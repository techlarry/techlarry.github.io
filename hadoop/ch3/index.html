<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Zhenhua Wang">
        <link rel="canonical" href="http://larryim.cc/note-big-data/hadoop/ch3/">
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Chapter 3: The Hadoop Distributed FileSystem - Zhenhua's Notes - Big Data</title>
        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.5.0.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../extra_css/custom.css" rel="stylesheet">
        <link href="../../extra_css/custom.js" rel="stylesheet">
        <link href="../../extra_css/friendly.css" rel="stylesheet">
        <link href="../../extra_css/theme.css" rel="stylesheet">
        <link href="../../extra_css/mkdocs/js/lunr-0.5.7.min.js" rel="stylesheet">
        <link href="../../extra_css/mkdocs/js/mustache.min.js" rel="stylesheet">
        <link href="../../extra_css/mkdocs/js/require.js" rel="stylesheet">
        <link href="../../extra_css/mkdocs/js/search.js" rel="stylesheet">
        <link href="../../extra_css/mkdocs/js/text.js" rel="stylesheet">
        <link href="../../extra_css/code-tab.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="../..">Zhenhua's Notes - Big Data</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                    <li >
                        <a href="../..">Home</a>
                    </li>
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">HADOOP <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../">Contents</a>
</li>
                            
<li >
    <a href="../ch1/">Chapter 1: Meet Hadoop</a>
</li>
                            
<li >
    <a href="../ch2/">Chapter 2: MapReduce</a>
</li>
                            
<li class="active">
    <a href="./">Chapter 3: The Hadoop Distributed FileSystem</a>
</li>
                            
<li >
    <a href="../ch4/">Chapter 4: YARN</a>
</li>
                            
<li >
    <a href="../ch5/">Chapter 5: Hadoop I/O</a>
</li>
                            
<li >
    <a href="../ch6/">Chapter 6: Developing a MapReduce Application</a>
</li>
                            
<li >
    <a href="../ch7/">Chapter 7: How MapReduce Works</a>
</li>
                            
<li >
    <a href="../ch8/">Chapter 8: MapReduce Types and Formats</a>
</li>
                            
<li >
    <a href="../ch9/">Chapter 9: MapReduce Features</a>
</li>
                            
<li >
    <a href="../ch10/">Chapter 10: Setting Up a Hadoop Cluster</a>
</li>
                            
<li >
    <a href="../ch11/">Chapter 11: Adminstering Hadoop</a>
</li>
                            
<li >
    <a href="../ch12/">Chapter 12: Avro</a>
</li>
                            
<li >
    <a href="../ch13/">Chapter 13: Parquet</a>
</li>
                            
<li >
    <a href="../ch14/">Chapter 14: Flume</a>
</li>
                            
<li >
    <a href="../ch15/">Chapter 15: Sqoop</a>
</li>
                            
<li >
    <a href="../ch16/">Chapter 16: Pig</a>
</li>
                            
<li >
    <a href="../ch17/">Chapter 17: Hive</a>
</li>
                            
<li >
    <a href="../ch18/">Chapter 18: Crunch</a>
</li>
                            
<li >
    <a href="../ch19/">Chapter 19: Spark</a>
</li>
                            
<li >
    <a href="../ch20/">Chapter 20: HBase</a>
</li>
                            
<li >
    <a href="../ch21/">Chapter 21: ZooKeeper</a>
</li>
                            
<li >
    <a href="../ch22/">Chapter 22: Composable Data at Center</a>
</li>
                            
<li >
    <a href="../ch23/">Chapter 23: Biological Data Science: Saving Lives with Software</a>
</li>
                            
<li >
    <a href="../ch24/">Chapter 24: Cascading</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">GDM <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../gdm/">Contents</a>
</li>
                            
<li >
    <a href="../../gdm/ch1/">Chapter 1: 简介</a>
</li>
                            
<li >
    <a href="../../gdm/ch2/">Chapter 2: 推荐系统入门</a>
</li>
                            
<li >
    <a href="../../gdm/ch3/">Chapter 3: 隐式评价和基于物品的过滤算法</a>
</li>
                            
<li >
    <a href="../../gdm/ch4/">Chapter 4: 分类</a>
</li>
                            
<li >
    <a href="../../gdm/ch5/">Chapter 5: 进一步探索分类</a>
</li>
                            
<li >
    <a href="../../gdm/ch6/">Chapter 6: 概率和朴素贝叶斯</a>
</li>
                            
<li >
    <a href="../../gdm/ch7/">Chapter 7: 朴素贝叶斯和文本数据</a>
</li>
                            
<li >
    <a href="../../gdm/ch8/">Chapter 8: 聚类</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Projects <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../projects/">Contents</a>
</li>
                            
<li >
    <a href="../../projects/SparkStreaming实时流处理项目/">SparkStreaming实时流处理</a>
</li>
                        </ul>
                    </li>
                    <li >
                        <a href="../../books/">Books</a>
                    </li>
                </ul>

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                    <li >
                        <a rel="next" href="../ch2/">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../ch4/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
            </ul>
        </div>
    </div>
</div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#hadoop-the-definitive-guide-3-the-hadoop-distributed-filesystem">Hadoop: The Definitive Guide 3 - The Hadoop Distributed FileSystem</a></li>
        <li class="main "><a href="#1-the-design-of-hdfs">1 The Design of HDFS</a></li>
        <li class="main "><a href="#2-hdfs-concepts">2 HDFS Concepts</a></li>
            <li><a href="#blocks">Blocks</a></li>
            <li><a href="#namenodes-and-datanodes">Namenodes and Datanodes</a></li>
            <li><a href="#block-caching">Block Caching</a></li>
            <li><a href="#hdfs-federation">HDFS Federation</a></li>
            <li><a href="#hdfs-high-availability">HDFS High Availability</a></li>
        <li class="main "><a href="#3-the-command-line-interface">3 The Command-Line Interface</a></li>
            <li><a href="#basic-filesystem-operations">Basic Filesystem Operations</a></li>
        <li class="main "><a href="#4-hadoop-filesystems">4 Hadoop Filesystems</a></li>
            <li><a href="#http">HTTP</a></li>
        <li class="main "><a href="#5-the-java-interface">5 The Java Interface</a></li>
            <li><a href="#reading-data-from-a-hadoop-url">Reading Data from a Hadoop URL</a></li>
            <li><a href="#reading-data-using-the-filesystem-api">Reading Data Using the FileSystem API</a></li>
            <li><a href="#fsdatainputstream">FSDataInputStream</a></li>
            <li><a href="#writing-data">Writing Data</a></li>
            <li><a href="#fsdataoutputstream">FSDataOutputStream</a></li>
            <li><a href="#directories">Directories</a></li>
            <li><a href="#querying-the-filesystem">Querying the Filesystem</a></li>
            <li><a href="#deleting-data">Deleting Data</a></li>
        <li class="main "><a href="#6-data-flow">6 Data Flow</a></li>
            <li><a href="#anatomy-of-a-file-read">Anatomy of a File Read</a></li>
            <li><a href="#anatomy-of-a-file-write">Anatomy of a File Write</a></li>
            <li><a href="#coherency-model">Coherency Model</a></li>
        <li class="main "><a href="#7-parallel-copying-with-distcp">7 Parallel Copying with distcp</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h3 id="hadoop-the-definitive-guide-3-the-hadoop-distributed-filesystem"><strong>Hadoop: The Definitive Guide 3 - The Hadoop Distributed FileSystem</strong><a class="headerlink" href="#hadoop-the-definitive-guide-3-the-hadoop-distributed-filesystem" title="Permanent link">&para;</a></h3>
<p>Filesystems that manage the storage across a network of machines are called <strong><em>distributed filesystems</em></strong> . Hadoop comes with a distributed filesystem called HDFS, which stands for <strong><em>Hadoop Distributed Filesystem</em></strong> .</p>
<h3 id="1-the-design-of-hdfs">1 The Design of HDFS<a class="headerlink" href="#1-the-design-of-hdfs" title="Permanent link">&para;</a></h3>
<p>HDFS is a filesystem designed for storing very large files with streaming data access patterns, running on clusters of commodity hardware.</p>
<ul>
<li>Very large files: files that are hundreds of megabytes, gigabytes, or terabytes in size.</li>
<li>Streaming data access: HDFS is built around the idea that the most efficient data processing pattern is a <strong><em>write-once, read-many-times</em></strong> pattern.</li>
<li>Commodity hardware: It’s designed to run on clusters of commodity hardware.</li>
</ul>
<p>These are areas where HDFS is not a good fit today:</p>
<ul>
<li>Low-latency data access</li>
<li>Lots of small files</li>
<li>Multiple writers, arbitrary file modifications</li>
</ul>
<h3 id="2-hdfs-concepts">2 HDFS Concepts<a class="headerlink" href="#2-hdfs-concepts" title="Permanent link">&para;</a></h3>
<h4 id="blocks">Blocks<a class="headerlink" href="#blocks" title="Permanent link">&para;</a></h4>
<p>A disk has a block size, which is the minimum amount of data that it can read or write. Filesystems for a single disk build on this by dealing with data in blocks, which are an integral multiple of the disk block size.</p>
<p>HDFS, too, has the concept of a <strong><em>block</em></strong>, but it is a much larger unit — 128 MB by default (typically a few kilobytes for ordinary file system). Unlike a filesystem for a single disk, a file in HDFS that is smaller than a single block does not occupy a full block’s worth of underlying storage. (For example, a 1 MB file stored with a block size of 128 MB uses 1 MB of disk space, not 128 MB.)</p>
<div class="admonition question">
<p class="admonition-title">Question</p>
<p>WHY IS A BLOCK IN HDFS SO LARGE? To minimize the cost of seeks.</p>
</div>
<p>Having a block abstraction for a distributed filesystem brings several benefits.</p>
<ul>
<li>A file can be larger than any single disk in the network.</li>
<li>Making the unit of abstraction a block rather than a file simplifies the storage subsystem.<ul>
<li>storage management: because blocks are a fixed size, it is easy to calculate how many can be stored on a given disk.</li>
<li>metadata concerns: because blocks are just chunks of data to be stored, file metadata such as permissions information does not need to be stored with the blocks.</li>
</ul>
</li>
<li>Blocks fit well with replication for providing fault tolerance and availability.<ul>
<li>To insure against corrupted blocks and disk and machine failure, each block is replicated to a small number of physically separate machines (typically three).</li>
</ul>
</li>
</ul>
<h4 id="namenodes-and-datanodes">Namenodes and Datanodes<a class="headerlink" href="#namenodes-and-datanodes" title="Permanent link">&para;</a></h4>
<p>An HDFS cluster has two types of nodes: a <strong><em>namenode</em></strong> (the master) and a number of <strong><em>datanodes</em></strong> (workers). </p>
<ul>
<li>The namenode manages the filesystem namespace. It maintains the filesystem tree and the metadata for all the files and directories in the tree. This information is stored persistently on the local disk in the form of two files: the namespace image and the edit log.</li>
<li>The namenode also knows the datanodes on which all the blocks for a given file are located;</li>
<li>Datanodes are the workhorses of the filesystem. They store and retrieve blocks when they are told to (by clients or the namenode), and they report back to the namenode periodically with lists of blocks that they are storing.</li>
</ul>
<p>If the machine running the namenode were obliterated, all the files on the filesystem would be lost since there would be no way of knowing how to reconstruct the files from the blocks on the datanodes. Possible solution:</p>
<ul>
<li>to back up the files that make up the persistent state of the filesystem metadata.</li>
<li>to run a secondary namenode, which keeps a copy of the merged namespace image.</li>
</ul>
<h4 id="block-caching">Block Caching<a class="headerlink" href="#block-caching" title="Permanent link">&para;</a></h4>
<p>For frequently accessed files, the blocks may be <em>explicitly</em> cached in the datanode’s memory, in an off-heap block cache. Users or applications instruct the namenode which files to cache (and for how long) by adding a <em>cache directive</em> to a <em>cache pool</em>.</p>
<h4 id="hdfs-federation">HDFS Federation<a class="headerlink" href="#hdfs-federation" title="Permanent link">&para;</a></h4>
<p>Problem: On very large clusters with many files, memory becomes the limiting factor for scaling, since namenode keeps a reference to every file and block in the filesystem in memory.</p>
<p>For example, a 200-node cluster with 24 TB of disk space per node, a block size of 128 MB, and a replication factor of 3 has room for about 2 million blocks (or more): <span><span class="MathJax_Preview">200\times 24TB⁄(128MB×3)</span><script type="math/tex">200\times 24TB⁄(128MB×3)</script></span>, So in this case, setting the namenode memory to 12,000 MB would be a good starting point.</p>
<p>Solution: HDFS federation, allows a cluster to scale by adding namenodes, each of which manages a portion of the filesystem namespace.</p>
<h4 id="hdfs-high-availability">HDFS High Availability<a class="headerlink" href="#hdfs-high-availability" title="Permanent link">&para;</a></h4>
<p>To remedy a failed namenode, a pair of namenodes in an <strong><em>active-standby</em></strong> configuration is introduced in Hadoop 2. In the event of the failure of the active namenode, the standby takes over its duties to continue servicing client requests <em>without</em> a significant interruption.</p>
<h3 id="3-the-command-line-interface">3 The Command-Line Interface<a class="headerlink" href="#3-the-command-line-interface" title="Permanent link">&para;</a></h3>
<h4 id="basic-filesystem-operations">Basic Filesystem Operations<a class="headerlink" href="#basic-filesystem-operations" title="Permanent link">&para;</a></h4>
<p>Hadoop’s filesystem shell command is <C>fs</C>, which supports a number of subcommands (type <c>hadoop fs -help</C> to get detailed help).</p>
<p>Copying a file from the local filesystem to HDFS:</p>
<p> <div class=codehilite><pre><span class=c1>#The local file is copied tothe HDFS instance running on localhost.</span>
$ hadoop fs -copyFromLocal test.copy /test.copy
<span class=c1># works as the same</span>
$ hadoop fs -copyFromLocal test.copy hdfs://localhost:9000/test2.copy
</pre></div></p>
<p>Copying the file from the HDFS to the local filesystem:</p>
<p> <div class=codehilite><pre>$ hadoop fs -copyToLocal /test.copy test.copy.txt
</pre></div></p>
<h3 id="4-hadoop-filesystems">4 Hadoop Filesystems<a class="headerlink" href="#4-hadoop-filesystems" title="Permanent link">&para;</a></h3>
<p>Hadoop has an abstract notion of filesystems, of which HDFS is just one implementation. The Java abstract class <C>org.apache.hadoop.fs.FileSystem</C> represents the client interface to a filesystem in Hadoop, and there are several concrete implementations.</p>
<table>
<thead>
<tr>
<th>Filesystem</th>
<th>URI scheme</th>
<th>Java implementation</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Local</td>
<td>file</td>
<td>fs.LocalFileSystem</td>
<td>A filesystem for a locally connected disk with client-side checksums</td>
</tr>
<tr>
<td>HDFS</td>
<td>hfs</td>
<td>hdfs.DistributedFileSystem</td>
<td>Hadoop’s distributed filesystem</td>
</tr>
<tr>
<td>WebHDFS</td>
<td>webhdfs</td>
<td>hdfs.web.WebHdfsFileSystem</td>
<td>Providing authenticated read/write access to HDFS over HTTP.</td>
</tr>
<tr>
<td>Secure WebHDFS</td>
<td>swebhdfs</td>
<td>hdfs.web.SWebHdfsFileSystem</td>
<td>The HTTPS version of WebHDFS.</td>
</tr>
</tbody>
</table>
<p>When you are processing large volumes of data you should choose a distributed filesystem that has the data locality optimization, notably HDFS.</p>
<h4 id="http">HTTP<a class="headerlink" href="#http" title="Permanent link">&para;</a></h4>
<p>The HTTP REST API exposed by the WebHDFS protocol makes it easier for other languages to interact with HDFS. Note that the HTTP interface is slower than the native Java client, so should be avoided for very large data transfers if possible.</p>
<p>There are two ways of accessing HDFS over HTTP:</p>
<ul>
<li>Directly, where the HDFS daemons serve HTTP requests to clients;</li>
<li>Via a proxy (or proxies), which accesses HDFS on the client’s behalf using the usual DistributedFileSystem API.</li>
</ul>
<p><img alt="" src="../figures/AccessingHdfsOverHttpOrHdfsProxies.jpg" /></p>
<p>HDFS proxy allows for stricter firewall and bandwidth-limiting policies to be put in place. It’s common to use a proxy for transfers between Hadoop clusters located in different data centers, or when accessing a Hadoop cluster running in the cloud from an external network.</p>
<h3 id="5-the-java-interface">5 The Java Interface<a class="headerlink" href="#5-the-java-interface" title="Permanent link">&para;</a></h3>
<p>Hadoop <C>FileSystem</C> class is the API for interacting with one of Hadoop’s filesystems. In general you should strive to <strong><em>write your code against the <C>FileSystem</C> abstract class</em></strong> , to retain portability across filesystems. This is very useful when testing your program, for example, because you can rapidly run tests using data stored on the local filesystem.</p>
<h4 id="reading-data-from-a-hadoop-url">Reading Data from a Hadoop URL<a class="headerlink" href="#reading-data-from-a-hadoop-url" title="Permanent link">&para;</a></h4>
<p>NOT recommended, because <C>setURLStreamHandlerFactory()</C> method can be called only once per JVM, which means that if some other part of your program sets it, you won't be able to use.</p>
<h4 id="reading-data-using-the-filesystem-api">Reading Data Using the FileSystem API<a class="headerlink" href="#reading-data-using-the-filesystem-api" title="Permanent link">&para;</a></h4>
<p>A file in a Hadoop filesystem is represented by a Hadoop <C>Path</C> object(<C>org.apache.hadoop.fs.Path</C>, not <C>java.io.File</C>). You can think of a <C>Path</C>  as a Hadoop filesystem URI, such as <code class="codehilite">hdfs://localhost/user/tom/test.copy</code></p>
<p>Since <C>FileSystem</C> is a general filesystem API, so the first step is to retrieve an instance for the filesystem we want. There are several static factory methods for getting a <C>FileSystem</C> instance:</p>
<p> <div class=codehilite><pre><span class=c1>// Returns the default filesystem</span>
<span class=kd>public</span> <span class=kd>static</span> <span class=n>FileSystem</span> <span class=nf>get</span><span class=o>(</span><span class=n>Configuration</span> <span class=n>conf</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span> 
<span class=c1>// Uses the given URI’s scheme and authority to determine the filesystem to use</span>
<span class=kd>public</span> <span class=kd>static</span> <span class=n>FileSystem</span> <span class=nf>get</span><span class=o>(</span><span class=n>URI</span> <span class=n>uri</span><span class=o>,</span> <span class=n>Configuration</span> <span class=n>conf</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span> 
<span class=c1>// Retrieves the filesystem as the given user</span>
<span class=kd>public</span> <span class=kd>static</span> <span class=n>FileSystem</span> <span class=nf>get</span><span class=o>(</span><span class=n>URI</span> <span class=n>uri</span><span class=o>,</span> <span class=n>Configuration</span> <span class=n>conf</span><span class=o>,</span> <span class=n>String</span> <span class=n>user</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
<span class=c1>// Retrieves a local filesystem instance</span>
<span class=kd>public</span> <span class=kd>static</span> <span class=n>LocalFileSystem</span> <span class=nf>getLocal</span><span class=o>(</span><span class=n>Configuration</span> <span class=n>conf</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
</pre></div></p>
<p>A <C>Configuration</C> object encapsulates a client or server's configuration, which is set using configuration files read from the classpath, such as <code class="codehilite">etc/hadoop/core-site.xml</code>.</p>
<p>With a <C>FileSystem</C> instance in hand, we invoke an <C>open()</C> method to get the input stream for a file:</p>
<p> <div class=codehilite><pre><span class=c1>// Uses a default buffer size of 4 KB</span>
<span class=kd>public</span> <span class=n>FSDataInputStream</span> <span class=nf>open</span><span class=o>(</span><span class=n>Path</span> <span class=n>f</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
<span class=c1>// Uses a buffer size of bufferSize</span>
<span class=kd>public</span> <span class=kd>abstract</span> <span class=n>FSDataInputStream</span> <span class=nf>open</span><span class=o>(</span><span class=n>Path</span> <span class=n>f</span><span class=o>,</span> <span class=kt>int</span> <span class=n>bufferSize</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
</pre></div></p>
<p>Displaying files from a Hadoop filesystem on standard output by using the FileSystem directly:</p>
<p> <div class=codehilite><pre><span class=c1>// $ hdfs://localhost:9000/test2.copy</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.conf.Configuration</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.fs.FileSystem</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.fs.Path</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.io.IOUtils</span><span class=o>;</span>

<span class=kn>import</span> <span class=nn>java.io.InputStream</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>java.net.URI</span><span class=o>;</span>

<span class=kd>public</span> <span class=kd>class</span> <span class=nc>FileSystemCat</span> <span class=o>{</span>
    <span class=kd>public</span> <span class=kd>static</span> <span class=kt>void</span> <span class=nf>main</span><span class=o>(</span><span class=n>String</span><span class=o>[]</span> <span class=n>args</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>Exception</span> <span class=o>{</span>
        <span class=n>String</span> <span class=n>uri</span> <span class=o>=</span> <span class=n>args</span><span class=o>[</span><span class=mi>0</span><span class=o>];</span>
        <span class=n>Configuration</span> <span class=n>conf</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Configuration</span><span class=o>();</span>
        <span class=n>FileSystem</span> <span class=n>fs</span> <span class=o>=</span> <span class=n>FileSystem</span><span class=o>.</span><span class=na>get</span><span class=o>(</span><span class=n>URI</span><span class=o>.</span><span class=na>create</span><span class=o>(</span><span class=n>uri</span><span class=o>),</span> <span class=n>conf</span><span class=o>);</span>
        <span class=n>InputStream</span> <span class=n>in</span> <span class=o>=</span> <span class=kc>null</span><span class=o>;</span>
        <span class=k>try</span> <span class=o>{</span>
            <span class=n>in</span> <span class=o>=</span> <span class=n>fs</span><span class=o>.</span><span class=na>open</span><span class=o>(</span><span class=k>new</span> <span class=n>Path</span><span class=o>(</span><span class=n>uri</span><span class=o>));</span>
            <span class=n>IOUtils</span><span class=o>.</span><span class=na>copyBytes</span><span class=o>(</span><span class=n>in</span><span class=o>,</span> <span class=n>System</span><span class=o>.</span><span class=na>out</span><span class=o>,</span> <span class=mi>4096</span><span class=o>,</span> <span class=kc>false</span><span class=o>);</span>
        <span class=o>}</span> <span class=k>finally</span> <span class=o>{</span>
            <span class=n>IOUtils</span><span class=o>.</span><span class=na>closeStream</span><span class=o>(</span><span class=n>in</span><span class=o>);</span>
        <span class=o>}</span>
    <span class=o>}</span>
<span class=o>}</span>
</pre></div></p>
<h4 id="fsdatainputstream">FSDataInputStream<a class="headerlink" href="#fsdatainputstream" title="Permanent link">&para;</a></h4>
<p>The <C>open()</C> method on <C>FileSystem</C> actually returns an <C>FSDataInputStream</C> rather than a standard java.io class. This class is a specialization of <C>java.io.DataInputStream</C> with support for random access, so you can read from any part of the stream:</p>
<p><img alt="FSDataInputStrea" src="../figures/FSDataInputStream.png" /></p>
<p>The <C>Seekable</C> interface permits seeking to a position in the file and provides a query method for the <em>current</em> offset from the start of the file (<C>getPos()</C>):</p>
<p> <div class=codehilite><pre><span class=kd>public</span> <span class=kd>interface</span> <span class=nc>Seekable</span> <span class=o>{</span> 
    <span class=kt>void</span> <span class=nf>seek</span><span class=o>(</span><span class=kt>long</span> <span class=n>pos</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span><span class=o>;</span> 
    <span class=kt>long</span> <span class=nf>getPos</span><span class=o>()</span> <span class=kd>throws</span> <span class=n>IOException</span><span class=o>;</span> <span class=o>}</span>
</pre></div></p>
<p>Displaying files from a Hadoop filesystem on standard output twice, by using <C>seek()</C>:</p>
<p> <div class=codehilite><pre><span class=c1>// hdfs://localhost:9000/test2.copy</span>

<span class=kn>import</span> <span class=nn>org.apache.hadoop.conf.Configuration</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.fs.FSDataInputStream</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.fs.FileSystem</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.fs.Path</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.io.IOUtils</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>java.net.URI</span><span class=o>;</span>

<span class=kd>public</span> <span class=kd>class</span> <span class=nc>FileSystemDoubleCat</span> <span class=o>{</span>
    <span class=kd>public</span> <span class=kd>static</span> <span class=kt>void</span> <span class=nf>main</span><span class=o>(</span><span class=n>String</span><span class=o>[]</span> <span class=n>args</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>Exception</span> <span class=o>{</span>
        <span class=n>String</span> <span class=n>uri</span> <span class=o>=</span> <span class=n>args</span><span class=o>[</span><span class=mi>0</span><span class=o>];</span>
        <span class=n>Configuration</span> <span class=n>conf</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Configuration</span><span class=o>();</span>
        <span class=n>FileSystem</span> <span class=n>fs</span> <span class=o>=</span> <span class=n>FileSystem</span><span class=o>.</span><span class=na>get</span><span class=o>(</span><span class=n>URI</span><span class=o>.</span><span class=na>create</span><span class=o>(</span><span class=n>uri</span><span class=o>),</span> <span class=n>conf</span><span class=o>);</span>
        <span class=n>FSDataInputStream</span> <span class=n>in</span> <span class=o>=</span> <span class=kc>null</span><span class=o>;</span>
        <span class=k>try</span> <span class=o>{</span>
            <span class=n>in</span> <span class=o>=</span> <span class=n>fs</span><span class=o>.</span><span class=na>open</span><span class=o>(</span><span class=k>new</span> <span class=n>Path</span><span class=o>(</span><span class=n>uri</span><span class=o>));</span>
            <span class=n>IOUtils</span><span class=o>.</span><span class=na>copyBytes</span><span class=o>(</span><span class=n>in</span><span class=o>,</span> <span class=n>System</span><span class=o>.</span><span class=na>out</span><span class=o>,</span> <span class=mi>4096</span><span class=o>,</span> <span class=kc>false</span><span class=o>);</span>
            <span class=n>in</span><span class=o>.</span><span class=na>seek</span><span class=o>(</span><span class=mi>0</span><span class=o>);</span> <span class=c1>// go back to the start of the file</span>
            <span class=n>IOUtils</span><span class=o>.</span><span class=na>copyBytes</span><span class=o>(</span><span class=n>in</span><span class=o>,</span> <span class=n>System</span><span class=o>.</span><span class=na>out</span><span class=o>,</span> <span class=mi>4096</span><span class=o>,</span> <span class=kc>false</span><span class=o>);</span>
        <span class=o>}</span> <span class=k>catch</span> <span class=o>(</span><span class=n>Exception</span> <span class=n>ex</span><span class=o>)</span> <span class=o>{</span>
            <span class=n>ex</span><span class=o>.</span><span class=na>printStackTrace</span><span class=o>();</span>
        <span class=o>}</span> <span class=k>finally</span> <span class=o>{</span>
            <span class=n>IOUtils</span><span class=o>.</span><span class=na>closeStream</span><span class=o>(</span><span class=n>in</span><span class=o>);</span>
        <span class=o>}</span>
    <span class=o>}</span>
<span class=o>}</span>
</pre></div></p>
<h4 id="writing-data">Writing Data<a class="headerlink" href="#writing-data" title="Permanent link">&para;</a></h4>
<p>The <C>FileSystem class</C> has a number of methods for creating a file. </p>
<p> <div class=codehilite><pre><span class=c1>// takes a Path object for the file to be created and returns an output stream to write to</span>
<span class=kd>public</span> <span class=n>FSDataOutputStream</span> <span class=nf>create</span><span class=o>(</span><span class=n>Path</span> <span class=n>f</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
<span class=c1>// appends to an existing file</span>
<span class=kd>public</span> <span class=n>FSDataOutputStream</span> <span class=nf>append</span><span class=o>(</span><span class=n>Path</span> <span class=n>f</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
</pre></div></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The <C>create()</C> methods create any parent directories of the file to be written that don’t already exist.</p>
</div>
<p>There’s an overloaded method of <create()> for passing a callback interface, <C>Progressable&lt;&gt;/C, so your application can be notified of the progress of the data being written to the datanodes:</p>
<p> <div class=codehilite><pre>public interface Progressable { 
    public void progress(); 
}
</pre></div></p>
<p>Here, we illustrate progress by printing a period every time the <C>progress()</C> method is called by Hadoop, which is after each 64 KB packet of data is written to the datanode pipeline.</p>
<p> <div class=codehilite><pre><span class=c1>// args: /Users/larry/test.copy hdfs://localhost:9000/test4.copy</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.conf.Configuration</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.fs.FileSystem</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.fs.Path</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.io.IOUtils</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.util.Progressable</span><span class=o>;</span>

<span class=kn>import</span> <span class=nn>java.io.*</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>java.net.URI</span><span class=o>;</span>

<span class=c1>// Copying a local file to a Hadoop filesystem</span>
<span class=kd>public</span> <span class=kd>class</span> <span class=nc>FileCopyWithProgress</span> <span class=o>{</span>

    <span class=kd>public</span> <span class=kd>static</span> <span class=kt>void</span> <span class=nf>main</span><span class=o>(</span><span class=n>String</span><span class=o>[]</span> <span class=n>args</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>Exception</span> <span class=o>{</span>
        <span class=n>String</span> <span class=n>localsrc</span> <span class=o>=</span> <span class=n>args</span><span class=o>[</span><span class=mi>0</span><span class=o>];</span>
        <span class=n>String</span> <span class=n>dstsrc</span> <span class=o>=</span> <span class=n>args</span><span class=o>[</span><span class=mi>1</span><span class=o>];</span>
        <span class=n>BufferedInputStream</span> <span class=n>in</span> <span class=o>=</span> <span class=k>new</span> <span class=n>BufferedInputStream</span><span class=o>(</span><span class=k>new</span> <span class=n>FileInputStream</span><span class=o>(</span><span class=n>localsrc</span><span class=o>));</span>

        <span class=n>Configuration</span> <span class=n>conf</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Configuration</span><span class=o>();</span>
        <span class=n>FileSystem</span> <span class=n>fs</span> <span class=o>=</span> <span class=n>FileSystem</span><span class=o>.</span><span class=na>get</span><span class=o>(</span><span class=n>URI</span><span class=o>.</span><span class=na>create</span><span class=o>(</span><span class=n>dstsrc</span><span class=o>),</span> <span class=n>conf</span><span class=o>);</span>
        <span class=k>try</span> <span class=o>{</span>
            <span class=n>OutputStream</span> <span class=n>out</span> <span class=o>=</span> <span class=n>fs</span><span class=o>.</span><span class=na>create</span><span class=o>(</span><span class=k>new</span> <span class=n>Path</span><span class=o>(</span><span class=n>dstsrc</span><span class=o>),</span> <span class=k>new</span> <span class=n>Progressable</span><span class=o>()</span> <span class=o>{</span>
                <span class=nd>@Override</span>
                <span class=kd>public</span> <span class=kt>void</span> <span class=nf>progress</span><span class=o>()</span> <span class=o>{</span>
                    <span class=n>System</span><span class=o>.</span><span class=na>out</span><span class=o>.</span><span class=na>println</span><span class=o>(</span><span class=s>&quot;.&quot;</span><span class=o>);</span>
                <span class=o>}</span>
            <span class=o>});</span>
            <span class=n>IOUtils</span><span class=o>.</span><span class=na>copyBytes</span><span class=o>(</span><span class=n>in</span><span class=o>,</span> <span class=n>out</span><span class=o>,</span> <span class=mi>4096</span><span class=o>,</span> <span class=kc>true</span><span class=o>);</span>

        <span class=o>}</span> <span class=k>finally</span> <span class=o>{</span>
            <span class=n>IOUtils</span><span class=o>.</span><span class=na>closeStream</span><span class=o>(</span><span class=n>in</span><span class=o>);</span>
        <span class=o>}</span> <span class=c1>//end try</span>
    <span class=o>}</span><span class=c1>// end main</span>
<span class=o>}</span>
</pre></div></p>
<h4 id="fsdataoutputstream">FSDataOutputStream<a class="headerlink" href="#fsdataoutputstream" title="Permanent link">&para;</a></h4>
<p>The <C>create()</C> method on <C>FileSystem</C> returns an <C>FSDataOutputStream</C>, which, like <C>FSDataInputStream</C>, has a method for querying the current position in the file:</p>
<p> <div class=codehilite><pre><span class=kd>public</span> <span class=kd>class</span> <span class=nc>FSDataOutputStream</span> <span class=kd>extends</span> <span class=n>DataOutputStream</span> <span class=kd>implements</span> <span class=n>Syncable</span> <span class=o>{</span>
    <span class=kd>public</span> <span class=kt>long</span> <span class=nf>getPos</span><span class=o>()</span> <span class=kd>throws</span> <span class=n>IOException</span> <span class=o>{</span> 
        <span class=c1>// implementation elided </span>
    <span class=o>}</span><span class=c1>// implementation elided</span>
<span class=o>}</span>
</pre></div></p>
<p>However, because HDFS allows only sequential writes to an open file or appends to an already written file, <C>FSDataOutputStream</C> does not permit seeking.</p>
<h4 id="directories">Directories<a class="headerlink" href="#directories" title="Permanent link">&para;</a></h4>
<p><C>FileSystem</C> provides a method to create a directory:</p>
<p> <div class=codehilite><pre><span class=kd>public</span> <span class=kt>boolean</span> <span class=nf>mkdirs</span><span class=o>(</span><span class=n>Path</span> <span class=n>f</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
</pre></div></p>
<p>This method creates all of the necessary parent directories if they don’t already exist.</p>
<h4 id="querying-the-filesystem">Querying the Filesystem<a class="headerlink" href="#querying-the-filesystem" title="Permanent link">&para;</a></h4>
<p><hh>File metadata: FileStatus</hh>
<hh>Listing files</hh>
<hh>File patterns</hh></p>
<h4 id="deleting-data">Deleting Data<a class="headerlink" href="#deleting-data" title="Permanent link">&para;</a></h4>
<p>Use the <C>delete()</C> method on <C>FileSystem</C> to permanently remove files or directories:</p>
<p> <div class=codehilite><pre><span class=kd>public</span> <span class=kt>boolean</span> <span class=nf>delete</span><span class=o>(</span><span class=n>Path</span> <span class=n>f</span><span class=o>,</span> <span class=kt>boolean</span> <span class=n>recursive</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
</pre></div></p>
<p>If <C>f</C> is a file or an empty directory, the value of <C>recursive</C> is ignored.</p>
<h3 id="6-data-flow">6 Data Flow<a class="headerlink" href="#6-data-flow" title="Permanent link">&para;</a></h3>
<h4 id="anatomy-of-a-file-read">Anatomy of a File Read<a class="headerlink" href="#anatomy-of-a-file-read" title="Permanent link">&para;</a></h4>
<p>The figure below shows the main sequence of events when reading a file.</p>
<p><img alt="" src="../figures/AClientReadingDataFromHDFS.jpg" /></p>
<ul>
<li>step 1: The client opens the file it wishes to read by calling <C>open()</C> on the <C>FileSystem</C> object, which for HDFS is an instance of <C>DistributedFileSystem</C>. </li>
<li>step 2: <C>DistributedFileSystem</C> calls the namenode, using remote procedure calls (RPCs), to determine the locations of the first few blocks in the file. </li>
<li>step 3: For each block, the namenode returns the addresses of the datanodes that have a copy of that block. Furthermore, the datanodes are sorted according to their proximity to the client. <ul>
<li>If the client is itself a datanode, the client will read from the local datanode if that datanode hosts a copy of the block.</li>
<li>The <C>DistributedFileSystem</C> returns an <C>FSDataInputStream</C> to the client for it to read data from. <C>FSDataInputStream</C> in turn wraps a <C>DFSInputStream</C>, which manages the datanode and namenode I/O.</li>
<li>The client then calls <C>read()</C> on the stream. </li>
</ul>
</li>
<li>step 4: <C>DFSInputStream</C>, which has stored the datanode addresses for the first few blocks in the file, then connects to the first (closest) datanode for the first block in the file. Data is streamed from the datanode back to the client, which calls <C>read()</C> repeatedly on the stream. </li>
<li>step 5: When the end of the block is reached, <C>DFSInputStream</C> will close the connection to the datanode, then find the best datanode for the next block. </li>
<li>step 6: This happens transparently to the client, which from its point of view is just reading a continuous stream.<ul>
<li>Blocks are read in order, with the <C>DFSInputStream</C> opening new connections to datanodes as the client reads through the stream. </li>
<li>It will also call the namenode to retrieve the datanode locations for the next batch of blocks as needed. When the client has finished reading, it calls <C>close()</C> on the <C>FSDataInputStream</C>.</li>
</ul>
</li>
</ul>
<h4 id="anatomy-of-a-file-write">Anatomy of a File Write<a class="headerlink" href="#anatomy-of-a-file-write" title="Permanent link">&para;</a></h4>
<p>The figure below illustrates the case of creating a new file, writing data to it, then closing the file.</p>
<p><img alt="" src="../figures/AClientWritingDataToHDFS.jpg" /></p>
<ul>
<li>step 1: The client creates the file by calling <C>create()</C> on <C>DistributedFileSystem</C>. </li>
<li>step 2: <C>DistributedFileSystem</C> makes an RPC call to the namenode to create a new file in the filesystem’s namespace, with no blocks associated with it. <ul>
<li>The namenode performs various checks to make sure the file doesn’t already exist and that the client has the right permissions to create the file. </li>
<li>If these checks pass, the namenode makes a record of the new file; otherwise, file creation fails and the client is thrown an <C>IOException</C>. </li>
<li>The <C>DistributedFileSystem</C> returns an <C>FSDataOutputStream</C> for the client to start writing data to. Just as in the read case, <C>FSDataOutputStream</C> wraps a <C>DFSOutputStream</C>, which handles communication with the datanodes and namenode.</li>
</ul>
</li>
<li>step 3: As the client writes data, the <C>DFSOutputStream</C> splits it into packets, which it writes to an internal queue called the <strong><em>data queue</em></strong> . The data queue is consumed by the <C>DataStreamer</C>, which is responsible for asking the namenode to allocate new blocks by picking a list of suitable datanodes to store the replicas. </li>
<li>step 4: The list of datanodes forms a pipeline, and here we’ll assume the replication level is three, so there are three nodes in the pipeline. The <C>DataStreamer</C> streams the packets to the first datanode in the pipeline, which stores each packet and forwards it to the second datanode in the pipeline. Similarly, the second datanode stores the packet and forwards it to the third (and last) datanode in the pipeline .</li>
<li>step 5: The <C>DFSOutputStream</C> also maintains an internal queue of packets that are waiting to be acknowledged by datanodes, called the <strong><em>ack queue</em></strong>. A packet is removed from the ack queue only when it has been acknowledged by all the datanodes in the pipeline.</li>
<li>step 6: When the client has finished writing data, it calls <C>close()</C> on the stream. This action flushes all the remaining packets to the datanode pipeline.</li>
<li>step 7:  It waits for acknowledgments before contacting the namenode to signal that the file is complete. </li>
</ul>
<h4 id="coherency-model">Coherency Model<a class="headerlink" href="#coherency-model" title="Permanent link">&para;</a></h4>
<p>A coherency model for a filesystem describes the data visibility of reads and writes for a file.</p>
<ul>
<li>Any content written to the file is not guaranteed to be visible, even if the stream is flushed.</li>
<li>Once more than a block’s worth of data has been written, the first block will be visible to new readers.</li>
<li>The <C>FSDataOutputStream.hflush()</C> method force all buffers to be flushed to the datanodes.<ul>
<li>The <C>hflush()</T> guarantees that the data written up to that point in the file has reached all the datanodes in the write pipeline and is visible to all new readers.</li>
<li>But it does not guarantee that the datanodes have written the data to disk, only that it’s in the datanodes’ memory.</li>
<li>Closing a file in HDFS performs an implicit <C>hflush()</C>.</li>
</ul>
</li>
<li>The <C>hsync()</C> method syncs to disk for a file descriptor.</li>
</ul>
<p> <div class=codehilite><pre><span class=n>FileOutputStream</span> <span class=n>out</span> <span class=o>=</span> <span class=k>new</span> <span class=n>FileOutputStream</span><span class=o>(</span><span class=n>localFile</span><span class=o>);</span> <span class=n>out</span><span class=o>.</span><span class=na>write</span><span class=o>(</span><span class=s>&quot;content&quot;</span><span class=o>.</span><span class=na>getBytes</span><span class=o>(</span><span class=s>&quot;UTF-8&quot;</span><span class=o>));</span> 
<span class=n>out</span><span class=o>.</span><span class=na>flush</span><span class=o>();</span> <span class=c1>// flush to operating system </span>
<span class=n>out</span><span class=o>.</span><span class=na>getFD</span><span class=o>().</span><span class=na>sync</span><span class=o>();</span> <span class=c1>// sync to disk </span>
<span class=n>assertThat</span><span class=o>(</span><span class=n>localFile</span><span class=o>.</span><span class=na>length</span><span class=o>(),</span> <span class=n>is</span><span class=o>(((</span><span class=kt>long</span><span class=o>)</span> <span class=s>&quot;content&quot;</span><span class=o>.</span><span class=na>length</span><span class=o>())));</span>
</pre></div></p>
<p><hh>Consequences for application design</hh></p>
<p>You should call <C>hflush()</C> at suitable points, such as after writing a certain number of records or number of bytes.</p>
<h3 id="7-parallel-copying-with-distcp">7 Parallel Copying with distcp<a class="headerlink" href="#7-parallel-copying-with-distcp" title="Permanent link">&para;</a></h3>
<p>The program <C>distcp</C> copys data to and from Hadoop filesystems in parallel.</p>
<p> <div class=codehilite><pre>$ hadoop distcp file1 file2
</pre></div></p>
<p><C>distcp</C> is implemented as a MapReduce job where the work of copying is done by the maps that run in parallel across the cluster, with no reducers.</p></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>var base_url = '../..';</script>
        <script src="../../js/base.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <script src="../../extra_javascript/tabhack.js"></script>
        <script src="../../search/require.js"></script>
        <script src="../../search/search.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td><kbd>&larr;</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td><kbd>&rarr;</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>


    </body>
</html>
