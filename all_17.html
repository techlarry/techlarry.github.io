<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  techlarry
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">HomePage</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_blank" href="wiki">WIKI</a></li>
        
        <li id=""><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">HomePage</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_blank" href="wiki">WIKI</a></li>
        
        <li><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="Leetcode.html">Leetcode</a></li>
        
            <li><a href="programming_language.html">编程语言</a></li>
        
            <li><a href="data_structure_and_algorithm.html">数据结构和算法</a></li>
        
            <li><a href="Course.html">Course</a></li>
        
            <li><a href="Python%E7%89%B9%E6%80%A7.html">Python特性</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html">Python科学计算三维可视化</a></li>
        
            <li><a href="English.html">English</a></li>
        
            <li><a href="Computer%20System.html">Computer System</a></li>
        
            <li><a href="Deep%20Learning.html">Deep Learning</a></li>
        
            <li><a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html">Linux 系统编程</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html">数据库</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="Big%20Data.html">Big Data</a></li>
        
            <li><a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html">文献阅读</a></li>
        
            <li><a href="Tools.html">Tools</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="what_is_a_tensorflow_session.html">
                
                  <h1>TensorFlow(3):What is a tensorflow session?</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>by <a href="http://danijar.com/what-is-a-tensorflow-session/">Danijar Hafner</a>, modified by <a href="http://larryim.cc">larry</a></p>

<p>I’ve seen a lot of confusion over the rules of <code>tf.Graph</code> and <code>tf.Session</code> in TensorFlow. It’s simple:</p>

<ul>
<li>A <code>graph</code> defines the computation. It doesn’t compute anything, it doesn’t hold any values, it just defines the operations that you specified in your code.</li>
<li>A <code>session</code> allows you to execute graphs or part of graphs. It allocates resources (on one or more machines) for that and holds the actual values of intermediate results and variables.</li>
</ul>

<p>Let’s look at an example.</p>

<h3 id="toc_0">Defining the Graph</h3>

<p>We define a graph with a variable and three operations: <code>x</code> returns the current value of our variable. <code>init</code> assigns the initial value of 42 to that variable. <code>x_assign</code> assigns the new value of 13 to that variable.</p>

<pre><code class="language-python">import tensorflow as tf
graph = tf.Graph()
with graph.as_default():
  x = tf.Variable(42, name=&#39;foo&#39;)
  init = tf.global_variables_initializer()
  x_assign = x.assign(13)
</code></pre>

<p>On a side note: TensorFlow creates a default graph for you, so we don’t need the first two lines of the code above. The default graph is also what the sessions in the next section use when not manually specifying a graph.</p>

<h3 id="toc_1">Running Computations in a Session</h3>

<p>To run any of the three defined operations, we need to create a session for that graph. The session will also allocate memory to store the current value of the variable.</p>

<pre><code class="language-python">with tf.Session(graph=graph) as sess:
  sess.run(init)
  sess.run(x_assign)
  print(sess.run(x))
</code></pre>

<pre><code class="language-text">13
</code></pre>

<p>As you can see, the value of our variable is only valid within one session. If we try to query the value afterwards in a second session, TensorFlow will raise an error because the variable is not initialized there.</p>

<pre><code class="language-python">with tf.Session(graph=graph) as sess:
  print(sess.run(x))
# Error: Attempting to use uninitialized value foo

</code></pre>

<p>Of course, we can use the graph in more than one session, we just have to initialize the variables again. The values in the new session will be completely independent from the first one:</p>

<pre><code class="language-python">with tf.Session(graph=graph) as sess:
  sess.run(init)
  print(sess.run(x))
</code></pre>

<pre><code class="language-text">42
</code></pre>

<p>Hopefully this short workthrough helped you to better understand <code>tf.Session</code>. </p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/8/18</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Tensorflow.html'>Tensorflow</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="TensorFlow_introduction.html">
                
                  <h1>TensorFlow(1): Tensorflow Introduction</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Install</a>
</li>
<li>
<a href="#toc_1">TensorFlow Python docset</a>
</li>
<li>
<a href="#toc_2">Basics</a>
<ul>
<li>
<ul>
<li>
<a href="#toc_3">Graph and Operation</a>
</li>
<li>
<a href="#toc_4">Session</a>
</li>
<li>
<a href="#toc_5">Steps</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_6">Learning Resources</a>
</li>
</ul>


<p><a href="https://www.tensorflow.org">Tensorflow</a> is an open-source software library for Machine Intelligence.</p>

<h2 id="toc_0">Install</h2>

<p>Using <code>conda</code> tool under <code>Anaconda</code> to install tensorflow is a very convenient and direct way.</p>

<h2 id="toc_1">TensorFlow Python docset</h2>

<p>Build TensorFlow Python docset is complex, many dependent utilities and procedures are needed. Fortunately,  a feed for TensorFlow Python docset has been built for us.</p>

<p>Add feed below to Dash/Zeal, and install docset. (<a href="https://github.com/ppwwyyxx/dash-docset-tensorflow">Project site</a>)</p>

<pre><code class="language-text">https://raw.githubusercontent.com/ppwwyyxx/dash-docset-tensorflow/master/TensorFlow.xml
</code></pre>

<h2 id="toc_2">Basics</h2>

<h4 id="toc_3">Graph and Operation</h4>

<p>A data flow <code>graph</code> representing a TensorFlow computation. It can be viewed via <code>TensorBoard</code>.</p>

<p>An <code>Operation</code> is a node in a TensorFlow <code>Graph</code> that takes zero or more <code>Tensor</code> object as input, and produces zero or more <code>Tensor</code> objects as output.</p>

<p>For example <code>c = tf.matmul(a, b)</code> creates an Operation of type <code>MatMul</code> that takes tensors \(a\) and \(b\) as input, and produces \(c\) as output.</p>

<h4 id="toc_4">Session</h4>

<p>If you are not using an <code>InteractiveSession</code>, then you should build the entire computation graph before starting a session and launching the graph.</p>

<p>Note that there are two typical ways to create and use sessions in tensorflow:</p>

<p><strong>Method 1</strong>:</p>

<pre><code class="language-python">sess = tf.Session()
# Run the variables initialization (if needed), run the operations
result = sess.run(..., feed_dict = {...})
sess.close() # Close the session
</code></pre>

<p><strong>Method 2</strong>:</p>

<pre><code class="language-python">with tf.Session() as sess: 
    # run the variables initialization (if needed), run the operations
    result = sess.run(..., feed_dict = {...})
    # This takes care of closing the session for you :)
</code></pre>

<h4 id="toc_5">Steps</h4>

<p>Writing and running programs in TensorFlow has the following steps:</p>

<ul>
<li>Create Tensors (variables) that are not yet executed/evaluated.</li>
<li>Write operations between those Tensors.</li>
<li>Initialize your Tensors.</li>
<li>Create a Session.</li>
<li>Run the Session. This will run the operations you&#39;d written above.</li>
</ul>

<h2 id="toc_6">Learning Resources</h2>

<ol>
<li><a href="https://web.stanford.edu/class/cs20si/">CS 20SI: Tensorflow for Deep Learning Research</a></li>
</ol>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/8/12</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Tensorflow.html'>Tensorflow</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="getting_started_with_tensorflow.html">
                
                  <h1>TensorFlow(2): Getting started with TensorFlow</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">TensorFlow Core tutorial</a>
<ul>
<li>
<a href="#toc_1">importing TensorFlow</a>
</li>
<li>
<a href="#toc_2">The Computational Graph</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">tf.train API</a>
<ul>
<li>
<a href="#toc_4">Complete program</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">tf.estimator</a>
<ul>
<li>
<a href="#toc_6">Basic usage</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">TensorFlow Core tutorial</h2>

<h3 id="toc_1">importing TensorFlow</h3>

<pre><code class="language-python">import tensorflow as tf
</code></pre>

<h3 id="toc_2">The Computational Graph</h3>

<p>A <code>computational graph</code> is a series of TensorFlow operations arranged into a graph of nodes. Let&#39;s build a simple computational graph. Each node takes zero or more tensors as inputs and produces a tensor as an output. One type of node is a constant. Like all TensorFlow constant, it takes no inputs, and it outputs a value it stores internally. We can create two floating point Tensor <code>node1</code> and <code>node2</code> as follows:</p>

<pre><code class="language-python">node1 = tf.constant(3.0, dtype=tf.float32)
node2 = tf.constant(4.0) # also tf.float32 implicitly
print(node1, node2)
</code></pre>

<pre><code class="language-text">Tensor(&quot;Const:0&quot;, shape=(), dtype=float32)
Tensor(&quot;Const_1:0&quot;, shape=(), dtype=float32)
</code></pre>

<p>To produce output values for <code>node1</code> and <code>nodes2</code>, evaluation is needed. To actually evaluate the nodes, we must run the computational graph within a <code>session</code>. A session encapsulates the control and state of the Tensorflow runtime.</p>

<p>The following code creates a <code>Session</code> object and then invokes its <code>run</code> method to run enough of the computational graph to evaluate <code>node1</code> and <code>node2</code>. By running the computational graph in a session as follows:</p>

<pre><code class="language-python">sess = tf.Session()
print(sess.run([node1, node2]))
</code></pre>

<pre><code class="language-text">[3.0, 4.0]
</code></pre>

<p>We can build more complicated computations by combining <code>Tensor</code> nodes with operations(Operations are also nodes). For example, we can add our two constant nodes and produce a new graph as follows:</p>

<pre><code class="language-python">node3 = tf.add(node1, node2)
print(&#39;node3:&#39;, node3)
print(&#39;sess.run(node3):&#39;, sess.run(node3))
</code></pre>

<pre><code class="language-text">node3: Tensor(&quot;Add:0&quot;, shape=(), dtype=float32)
sess.run(node3): 7.0
</code></pre>

<p>TensorFlow provides a utility called <code>TensorBoard</code> that can display a picture of the computational graph. Here is a screenshot showing how <code>TensorBoard</code> visualizes the graph:<br/>
<img src="media/15033740189206/getting_started_add.png" alt=""/></p>

<p>As it stands, this graph is not especially interesting because it always produces a constant result. A graph can be parameterized to accept external inputs, known as <code>placeholders</code>. A <code>placeholder</code> is a promise to provide a value later.</p>

<pre><code class="language-python">a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)
adder_node = a + b # + provides a shortcut for tf.add(a,b)
</code></pre>

<p>The preceding three lines are a bit like a function or a lambda in which we define two input parameters (a and b) and then an operation on them. We can evaluate this graph with multiple inputs by using the feed_dict argument to the <code>run method</code> to feed concrete values to the placeholder:</p>

<pre><code class="language-python">print(sess.run(adder_node, {a:3, b:4.5}))
print(sess.run(adder_node, {a:[1,3], b:[2,4]}))
</code></pre>

<pre><code class="language-text">7.5
[ 3.  7.]
</code></pre>

<p>In <code>TensorBoard</code>, the graph looks like this:</p>

<p><img src="media/15033740189206/getting_started_adder.png" alt=""/></p>

<p>We can make the computational graph more complex by adding another operation. For example,</p>

<pre><code class="language-python">add_and_triple = adder_node * 3.
print(sess.run(add_and_triple,{a:3, b:4.5}))
</code></pre>

<pre><code class="language-text">22.5
</code></pre>

<p>The preceding computational graph would look as follows in <code>TensorBoard</code>:<br/>
<img src="media/15033740189206/getting_started_triple.png" alt=""/></p>

<p>In machine learning we will typically want a model that can take arbitrary inputs, such as the one above. To make the model trainable, we need to be able to modify the graph to get new outputs with the same input. <code>variables</code> allow us to add trainable parameters to a graph. They are constructed with a type and initial value:</p>

<pre><code class="language-python">W = tf.Variable([.3], dtype=tf.float32)
b = tf.Variable([-.3], dtype=tf.float32)
x = tf.placeholder(tf.float32)
linear_model = W*x +b 
</code></pre>

<p>Constants are initialized when you call <code>tf.constant</code>, and their value can never change. By contrast, variables are not initialized when you call <code>tf.Variable</code>. To initialize all the variables in a TensorFlow program, you must explicitly call a special operation as follows:</p>

<pre><code class="language-python">init = tf.global_variables_initializer()
sess.run(init)
</code></pre>

<p>It is important to realize <code>init</code> is a handle to the TensorFlow sub-graph that initializes all the global variables. Until we call <code>sess.run</code>, the variables are uninitialized.</p>

<p>Since <code>x</code> is a placeholder, we can evaluate <code>linear_model</code> for several values of <code>x</code> simultaneously as follows:</p>

<pre><code class="language-python">print(sess.run(linear_model,{x:[1,2,3,4]}))
</code></pre>

<pre><code class="language-text">[ 0.          0.30000001  0.60000002  0.90000004]
</code></pre>

<p>We&#39;ve create a model, but we don&#39;t know how good it is yet. To evaluate the model on training data, we need a <code>y</code> placeholder to provide the desired values, and we need to write a loss function.</p>

<p>A <code>loss function</code> measures how far apart the current model is from the provided data. We&#39;ll use a standard loss model for linear regression, which sums the squares of the deltas between the current model and the provided data. <code>linear_model -y</code> creates a vector where each element is the corresponding example&#39;s error delta. We call <code>tf.square</code> to square that error. Then, we sum all the squared errors to create a single scalar that abstacts the error of all examples using <code>tf.reduce_sum</code>:</p>

<pre><code class="language-python">y = tf.placeholder(tf.float32)
squared_deltas = tf.square(linear_model - y)
loss = tf.reduce_sum(squared_deltas)
print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))
</code></pre>

<pre><code class="language-text">23.66
</code></pre>

<p>We could improve this manually by reassigning the values of <code>W</code> and <code>b</code> to the perfect values of -1 and 1. A variable is initialized to the value provided to <code>tf.Variable</code> but can be changed using operations like <code>tf.assign</code>. FOr example, <code>W=-1</code> and <code>b=1</code> are the optimal parameters for our model. We can change <code>W</code> and <code>b</code> accordingly:</p>

<pre><code class="language-python">fixW = tf.assign(W, [-1.])
fixb = tf.assign(b, [1.])
sess.run([fixW, fixb])
print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))
</code></pre>

<pre><code class="language-text">0.0
</code></pre>

<p>We guessed the &quot;perfect&quot; values of <code>W</code> and <code>b</code>, but the whole point of machine learning is to find the correct model parameters automatically. We will show how to accomplish this in the next section.</p>

<h2 id="toc_3">tf.train API</h2>

<p>A complete discussion of machine learning is out of the scope of this tutorial. However, TensorFlow provides <code>optimizers</code> that slowly change each variable in order to minimize the loss function. The simplest optimizer is <code>gradient descent</code>. It modifies each variable according to the magnitude of the derivative of loss with respect to that variable. In general, computing symbolic derivatives manually is tedious and error-prone. Consequently, <code>TensorFlow</code> can automatically produce derivatives given only a description of the model using the function <code>tf.gradients</code>. For simplicity, <code>optimizers</code> typically do this for you. For example,</p>

<pre><code class="language-python">optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)

sess.run(init) # reset values to incorrect defaults
for i in range(1000):
    sess.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})
print(sess.run([W,b]))
</code></pre>

<pre><code class="language-text">[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]
</code></pre>

<p>Now we have done actual machine learning! Although doing this simple linear regression doesn&#39;t require much TensorFlow core code, more complicated models and methods to feed data into your model necessitate more code. Thus TensorFlow provides higher level abstractions for common patterns, structures, and functionality. We will learn how to use some of these abstractions in the next section.</p>

<h3 id="toc_4">Complete program</h3>

<pre><code class="language-python">import tensorflow as tf

# Model parameters
W = tf.Variable([.3], dtype=tf.float32)
b = tf.Variable([-.3], dtype=tf.float32)
# Model input and output
x = tf.placeholder(tf.float32)
linear_model = W * x + b
y = tf.placeholder(tf.float32)

# loss
loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares
# optimizer
optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)

# training data
x_train = [1, 2, 3, 4]
y_train = [0, -1, -2, -3]
# training loop
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init) # reset values to wrong
for i in range(1000):
  sess.run(train, {x: x_train, y: y_train})

# evaluate training accuracy
curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})
print(&quot;W: %s b: %s loss: %s&quot;%(curr_W, curr_b, curr_loss))
</code></pre>

<pre><code class="language-text">W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11
</code></pre>

<p>This more complicated program can still be visualized in TensorBoard:</p>

<p><img src="media/15033740189206/getting_started_final.png" alt=""/></p>

<h2 id="toc_5">tf.estimator</h2>

<p><code>tf.estimator</code> is a high-level TensorFlow library that simplifies the mechanics of machine learning, including the following:</p>

<ul>
<li>running training loops</li>
<li>running evaluation loops</li>
<li>managing data sets</li>
<li><code>tf.estimator</code> defines many common models.</li>
</ul>

<h3 id="toc_6">Basic usage</h3>

<p>Notice how much simpler the linear regression program becomes with <code>tf.estimator</code>:</p>

<pre><code class="language-python"># NumPy is often used to load, manipulate and preprocess data.
import numpy as np

# Declare list of features. We only have one numeric feature. There are many
# other types of columns that are more complicated and useful.
feature_columns = [tf.feature_column.numeric_column(&quot;x&quot;, shape=[1])]

# An estimator is the front end to invoke training (fitting) and evaluation
# (inference). There are many predefined types like linear regression,
# linear classification, and many neural network classifiers and regressors.
# The following code provides an estimator that does linear regression.
estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns)

# TensorFlow provides many helper methods to read and set up data sets.
# Here we use two data sets: one for training and one for evaluation
# We have to tell the function how many batches
# of data (num_epochs) we want and how big each batch should be.
x_train = np.array([1., 2., 3., 4.])
y_train = np.array([0., -1., -2., -3.])
x_eval = np.array([2., 5., 8., 1.])
y_eval = np.array([-1.01, -4.1, -7, 0.])
input_fn = tf.estimator.inputs.numpy_input_fn(
    {&quot;x&quot;: x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)
train_input_fn = tf.estimator.inputs.numpy_input_fn(
    {&quot;x&quot;: x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)
eval_input_fn = tf.estimator.inputs.numpy_input_fn(
    {&quot;x&quot;: x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)

# We can invoke 1000 training steps by invoking the  method and passing the
# training data set.
estimator.train(input_fn=input_fn, steps=1000)

# Here we evaluate how well our model did.
train_metrics = estimator.evaluate(input_fn=train_input_fn)
eval_metrics = estimator.evaluate(input_fn=eval_input_fn)
print(&quot;train metrics: %r&quot;% train_metrics)
print(&quot;eval metrics: %r&quot;% eval_metrics)
</code></pre>

<pre><code class="language-text">INFO:tensorflow:Using default config.
WARNING:tensorflow:Using temporary folder as model directory: /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a
INFO:tensorflow:Using config: {&#39;_model_dir&#39;: &#39;/var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a&#39;, &#39;_tf_random_seed&#39;: 1, &#39;_save_summary_steps&#39;: 100, &#39;_save_checkpoints_secs&#39;: 600, &#39;_save_checkpoints_steps&#39;: None, &#39;_session_config&#39;: None, &#39;_keep_checkpoint_max&#39;: 5, &#39;_keep_checkpoint_every_n_hours&#39;: 10000, &#39;_log_step_count_steps&#39;: 100}
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Saving checkpoints for 1 into /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a/model.ckpt.
INFO:tensorflow:loss = 19.0, step = 1
INFO:tensorflow:global_step/sec: 592.325
INFO:tensorflow:loss = 0.192443, step = 101 (0.171 sec)
INFO:tensorflow:global_step/sec: 710.314
INFO:tensorflow:loss = 0.0370785, step = 201 (0.141 sec)
INFO:tensorflow:global_step/sec: 669.791
INFO:tensorflow:loss = 0.0173565, step = 301 (0.150 sec)
INFO:tensorflow:global_step/sec: 729.609
INFO:tensorflow:loss = 0.00361388, step = 401 (0.136 sec)
INFO:tensorflow:global_step/sec: 814
INFO:tensorflow:loss = 0.000215951, step = 501 (0.123 sec)
INFO:tensorflow:global_step/sec: 793.172
INFO:tensorflow:loss = 0.0001734, step = 601 (0.127 sec)
INFO:tensorflow:global_step/sec: 776.415
INFO:tensorflow:loss = 3.66416e-05, step = 701 (0.128 sec)
INFO:tensorflow:global_step/sec: 845.781
INFO:tensorflow:loss = 3.03422e-06, step = 801 (0.118 sec)
INFO:tensorflow:global_step/sec: 849.689
INFO:tensorflow:loss = 1.18453e-06, step = 901 (0.118 sec)
INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a/model.ckpt.
INFO:tensorflow:Loss for final step: 3.72255e-07.
INFO:tensorflow:Starting evaluation at 2017-08-21-05:57:38
INFO:tensorflow:Restoring parameters from /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a/model.ckpt-1000
INFO:tensorflow:Finished evaluation at 2017-08-21-05:57:40
INFO:tensorflow:Saving dict for global step 1000: average_loss = 6.05797e-08, global_step = 1000, loss = 2.42319e-07
INFO:tensorflow:Starting evaluation at 2017-08-21-05:57:40
INFO:tensorflow:Restoring parameters from /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a/model.ckpt-1000
INFO:tensorflow:Finished evaluation at 2017-08-21-05:57:42
INFO:tensorflow:Saving dict for global step 1000: average_loss = 0.00254753, global_step = 1000, loss = 0.0101901
train metrics: {&#39;average_loss&#39;: 6.057968e-08, &#39;loss&#39;: 2.4231872e-07, &#39;global_step&#39;: 1000}
eval metrics: {&#39;average_loss&#39;: 0.0025475256, &#39;loss&#39;: 0.010190102, &#39;global_step&#39;: 1000}
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/8/20</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Tensorflow.html'>Tensorflow</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="convolutional-neural-networks.html">
                
                  <h1>Convolutional Neural Networks</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Architecture</a>
<ul>
<li>
<ul>
<li>
<a href="#toc_1">Conv layers</a>
</li>
<li>
<a href="#toc_2">Pooling</a>
</li>
<li>
<a href="#toc_3">Fully-connected layers</a>
</li>
<li>
<a href="#toc_4">Layer Patterns</a>
</li>
<li>
<a href="#toc_5">Layer Sizing Patterns</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">Computational Considerations</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_7">The memory bottleneck is the largest bottleneck when constructing ConvNet architectures.</a>


<h2 id="toc_0">Architecture</h2>

<p>Three main types of layers to build ConvNet architectures: <strong>Convolutional Layer, Pooling Layer and Fully-Connected Layer</strong>.</p>

<p>The layers of a ConvNet have neurons arranged in 3 dimensions: <strong>width, height, depth</strong>.</p>

<p><img src="media/15042285789723/15043413926666.jpg" alt="convolutional"/></p>

<p>A simple ConvNet for <a href="https://www.cs.toronto.edu/%7Ekriz/cifar.html">CIFAR-10</a> classification could have the architecture as follows:</p>

<ul>
<li><strong>INPUT</strong> [width x height x color channel] will hold the raw pixel values of the image, in this case an image of width, height, and with three color channels R,G,B.</li>
<li><strong>CONV layer</strong> will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume.</li>
<li><strong>RELU layer</strong> will apply an elementwise activation function, such as the <code>max(0,x)</code> thresholding at zero. </li>
<li><strong>POOL layer</strong> will perform a downsampling operation along the spatial dimensions (width, height).</li>
<li><strong>FC</strong> (i.e. fully-connected) layer will compute the class scores, where each score corresponding to the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.</li>
</ul>

<h4 id="toc_1">Conv layers</h4>

<p>Four hyperparameters <strong>depth</strong>(K), <strong>spatial extent</strong>(\(F\)), <strong>stride</strong>(\(S\)) and <strong>zero-padding</strong>(\(P\)) control the size of the output volume from the input volume (\(W\)).</p>

<p><strong>Summary</strong>. To summarize, the Conv Layer:</p>

<ul>
<li>Accepts a volume of size \(W_1 \times H_1 \times D_1\)</li>
<li>Requires four hyperparameters: 
<ul>
<li>Number of filters \(K\), </li>
<li>their spatial extent \(F\), </li>
<li>the stride \(S\), </li>
<li>the amount of zero padding \(P\).</li>
</ul></li>
<li>Produces a volume of size \(W_2 \times H_2 \times D_2\) where:
<ul>
<li>\(W_2 = (W_1 - F + 2P)/S + 1\)</li>
<li>\(H_2 = (H_1 - F + 2P)/S + 1\) (i.e. width and height are computed equally by symmetry)</li>
<li>\(D_2 = K\)</li>
</ul></li>
<li>With parameter sharing, it introduces \(F \cdot F \cdot D_1\) weights per filter, for a total of \((F \cdot F \cdot D_1) \cdot K\) weights and \(K\) biases.</li>
<li>In the output volume, the \(d\)-th depth slice (of size \(W_2 \times H_2\)) is the result of performing a valid convolution of the \(d\)-th filter over the input volume with a stride of \(S\), and then offset by \(d\)-th bias.</li>
</ul>

<p><strong>parameter sharing</strong>: the neurons in each depth slice(i.e \(K\)) to use the same weights and bias. </p>

<h4 id="toc_2">Pooling</h4>

<p><code>max pooling</code> is the most common function performed on the pooling units, others like <code>average pooling</code> or <code>L2-norm pooling</code> is work worse in practice. And Many people dislike the pooling operation and think that we can get away without it.</p>

<h4 id="toc_3">Fully-connected layers</h4>

<p>Neurons in a fully connected layer have full connections to all activations in the previous layer.</p>

<h4 id="toc_4">Layer Patterns</h4>

<p>The most common form of a ConvNet architecture stacks a few CONV-RELU layers, follows them with POOL layers, and <strong>repeats this pattern until the image has been merged spatially to a small size</strong>. At some point, it is common to transition to fully-connected layers. The last fully-connected layer holds the output, such as the class scores. In other words, the most common ConvNet architecture follows the pattern:</p>

<p>\[INPUT \rightarrow  [[CONV \rightarrow  RELU]*N \rightarrow  POOL?]*M \rightarrow [FC -&gt; RELU]*K \rightarrow  FC\]</p>

<p>where the <code>*</code> indicates repetition, and the <code>POOL?</code> indicates an optional pooling layer. Moreover, <code>N &gt;= 0</code> (and usually <code>N &lt;= 3</code>), <code>M &gt;= 0</code>, <code>K &gt;= 0</code> (and usually <code>K &lt; 3</code>). For example, here are some common ConvNet architectures you may see that follow this pattern:</p>

<p><strong>Note</strong>: <code>INPUT -&gt; FC</code>, implements a linear classifier. Here <code>N = M = K = 0</code>.<br/>
<strong>Note</strong>: Prefer a stack of small filter CONV to one large receptive field CONV layer, because of few parameters needed and expressing more powerful features of the input.</p>

<h4 id="toc_5">Layer Sizing Patterns</h4>

<ul>
<li><strong>input layer</strong>:  Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512.</li>
<li><strong>conv layers</strong>: small filters(\(3\times3\) or \(5\times5\)), with \(S=1\). Crucially, <em>padding the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input.</em> </li>
<li><strong>pool layers</strong>: use max-pooling with \(F=2, S=2\) or \(F=3, S=2\)</li>
</ul>

<h3 id="toc_6">Computational Considerations</h3>

<h1 id="toc_7">The memory bottleneck is the largest bottleneck when constructing ConvNet architectures.</h1>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/2</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Deep%20Learning.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="virtualenv.html">
                
                  <h1>Python `virtualenv` on mac</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">Install</h2>

<p>Install <code>virtualenv</code> using <code>conda</code> instead of <code>pip</code>, because it might raise error (see on <a href="virtualenv%20--no-site-packages%20venv">StackOverflow</a>)</p>

<pre><code class="language-bash">conda install virtualenv
</code></pre>

<h2 id="toc_1">create your environment</h2>

<p>Now you can create your python environment for your particular programs. For example, under the folder <code>your project</code>, you create an environment called <code>.venv</code> by:</p>

<pre><code class="language-bash">virtualenv --no-site-packages .venv
</code></pre>

<p>The command <code>--no-site-packages</code> requires the environment should not access to global site-packages (as default now).</p>

<p>Before running your program in your created environment, you need to activate it:</p>

<pre><code class="language-python">source .venv/bin/activate
</code></pre>

<p>And remember to deactivate it whenever you are done.:</p>

<pre><code class="language-text">deactivate
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/2</span>
                    
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all_16.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_18.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">他山之石，可以攻玉</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Leetcode.html"><strong>Leetcode</strong></a>
        
            <a href="programming_language.html"><strong>编程语言</strong></a>
        
            <a href="data_structure_and_algorithm.html"><strong>数据结构和算法</strong></a>
        
            <a href="Course.html"><strong>Course</strong></a>
        
            <a href="Python%E7%89%B9%E6%80%A7.html"><strong>Python特性</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>Python科学计算三维可视化</strong></a>
        
            <a href="English.html"><strong>English</strong></a>
        
            <a href="Computer%20System.html"><strong>Computer System</strong></a>
        
            <a href="Deep%20Learning.html"><strong>Deep Learning</strong></a>
        
            <a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html"><strong>Linux 系统编程</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html"><strong>数据库</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="Big%20Data.html"><strong>Big Data</strong></a>
        
            <a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html"><strong>文献阅读</strong></a>
        
            <a href="Tools.html"><strong>Tools</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="csapp-internet-programming.html">CSAPP - 网络编程</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os-concets-processes.html">Operating System Concepts 3 - Processes</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os_concepts_threads_and_concurrency.html">Operating System Concepts 4 - Threads & Concurrency</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="head-first_java_note.html">Head first Java</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="cpp_by_diessection.html">[NOTE] C++ By Dissection</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
