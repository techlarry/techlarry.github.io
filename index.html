<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  techlarry
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">HomePage</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_blank" href="wiki">WIKI</a></li>
        
        <li id=""><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">HomePage</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_blank" href="wiki">WIKI</a></li>
        
        <li><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="Leetcode.html">Leetcode</a></li>
        
            <li><a href="programming_language.html">编程语言</a></li>
        
            <li><a href="data_structure_and_algorithm.html">数据结构和算法</a></li>
        
            <li><a href="Python%E7%89%B9%E6%80%A7.html">Python特性</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html">Python科学计算三维可视化</a></li>
        
            <li><a href="English.html">English</a></li>
        
            <li><a href="Computer%20System.html">Computer System</a></li>
        
            <li><a href="Deep%20Learning.html">Deep Learning</a></li>
        
            <li><a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html">Linux 系统编程</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html">数据库</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="Big%20Data.html">Big Data</a></li>
        
            <li><a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html">文献阅读</a></li>
        
            <li><a href="Tools.html">Tools</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="os_concepts_CPU_scheduling.html">
                
                  <h1>Operating System Concepts 5 - CPU Scheduling</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">1 Basic Concepts</a>
<ul>
<li>
<a href="#toc_1">1.1 CPU-I/O Burst Cycle</a>
</li>
<li>
<a href="#toc_2">1.2 CPU Scheduler</a>
</li>
<li>
<a href="#toc_3">1.3 Preemptive and Nonpreemptive Scheduling</a>
</li>
<li>
<a href="#toc_4">1.4 Dispatcher</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">2 Scheduling Criteria</a>
</li>
<li>
<a href="#toc_6">3 Scheduling Algorithms</a>
<ul>
<li>
<a href="#toc_7">3.1 First-Come,First-Served scheduling, FCFS</a>
</li>
<li>
<a href="#toc_8">3.2 Shortest-job-first scheduling, SJF</a>
</li>
<li>
<a href="#toc_9">3.3 Round-Robin scheduling, RR</a>
</li>
<li>
<a href="#toc_10">3.4 Priority scheduling algorithm</a>
</li>
<li>
<a href="#toc_11">3.5 Multilevel Queue Scheduling</a>
</li>
<li>
<a href="#toc_12">3.6 Multilevel Feedback-Queue Scheduling</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">4 Thread Scheduling</a>
<ul>
<li>
<a href="#toc_14">4.1 Contention Scope</a>
</li>
<li>
<a href="#toc_15">4.2 Pthread Scheduling</a>
</li>
</ul>
</li>
<li>
<a href="#toc_16">5 Multi-Processor Scheduling</a>
<ul>
<li>
<a href="#toc_17">5.1 Approaches to Multiple-Processor Scheduling</a>
</li>
<li>
<a href="#toc_18">5.2 Multicore Processors</a>
</li>
<li>
<a href="#toc_19">5.3 Load Balancing</a>
</li>
<li>
<a href="#toc_20">5.4 Processor Afﬁnity</a>
</li>
</ul>
</li>
<li>
<a href="#toc_21">6 Real-Time CPU Scheduling</a>
</li>
<li>
<a href="#toc_22">7 Linux Scheduling</a>
</li>
</ul>


<p>On modern operating systems it is <strong>kernel-level threads</strong> —not processes—that are in fact being scheduled by the operating system. </p>

<ul>
<li>User-level threads are managed by a thread library, and the kernel is <em>unaware</em> of them.</li>
<li>To run on a CPU, user-level threads must ultimately be mapped to an associated kernel-level thread, although this mapping may be indirect and may use a lightweight process (LWP).</li>
</ul>

<h2 id="toc_0">1 Basic Concepts</h2>

<h3 id="toc_1">1.1 CPU-I/O Burst Cycle</h3>

<p>Process execution consists of a <strong>cycle</strong> of CPU execution and I/O wait. 进程执行由CPU执行周期和I/O等待周期组成。</p>

<ul>
<li>Processes alternate between these two states. 进程在这两个状态之间切换。</li>
<li>Process execution begins with a <strong>CPU burst</strong>, which is followed by an <strong>I/O burst</strong> and so on. 进程执行从CPU区间开始，在这之后是I/O区间。</li>
</ul>

<p>进程在CPU区间和I/O区间之间切换：<br/>
<img src="media/15326899337167/alternating%20sequence%20of%20CPU%20and%20I:O%20bursts.png" alt="alternating sequence of CPU and I:O bursts"/></p>

<p>The durations of CPU bursts tend to have a frequency curve similar to the figure below. </p>

<ul>
<li>The curve is generally characterized as <strong>exponential</strong> or hyperexpoential(超指数).</li>
<li>A large number of short CPU bursts and a small number of long CPU burst.</li>
<li>An I/O-bounded program typically has many short CPU bursts. I/O密集程序通常具有很多短CPU区间。</li>
<li>A CPU-bound program might have a few long CPU bursts.CPU密集程序可能有少量的长CPU区间。</li>
<li>The distribution can be important when implementing a CPU-scheduling algorithm. 分布有助于选择合适的CPU调度算法。</li>
</ul>

<p><img src="media/15326899337167/Histogram%20of%20CPU-burst%20durations.png" alt="Histogram of CPU-burst durations"/></p>

<h3 id="toc_2">1.2 CPU Scheduler</h3>

<p>Whenever the CPU becomes idle, the operating system must select one of the processes in the <strong>ready queue</strong>(就绪队列) to be executed. 每当CPU空闲时，操作系统就必须从就绪队列中选择一个进程来执行。</p>

<ul>
<li>The selection process is carried out by the <strong>CPU scheduler</strong>(CPU调度程序).  进程选择由CPU调度程序执行。</li>
<li>CPU scheduler selects a process from the processes in memory that are ready to execute and allocates the CPU to that process. 调度程序从内存中选择一个能够执行的进程，并为之分配CPU。</li>
<li>A ready queue can be implemented as a FIFO queue, a priority queue, a tree, or simply an unordered linked list. 就绪队列可以是FIFO队列，优先队列、树或无序链表。</li>
</ul>

<h3 id="toc_3">1.3 Preemptive and Nonpreemptive Scheduling</h3>

<p>CPU-scheduling decisions may take place under the following four circumstances: </p>

<ol>
<li>When a process switches from the running state to the waiting state (for example, as the result of an I/O request or an invocation of <code>wait()</code> for the termination of a child process) 当一个进程从运行状态切换到等待状态（如：I/O请求，或者调用wait等待一个子进程的终止） </li>
<li>When a process switches from the running state to the ready state (for example, when an interrupt occurs) 当一个进程从运行状态切换到就绪状态（如：出现中断） </li>
<li>When a process switches from the waiting state to the ready state (for example, at completion of I/O) 当一个进程从等待状态切换到就绪状态（如：I/O完成） </li>
<li>When a process terminates 当一个进程终止时</li>
</ol>

<p>When scheduling takes place only under circumstances 1 and 4, the scheduling scheme is <strong>nonpreemptive</strong>(非抢占的) or <strong>cooperative</strong>(协作的). Otherwise, it is <strong>preemptive</strong>(抢占的).</p>

<ul>
<li>Under nonpreemptive scheduling, once the CPU has been allocated to a process, the process keeps the CPU until it releases it either by terminating or by switching to the waiting state.</li>
<li>Virtually all modern Operating systems use preemptive scheduling algorithms. </li>
</ul>

<h3 id="toc_4">1.4 Dispatcher</h3>

<p>The <strong>dispatcher</strong>(分派程序) is the module that gives control of the CPU&#39;s core to the process selected by the CPU scheduler. This function involves the following:</p>

<ul>
<li>Switching context from one process to another</li>
<li>Switching to user mode</li>
<li>Jumping to the proper location in the user program to resume that program</li>
</ul>

<p><strong>Dispatch latency</strong> (分派延迟) is the time it takes for the dispatcher to stop one process and start another running.</p>

<p><img src="media/15326899337167/the%20role%20of%20dispatcher.png" alt="the role of dispatcher"/></p>

<h2 id="toc_5">2 Scheduling Criteria</h2>

<p>Scheduling criteria（调度准则) include the following:</p>

<ul>
<li><strong>CPU utilization</strong> (CPU利用率)</li>
<li><strong>Throughput</strong> (吞吐量)： the number of processes that are completed per time unit.</li>
<li><strong>Turnaround time</strong> (周转时间): the interval from the time of submission of a process to the time of completion.</li>
<li><strong>Waiting time</strong> (等待时间): the sum of time spent waiting in the ready queue.</li>
<li><strong>Response time</strong> (响应时间): the time from the submission of a request until the first response is produced.</li>
</ul>

<h2 id="toc_6">3 Scheduling Algorithms</h2>

<h3 id="toc_7">3.1 First-Come,First-Served scheduling, FCFS</h3>

<p>By far the simplest CPU-scheduling algorithm is the <strong>first-come first serve scheduling</strong> (先到先服务调度, FCFS) algorithm.</p>

<ul>
<li>The implementation of FCFS policy is easily managed with a <strong>FIFO queue</strong>.</li>
<li>The average <strong>waiting time</strong> under the FCFS policy is often quite <strong>long</strong>.</li>
<li><strong>Convoy effect</strong>(护航效果) occurs when all the other processes wait for the one big process to get off the CPU. 所有其他进程都等待一个大进程释放CPU，这称之为护航效果。</li>
<li>The FCFS scheduling algorithm is <strong>nonpreemptive</strong>. FCFS调度算法是非抢占的。</li>
</ul>

<h3 id="toc_8">3.2 Shortest-job-first scheduling, SJF</h3>

<p>The <strong>shortest-job-first scheduling</strong> (最短作业优先调度, SJF) algorithm associates with each process the length of the process&#39;s next CPU burst.</p>

<ul>
<li>When the CPU is available, it is assigned to the process that has the smallest <strong>next</strong> CPU burst.</li>
<li>It gives the <strong>minimum</strong> average waiting time for a given set of processes.</li>
<li>The SJF algorithm can be either preemptive or nonpreemptive.
<ul>
<li>Preempt the currently executing process: when a new process arrives at the ready queue while a previous process is still executing. The next CPU burst of the newly arrived process may be shorter than what is left of the currently executing process. </li>
</ul></li>
</ul>

<p>The next CPU burst is generally predicted as an <strong>exponential average</strong> of the measured lengths of previous CPU bursts. Let \(t_n\) be the length of the \(n\)th CPU burst, and let \(\tau_{n+1}\) be predicted value for the next CPU burst:</p>

<p>\[\tau_{n+1}= \alpha \tau_n + (1-\alpha) \tau_n\]</p>

<p>where \(0\le\alpha \le 1\), commonly \(\alpha = 1/2\).</p>

<h3 id="toc_9">3.3 Round-Robin scheduling, RR</h3>

<p>The <strong>round-robin scheduling</strong>(轮转调度) algorithm is similar to FCFS scheduling, but switch occurs after 1 <strong>time quantum</strong> (时间片).</p>

<ul>
<li>Time quantum is a small unit of time, generally from 10 to 100 milliseconds in length.</li>
<li>The ready queue is treated as a circular queue.</li>
<li>If the process have a CPU burst of less than 1 time quantum, the  process itself will release the CPU voluntarily.</li>
<li>otherwise, a context switch will be executed, and the process will be put at the tail of the ready queue.</li>
</ul>

<p>The performance of the RR algorithm depends heavily on the size of the time quantum.</p>

<ul>
<li>If extremely large, the RR policy is the same as the FCFS policy.</li>
<li>If extremely small, it&#39;ll result in a large number of context switches.</li>
</ul>

<h3 id="toc_10">3.4 Priority scheduling algorithm</h3>

<p>The <strong>priority-scheduling</strong>(优先级调度) algorithm associate each process a priority, and the CPU allocated to the process with the highest priority.</p>

<ul>
<li>FCFS: equal-priority</li>
<li>SJF: the priority is the inverse of the next CPU burst.</li>
</ul>

<p>ISSUE: <strong>Indefinite blocking</strong>(无限阻塞), or <strong>starvation</strong>(饥饿) occurs when some low-priority processes waiting indefinitely.</p>

<p>SOLUTION: <strong>Aging</strong>(老化) involves gradually increasing the priority of processes that wait in the system for a long time.</p>

<h3 id="toc_11">3.5 Multilevel Queue Scheduling</h3>

<p>For <strong>multilevel queue scheduling</strong>(多级队列调度), there are separate queues for each distinct priority, and priority scheduling simply schedules the process in the highest-priority queue.</p>

<p>A multilevel queue scheduling algorithm can be used to partition processes into several separate queuse based on the process type.<br/>
<img src="media/15326899337167/multilevel-queue-scheduling.png" alt="multilevel-queue-scheduling"/></p>

<p>In addition, there must be scheduling <u><em>among the queues</em></u> :</p>

<ul>
<li><strong>Fixed-priority preemptive scheduling</strong>(固定优先级抢占调度): Each queue has absolute priority over lower-priority queues
<ul>
<li>eg. no process in the batch queue, could run unless the queues for real-time processes, system processes, and interactive processes were all empty. </li>
</ul></li>
<li><strong>Time-slice among queues</strong>(队列之间划分时间片): each queue gets a certain portion of the CPU time.
<ul>
<li>eg. the foreground queue can be given 80 percent of the CPU time for RR scheduling among its processes, while the background queue receives 20 percent of the CPU to give to its processes on an FCFS basis.</li>
</ul></li>
</ul>

<h3 id="toc_12">3.6 Multilevel Feedback-Queue Scheduling</h3>

<p>The <strong>multilevel feedback queue scheduling</strong>(多级反馈队列调度) algorithm allows a process to move between queues.</p>

<ul>
<li>If a process uses too much CPU time, it will be moved to a lower-priority queue.
<ul>
<li>It leaves I/O-bound and interactive processes—which are typically characterized by short CPU bursts —in the higher-priority queues. </li>
</ul></li>
<li>A process that waits too long in a lower-priority queue may be moved to a higher-priority queue.
<ul>
<li>This form of aging prevent starvation.</li>
</ul></li>
</ul>

<p>In general, a multilevel feedback queue scheduler is defined by the following parameters:</p>

<ul>
<li>The number of queues</li>
<li>The scheduling algorithm for each queue</li>
<li>The method used to determine when to upgrade a process to a higher priority queue</li>
<li>The method used to determine when to demote a process to a lower priority queue</li>
<li>The method used to determine which queue a process will enter when that process needs service</li>
</ul>

<h2 id="toc_13">4 Thread Scheduling</h2>

<h3 id="toc_14">4.1 Contention Scope</h3>

<p><strong>Process contention scope</strong> (PCS，进程竞争范围), occurs when competition for the CPU takes place among threads belonging to the same process.</p>

<ul>
<li>the thread library schedules user-level threads to run on an available LWP, on systems implementing the many-to-one and many-to-many models.</li>
</ul>

<p>To decide which kernel-level thread to schedule onto a CPU, the kernel uses <strong>system-contention scope</strong> (SCS, 系统竞争范围).</p>

<ul>
<li>Systems using the one-to-one model, such as Windows and Linux schedule threads using only SCS.</li>
</ul>

<h3 id="toc_15">4.2 Pthread Scheduling</h3>

<p><strong>Pthreads</strong> identifies the following contention scope values:</p>

<ul>
<li><code>PTHREAD_SCOPE_PROCESS</code> schedules threads using PCS scheduling.</li>
<li><code>PTHREAD_SCOPE_SYSTEM</code> schedules threads using SCS scheduling.</li>
</ul>

<p>The Pthread IPC (Interprocess Communication) provides two functions for setting—and getting—the contention scope policy:</p>

<ul>
<li><code>pthread_attr_setscope(pthread_attr_t *attr, int scope)</code></li>
<li><code>pthread_attr_getscope(pthread_attr_t *attr, int *scope)</code></li>
</ul>

<pre><code class="language-c">#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;
#define NUM_THREADS 5

/* the thread runs in this function */
void *runner(void *param); 

int main(int argc, char *argv[])
{
    int i, scope;
    pthread_t tid[NUM_THREADS];     /* the thread identifier */
    pthread_attr_t attr;        /* set of attributes for the thread */

    /* get the default attributes */
    pthread_attr_init(&amp;attr);

    /* first inquire on the current scope */
    if (pthread_attr_getscope(&amp;attr,&amp;scope) != 0)
        fprintf(stderr, &quot;Unable to get scheduling scope.\n&quot;);
    else {
        if (scope == PTHREAD_SCOPE_PROCESS)
            printf(&quot;PTHREAD_SCOPE_PROCESS\n&quot;);
        else if (scope == PTHREAD_SCOPE_SYSTEM)
            printf(&quot;PTHREAD_SCOPE_SYSTEM\n&quot;);
        else 
            fprintf(stderr,&quot;Illegal scope value.\n&quot;);
    }
    
    /* set the scheduling algorithm to PCS or SCS */
    if (pthread_attr_setscope(&amp;attr, PTHREAD_SCOPE_SYSTEM) != 0)
        printf(&quot;unable to set scheduling policy.\n&quot;);

    /* create the threads */
    for (i = 0; i &lt; NUM_THREADS; i++) 
        pthread_create(&amp;tid[i],&amp;attr,runner,NULL); 

    /**
     * Now join on each thread
     */
    for (i = 0; i &lt; NUM_THREADS; i++) 
        pthread_join(tid[i], NULL);
}

/**
 * The thread will begin control in this function.
 */
void *runner(void *param) 
{
    /* do some work ... */

    pthread_exit(0);
}
</code></pre>

<h2 id="toc_16">5 Multi-Processor Scheduling</h2>

<h3 id="toc_17">5.1 Approaches to Multiple-Processor Scheduling</h3>

<p><strong>Asymmetric multiprocessing</strong> (AMP，非对称多处理)</p>

<ul>
<li>all scheduling decisions, I/O processing, and other system activities handled by a single processor -- the master server; the other processors execute only user code.  让一个处理器（主服务器）处理所有的调度决定、I/O处理以及其他系统活动，其他的处理器只执行用户代码。</li>
<li>it is simple because only one core accesses the system data structures, reducing the need for data sharing. 简单，因为只有一个处理器访问系统数据结构，减轻了数据共享的需要。</li>
<li>the master server becomes a potential bottleneck where overall system performance may be reduced.</li>
</ul>

<p><strong>Symmetric multiprocessing</strong> (SMP， 对称多处理)</p>

<ul>
<li>each processor is self-scheduling</li>
<li>it provides two possible strategies for organizing the threads eligible to be scheduled:
<ul>
<li>All threads may be in a _common ready queue_.
<ul>
<li>use some form of locking to protect the common ready queue from race condition</li>
<li>all accesses to the queue would require lock ownership, it would be a performance bottleneck.</li>
</ul></li>
<li>Each processor may have its own <u>private queue</u> of threads.
<ul>
<li>most common approach on systems supporting SMP</li>
<li>more efficient use of cache memory.</li>
</ul></li>
</ul></li>
</ul>

<p><img src="media/15326899337167/organization%20of%20ready%20queues.png" alt="organization of ready queues"/></p>

<h3 id="toc_18">5.2 Multicore Processors</h3>

<p><u>Issue</u> : memory stalls occurs when a processor accesses memory, it spends a significant amount of time waiting for the data to become available.</p>

<ul>
<li>occurs primarily because modern processors operate at much faster speeds than memory</li>
<li>occur because of a cache miss</li>
</ul>

<p><img src="media/15326899337167/memory%20stall.png" alt="memory stall"/><br/>
<u>Solution</u> : many recent hardware designs have implemented multithreaded processing cores in which two (or more) <strong>hardware threads</strong>(硬件线程) are assigned to each core.</p>

<ul>
<li>If one hardware thread stalls while waiting for memory, the core can switch to another thread.</li>
<li>From an operating system perspective, each hardware thread maintains its architectural state, such as instruction pointer and register set, and thus appears as a logical CPU that is available to run a software thread. This technique is known as <strong>chip multithreading</strong> (CMT, 芯片多线程). Intel use the term <strong>hyper-threading</strong>(超线程).</li>
<li><strong>NOTE</strong>: the resources of the physical core (such as caches and pipelines) are shared among its hardware threads, and a processing core can only execute one hardware thread at a time.</li>
</ul>

<p><img src="media/15326899337167/Chip%20multithreading.png" alt="Chip multithreading"/></p>

<p>Two levels of scheduling needed:</p>

<ul>
<li>It chooses which software thread to run on each hardware thread.
<ul>
<li>It may choose any scheduling algorithm. </li>
</ul></li>
<li>It chooses which hardware thread to run on CPU.
<ul>
<li>Use a simple round-robin algorithm</li>
<li>assigned to each hardware thread a dynamic urgency value ranging from 0 to 7, with 0 representing the lowest urgency and 7 the highest. </li>
</ul></li>
</ul>

<p><img src="media/15326899337167/two%20levels%20of%20scheduling.png" alt="two levels of scheduling"/></p>

<h3 id="toc_19">5.3 Load Balancing</h3>

<p><strong>Load balancing</strong>(负载均衡) attempts to keep the workload evenly distributed across all processors in an SMP system.</p>

<p>Two general approaches to load balancing:</p>

<ul>
<li><strong>Push migration</strong>: a specific task periodically checks the load on each processor and -- if it finds an imbalance -- evenly distributes the load by moving (or pushing) threads from overloaded to idle or less-busy processors.</li>
<li><strong>Pull migration</strong>: an idle processor pulls a waiting task from a busy processor.</li>
<li>They are not mutually exclusive and are, in fact, often implemented in parallel on load-balancing systems.</li>
</ul>

<h3 id="toc_20">5.4 Processor Afﬁnity</h3>

<p>Because of the high cost of invalidating and repopulating caches, most operating systems with SMP support try to <u>avoid migrating</u> a thread from one processor to another and instead attempt to keep a thread running on the same processor and take advantage of a warm cache. This is known as <strong>processor affinity</strong>(处理器亲和性)。</p>

<p>Common ready queue and per-processor ready queue(section 5.1):</p>

<ul>
<li>If we adopt the approach of a common ready queue, a thread may be selected for execution by any processor. Thus, if a thread is scheduled on a new processor, that processor’s cache must be repopulated.</li>
<li>With private, per-processor ready queues, a thread is always scheduled on the same processor and can therefore benefit from the contents of a warm cache.</li>
</ul>

<p>The main-memory architecture of a system can affect processor affinity issues as well. <strong>Non-uniform memory access</strong>(NUMA, 非一致性内存访问) where there are two physical processor chips each with their own CPU and local memory. A CPU has faster access to its local memory than to memory local to another CPU.</p>

<p><img src="media/15326899337167/numa%20and%20CPU%20scheduling.png" alt="numa and CPU scheduling"/></p>

<p>Interestingly, load balancing often <strong>counteracts</strong> the benefits of processor affinity.</p>

<h2 id="toc_21">6 Real-Time CPU Scheduling</h2>

<p>[to be continued]</p>

<h2 id="toc_22">7 Linux Scheduling</h2>

<p>The <strong><em>Completely Fair Scheduler</em></strong>（CFS，完全公平调度算法) is the default Linux scheduling algorithm.</p>

<ul>
<li>Each task has a <strong>virtual runtime</strong> value, which is its actual runtime normalized to the number of ready tasks.</li>
<li>Task priority is incorporated as a <strong>decay factor</strong> into this<br/>
formula. 
<ul>
<li>Lower-priority tasks have higher rates of decay than higher-priority tasks.</li>
</ul></li>
<li>The CPU is allocated to the task with the smallest virtual<br/>
runtime value.</li>
</ul>

<p>Standard Linux kernels implement two <strong>scheduling classes</strong>(调度类): </p>

<ul>
<li>a default scheduling class using the CFS scheduling algorithm </li>
<li>a real-time scheduling class.</li>
</ul>

<p>Each runnable task is placed in a <strong>red-black tree</strong> - a balanced binary search tree whose key is based on the value of virtual runtime <code>vruntime</code>.</p>

<ul>
<li>discover the leftmost node will require \(O(\log N)\) operations.</li>
<li>Linux scheduler caches the leftmost node in the variable <code>rb_leftmost</code>, and requires only retrieving the cached value.</li>
</ul>

<p><img src="media/15326899337167/15327413379278.gif" alt=""/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/3/27</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Computer%20System.html'>Computer System</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="os_concepts_synchronization_tools.html">
                
                  <h1>Operating System Concepts 6 - Synchronization Tools</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">1 Background</a>
</li>
<li>
<a href="#toc_1">2 The Critical-Section problem</a>
</li>
<li>
<a href="#toc_2">3 Peterson&#39;s Solution</a>
</li>
<li>
<a href="#toc_3">4 Hardware support for Synchronization</a>
<ul>
<li>
<a href="#toc_4">4.1 Memory barriers</a>
</li>
<li>
<a href="#toc_5">4.2 Hardware instructions</a>
</li>
<li>
<a href="#toc_6">4.3 Atomic variables</a>
</li>
</ul>
</li>
<li>
<a href="#toc_7">5 Mutex locks</a>
</li>
<li>
<a href="#toc_8">6 Semaphores</a>
</li>
<li>
<a href="#toc_9">7 Monitors</a>
</li>
<li>
<a href="#toc_10">8 Liveness</a>
<ul>
<li>
<a href="#toc_11">8.1 Deadlock</a>
</li>
<li>
<a href="#toc_12">8.2 Priority Inversion</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">9 Evaluation</a>
</li>
</ul>


<h2 id="toc_0">1 Background</h2>

<p>A <strong>race condition</strong>(竞争条件) occurs when several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place.</p>

<p>多个进程并发访问和操作同一数据，且执行结果与访问发生的特定顺序有关，称之为竞争条件。</p>

<h2 id="toc_1">2 The Critical-Section problem</h2>

<p>A <strong>critical section</strong>（临界区） is a section of code, in which the process may be accessing and updating data that is shared with at least one other process.</p>

<ul>
<li>When one process is executing in its critical section, no other process is allowed to execute in its critical section.</li>
</ul>

<p>The <strong>critical-section problem</strong>（临界区问题） is to design a protocol that the processes can use to synchronize their activity so as to cooperatively share data.</p>

<ul>
<li>Each process must request permission to enter its critical section.</li>
<li>The section of code implementing this request is the <strong>entry section</strong>（进入区）</li>
<li>The critical section may be followed by an <strong>exit section</strong> (退出区)。</li>
<li>The remaining code is the **remainder section **（剩余区)。</li>
</ul>

<p><img src="media/15326525991493/general%20structure%20of%20a%20typical%20process.png" alt="general structure of a typical process"/></p>

<p>A solution to the critical-section problem must satisfy the following three requirements:</p>

<ol>
<li><strong>Mutual exclusion</strong> (互斥): If process \(P_i\) is executing in its critical section, then no other processes can be executing in their critical sections. 如果进程\(P_i\)在其临界区内执行，那么其他进程都不能在其临界区内执行；</li>
<li><p><strong>Progress</strong> (前进): If no process is executing in its critical section and some processes wish to enter their critical sections, then only those processes that are not executing in their remainder sections can participate in deciding which will enter its critical section next, and this selection cannot be postponed indefinitely. 如果没有进程在其临界区内执行且有进程需进入临界区，那么只有那么不在剩余区内执行的进程可参加选择，以确定谁能下一个进入临界区，且这种选择不能无限推迟；</p></li>
<li><p><strong>Bounded waiting</strong> (有限等待): There exists a bound, or limit, on the number of times that other processes are allowed to enter their critical sections after a process has made a request to enter its critical section and before that request is granted. 从一个进程做出进入临界区的请求，直到该请求允许为止，其他进程允许进入其临界区内的次数有上限。</p></li>
</ol>

<p>Two general approaches are used to handle critical sections in operating systems: <strong>preemptive kernels</strong>（抢占内核） and <strong>nonpreemptive kernels</strong>（非抢占内核）.</p>

<ul>
<li>A preemptive kernel allows a process to be preempted while it is running in kernel mode. 抢占内核允许处于内核模式的进程被抢占。</li>
<li>A nonpreemptive kernel does not allow a process running in kernel mode to be preempted.A kernel-model process will run until it exists kernel mode, blocks, or voluntarily yields control of the CPU.非抢占内核不允许内核模式的进程被抢占。</li>
<li>A nonpreemptive kernel is essentially free from race conditions on kernel data structures, as only on process is active in the kernel at at time. 非抢占内核的内核从根本上不会导致竞争条件，因为在内核中一次只有一个进程是活跃的。</li>
<li>Preemptive kernels must be carefully designed to ensure that shared kernel data are free from race conditions. 对于抢占内核需要认真设计以确保共享内和数据免于竞争条件。</li>
<li>A preemptive kernel may be more responsive, since there is less risk that a kernel-model process will run for an arbitrarily long period before relinquishing the processor to waiting process. 抢占内核的响应更快，因为处于内核模式的进程在释放CPU之前不会运行过久。</li>
<li>A preemptive kernel is more suitable for real-time programming, as it will allow a real-time process to preemptive a process currently running in the kernel. 抢占内核更适合实时编程，因为它能允许实时进程抢占处于内核模式运行的其他进程。</li>
</ul>

<h2 id="toc_2">3 Peterson&#39;s Solution</h2>

<p><strong>Peterson’s solution</strong>(Peterson 算法) is restricted to two processes that alternate execution between their critical sections and remainder sections. The processes are numbered \(P_0\) and \(P_1\). For convenience, when presenting \(P_i\), we use \(P_j\) to denote the other process; that is \(j\) equals \(1-i\).</p>

<p>Peterson&#39;s solution requires the two processes to share two data items:</p>

<pre><code class="language-c">int turn;
boolean flag[2];
</code></pre>

<p>The structure of process \(P_i\) in Peterson&#39;s solution.</p>

<pre><code class="language-c">while (true) {
    flag[i] = true; 
    turn = j; 
    while (flag[j] &amp;&amp; turn == j) 
        ;
    /* critical section */
    flag[i] = false;
    /*remainder section */
}
</code></pre>

<ul>
<li>The variable <code>turn</code> indicates whose turn it is to enter its critical section.</li>
<li>The <code>flag</code> array is used to indicate if a process is ready to enter its critical section.</li>
</ul>

<p><strong>Note</strong>:  Peterson’s solution is <strong>not guaranteed</strong> to work on modern computer architectures for the primary reason that, to improve system performance, <strong>processors and/or compilers may reorder read and write operations that have no dependencies</strong>.</p>

<p>If the assignments of the first two statements that appear in the entry section of Peterson&#39;s solution are reordered. It is possible that both threads may be active in their critical sections at the same time.<br/>
<img src="media/15326525991493/the%20effects%20of%20instruction%20reordering%20in%20Peterson&#x27;s%20solution.png" alt="the effects of instruction reordering in Peterson&#39;s solution"/></p>

<h2 id="toc_3">4 Hardware support for Synchronization</h2>

<p>Hardware support for the critical-section problem includes, </p>

<ul>
<li>memory barriers</li>
<li>hardware instructions</li>
<li>atomic variables</li>
</ul>

<h3 id="toc_4">4.1 Memory barriers</h3>

<p>How a computer architecture determines what memory guarantees it will provide to an application program is known as its <strong>memory model</strong>(内存模型). In general, a memory model falls into one of two categories:</p>

<ol>
<li><strong>Strongly ordered</strong>, where a memory modification on one processor is immediately visible to all other processors.</li>
<li><strong>Weakly ordered</strong>, where modifications to memory on one processor may not be immediately visible to other processors.</li>
</ol>

<p>Computer architectures provide instructions that can <em>force</em> any changes in memory to be propagated to all other processors, thereby ensuring that memory modifications are visible to threads running on other processors. Such instruction are known as <strong>memory barriers</strong>(内存屏障).</p>

<ul>
<li>When a memory barrier instruction is performed, the system ensures that all loads and stores are completed before any subsequent load or store operations are performed.</li>
</ul>

<h3 id="toc_5">4.2 Hardware instructions</h3>

<p>Many modern computer systems provide special hardware instructions that allow either to test and modify the content of a word or to swap the contents of two words atomically - that is, one uninterruptible unit.</p>

<p>The definition of the atomic <code>test_and_set()</code> instruction:</p>

<pre><code class="language-c">boolean test and set(boolean *target) { 
    boolean rv = *target; 
    *target = true;
    return rv;
}
</code></pre>

<p>Mutual-exclusion implementation with <code>test_and_set()</code>:</p>

<pre><code class="language-text">do {
    while (test and set(&amp;lock)) 
        ; /* do nothing */
    /* critical section */
    lock = false;
    /* remainder section */ } 
while (true);
</code></pre>

<p>The definition of the atomic <code>compare_and_swap()</code>（CAS）instruction:</p>

<pre><code class="language-c">int compare and swap(int *value, int expected, int new value) { 
    int temp = *value;
    if (*value == expected) 
        *value = new value;
    return temp;
}
</code></pre>

<p>Mutual exclusion with the <code>compare_and_swap()</code> instruction:</p>

<pre><code class="language-c">while (true) {
    while (compare and swap(&amp;lock, 0, 1) != 0) 
        ; /* do nothing */
    /* critical section */
    lock = 0;
    /* remainder section */
}
</code></pre>

<h3 id="toc_6">4.3 Atomic variables</h3>

<p><strong>Atomic variables</strong> (原子变量) provides atomic operations on basic data types such as integers and booleans. Their use is often limited to single updates of shared data such as counters and sequence generators.</p>

<h2 id="toc_7">5 Mutex locks</h2>

<p>ISSUE: The hardware-based solutions are complicated as well as generally inaccessible to application programmers.</p>

<p>SOLUTION: Operating-system designers build higher-level software tools. The simplest of these tools is the <strong>mutex lock</strong>(互斥锁)。</p>

<ul>
<li>A process must <strong>acquire</strong> the lock before entering a critical section; </li>
<li>A process <strong>releases</strong> the lock when it exists the critical section.</li>
<li>A mutex lock has a boolean variable <strong>available</strong>, whose value indicates if the lock is available or not.</li>
<li>Calls to either <code>acquire()</code> or <code>release()</code> must be performed atomically. Thus mutex locks can be implemented using the CAS operation.</li>
</ul>

<p>Solution to the critical-section problem using mutex locks:<br/>
<img src="media/15326525991493/mutex%20lock.png" alt="mutex lock"/></p>

<p>The definition of <code>acquire()</code> is as follows:</p>

<pre><code class="language-c">acquire() { 
    while (!available) ;
        /* busy wait */ 
    available = false; 
}
</code></pre>

<p>The definition of <code>release()</code> is as follows:</p>

<pre><code class="language-c">release(){
    available = true;
}
</code></pre>

<p>The main disadvantage of the implementation is that it requires <strong>busy waiting</strong>.</p>

<ul>
<li>while  a process is in its critical section, any other process that tries enter its critical section must loop continuously in the call to <code>acquire()</code>.</li>
<li>it wastes CPU cycles.</li>
</ul>

<p>Because the process &quot;spins&quot; while waiting for the lock to become available, this type of mutex lock is also called a <code>spinlock</code>（自旋锁）。</p>

<ul>
<li>advantage: no context switch is required</li>
</ul>

<p>Spinlocks are not appropriate for single-processor systems yet are often used in multiprocessor systems.</p>

<p>在UNIX中，自旋锁相关的API：</p>

<pre><code class="language-c">// 初始化自旋锁： 用来申请使用自旋锁所需要的资源并且将它初始化为非锁定状态
int pthread_spin_init(pthread_spinlock_t *, int);
// 获得一个自旋锁：如果该自旋锁当前没有被其它线程所持有，则调用该函数的线程获得该自旋锁.
// 否则该函数在获得自旋锁之前不会返回。
int pthread_spin_lock(pthread_spinlock_t *);
//释放指定的自旋锁
int pthread_spin_unlock(pthread_spinlock_t *);
// 销毁一个自旋锁
int pthread_spin_destroy(pthread_spinlock_t *);
</code></pre>

<h2 id="toc_8">6 Semaphores</h2>

<p>A <strong>semaphore</strong>(信号量) S is an integer variable that, apart from initialization, is accessed only through two standard atomic operations: <code>wait()</code> and <code>signal()</code>. 信号量S是个整数变量，除了初始化外，它只能通过两个标准原子操作：<code>wait()</code>和<code>signal()</code>来访问。</p>

<p>The definition of <code>wait()</code> is as follows:</p>

<pre><code class="language-c">wait(S){
    while (S &lt;= 0)
        ;// busy wait
    S--;
{
</code></pre>

<p>The definition of <code>signal()</code> is as follows:</p>

<pre><code class="language-c">signal(S){
    S++;
}
</code></pre>

<p>All modifications to the integer value of the semaphore in the <code>wait()</code> and <code>signal()</code> operations must be executed atomically.  在<code>wait()</code>和<code>signal()</code>操作中，对信号量整型值的修改必须不可分地执行。</p>

<p>Operating systems often distinguish between counting and binary semaphores.通常操作系统区分计数信号量和二进制信号量。</p>

<ul>
<li>The value of a <strong>counting semaphore</strong>(计数信号量) can range over an unrestricted domain.计数信号量的值域不受限制。</li>
<li>The value of a <strong>binary semaphore</strong>(二进制信号量) can range only between 0 and 1. 二进制信号量的值只能为0或1。</li>
</ul>

<p>Counting semaphores can be used to control access to  a given resource consisting of a finite number of instances.</p>

<ul>
<li>The semaphore is initialized to the number of resources available. </li>
<li>Each process that wishes to use a resource performs a <code>wait()</code>operation on the semaphore (thereby decrementing the count). </li>
<li>When a process releases a resource, it performs a <code>signal()</code> operation (incrementing the count). </li>
<li>When the count for the semaphore goes to 0, all resources are being used. After that, processes that wish to use a resource will block until the count becomes greater than 0.</li>
</ul>

<h2 id="toc_9">7 Monitors</h2>

<p>Issues: various types of errors can be generated easily when programmers use semaphores or mutex locks incorrectly to solve the critical-section problem.</p>

<ul>
<li>interchanges the order of <code>wait()</code> and <code>signal()</code></li>
<li>replaces <code>signal()</code> with <code>wait()</code></li>
<li>omits <code>wait()</code> or <code>signal()</code></li>
</ul>

<p>Solution: An abstract data type, <strong>monitor</strong>(管程), includes a set of programmer-defined operation related to mutual exclusion within the monitor. A monitor uses <strong>condition variables</strong> that allow processes to wait for certain conditions to become true and to signal one another when conditions have been set to true.</p>

<p>Pseudocode syntax of a monitor:</p>

<pre><code class="language-c">monitor monitor name { /* shared variable declarations */
    function P1 ( . . . ) { . . .}
    function P2 ( . . . ) { . . .}
        .
        .
    function Pn ( . . . ) { . . .}
    initialization code ( . . . ) { . . .}
}
</code></pre>

<p><img src="media/15326525991493/monitor%20with%20condition%20variables.png" alt="monitor with condition variables"/></p>

<h2 id="toc_10">8 Liveness</h2>

<h3 id="toc_11">8.1 Deadlock</h3>

<p><strong>deadlocked</strong>(死锁): two or more processes are waiting indefinitely for an event.</p>

<p>A set of processes is in a deadlocked state when every process in the set is waiting for an event that can be caused only by another process in the set.</p>

<h3 id="toc_12">8.2 Priority Inversion</h3>

<p>A scheduling challenge arises when a higher-priority process needs to read or modify kernel data that are currently being accessed by a lower-priority process—or a chain of lower-priority processes. </p>

<ul>
<li>Since kernel data are typically protected with a lock, the higher-priority process will have to wait for a lower-priority one to finish with the resource. </li>
<li>The situation becomes more complicated if the lower-priority process is preempted in favor of another process with a higher priority.</li>
</ul>

<p>As an example, assume we have three processes—\(L\), \(M\), and \(H\)—whose priorities follow the order \(L &lt; M &lt; H\). </p>

<ul>
<li>Assume that process \(H\) requires a semaphore \(S\), which is currently being accessed by process \(L\). </li>
<li>Ordinarily, process \(H\) would wait for \(L\) to finish using resource S. </li>
<li>However, now suppose that process \(M\) becomes runnable, thereby preempting process \(L\). </li>
<li>Indirectly, a process with a lower priority—process \(M\)—has affected how long process \(H\) must wait for \(L\) to relinquish resource \(S\).</li>
</ul>

<p>This liveness problem is known as <strong>priority inversion</strong>（优先级反转）, and it can occur only in systems with more than two priorities. </p>

<p>Solution：  priority-inheritance protocol(优先级继承协议)：</p>

<ul>
<li>All processes that are accessing resources needed by a higher-priority process inherit the higher priority until they are finished with the resources. </li>
<li>When they are finished, priorities revert to original values.</li>
</ul>

<h2 id="toc_13">9 Evaluation</h2>

<p>Performance differences between CAS-based synchronization and traditional synchronization (such as mutex locks and semaphores) under varying contention loads:</p>

<ul>
<li><strong>Uncontended</strong>： Although both options are generally fast, CAS protection will be somewhat faster than traditional synchronization.</li>
<li><strong>Moderate contention</strong>： CAS protection will be faster—possibly much faster —than traditional synchronization.</li>
<li><strong>High contention</strong>： Under very highly contended loads, traditional synchronization will ultimately be faster than CAS-based synchronization.</li>
</ul>

<p>Higher-level tools such as monitors and condition variables may have significant overhead, and may be less likely to scale in highly contended situations.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/3/27</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Computer%20System.html'>Computer System</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="os_concepts_synchronization_examples.html">
                
                  <h1>Operating System Concepts 7 - Synchronization Examples</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	
                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/3/29</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Computer%20System.html'>Computer System</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="os-concets-processes.html">
                
                  <h1>Operating System Concepts 3 - Processes</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">1 Process concept</a>
<ul>
<li>
<a href="#toc_1">1.1 The process</a>
</li>
<li>
<a href="#toc_2">1.2 Process state</a>
</li>
<li>
<a href="#toc_3">1.3 Process control block</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">2 Process scheduling 进程调度</a>
<ul>
<li>
<a href="#toc_5">2.1 Scheduling Queues</a>
</li>
<li>
<a href="#toc_6">2.2 context switch</a>
</li>
</ul>
</li>
<li>
<a href="#toc_7">3 Operating on Processes</a>
<ul>
<li>
<a href="#toc_8">3.1 Process creation</a>
</li>
<li>
<a href="#toc_9">3.2 Process termination</a>
</li>
</ul>
</li>
<li>
<a href="#toc_10">4 Interprocess communication</a>
</li>
<li>
<a href="#toc_11">5 IPC in shared-memory system</a>
</li>
<li>
<a href="#toc_12">6 IPC in message-passing system</a>
<ul>
<li>
<a href="#toc_13">6.1 Direct/Indirect communication</a>
<ul>
<li>
<a href="#toc_14">(1) Direct Communication</a>
</li>
<li>
<a href="#toc_15">(2) Indirect Communication</a>
</li>
</ul>
</li>
<li>
<a href="#toc_16">6.2 Synchronization</a>
</li>
<li>
<a href="#toc_17">6.3 Buffering</a>
</li>
</ul>
</li>
<li>
<a href="#toc_18">7 Examples of IPC</a>
<ul>
<li>
<a href="#toc_19">7.1 Mach Message Passing</a>
</li>
<li>
<a href="#toc_20">7.2 Pipes</a>
<ul>
<li>
<a href="#toc_21">(1) Ordinary pipes</a>
</li>
<li>
<a href="#toc_22">(2) Named pipes</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_23">8 Communication in Client-server system</a>
<ul>
<li>
<a href="#toc_24">8.1 Sockets</a>
</li>
<li>
<a href="#toc_25">8.2 Remote procedure calls</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">1 Process concept</h2>

<h3 id="toc_1">1.1 The process</h3>

<p><strong>Process</strong> (进程) is a program in execution.</p>

<ul>
<li>Process is the unit of work in a modern computing system</li>
</ul>

<p>The status of the <strong>current</strong> activity of a process is represented by the value of the <strong>program counter</strong> and the contents of the processor&#39;s <strong>registers</strong>.</p>

<p>A program by itself is not a process.</p>

<ul>
<li>A program is a <strong>passive</strong> entity, such as a file containing a list of instructions stored on disk</li>
<li>A process is an <strong>active</strong> entity, with a program counter specifying the next instruction to execute</li>
</ul>

<h3 id="toc_2">1.2 Process state</h3>

<p>A process may be in one of the following states:</p>

<ul>
<li><strong>New</strong>(新建). The process is being created. 进程正在被创建</li>
<li><strong>Running</strong>(运行). Instructions are being executed.指令正在被执行</li>
<li><strong>Waiting</strong>(等待). The process is waiting for some event to occur(such as an I/O completion or reception of a signal). 进程等待某些事件发生</li>
<li><strong>Ready</strong>(就绪). The process is waiting to be assigned to a processor.进程等待分配处理器</li>
<li><strong>Terminated</strong>(终止). The process has finished execution.进程执行完毕</li>
</ul>

<p>Diagram of process state:<br/>
<img src="media/15317585001692/diagramofprocessstate.png" alt="Diagram of process state"/></p>

<h3 id="toc_3">1.3 Process control block</h3>

<p>Each process is represented by a <strong>process control block</strong>(PCB, 进程控制块), it contains</p>

<ul>
<li><strong>Process state</strong>(进程状态)</li>
<li><strong>Program counter</strong>(程序计数器)</li>
<li><strong>CPU registers</strong>(CPU寄存器)</li>
<li><strong>CPU-scheduling information</strong>(CPU调度信息): a process priority, pointers to scheduling queues, and any other scheduling parameters.</li>
<li><strong>Memory-management information</strong>(内存管理信息)</li>
<li><strong>Accounting information</strong>(记账信息): the amount of CPU and real time used, time limits, account numbers, process numbers and so on.</li>
<li><strong>I/O status information</strong>(I/O状态信息): the list of I/O devices allocated to the process, a list of open files</li>
</ul>

<p>Process Control Block:<br/>
<img src="media/15317585001692/processcontrolblock.png" alt="process control block"/></p>

<p>The process control block in Linux is represented by the C structure <code>task_struct</code> (&#39;include/linux/sched.h&#39;)， <a href="https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h#L592">CODE LINK</a></p>

<ul>
<li>Within the Linux kernel, all active processes are represented using a <strong>doubly linked list</strong> of task struct.</li>
</ul>

<p>Task_strut:<br/>
<img src="media/15317585001692/task_strcut%20in%20Linux.png" alt="task_strcut in Linux"/></p>

<h2 id="toc_4">2 Process scheduling 进程调度</h2>

<p>The <strong>process scheduler</strong>(进程调度程序) selects an available process for program execution on a core.</p>

<ul>
<li>Each CPU core can run one process at a time.</li>
<li>The number of processes currently in memory is known as the <strong>degree of multiprogramming</strong>.</li>
</ul>

<h3 id="toc_5">2.1 Scheduling Queues</h3>

<p><strong>Ready queue</strong>(就绪队列): the status of processes are ready.</p>

<ul>
<li>generally stored as a linked list, its header contains pointers to the first PCB in the list, each PCB includes a pointer field that points to next PCB in the ready queue.</li>
</ul>

<p><strong>Wait Queue</strong>(等待队列): the status of processes are waiting.</p>

<p>Queueing-diagram representation of process scheduling: <br/>
<img src="media/15317585001692/Queueing-diagram%20representation%20of%20process%20scheduling.png" alt="Queueing-diagram representation of process scheduling"/></p>

<h3 id="toc_6">2.2 context switch</h3>

<p>Here the <strong><em>context</em></strong> of a process is represented in the PCB of the process, including the value of the CPU registers, the process state, and memory-management information.</p>

<p>An operating system performs a <strong>context switch</strong>（上下文切换) when it switches from running one process to running another.</p>

<ul>
<li>The kernel <strong>saves</strong> the context of the old process into its PCB and <strong>restore</strong> the saved context of the new process scheduled to run.</li>
<li>Context-switch time is overhead; the system does no useful work while switching. 
<ul>
<li>A typical speed is a several microseconds. </li>
</ul></li>
<li>Context-switch times are <strong>highly</strong> dependent on hardware support.</li>
</ul>

<p>Context switch from an old process to a new process:<br/>
<img src="media/15317585001692/context%20switch%20from%20process%20to%20process.png" alt="context switch from process to process"/></p>

<h2 id="toc_7">3 Operating on Processes</h2>

<h3 id="toc_8">3.1 Process creation</h3>

<p>A process may <strong>create</strong> several new processes.</p>

<ul>
<li>the creating process is called a <strong>parent process</strong>.</li>
<li>the new process is called a <strong>child process</strong> .</li>
</ul>

<p><img src="media/15317585001692/process%20creating%20using%20the%20fork--%20system%20call.png" alt="process creating using the fork-- system cal"/></p>

<h3 id="toc_9">3.2 Process termination</h3>

<p>A process <strong>terminates</strong> when it finishes executing its final statement and asks the operating system to delete it by using the <code>exit()</code> system call.</p>

<ul>
<li><strong>cascading termination</strong>(级联终止):  if a process terminates (either normally or abnormally), then all its children must also be terminated. </li>
<li>A process that has terminated, but whose parent has not yet called <code>wait()</code>, is known as a <strong>zombie process</strong>(僵尸进程).</li>
<li>if a parent did not invoke <code>wait()</code> and instead terminated, then leaving its child processes as <strong>orphan processes</strong>(孤儿进程).
<ul>
<li>Unix system may assign the <code>init</code> process as the new parent to orphan processes, and the <code>init</code> process periodically invokes <code>wait()</code>.</li>
</ul></li>
</ul>

<h2 id="toc_10">4 Interprocess communication</h2>

<p>Processes may be either <strong>independent processes</strong>(独立进程) or <strong>cooperating processes</strong>(协同进程).</p>

<ul>
<li>A process is <strong><em>independent</em></strong> if it does not share data with any other processes executing in the system.</li>
<li>A process is <strong><em>cooperating</em></strong> if it can affect or be affected by the other processes executing in the system.</li>
</ul>

<p>Advantages of  process cooperation:</p>

<ul>
<li>Information sharing 信息共享</li>
<li>Computation speedup 加速运算</li>
<li>Modularity 模块化</li>
</ul>

<p>Cooperating process require an <strong>interprocess communication</strong> (IPC，进程间通信) mechanism that will allow them to <strong>exchange</strong> data. There are two fundamental models of IPC:</p>

<ul>
<li><strong>shared memory</strong>（共享内存）: a region of memory is shared by cooperating process. Process can exchange information by reading and writing data to the shared region.
<ul>
<li>Shared memory can be <strong>faster</strong> than message passing.</li>
</ul></li>
<li><strong>message passing</strong>(消息传递)： communication takes place by means of messages exchanged between the cooperating processes.
<ul>
<li>Message passing is useful for exchanging <strong>smaller</strong> amounts of data, because no conflicts need be avoided.</li>
<li>Message passing is easier to implement in a distributed system than shared memory.</li>
</ul></li>
</ul>

<p><img src="media/15317585001692/sharedmemory%20and%20message%20passing.png" alt="sharedmemory and message passing"/></p>

<h2 id="toc_11">5 IPC in shared-memory system</h2>

<p>Here, we explore the POSIX API for shared memory. POSIX shared memory is organized using <strong>memory-mapped files</strong> (内存映射文件), which associate the region of shared memory with a file. A process must first create a shared-memory object using the <code>shm_open()</code> system call, as follows:</p>

<pre><code class="language-c">fd = shm_open(name, O_CREAT | O_RDWR, 0666);
ftruncate(fd, 4096);
mmap(0, SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
</code></pre>

<ul>
<li>A successful call to <code>shm_open()</code> returns an integer file descriptor for the shared-memory object.</li>
<li>Once the object is established, the <code>ftruncate</code> function is used to configure the size of the object in bytes.</li>
<li>Finally, the <code>mmap()</code> function establishes a memory-mapped file containing the shared-memory object. It returns a pointer to the shared</li>
</ul>

<h2 id="toc_12">6 IPC in message-passing system</h2>

<p>A message-passing facility provides at least two operations:</p>

<ul>
<li>send(message)</li>
<li>receive(message)</li>
</ul>

<p>If P and Q wish to communicate, they need to</p>

<ul>
<li>establish a <strong>communication link</strong>(通信连接) between them</li>
<li>exchange messages via send/receive </li>
</ul>

<p>Here are several methods for logically implementing a <em>communication link</em> between processes:</p>

<ul>
<li>Direct or indirect communication 直接/间接通信</li>
<li>Synchronous or asynchronous communication 同步/异步同步</li>
<li>Automatic or explicit buffering 自动/显式缓冲</li>
</ul>

<h3 id="toc_13">6.1 Direct/Indirect communication</h3>

<h4 id="toc_14">(1) Direct Communication</h4>

<p>Under <strong>direct communication</strong>, each process that wants to communicate must explicitly name the recipient or sender of the communication.</p>

<ul>
<li>send(P, message) - send a message to process P.</li>
<li>receive(Q, message) - receive a message from process Q</li>
</ul>

<p>A communication link in this scheme has the following properties:</p>

<ul>
<li>A link is established <strong>automatically</strong> between every pair of processes that want to communicate.</li>
<li>A link is associated with <strong>exactly two</strong> processes.</li>
<li>Between each pair of processes, there exists exactly one link.</li>
</ul>

<p>Cons:</p>

<ul>
<li>limited modularity of the resulting process definitions. Changing the identifier of a process may necessinate examining all other process definitions.</li>
<li>any such hard-coding techniques, are less desirable.</li>
</ul>

<h4 id="toc_15">(2) Indirect Communication</h4>

<p>With <strong>indirect communication</strong>, the message are sent to and receive from <strong>mailboxes</strong>, or <strong>ports</strong>.</p>

<ul>
<li>send(A, message) - send a message to mailbox A</li>
<li>receive(A, message) - receive a message from mailbox A</li>
</ul>

<p>A mailbox can be viewed abstractly as an object into which messages can be placed by processes and from which messages can be removed.</p>

<ul>
<li>Each mailbox has an <strong>unique</strong> identification.</li>
<li>Two processes can communicate only if they have a shared mailbox.</li>
</ul>

<p>In this scheme, a communication link has the following properties:</p>

<ul>
<li>A link is established between a pair of processes only if both members of the pair have a shared mailbox.</li>
<li>A link may be associated with more than two processes.</li>
<li>Between each pair of communicating processes, a number of different links may exist, with each link corresponding to one mailbox.</li>
</ul>

<p>A mailbox may be owned either by a process or by the operating system.</p>

<p>If the mailbox is owned by a process</p>

<ul>
<li>We distinguish between the <strong>owner</strong> (which can only receive messages through his mailbox) and the <strong>user</strong> (which can only send messages to the mailbox)</li>
<li>Each mailbox has a unique owner.</li>
<li>When a process that owns a mailbox terminates, the mailbox disappears.</li>
<li>The process that creates a new mailbox is that mailbox&#39;s owner by default.</li>
</ul>

<h3 id="toc_16">6.2 Synchronization</h3>

<p>Message passing may be either <strong>blocking</strong> or <strong>nonblocking</strong> - also known as <strong>synchronous</strong> and <strong>asynchronous</strong>.</p>

<h3 id="toc_17">6.3 Buffering</h3>

<p>Messages exchanged by communicating processes reside in a temporary queue, whether communication is direct or indirect. Basically, it can be implemented in three ways:</p>

<ul>
<li>Zero capacity（零容量）-- no buffering
<ul>
<li>The link cannot have any messages waiting in it.</li>
<li>The sender must block until the recipient receives the message. </li>
</ul></li>
<li>Bounded capacity（有界容量）-- automatic buffering
<ul>
<li>The queue has finite length n, at most n message can reside in it.<br/></li>
<li>The sender must block until space is available in the queue if the link is full.<br/></li>
</ul></li>
<li>Unbounded capacity （无界容量） -- automatic buffering
<ul>
<li>Any number of messages can wait in it.</li>
<li>The sender never blocks. </li>
</ul></li>
</ul>

<h2 id="toc_18">7 Examples of IPC</h2>

<h3 id="toc_19">7.1 Mach Message Passing</h3>

<p>Mach was especially designed for distributed systems. Its kernel supports the creation and destruction of multiple <strong>tasks</strong>, which are similar to processes but have multiple threads of control and fewer associated resources.  </p>

<p>Messages are sent to, and received from, mailboxes, which are called <strong>ports</strong> in Mach. </p>

<ul>
<li>Ports are <strong>finite in size</strong> and <strong>unidirectional</strong>.</li>
<li>For two-way communication, a message is sent to one port, and a response is sent to a separate <strong>reply</strong> port.</li>
<li>Associated with each port is a collection of <strong>port rights</strong>, which  identify the capabilities necessary for a task to interact with the port.</li>
</ul>

<p>Functions:</p>

<ul>
<li><code>mach_port_allocate()</code> creates a new port and allocates space for its queue of messages.</li>
<li><code>mach_msg()</code> is the standard API for both sending and receiving messages.</li>
</ul>

<pre><code class="language-c">#include &lt;mach/mach.h&gt;

struct message {
    mach_msg_header_t header;
    int data;
};

mach_port_t client;
mach_port_t server;

/* Client Code */

struct message message;

// construct the header
message.header.msgh_size = sizeof(message);
message.header.msgh_remote_port = server;
message.header.msgh_local_port = client;

// send the message
mach msg(&amp;message.header, // message header
         MACH_SEND_MSG, // sending a message
         sizeof(message), // size of message sent
         0, // maximum size of received message - unnecessary
         MACH_PORT_NULL, // name of receive port - unnecessary
         MACH_MSG_TIMEOUT_NONE, // no time outs MACH PORT NULL // no notify port
);

/* Server Code */

struct message message;

// receive the message
mach_msg(&amp;message.header, // message header
  MACH_RCV_MSG, // sending a message  0, // size of message sent
  sizeof(message), // maximum size of received message
  server, // name of receive port
  MACH_MSG_TIMEOUT_NONE, // no time outs
  MACH_PORT_NULL // no notify port
);
</code></pre>

<h3 id="toc_20">7.2 Pipes</h3>

<p>A <strong>pipe</strong> acts as a conduit allowing two processes to communicate. Pipes were one of the first IPC mechanisms in early UNIX systems. There are two common types of pipes used on both UNIX and Windows systems: <strong>ordinary pipes</strong> and <strong>named pipes</strong>.</p>

<h4 id="toc_21">(1) Ordinary pipes</h4>

<p><strong>Ordinary pipes</strong> allow two processes to communicate in standard producer-consumer fashion: the producer writes to one end of the pipe (the <strong>write end</strong>) and the consumer reads from the other end (the <strong>read end</strong>).</p>

<ul>
<li>Ordinary pipes are <strong>unidirectional</strong>, allowing only one-way communication.</li>
<li>Function <code>pipe(int fd[])</code> constructs an ordinary pipe, where <code>fd</code> is a file descriptor.</li>
<li>UNIX treats a pipe as <em>a special type of file</em>. Pipes can be accessed using ordinary <code>read()</code> and <code>write()</code> system calls.</li>
<li>Ordinary pipes <strong>exit only</strong> while the processes are communicating with each other.</li>
</ul>

<p><img src="media/15317585001692/file%20descriptors%20for%20an%20ordinary%20pipes.png" alt="file descriptors for an ordinary pipes"/></p>

<pre><code class="language-c">#include &lt;sys/types.h&gt;
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;unistd.h&gt;

#define BUFFER_SIZE 25
#define READ_END 0
#define WRITE_END 1

int main(void)
{
        char write_msg[BUFFER_SIZE] = &quot;Greetings&quot;;
        char read_msg[BUFFER_SIZE];
        int fd[2];
        pid_t pid;

        /* create the pipe */
        if (pipe(fd) == -1){
                fprintf(stderr, &quot;Pipe failed&quot;);
                return 1;
        }

        /* fork a child process */
        pid = fork();

        if (pid&gt;0){ /* parent process */
                close(fd[READ_END]);/* close the unused end of the pipe */
                write(fd[WRITE_END], write_msg, strlen(write_msg)+1); /* write to the pipe */
                close(fd[WRITE_END]);  /* close the write end of the pipe */
        }
        else if (pid==0){ /* child process */
                close(fd[WRITE_END]); /* close the unused end of the pipe */
                read(fd[READ_END], read_msg, BUFFER_SIZE); /* read from the pipe */
                printf(&quot;read: %s\n&quot;, read_msg);
                close(fd[READ_END]); /* close the read end of the pipe */
        }
        return 0;

}
</code></pre>

<h4 id="toc_22">(2) Named pipes</h4>

<p><strong>Named pipes</strong>（命名管道） can be <strong>bidirectional</strong>, and no parent-child relationship is required.</p>

<ul>
<li>Named pipes are referred to as <strong>FIFOs</strong> in UNIX system.</li>
<li>Once created, they appear as typical files in the file system.</li>
<li>The communicating processes for named pipes must reside on the same machine.</li>
</ul>

<p>A FIFO is created with the <code>mkfifo()</code> system call and manipulated with the ordinary <code>open()</code>, <code>read()</code>, <code>write()</code>, and <code>close</code> system calls.：</p>

<pre><code class="language-c">int mkfifo(const char *filename, mode_t mode);
</code></pre>

<p><code>fifowrite.c</code>:</p>

<pre><code class="language-c">#include&lt;sys/types.h&gt;
#include&lt;stdlib.h&gt;
#include&lt;stdio.h&gt;
#include&lt;fcntl.h&gt;
#include&lt;limits.h&gt;
int main()
{
    const char *fifo_name = &quot;/tmp/my_fifo&quot;;
    int pipe_fd = -1;
    int data_fd = -1;
    int res = 0;
    const int open_mode = O_WRONLY;
    char buffer[PIPE_BUF+1];
    if(access(fifo_name,F_OK)==-1)
    {
        res = mkfifo(fifo_name,0777);
        if(res!=0)
        {
            fprintf(stderr,&quot;could not create fifo\n&quot;);
            exit(EXIT_FAILURE);
        }
    }
    printf(&quot;process %d opening fifo O_WRONLY\n&quot;,getpid());
    pipe_fd = open(fifo_name,open_mode);
    data_fd = open(&quot;data.txt&quot;,O_RDONLY);
    printf(&quot;process %d result %d\n&quot;,getpid(),pipe_fd);
    if(pipe_fd!=-1)
    {
        int bytes_read = 0;
        bytes_read = read(data_fd,buffer,PIPE_BUF);
        while(bytes_read&gt;0)
        {
            res = write(pipe_fd,buffer,bytes_read);
            if(res==-1)
            {
                fprintf(stderr,&quot;write error\n&quot;);
                exit(EXIT_FAILURE);
            }
            bytes_read = read(data_fd,buffer,PIPE_BUF);
            buffer[bytes_read]=&#39;\0&#39;;
        }
        close(pipe_fd);
        close(data_fd);
    }
    else{
        exit(EXIT_FAILURE);
    }
    printf(&quot;process %d finished.\n&quot;,getpid());
    exit(EXIT_SUCCESS);
}
</code></pre>

<p><code>fiforead.c</code>:</p>

<pre><code class="language-c">#include&lt;stdlib.h&gt;
#include&lt;stdio.h&gt;
#include&lt;sys/types.h&gt;
#include&lt;fcntl.h&gt;
#include&lt;limits.h&gt;
int main()
{
    const char *fifo_name = &quot;/tmp/my_fifo&quot;;
    int pipe_fd = -1;
    int data_fd = -1;
    int res = 0;
    int open_mode = O_RDONLY;
    char buffer[PIPE_BUF+1];
    int bytes_read = 0;
    int bytes_write = 0;
    memset(buffer,&#39;\0&#39;,sizeof(buffer));

    printf(&quot;process %d opening FIFO O_RDONLY\n&quot;,getpid());
    pipe_fd = open(fifo_name,open_mode);
    data_fd = open(&quot;dataformfifo.txt&quot;,O_WRONLY|O_CREAT,0644);
    printf(&quot;process %d result %d\n&quot;,getpid(),pipe_fd);
    if(pipe_fd!=-1)
    {
        do{
            res = read(pipe_fd,buffer,PIPE_BUF);
            bytes_write = write(data_fd,buffer,res);
            bytes_read +=res;
        }while(res&gt;0);
        close(pipe_fd);
        close(data_fd);
    }
    else{
        exit(EXIT_FAILURE);
    }
    printf(&quot;process %d finished,%d bytes read\n&quot;,getpid(),bytes_read);
    exit(EXIT_SUCCESS);
}
</code></pre>

<h2 id="toc_23">8 Communication in Client-server system</h2>

<p>In this section, we explore two other strategies for communication in client-server system: <strong>sockets</strong> and <strong>remote procedure calls</strong>(RPCs)</p>

<h3 id="toc_24">8.1 Sockets</h3>

<p>A <strong>socket</strong>（套接字）is defined as an endpoint for communication. A socket is identified by an IP address concatenated with a port number.</p>

<p>Communication using sockets：<br/>
<img src="media/15317585001692/communication%20using%20sockets.png" alt="communication using sockets"/></p>

<p>Servers implementing specific services (such as SSH, FTP, and HTTP) listen to well-known ports. Once a request is received, the server accepts a connection from the client socket to complete the connection.</p>

<h3 id="toc_25">8.2 Remote procedure calls</h3>

<p><strong>Remote Procedure Call</strong>（远程过程调用）allows programs on different machines to interact using simple procedure call/return semantics, just as if the two programs were in the same computer。</p>

<p>RPC between a client and a serve：<br/>
<img src="media/15317585001692/RPC%20between%20a%20client%20and%20a%20server.png" alt="RPC between a client and a serve"/></p>

<p>RPC hides all the network code into the stub procedures. This prevents the application programs, the client and the server, from having to worry about details such as sockets, network byte order, and the like.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/2/25</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Computer%20System.html'>Computer System</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="diagrammatize_TCP_IP.html">
                
                  <h1>图解TCP/IP</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>在计算机通信中，事先达成一个详细的约定，并遵循这一约定进行处理，这种约定其实就是<strong>协议</strong>。</p>

<p><strong>计算机网络体系结构</strong>将网络协议进行了系统的归纳。TCP/IP就是IP、TCP、HTTP等协议的集合。除此之外，还有很多其他类型的网络体系结构。</p>

<p><img src="media/15166008990128/15322220890961.jpg" alt=""/></p>

<h2 id="toc_0">OSI模型</h2>

<p><em><u>开放式系统互联通信参考模型</u></em>(Open System Interconnection Reference Model， 简称<strong>OSI模型</strong>)。在这一模型中，每个分层都接收由它下一层所提供的特定服务，并且负责为自己的上一层提供特定的服务。上下层之间进行交互时所遵循的约定叫做<strong>接口</strong>。统一层之间的交互所遵循的约定叫做<strong>协议</strong>。</p>

<ul>
<li>分层可以将每个独立使用，即使系统中某些分层发生变化，也不会波及整个系统</li>
<li>分层能够细分通信功能，更易于单独实现每个分层的协议，并界定各个分层的具体责任和义务。</li>
</ul>

<p><img src="media/15166008990128/15322230855436.jpg" alt=""/></p>

<ul>
<li>应⽤层：为应⽤程序提供服务并规定应⽤程序中通信相关的细节。包括⽂件传输、电⼦邮件、远程登录（虚拟终端）等协议。 </li>
<li>表⽰层：将应⽤处理的信息转换为适合⽹络传输的格式，或将来⾃下⼀层的数据转换为上层能够处理的格式。因此它主要负责数据格式的转换。具体来说，就是将设备固有的数据格式转换为⽹络标准传输格式。不同设备对同⼀⽐特流解释的结果可能会不同。因此，使它们保持⼀致是这 ⼀层的主要作⽤。 </li>
<li>会话层： 负责建⽴和断开通信连接（数据流动的逻辑通路），以及数据的分割等数据传输相关的管理。 </li>
<li>传输层： 起着可靠传输的作⽤。只在通信双⽅节点上进⾏处理，⽽⽆需在路由器上处理。 </li>
<li>⽹络层：将数据传输到⽬标地址。⽬标地址可以是多个⽹络通过路由器连接⽽成的某⼀个地址。因此这⼀层主要负责寻址和路由选择。 </li>
<li>数据链路层：负责物理层⾯上互连的、节点之间的通信传输。例如与1个以太⽹相连的2个节点之间的通信。 将0、1序列划分为具有意义的数据帧传送给对端（数据帧的⽣成与接收）。 </li>
<li>物理层：负责0、1⽐特流（0、1序列）与电压的⾼低、光的闪灭之间的互换。</li>
</ul>

<h2 id="toc_1">TCP/IP协议</h2>

<p><img src="media/15166008990128/15322239071091.jpg" alt=""/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/1/22</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Computer%20System.html'>Computer System</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_1.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">他山之石，可以攻玉</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Leetcode.html"><strong>Leetcode</strong></a>
        
            <a href="programming_language.html"><strong>编程语言</strong></a>
        
            <a href="data_structure_and_algorithm.html"><strong>数据结构和算法</strong></a>
        
            <a href="Python%E7%89%B9%E6%80%A7.html"><strong>Python特性</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>Python科学计算三维可视化</strong></a>
        
            <a href="English.html"><strong>English</strong></a>
        
            <a href="Computer%20System.html"><strong>Computer System</strong></a>
        
            <a href="Deep%20Learning.html"><strong>Deep Learning</strong></a>
        
            <a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html"><strong>Linux 系统编程</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html"><strong>数据库</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="Big%20Data.html"><strong>Big Data</strong></a>
        
            <a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html"><strong>文献阅读</strong></a>
        
            <a href="Tools.html"><strong>Tools</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="os_concepts_CPU_scheduling.html">Operating System Concepts 5 - CPU Scheduling</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os_concepts_synchronization_tools.html">Operating System Concepts 6 - Synchronization Tools</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os_concepts_synchronization_examples.html">Operating System Concepts 7 - Synchronization Examples</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os-concets-processes.html">Operating System Concepts 3 - Processes</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="diagrammatize_TCP_IP.html">图解TCP/IP</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
