<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  Tensorflow - techlarry
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        <li id=""><a target="_self" href="category.html">Category</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">Home</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        
        <li><a target="_self" href="category.html">Category</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="Leetcode.html">Leetcode</a></li>
        
            <li><a href="Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html">PythonÊï∞ÊçÆÁªìÊûÑ‰∏éÁÆóÊ≥ï</a></li>
        
            <li><a href="Python%E7%89%B9%E6%80%A7.html">PythonÁâπÊÄß</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">Êú∫Âô®Â≠¶‰π†</a></li>
        
            <li><a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html">PythonÁßëÂ≠¶ËÆ°ÁÆó‰∏âÁª¥ÂèØËßÜÂåñ</a></li>
        
            <li><a href="English.html">English</a></li>
        
            <li><a href="%20%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA.html"> ÁêÜËß£ËÆ°ÁÆóÊú∫</a></li>
        
            <li><a href="Deep%20Learning.html">Deep Learning</a></li>
        
            <li><a href="Latex.html">Latex</a></li>
        
            <li><a href="%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html">Êìç‰ΩúÁ≥ªÁªü</a></li>
        
            <li><a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html">Linux Á≥ªÁªüÁºñÁ®ã</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html">Êï∞ÊçÆÂ∫ì</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="%E5%B7%A5%E4%BD%9C%E4%B8%8E%E5%AD%A6%E4%B9%A0.html">Â∑•‰Ωú‰∏éÂ≠¶‰π†</a></li>
        
            <li><a href="Data%20Science.html">Data Science</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="15046658203135.html">
                
                  <h1>TensorFlow Q</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h3 id="toc_0">Q1: what&#39;s the difference between <code>tf.placeholder</code> and <code>tf.Variable</code></h3>

<p>In general, we use <code>tf.placeholder</code> to feed actual training examples, while using <code>tf.Variable</code> for parameters such as weights (\(W\)) and biases (\(b\)) for models.</p>

<p>With <code>tf.Variable</code>, we have to provide an initial value when declaring it. And we don&#39;t have to provide an initial value until running time with a <code>feed_dict</code>.</p>

<h3 id="toc_1">Q2: what&#39;s the difference between <code>tf.random_normal</code> and <code>tf.trucated_normal</code></h3>

<p><code>tf.trucated_normal</code> generates values following a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.</p>

<pre><code>import tensorflow as tf
import matplotlib.pyplot as plt
%matplotlib inline
Session = tf.InteractiveSession()
</code></pre>

<pre><code>mean = 0.0
std = 1.0
shape = (10000,)
hist_range = (-5, 5)

A = tf.truncated_normal(shape, mean, std)
B = tf.random_normal(shape, mean, std)
a, b = Session.run([A, B])
</code></pre>

<pre><code>f, (ax1, ax2) = plt.subplots(2,1)
f1 = ax1.hist(a, bins= 100, range=hist_range)
f2 = ax2.hist(b, bins= 100, range=hist_range)
</code></pre>

<p><img src="media/15046658203135/output_2_0.png" alt="output_2_0"/></p>

<h3 id="toc_2">Q3: What&#39;s the difference between <code>tf.Variable</code> and <code>tf.get_variable</code></h3>

<p><code>tf.get_variable</code> will make it way easier to refactor code if you need to share variables at any time.</p>

<pre><code class="language-python">import tensorflow as tf

with tf.variable_scope(&quot;scope1&quot;):
    w1 = tf.get_variable(&quot;w1&quot;, shape=[2,3])
    w2 = tf.Variable(1.0, name=&quot;w2&quot;)
with tf.variable_scope(&quot;scope1&quot;, reuse=True):
    w1_p = tf.get_variable(&quot;w1&quot;, shape=[2,3])
    w2_p = tf.Variable(1.0, name=&quot;w2&quot;)

print(w1 is w1_p, w2 is w2_p)

#Output: True  False
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/6</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Tensorflow.html'>Tensorflow</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15035780907547.html">
                
                  <h1>TensorFlow(5): Vector and Matrix Product in Numpy and TensorFlow</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">Numpy</h2>

<p>Following are common vector and matrix product operations in Numpy, they are quite simple and straightforward:</p>

<ul>
<li><p>Inner Product   \(\quad a^Tb\quad \):  <code>np.inner()</code></p></li>
<li><p>Outer Product  \(\quad ab^T\quad \):  <code>np.outer()</code></p></li>
<li><p>Dot Product  \(\quad a \cdot b = \sum a_ib_i\quad \): <code>np.dot()</code></p></li>
<li><p>Elementwise Product  \(\quad c_i = a_ib_i\quad \): <code>np.multiply()</code></p></li>
</ul>

<p>Note: inner product is defined on vector spaces over a field ùïÇ (finite or infinite dimensional). Dot product refers specifically to the product of vectors in \(‚Ñù^n\)</p>

<p>The difference between the following implementations of the dot/inner/outer/elementwise product are demonstrated as follows:</p>

<pre><code class="language-python">W = np.ones((2, 7), dtype=&#39;float32&#39;)
x1 = [9, 2, 5, 0, 0, 7, 5]
x2 = [9, 2, 2, 9, 0, 9, 2]
print(&#39;vector dot product&#39;, np.dot(x1,x2)) # dot product
print(&#39;inner&#39;, np.inner(x1,x2)) # inner product
print(&#39;outter&#39;, np.outer(x1,x2)) # outter product
print(&#39;element-wsie&#39;, np.multiply(x1,x2)) # Element-wise product
print(&#39;matrix dot product&#39;, np.dot(W, x1)) # dot product
</code></pre>

<pre><code>dot for vector 168
inner 168
outter [[81 18 18 81  0 81 18]
 [18  4  4 18  0 18  4]
 [45 10 10 45  0 45 10]
 [ 0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0]
 [63 14 14 63  0 63 14]
 [45 10 10 45  0 45 10]]
element-wsie [81  4 10  0  0 63 10]
dot for matrix [ 28.  28.]
</code></pre>

<h2 id="toc_1">TensorFlow</h2>

<p>Vector inner/outer Product are a bit complex in TensorFlow. </p>

<pre><code class="language-python">import tensorflow as tf
import numpy as np

x = tf.Variable([[1, -2, 3]], tf.float32, name=&#39;x&#39;)
y = tf.Variable([[-1, 2, -3]], tf.float32, name=&#39;y&#39;)

## inner product
inner_product1 = tf.reduce_sum(tf.multiply(x, y))
inner_product2 = tf.matmul(x, y, transpose_a=False, transpose_b= True) # different from inner_product1

## outer product
outer_product2 = tf.matmul(x, y, transpose_a= True)

## matrix dot product
X = tf.constant(np.random.randn(3,3), name=&#39;X&#39;)
W = tf.constant(np.random.randn(3,3), name=&#39;W&#39;)
matrix_product = tf.matmul(W, X)

sess = tf.InteractiveSession()
init_op = tf.global_variables_initializer()

# run
sess.run(init_op)
print(sess.run(inner_product1))
print(sess.run(inner_product2))
print(sess.run(outer_product2))
print(sess.run(matrix_product))
</code></pre>

<pre><code>-14
[[-14]]
[[-1  2 -3]
 [ 2 -4  6]
 [-3  6 -9]]
[[-0.88722509 -0.94128018 -2.1999658 ]
 [-0.67967623  1.33193446 -0.75612585]
 [ 0.31741623  1.3271727  -0.04311113]]
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/8/24</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Tensorflow.html'>Tensorflow</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="tensorflow_operation.html">
                
                  <h1>TensorFlow(4):TensorFlow Operation</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">1 Visualize with TensorBoard</a>
<ul>
<li>
<a href="#toc_1">Explicitly name operation, variable</a>
</li>
</ul>
</li>
<li>
<a href="#toc_2">2 Constant types</a>
<ul>
<li>
<a href="#toc_3">Tensors filled with a specific value</a>
</li>
<li>
<a href="#toc_4">Constants as sequences</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">3 Math Operations</a>
</li>
<li>
<a href="#toc_6">4 TensorFlow data types:</a>
<ul>
<li>
<a href="#toc_7">Python Native Types</a>
</li>
<li>
<a href="#toc_8">TensorFlow Native Types</a>
</li>
<li>
<a href="#toc_9">Numpy Data Types</a>
</li>
<li>
<a href="#toc_10">Constant</a>
</li>
<li>
<a href="#toc_11">Variables</a>
<ul>
<li>
<a href="#toc_12">Each session maintains its own copy of variable</a>
</li>
<li>
<a href="#toc_13">Use a variable to initialize another variables</a>
</li>
<li>
<a href="#toc_14">Session vs InteractiveSession,</a>
</li>
</ul>
</li>
<li>
<a href="#toc_15">Placeholder</a>
</li>
<li>
<a href="#toc_16">Lazy loading</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">1 Visualize with TensorBoard</h2>

<pre><code class="language-python">import tensorflow as tf
a = tf.constant(2, name=&#39;a&#39;)
b = tf.constant(3, name=&#39;b&#39;)
x = tf.add(a, b, name=&#39;add&#39;)
with tf.Session() as sess:
    writer = tf.summary.FileWriter(&#39;./graphs&#39;, sess.graph)
    print(sess.run(x))media/15033836099887
writer.close() # close the writer when you&#39;re done using it.
</code></pre>

<pre><code>5
</code></pre>

<p>Bash command (to view TensorBoard):</p>

<pre><code class="language-bash"> tensorboard --logdir=&#39;./graphs&#39; --port 6006
 # open http://localhost:6006/#graphs in your browser
</code></pre>

<h3 id="toc_1">Explicitly name operation, variable</h3>

<pre><code class="language-python">a = tf.constant(2, name=&#39;a&#39;)
b = tf.constant(3, name=&#39;b&#39;)
x = tf.add(a,b,name=&#39;add&#39;)
with tf.Session() as sess:
    writer = tf.summary.FileWriter(&#39;./graphs&#39;, sess.graph)
    print(sess.run(x))
writer.close() # close the writer when you&#39;re done using it.
</code></pre>

<pre><code>5
</code></pre>

<p>The figure produced by TensorBoard is as follows:</p>

<p><img src="media/15033836099887/explicit_name.png" alt=""/></p>

<p><strong>Note</strong>:  Learn to use TensorBoard well and often. It will help a lot when you build complicated models.</p>

<h2 id="toc_2">2 Constant types</h2>

<h3 id="toc_3">Tensors filled with a specific value</h3>

<p>Using <code>tensorflow.zeros</code> to fill tensor with zeros, which is similar to <code>Numpy</code>:</p>

<pre><code class="language-python">tf.zeros(shape, dtype=tf.float32, name=None)
</code></pre>

<p>For example,</p>

<pre><code class="language-python">x = tf.zeros([2,3], tf.int32)
with tf.Session() as sess:
    print(sess.run(x))
</code></pre>

<pre><code>[[0 0 0]
 [0 0 0]]
</code></pre>

<p><code>tensorflow.zeros_like</code> return an tensor of zeros with the same shape and type as a given tensor. For example, we may want to have a tensor filled with zeros, with the same shape as <code>x</code>:</p>

<pre><code class="language-python">y = tf.zeros_like(x)
with tf.Session() as sess:
    print(sess.run(y))
</code></pre>

<pre><code>[[0 0 0]
 [0 0 0]]
</code></pre>

<p>There are other command to fill tensor with a specific value, such as <code>tensorflow.ones</code>, <code>tensorflow.ones_like</code>, which of usage is similar to <code>tensorflow.zeros</code>, <code>tensorflow.zeros_like</code>.</p>

<p><code>tensorflow.fill</code> creates a tensor filled with a scalar value:</p>

<pre><code class="language-python">tf.fill(dims, value, name=None)
</code></pre>

<pre><code class="language-python">z = tf.fill([3,4],3)
with tf.Session() as sess:
    print(sess.run(z))
</code></pre>

<pre><code>[[3 3 3 3]
 [3 3 3 3]
 [3 3 3 3]]
</code></pre>

<h3 id="toc_4">Constants as sequences</h3>

<p>You can create constants that are sequences, using <code>tf.linspace</code>, <code>tf.range</code>:</p>

<pre><code class="language-python">tf.linspace(start, stop, num, name=None)

# create a sequence of num evenly-spaced values are generated beginning at  start. If num &gt; 1, the values in the sequence increase by stop - start / num - 1, so that the last one is exactly stop.
# start, stop, num must be scalars
# comparable to but slightly different from numpy.linspace
# numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)

tf.range(start, limit=None, delta=1, dtype=None, name=&#39;range&#39;)
# create a sequence of numbers that begins at start and extends by increments of delta up to but not including limit
# slight different from range in Python
</code></pre>

<pre><code class="language-python">x = tf.linspace(10.0, 13.0, 4, name=&#39;linspace&#39;)
y = tf.range(3, 18)
z= tf.range(3, 18, 3)
with tf.Session() as sess:
    print(sess.run(x))
    print(sess.run(y))
    print(sess.run(z))
</code></pre>

<pre><code>[ 10.  11.  12.  13.]
[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
[ 3  6  9 12 15]
</code></pre>

<h2 id="toc_5">3 Math Operations</h2>

<p><img src="media/15033836099887/math_operations.png" alt=""/></p>

<pre><code class="language-python">a = tf.constant([[3,6],[0,0]])
b = tf.constant([[0,0],[2,2]])
x1 = tf.add(a, b)
x2 = tf.add_n([a,b,b]) # &gt;&gt; [7 10]. Equivalent to a + b + b
x3 = tf.multiply(a, b) # &gt;&gt; [6 12] because mul is element wise
x4 = tf.matmul(a, b) # &gt;&gt; ValueError
x5 = tf.matmul(tf.reshape(a, [4, 1]), tf.reshape(b, [1, 4])) # &gt;&gt; [[18]]

with tf.Session() as sess:
    sess.run(a)
    sess.run(b)
    print(&#39;x1:\n&#39;, sess.run(x1))
    print(&#39;x2:\n&#39;, sess.run(x2))
    print(&#39;x3:\n&#39;, sess.run(x3))
    print(&#39;x4:\n&#39;, sess.run(x4))
    print(&#39;x5:\n&#39;, sess.run(x5))
</code></pre>

<pre><code>x1:
 [[3 6]
 [2 2]]
x2:
 [[3 6]
 [4 4]]
x3:
 [[0 0]
 [0 0]]
x4:
 [[12 12]
 [ 0  0]]
x5:
 [[ 0  0  6  6]
 [ 0  0 12 12]
 [ 0  0  0  0]
 [ 0  0  0  0]]
</code></pre>

<h2 id="toc_6">4 TensorFlow data types:</h2>

<h3 id="toc_7">Python Native Types</h3>

<p>TensorFlow takes Python natives types: <code>boolean</code>, <code>numeric</code> (<code>int</code>, <code>float</code>), <code>strings</code></p>

<p>TensorFlow takes in Python native types such as Python boolean values, numeric values (integers, floats), and strings. Single values will be converted to 0-d tensors (or scalars), lists of values will be converted to 1-d tensors (vectors), lists of lists of values will be converted to 2-d tensors (matrices), and so on.</p>

<pre><code class="language-python">tf.InteractiveSession() # open tensorflow interactivesession
t_0 = 19   # Treated as a 0-d tensor, or &quot;scalar&quot; 
print(&#39;t_0:&#39;,t_0)
print(tf.zeros_like(t_0))   # ==&gt; 0
print(tf.ones_like(t_0))   # ==&gt; 1
t_1 = [b&quot;apple&quot; ,  b&quot;peach&quot; ,  b&quot;grape&quot;]   # treated as a 1-d tensor, or &quot;vector&quot; 
print(&#39;t_1:&#39;,t_1)
print(tf.zeros_like(t_1))   # ==&gt; [&#39;&#39; &#39;&#39; &#39;&#39;]
t_2= [[ True, False, False],  [False, False, True], [False, True ,   False ]]   # treated as a 2-d tensor, or &quot;matrix&quot;
print(&#39;t_2:&#39;,t_2)
print(tf.zeros_like(t_2))   # ==&gt; 2x2 tensor, all elements are False 
print(tf.ones_like(t_2))   # ==&gt; 2x2 tensor, all elements are True
</code></pre>

<pre><code>t_0: 19
Tensor(&quot;zeros_like_32:0&quot;, shape=(), dtype=int32)
Tensor(&quot;ones_like_20:0&quot;, shape=(), dtype=int32)
t_1: [b&#39;apple&#39;, b&#39;peach&#39;, b&#39;grape&#39;]
Tensor(&quot;zeros_like_33:0&quot;, shape=(3,), dtype=string)
t_2: [[True, False, False], [False, False, True], [False, True, False]]
Tensor(&quot;zeros_like_34:0&quot;, shape=(3, 3), dtype=bool)
Tensor(&quot;ones_like_21:0&quot;, shape=(3, 3), dtype=bool)
</code></pre>

<p><strong>Note: Do not use Python native types for tensors because TensorFlow has to infer Python type.</strong></p>

<h3 id="toc_8">TensorFlow Native Types</h3>

<p>Like <code>NumPy</code>, <code>TensorFlow</code> also its own data types such as <code>tf.int32</code>, <code>tf.float32</code>. Below is a list of current TensorFlow data types.</p>

<p><img src="media/15033836099887/tensorflow_data_types.png" alt=""/></p>

<h3 id="toc_9">Numpy Data Types</h3>

<p>By now, you‚Äôve probably noticed the similarity between <code>NumPy</code> and <code>TensorFlow</code>. <code>TensorFlow</code> was designed to integrate seamlessly with <code>Numpy</code>, the package that has become the  lingua franca of data science.</p>

<p>TensorFlow‚Äôs data types are based on those of NumPy; in fact, <code>np.int32 == tf.int32</code> returns <code>True</code>. You can pass <code>NumPy</code> types to <code>TensorFlow</code> ops.</p>

<p>Example:</p>

<pre><code class="language-python">import numpy as np
tf.ones([2, 2],  np.float32)
</code></pre>

<pre><code>&lt;tf.Tensor &#39;ones:0&#39; shape=(2, 2) dtype=float32&gt;
</code></pre>

<pre><code class="language-python">x = np.zeros((2,2))
tf.ones_like(x)
</code></pre>

<pre><code>&lt;tf.Tensor &#39;ones_like_22:0&#39; shape=(2, 2) dtype=float64&gt;
</code></pre>

<h3 id="toc_10">Constant</h3>

<p>Constants are stored in the graph definition. This makes loading graphs expensive when constants are big. <strong>Only use constants for primitive types, use variables or readers for more data that requires more memory</strong>.</p>

<pre><code class="language-python">g = tf.Graph() # to add operators to a graph, set it as default:
with g.as_default():
    my_const = tf.constant([1.0, 2.0], name=&quot;my_const&quot;)
    with tf.Session() as sess:
        print(sess.graph.as_graph_def())
</code></pre>

<pre><code>node {
  name: &quot;my_const&quot;
  op: &quot;Const&quot;
  attr {
    key: &quot;dtype&quot;
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: &quot;value&quot;
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: &quot;\000\000\200?\000\000\000@&quot;
      }
    }
  }
}
versions {
  producer: 24
}
</code></pre>

<h3 id="toc_11">Variables</h3>

<p><code>tf.constant</code> is an operation, but <code>tf.Variable</code> is a class. <code>tf.Variables</code> holds several operations:</p>

<pre><code class="language-python">tf.InteractiveSession()
xx = tf.Variable(23, name=&#39;scalar&#39;)
xx.initializer # init op
xx.value() # read op
assign_op = xx.assign(5)

</code></pre>

<p>You have to initialize <code>variables</code>, The easiest way is initializing all variables at once:</p>

<pre><code class="language-python">init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    print(xx.eval())
    sess.run(assign_op)
    print(xx.eval())
</code></pre>

<pre><code>23
5
</code></pre>

<h4 id="toc_12">Each session maintains its own copy of variable</h4>

<pre><code class="language-python">W = tf.Variable(10, name=&#39;W&#39;)
sess1 = tf.Session()
sess2 = tf.Session()
sess1.run(W.initializer)
sess2.run(W.initializer)
print(sess1.run(W.assign_add(10)))
print(sess2.run(W.assign_sub(2))) # not 18!

sess1.close()
sess2.close()
</code></pre>

<pre><code>20
8
</code></pre>

<h4 id="toc_13">Use a variable to initialize another variables</h4>

<pre><code class="language-python"># want to declare U = 2*W
# W is random tensor
W = tf.Variable(tf.truncated_normal([4, 2]))
U = tf.Variable(2*W.initialized_value())
with tf.Session() as sess:
    sess.run(U.initializer)
    print(U.eval())
</code></pre>

<pre><code>[[ 1.11442947 -3.1675539 ]
 [ 3.02267933 -0.81786388]
 [ 2.57613969 -0.98440802]
 [ 0.6298722  -0.38194153]]
</code></pre>

<h4 id="toc_14">Session vs InteractiveSession,</h4>

<p>You sometimes see InteractiveSession instead of Session. The only difference is an InteractiveSession makes itself the default.</p>

<pre><code class="language-python">sess = tf.InteractiveSession()
a = tf.constant(5.0)
b = tf.constant(6.0)
c = a*b
# We can just use `c.eval()` with out specifying the context `sess`
print(c.eval())
sess.close()
</code></pre>

<pre><code>30.0
</code></pre>

<h3 id="toc_15">Placeholder</h3>

<p>A TensorFlow program often has 2 phases:</p>

<ol>
<li>Assemble a graph</li>
<li>Use a session to execute operations in the graph</li>
</ol>

<p>\(\rightarrow\) can assemble the graph without knowing the values needed for computation</p>

<p><strong>Analogy</strong>: Can define the function \(f(x,y) = x*2+y\) without knowing value of \(x\) or \(y\).</p>

<p>So using <code>placeholders</code>, we can later supply their data when they needed to execute the computation.</p>

<pre><code>tf.placeholder(dtype, shape=None, name=None)
</code></pre>

<p><code>shape=None</code> means that tensor of nay shape will be accepted as value for placeholder. Note: <strong><code>shape=None</code> is easy to construct graphs, but nightmarish for debugging</strong>.</p>

<p>To make <code>shape</code>  flexible, <code>None</code> can be used in the <code>shape</code> argument:</p>

<pre><code>    X = tf.placeholder(dtype=tf.float32, shape=[n_x, None], name=&#39;X&#39;)
</code></pre>

<h3 id="toc_16">Lazy loading</h3>

<p><code>Lazy loading</code> means defer creating/initializing an object until it is needed. In the context of TensorFlow, it means you defer creating an op until you need to compute it. </p>

<p>Normal loading:</p>

<pre><code class="language-python">g = tf.Graph()
with g.as_default():
    x = tf.Variable(10, name=&#39;x&#39;)
    y = tf.Variable(20, name=&#39;y&#39;)
    z = tf.add(x,y) # you create the node for add node before executing the graph

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for _ in range(10):
            sess.run(z)

</code></pre>

<p>Lazy loading:</p>

<pre><code class="language-python">g = tf.Graph()
with g.as_default():
    x = tf.Variable(10, name=&#39;x&#39;)
    y = tf.Variable(20, name=&#39;y&#39;)

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        writer = tf.summary.FileWriter(&#39;./my_graph/12&#39;, sess.graph)
        for _ in range(10):
            sess.run(tf.add(x,y)) # someone decides to be clever to save one line of code
        writer.close()
</code></pre>

<p>Note: In Lazy loading, Node <code>ADD</code> added 10 times to the graph definition. Image you want to compute an operations thousands of times, you graph gets bloated slow to load, and expensive to pass around.</p>

<p><strong>Solution</strong>: </p>

<ol>
<li>Separate definition of ops from computing/running ops</li>
<li>Use Python property to ensure function is also loaded once the first time it is called.</li>
</ol>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/8/20</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Tensorflow.html'>Tensorflow</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="getting_started_with_tensorflow.html">
                
                  <h1>TensorFlow(2): Getting started with TensorFlow</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">TensorFlow Core tutorial</a>
<ul>
<li>
<a href="#toc_1">importing TensorFlow</a>
</li>
<li>
<a href="#toc_2">The Computational Graph</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">tf.train API</a>
<ul>
<li>
<a href="#toc_4">Complete program</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">tf.estimator</a>
<ul>
<li>
<a href="#toc_6">Basic usage</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">TensorFlow Core tutorial</h2>

<h3 id="toc_1">importing TensorFlow</h3>

<pre><code class="language-python">import tensorflow as tf
</code></pre>

<h3 id="toc_2">The Computational Graph</h3>

<p>A <code>computational graph</code> is a series of TensorFlow operations arranged into a graph of nodes. Let&#39;s build a simple computational graph. Each node takes zero or more tensors as inputs and produces a tensor as an output. One type of node is a constant. Like all TensorFlow constant, it takes no inputs, and it outputs a value it stores internally. We can create two floating point Tensor <code>node1</code> and <code>node2</code> as follows:</p>

<pre><code class="language-python">node1 = tf.constant(3.0, dtype=tf.float32)
node2 = tf.constant(4.0) # also tf.float32 implicitly
print(node1, node2)
</code></pre>

<pre><code>Tensor(&quot;Const:0&quot;, shape=(), dtype=float32)
Tensor(&quot;Const_1:0&quot;, shape=(), dtype=float32)
</code></pre>

<p>To produce output values for <code>node1</code> and <code>nodes2</code>, evaluation is needed. To actually evaluate the nodes, we must run the computational graph within a <code>session</code>. A session encapsulates the control and state of the Tensorflow runtime.</p>

<p>The following code creates a <code>Session</code> object and then invokes its <code>run</code> method to run enough of the computational graph to evaluate <code>node1</code> and <code>node2</code>. By running the computational graph in a session as follows:</p>

<pre><code class="language-python">sess = tf.Session()
print(sess.run([node1, node2]))
</code></pre>

<pre><code>[3.0, 4.0]
</code></pre>

<p>We can build more complicated computations by combining <code>Tensor</code> nodes with operations(Operations are also nodes). For example, we can add our two constant nodes and produce a new graph as follows:</p>

<pre><code class="language-python">node3 = tf.add(node1, node2)
print(&#39;node3:&#39;, node3)
print(&#39;sess.run(node3):&#39;, sess.run(node3))
</code></pre>

<pre><code>node3: Tensor(&quot;Add:0&quot;, shape=(), dtype=float32)
sess.run(node3): 7.0
</code></pre>

<p>TensorFlow provides a utility called <code>TensorBoard</code> that can display a picture of the computational graph. Here is a screenshot showing how <code>TensorBoard</code> visualizes the graph:<br/>
<img src="media/15033740189206/getting_started_add.png" alt=""/></p>

<p>As it stands, this graph is not especially interesting because it always produces a constant result. A graph can be parameterized to accept external inputs, known as <code>placeholders</code>. A <code>placeholder</code> is a promise to provide a value later.</p>

<pre><code class="language-python">a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)
adder_node = a + b # + provides a shortcut for tf.add(a,b)
</code></pre>

<p>The preceding three lines are a bit like a function or a lambda in which we define two input parameters (a and b) and then an operation on them. We can evaluate this graph with multiple inputs by using the feed_dict argument to the <code>run method</code> to feed concrete values to the placeholder:</p>

<pre><code class="language-python">print(sess.run(adder_node, {a:3, b:4.5}))
print(sess.run(adder_node, {a:[1,3], b:[2,4]}))
</code></pre>

<pre><code>7.5
[ 3.  7.]
</code></pre>

<p>In <code>TensorBoard</code>, the graph looks like this:</p>

<p><img src="media/15033740189206/getting_started_adder.png" alt=""/></p>

<p>We can make the computational graph more complex by adding another operation. For example,</p>

<pre><code class="language-python">add_and_triple = adder_node * 3.
print(sess.run(add_and_triple,{a:3, b:4.5}))
</code></pre>

<pre><code>22.5
</code></pre>

<p>The preceding computational graph would look as follows in <code>TensorBoard</code>:<br/>
<img src="media/15033740189206/getting_started_triple.png" alt=""/></p>

<p>In machine learning we will typically want a model that can take arbitrary inputs, such as the one above. To make the model trainable, we need to be able to modify the graph to get new outputs with the same input. <code>variables</code> allow us to add trainable parameters to a graph. They are constructed with a type and initial value:</p>

<pre><code class="language-python">W = tf.Variable([.3], dtype=tf.float32)
b = tf.Variable([-.3], dtype=tf.float32)
x = tf.placeholder(tf.float32)
linear_model = W*x +b 
</code></pre>

<p>Constants are initialized when you call <code>tf.constant</code>, and their value can never change. By contrast, variables are not initialized when you call <code>tf.Variable</code>. To initialize all the variables in a TensorFlow program, you must explicitly call a special operation as follows:</p>

<pre><code class="language-python">init = tf.global_variables_initializer()
sess.run(init)
</code></pre>

<p>It is important to realize <code>init</code> is a handle to the TensorFlow sub-graph that initializes all the global variables. Until we call <code>sess.run</code>, the variables are uninitialized.</p>

<p>Since <code>x</code> is a placeholder, we can evaluate <code>linear_model</code> for several values of <code>x</code> simultaneously as follows:</p>

<pre><code class="language-python">print(sess.run(linear_model,{x:[1,2,3,4]}))
</code></pre>

<pre><code>[ 0.          0.30000001  0.60000002  0.90000004]
</code></pre>

<p>We&#39;ve create a model, but we don&#39;t know how good it is yet. To evaluate the model on training data, we need a <code>y</code> placeholder to provide the desired values, and we need to write a loss function.</p>

<p>A <code>loss function</code> measures how far apart the current model is from the provided data. We&#39;ll use a standard loss model for linear regression, which sums the squares of the deltas between the current model and the provided data. <code>linear_model -y</code> creates a vector where each element is the corresponding example&#39;s error delta. We call <code>tf.square</code> to square that error. Then, we sum all the squared errors to create a single scalar that abstacts the error of all examples using <code>tf.reduce_sum</code>:</p>

<pre><code class="language-python">y = tf.placeholder(tf.float32)
squared_deltas = tf.square(linear_model - y)
loss = tf.reduce_sum(squared_deltas)
print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))
</code></pre>

<pre><code>23.66
</code></pre>

<p>We could improve this manually by reassigning the values of <code>W</code> and <code>b</code> to the perfect values of -1 and 1. A variable is initialized to the value provided to <code>tf.Variable</code> but can be changed using operations like <code>tf.assign</code>. FOr example, <code>W=-1</code> and <code>b=1</code> are the optimal parameters for our model. We can change <code>W</code> and <code>b</code> accordingly:</p>

<pre><code class="language-python">fixW = tf.assign(W, [-1.])
fixb = tf.assign(b, [1.])
sess.run([fixW, fixb])
print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))
</code></pre>

<pre><code>0.0
</code></pre>

<p>We guessed the &quot;perfect&quot; values of <code>W</code> and <code>b</code>, but the whole point of machine learning is to find the correct model parameters automatically. We will show how to accomplish this in the next section.</p>

<h2 id="toc_3">tf.train API</h2>

<p>A complete discussion of machine learning is out of the scope of this tutorial. However, TensorFlow provides <code>optimizers</code> that slowly change each variable in order to minimize the loss function. The simplest optimizer is <code>gradient descent</code>. It modifies each variable according to the magnitude of the derivative of loss with respect to that variable. In general, computing symbolic derivatives manually is tedious and error-prone. Consequently, <code>TensorFlow</code> can automatically produce derivatives given only a description of the model using the function <code>tf.gradients</code>. For simplicity, <code>optimizers</code> typically do this for you. For example,</p>

<pre><code class="language-python">optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)

sess.run(init) # reset values to incorrect defaults
for i in range(1000):
    sess.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})
print(sess.run([W,b]))
</code></pre>

<pre><code>[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]
</code></pre>

<p>Now we have done actual machine learning! Although doing this simple linear regression doesn&#39;t require much TensorFlow core code, more complicated models and methods to feed data into your model necessitate more code. Thus TensorFlow provides higher level abstractions for common patterns, structures, and functionality. We will learn how to use some of these abstractions in the next section.</p>

<h3 id="toc_4">Complete program</h3>

<pre><code class="language-python">import tensorflow as tf

# Model parameters
W = tf.Variable([.3], dtype=tf.float32)
b = tf.Variable([-.3], dtype=tf.float32)
# Model input and output
x = tf.placeholder(tf.float32)
linear_model = W * x + b
y = tf.placeholder(tf.float32)

# loss
loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares
# optimizer
optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)

# training data
x_train = [1, 2, 3, 4]
y_train = [0, -1, -2, -3]
# training loop
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init) # reset values to wrong
for i in range(1000):
  sess.run(train, {x: x_train, y: y_train})

# evaluate training accuracy
curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})
print(&quot;W: %s b: %s loss: %s&quot;%(curr_W, curr_b, curr_loss))
</code></pre>

<pre><code>W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11
</code></pre>

<p>This more complicated program can still be visualized in TensorBoard:</p>

<p><img src="media/15033740189206/getting_started_final.png" alt=""/></p>

<h2 id="toc_5">tf.estimator</h2>

<p><code>tf.estimator</code> is a high-level TensorFlow library that simplifies the mechanics of machine learning, including the following:</p>

<ul>
<li>running training loops</li>
<li>running evaluation loops</li>
<li>managing data sets</li>
<li><code>tf.estimator</code> defines many common models.</li>
</ul>

<h3 id="toc_6">Basic usage</h3>

<p>Notice how much simpler the linear regression program becomes with <code>tf.estimator</code>:</p>

<pre><code class="language-python"># NumPy is often used to load, manipulate and preprocess data.
import numpy as np

# Declare list of features. We only have one numeric feature. There are many
# other types of columns that are more complicated and useful.
feature_columns = [tf.feature_column.numeric_column(&quot;x&quot;, shape=[1])]

# An estimator is the front end to invoke training (fitting) and evaluation
# (inference). There are many predefined types like linear regression,
# linear classification, and many neural network classifiers and regressors.
# The following code provides an estimator that does linear regression.
estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns)

# TensorFlow provides many helper methods to read and set up data sets.
# Here we use two data sets: one for training and one for evaluation
# We have to tell the function how many batches
# of data (num_epochs) we want and how big each batch should be.
x_train = np.array([1., 2., 3., 4.])
y_train = np.array([0., -1., -2., -3.])
x_eval = np.array([2., 5., 8., 1.])
y_eval = np.array([-1.01, -4.1, -7, 0.])
input_fn = tf.estimator.inputs.numpy_input_fn(
    {&quot;x&quot;: x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)
train_input_fn = tf.estimator.inputs.numpy_input_fn(
    {&quot;x&quot;: x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)
eval_input_fn = tf.estimator.inputs.numpy_input_fn(
    {&quot;x&quot;: x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)

# We can invoke 1000 training steps by invoking the  method and passing the
# training data set.
estimator.train(input_fn=input_fn, steps=1000)

# Here we evaluate how well our model did.
train_metrics = estimator.evaluate(input_fn=train_input_fn)
eval_metrics = estimator.evaluate(input_fn=eval_input_fn)
print(&quot;train metrics: %r&quot;% train_metrics)
print(&quot;eval metrics: %r&quot;% eval_metrics)
</code></pre>

<pre><code>INFO:tensorflow:Using default config.
WARNING:tensorflow:Using temporary folder as model directory: /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a
INFO:tensorflow:Using config: {&#39;_model_dir&#39;: &#39;/var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a&#39;, &#39;_tf_random_seed&#39;: 1, &#39;_save_summary_steps&#39;: 100, &#39;_save_checkpoints_secs&#39;: 600, &#39;_save_checkpoints_steps&#39;: None, &#39;_session_config&#39;: None, &#39;_keep_checkpoint_max&#39;: 5, &#39;_keep_checkpoint_every_n_hours&#39;: 10000, &#39;_log_step_count_steps&#39;: 100}
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Saving checkpoints for 1 into /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a/model.ckpt.
INFO:tensorflow:loss = 19.0, step = 1
INFO:tensorflow:global_step/sec: 592.325
INFO:tensorflow:loss = 0.192443, step = 101 (0.171 sec)
INFO:tensorflow:global_step/sec: 710.314
INFO:tensorflow:loss = 0.0370785, step = 201 (0.141 sec)
INFO:tensorflow:global_step/sec: 669.791
INFO:tensorflow:loss = 0.0173565, step = 301 (0.150 sec)
INFO:tensorflow:global_step/sec: 729.609
INFO:tensorflow:loss = 0.00361388, step = 401 (0.136 sec)
INFO:tensorflow:global_step/sec: 814
INFO:tensorflow:loss = 0.000215951, step = 501 (0.123 sec)
INFO:tensorflow:global_step/sec: 793.172
INFO:tensorflow:loss = 0.0001734, step = 601 (0.127 sec)
INFO:tensorflow:global_step/sec: 776.415
INFO:tensorflow:loss = 3.66416e-05, step = 701 (0.128 sec)
INFO:tensorflow:global_step/sec: 845.781
INFO:tensorflow:loss = 3.03422e-06, step = 801 (0.118 sec)
INFO:tensorflow:global_step/sec: 849.689
INFO:tensorflow:loss = 1.18453e-06, step = 901 (0.118 sec)
INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a/model.ckpt.
INFO:tensorflow:Loss for final step: 3.72255e-07.
INFO:tensorflow:Starting evaluation at 2017-08-21-05:57:38
INFO:tensorflow:Restoring parameters from /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a/model.ckpt-1000
INFO:tensorflow:Finished evaluation at 2017-08-21-05:57:40
INFO:tensorflow:Saving dict for global step 1000: average_loss = 6.05797e-08, global_step = 1000, loss = 2.42319e-07
INFO:tensorflow:Starting evaluation at 2017-08-21-05:57:40
INFO:tensorflow:Restoring parameters from /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a/model.ckpt-1000
INFO:tensorflow:Finished evaluation at 2017-08-21-05:57:42
INFO:tensorflow:Saving dict for global step 1000: average_loss = 0.00254753, global_step = 1000, loss = 0.0101901
train metrics: {&#39;average_loss&#39;: 6.057968e-08, &#39;loss&#39;: 2.4231872e-07, &#39;global_step&#39;: 1000}
eval metrics: {&#39;average_loss&#39;: 0.0025475256, &#39;loss&#39;: 0.010190102, &#39;global_step&#39;: 1000}
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/8/20</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Tensorflow.html'>Tensorflow</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="what_is_a_tensorflow_session.html">
                
                  <h1>TensorFlow(3):What is a tensorflow session?</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>by <a href="http://danijar.com/what-is-a-tensorflow-session/">Danijar Hafner</a>, modified by <a href="http://larryim.cc">larry</a></p>

<p>I‚Äôve seen a lot of confusion over the rules of <code>tf.Graph</code> and <code>tf.Session</code> in TensorFlow. It‚Äôs simple:</p>

<ul>
<li>A <code>graph</code> defines the computation. It doesn‚Äôt compute anything, it doesn‚Äôt hold any values, it just defines the operations that you specified in your code.</li>
<li>A <code>session</code> allows you to execute graphs or part of graphs. It allocates resources (on one or more machines) for that and holds the actual values of intermediate results and variables.</li>
</ul>

<p>Let‚Äôs look at an example.</p>

<h3 id="toc_0">Defining the Graph</h3>

<p>We define a graph with a variable and three operations: <code>x</code> returns the current value of our variable. <code>init</code> assigns the initial value of 42 to that variable. <code>x_assign</code> assigns the new value of 13 to that variable.</p>

<pre><code class="language-python">import tensorflow as tf
graph = tf.Graph()
with graph.as_default():
  x = tf.Variable(42, name=&#39;foo&#39;)
  init = tf.global_variables_initializer()
  x_assign = x.assign(13)
</code></pre>

<p>On a side note: TensorFlow creates a default graph for you, so we don‚Äôt need the first two lines of the code above. The default graph is also what the sessions in the next section use when not manually specifying a graph.</p>

<h3 id="toc_1">Running Computations in a Session</h3>

<p>To run any of the three defined operations, we need to create a session for that graph. The session will also allocate memory to store the current value of the variable.</p>

<pre><code class="language-python">with tf.Session(graph=graph) as sess:
  sess.run(init)
  sess.run(x_assign)
  print(sess.run(x))
</code></pre>

<pre><code>13
</code></pre>

<p>As you can see, the value of our variable is only valid within one session. If we try to query the value afterwards in a second session, TensorFlow will raise an error because the variable is not initialized there.</p>

<pre><code class="language-python">with tf.Session(graph=graph) as sess:
  print(sess.run(x))
# Error: Attempting to use uninitialized value foo

</code></pre>

<p>Of course, we can use the graph in more than one session, we just have to initialize the variables again. The values in the new session will be completely independent from the first one:</p>

<pre><code class="language-python">with tf.Session(graph=graph) as sess:
  sess.run(init)
  print(sess.run(x))
</code></pre>

<pre><code>42
</code></pre>

<p>Hopefully this short workthrough helped you to better understand <code>tf.Session</code>. </p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/8/18</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Tensorflow.html'>Tensorflow</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="Tensorflow_1.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">‰ªñÂ±±‰πãÁü≥ÔºåÂèØ‰ª•ÊîªÁéâ</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Leetcode.html"><strong>Leetcode</strong></a>
        
            <a href="Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html"><strong>PythonÊï∞ÊçÆÁªìÊûÑ‰∏éÁÆóÊ≥ï</strong></a>
        
            <a href="Python%E7%89%B9%E6%80%A7.html"><strong>PythonÁâπÊÄß</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>Êú∫Âô®Â≠¶‰π†</strong></a>
        
            <a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>PythonÁßëÂ≠¶ËÆ°ÁÆó‰∏âÁª¥ÂèØËßÜÂåñ</strong></a>
        
            <a href="English.html"><strong>English</strong></a>
        
            <a href="%20%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA.html"><strong> ÁêÜËß£ËÆ°ÁÆóÊú∫</strong></a>
        
            <a href="Deep%20Learning.html"><strong>Deep Learning</strong></a>
        
            <a href="Latex.html"><strong>Latex</strong></a>
        
            <a href="%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html"><strong>Êìç‰ΩúÁ≥ªÁªü</strong></a>
        
            <a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html"><strong>Linux Á≥ªÁªüÁºñÁ®ã</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html"><strong>Êï∞ÊçÆÂ∫ì</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="%E5%B7%A5%E4%BD%9C%E4%B8%8E%E5%AD%A6%E4%B9%A0.html"><strong>Â∑•‰Ωú‰∏éÂ≠¶‰π†</strong></a>
        
            <a href="Data%20Science.html"><strong>Data Science</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15088177470889.html">Machine Learning Foundations (8): The noise and error</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15088177317059.html">Machine Learning Foundations (7): The VC dimension</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15088176831066.html">Machine Learning Foundations (6): Theory of generalization</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15088159325268.html">Machine Learning Foundations (5): Training versus Testing</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="active_learning_intro.html">‰∏ªÂä®Â≠¶‰π†</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
