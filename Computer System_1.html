<!doctype html>
<html class="no-js" lang="en">
  <head>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  Computer System - techlarry
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?fdc936c9f5a3b72177541183cdeb8cb3";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
  </script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">HomePage</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_blank" href="wiki">WIKI</a></li>
        
        <li id=""><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        <li id=""><a target="_blank" href="note-os">NOTE-OS</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">HomePage</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_blank" href="wiki">WIKI</a></li>
        
        <li><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        
        <li><a target="_blank" href="note-os">NOTE-OS</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="Leetcode.html">Leetcode</a></li>
        
            <li><a href="programming_language.html">编程语言</a></li>
        
            <li><a href="data_structure_and_algorithm.html">数据结构和算法</a></li>
        
            <li><a href="Python%E7%89%B9%E6%80%A7.html">Python特性</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="English.html">English</a></li>
        
            <li><a href="Computer%20System.html">Computer System</a></li>
        
            <li><a href="Deep%20Learning.html">Deep Learning</a></li>
        
            <li><a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html">Linux 系统编程</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html">数据库</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="Big%20Data.html">Big Data</a></li>
        
            <li><a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html">文献阅读</a></li>
        
            <li><a href="Tools.html">Tools</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="os_concepts_CPU_scheduling.html">
                
                  <h1>Operating System Concepts 5 - CPU Scheduling</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">1 Basic Concepts</a>
<ul>
<li>
<a href="#toc_1">1.1 CPU-I/O Burst Cycle</a>
</li>
<li>
<a href="#toc_2">1.2 CPU Scheduler</a>
</li>
<li>
<a href="#toc_3">1.3 Preemptive and Nonpreemptive Scheduling</a>
</li>
<li>
<a href="#toc_4">1.4 Dispatcher</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">2 Scheduling Criteria</a>
</li>
<li>
<a href="#toc_6">3 Scheduling Algorithms</a>
<ul>
<li>
<a href="#toc_7">3.1 First-Come,First-Served scheduling, FCFS</a>
</li>
<li>
<a href="#toc_8">3.2 Shortest-job-first scheduling, SJF</a>
</li>
<li>
<a href="#toc_9">3.3 Round-Robin scheduling, RR</a>
</li>
<li>
<a href="#toc_10">3.4 Priority scheduling algorithm</a>
</li>
<li>
<a href="#toc_11">3.5 Multilevel Queue Scheduling</a>
</li>
<li>
<a href="#toc_12">3.6 Multilevel Feedback-Queue Scheduling</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">4 Thread Scheduling</a>
<ul>
<li>
<a href="#toc_14">4.1 Contention Scope</a>
</li>
<li>
<a href="#toc_15">4.2 Pthread Scheduling</a>
</li>
</ul>
</li>
<li>
<a href="#toc_16">5 Multi-Processor Scheduling</a>
<ul>
<li>
<a href="#toc_17">5.1 Approaches to Multiple-Processor Scheduling</a>
</li>
<li>
<a href="#toc_18">5.2 Multicore Processors</a>
</li>
<li>
<a href="#toc_19">5.3 Load Balancing</a>
</li>
<li>
<a href="#toc_20">5.4 Processor Afﬁnity</a>
</li>
</ul>
</li>
<li>
<a href="#toc_21">6 Real-Time CPU Scheduling</a>
</li>
<li>
<a href="#toc_22">7 Linux Scheduling</a>
</li>
</ul>


<p>On modern operating systems it is <strong>kernel-level threads</strong> —not processes—that are in fact being scheduled by the operating system. </p>

<ul>
<li>User-level threads are managed by a thread library, and the kernel is <em>unaware</em> of them.</li>
<li>To run on a CPU, user-level threads must ultimately be mapped to an associated kernel-level thread, although this mapping may be indirect and may use a lightweight process (LWP).</li>
</ul>

<h2 id="toc_0">1 Basic Concepts</h2>

<h3 id="toc_1">1.1 CPU-I/O Burst Cycle</h3>

<p>Process execution consists of a <strong>cycle</strong> of CPU execution and I/O wait. 进程执行由CPU执行周期和I/O等待周期组成。</p>

<ul>
<li>Processes alternate between these two states. 进程在这两个状态之间切换。</li>
<li>Process execution begins with a <strong>CPU burst</strong>, which is followed by an <strong>I/O burst</strong> and so on. 进程执行从CPU区间开始，在这之后是I/O区间。</li>
</ul>

<p>进程在CPU区间和I/O区间之间切换：<br/>
<img src="media/15326899337167/alternating%20sequence%20of%20CPU%20and%20I:O%20bursts.png" alt="alternating sequence of CPU and I:O bursts"/></p>

<p>The durations of CPU bursts tend to have a frequency curve similar to the figure below. </p>

<ul>
<li>The curve is generally characterized as <strong>exponential</strong> or hyperexpoential(超指数).</li>
<li>A large number of short CPU bursts and a small number of long CPU burst.</li>
<li>An I/O-bounded program typically has many short CPU bursts. I/O密集程序通常具有很多短CPU区间。</li>
<li>A CPU-bound program might have a few long CPU bursts.CPU密集程序可能有少量的长CPU区间。</li>
<li>The distribution can be important when implementing a CPU-scheduling algorithm. 分布有助于选择合适的CPU调度算法。</li>
</ul>

<p><img src="media/15326899337167/Histogram%20of%20CPU-burst%20durations.png" alt="Histogram of CPU-burst durations"/></p>

<h3 id="toc_2">1.2 CPU Scheduler</h3>

<p>Whenever the CPU becomes idle, the operating system must select one of the processes in the <strong>ready queue</strong>(就绪队列) to be executed. 每当CPU空闲时，操作系统就必须从就绪队列中选择一个进程来执行。</p>

<ul>
<li>The selection process is carried out by the <strong>CPU scheduler</strong>(CPU调度程序).  进程选择由CPU调度程序执行。</li>
<li>CPU scheduler selects a process from the processes in memory that are ready to execute and allocates the CPU to that process. 调度程序从内存中选择一个能够执行的进程，并为之分配CPU。</li>
<li>A ready queue can be implemented as a FIFO queue, a priority queue, a tree, or simply an unordered linked list. 就绪队列可以是FIFO队列，优先队列、树或无序链表。</li>
</ul>

<h3 id="toc_3">1.3 Preemptive and Nonpreemptive Scheduling</h3>

<p>CPU-scheduling decisions may take place under the following four circumstances: </p>

<ol>
<li>When a process switches from the running state to the waiting state (for example, as the result of an I/O request or an invocation of <code>wait()</code> for the termination of a child process) 当一个进程从运行状态切换到等待状态（如：I/O请求，或者调用wait等待一个子进程的终止） </li>
<li>When a process switches from the running state to the ready state (for example, when an interrupt occurs) 当一个进程从运行状态切换到就绪状态（如：出现中断） </li>
<li>When a process switches from the waiting state to the ready state (for example, at completion of I/O) 当一个进程从等待状态切换到就绪状态（如：I/O完成） </li>
<li>When a process terminates 当一个进程终止时</li>
</ol>

<p>When scheduling takes place only under circumstances 1 and 4, the scheduling scheme is <strong>nonpreemptive</strong>(非抢占的) or <strong>cooperative</strong>(协作的). Otherwise, it is <strong>preemptive</strong>(抢占的).</p>

<ul>
<li>Under nonpreemptive scheduling, once the CPU has been allocated to a process, the process keeps the CPU until it releases it either by terminating or by switching to the waiting state.</li>
<li>Virtually all modern Operating systems use preemptive scheduling algorithms. </li>
</ul>

<h3 id="toc_4">1.4 Dispatcher</h3>

<p>The <strong>dispatcher</strong>(分派程序) is the module that gives control of the CPU&#39;s core to the process selected by the CPU scheduler. This function involves the following:</p>

<ul>
<li>Switching context from one process to another</li>
<li>Switching to user mode</li>
<li>Jumping to the proper location in the user program to resume that program</li>
</ul>

<p><strong>Dispatch latency</strong> (分派延迟) is the time it takes for the dispatcher to stop one process and start another running.</p>

<p><img src="media/15326899337167/the%20role%20of%20dispatcher.png" alt="the role of dispatcher"/></p>

<h2 id="toc_5">2 Scheduling Criteria</h2>

<p>Scheduling criteria（调度准则) include the following:</p>

<ul>
<li><strong>CPU utilization</strong> (CPU利用率)</li>
<li><strong>Throughput</strong> (吞吐量)： the number of processes that are completed per time unit.</li>
<li><strong>Turnaround time</strong> (周转时间): the interval from the time of submission of a process to the time of completion.</li>
<li><strong>Waiting time</strong> (等待时间): the sum of time spent waiting in the ready queue.</li>
<li><strong>Response time</strong> (响应时间): the time from the submission of a request until the first response is produced.</li>
</ul>

<h2 id="toc_6">3 Scheduling Algorithms</h2>

<h3 id="toc_7">3.1 First-Come,First-Served scheduling, FCFS</h3>

<p>By far the simplest CPU-scheduling algorithm is the <strong>first-come first serve scheduling</strong> (先到先服务调度, FCFS) algorithm.</p>

<ul>
<li>The implementation of FCFS policy is easily managed with a <strong>FIFO queue</strong>.</li>
<li>The average <strong>waiting time</strong> under the FCFS policy is often quite <strong>long</strong>.</li>
<li><strong>Convoy effect</strong>(护航效果) occurs when all the other processes wait for the one big process to get off the CPU. 所有其他进程都等待一个大进程释放CPU，这称之为护航效果。</li>
<li>The FCFS scheduling algorithm is <strong>nonpreemptive</strong>. FCFS调度算法是非抢占的。</li>
</ul>

<h3 id="toc_8">3.2 Shortest-job-first scheduling, SJF</h3>

<p>The <strong>shortest-job-first scheduling</strong> (最短作业优先调度, SJF) algorithm associates with each process the length of the process&#39;s next CPU burst.</p>

<ul>
<li>When the CPU is available, it is assigned to the process that has the smallest <strong>next</strong> CPU burst.</li>
<li>It gives the <strong>minimum</strong> average waiting time for a given set of processes.</li>
<li>The SJF algorithm can be either preemptive or nonpreemptive.
<ul>
<li>Preempt the currently executing process: when a new process arrives at the ready queue while a previous process is still executing. The next CPU burst of the newly arrived process may be shorter than what is left of the currently executing process. </li>
</ul></li>
</ul>

<p>The next CPU burst is generally predicted as an <strong>exponential average</strong> of the measured lengths of previous CPU bursts. Let \(t_n\) be the length of the \(n\)th CPU burst, and let \(\tau_{n+1}\) be predicted value for the next CPU burst:</p>

<p>\[\tau_{n+1}= \alpha \tau_n + (1-\alpha) \tau_n\]</p>

<p>where \(0\le\alpha \le 1\), commonly \(\alpha = 1/2\).</p>

<h3 id="toc_9">3.3 Round-Robin scheduling, RR</h3>

<p>The <strong>round-robin scheduling</strong>(轮转调度) algorithm is similar to FCFS scheduling, but switch occurs after 1 <strong>time quantum</strong> (时间片).</p>

<ul>
<li>Time quantum is a small unit of time, generally from 10 to 100 milliseconds in length.</li>
<li>The ready queue is treated as a circular queue.</li>
<li>If the process have a CPU burst of less than 1 time quantum, the  process itself will release the CPU voluntarily.</li>
<li>otherwise, a context switch will be executed, and the process will be put at the tail of the ready queue.</li>
</ul>

<p>The performance of the RR algorithm depends heavily on the size of the time quantum.</p>

<ul>
<li>If extremely large, the RR policy is the same as the FCFS policy.</li>
<li>If extremely small, it&#39;ll result in a large number of context switches.</li>
</ul>

<h3 id="toc_10">3.4 Priority scheduling algorithm</h3>

<p>The <strong>priority-scheduling</strong>(优先级调度) algorithm associate each process a priority, and the CPU allocated to the process with the highest priority.</p>

<ul>
<li>FCFS: equal-priority</li>
<li>SJF: the priority is the inverse of the next CPU burst.</li>
</ul>

<p>ISSUE: <strong>Indefinite blocking</strong>(无限阻塞), or <strong>starvation</strong>(饥饿) occurs when some low-priority processes waiting indefinitely.</p>

<p>SOLUTION: <strong>Aging</strong>(老化) involves gradually increasing the priority of processes that wait in the system for a long time.</p>

<h3 id="toc_11">3.5 Multilevel Queue Scheduling</h3>

<p>For <strong>multilevel queue scheduling</strong>(多级队列调度), there are separate queues for each distinct priority, and priority scheduling simply schedules the process in the highest-priority queue.</p>

<p>A multilevel queue scheduling algorithm can be used to partition processes into several separate queuse based on the process type.<br/>
<img src="media/15326899337167/multilevel-queue-scheduling.png" alt="multilevel-queue-scheduling"/></p>

<p>In addition, there must be scheduling <u><em>among the queues</em></u> :</p>

<ul>
<li><strong>Fixed-priority preemptive scheduling</strong>(固定优先级抢占调度): Each queue has absolute priority over lower-priority queues
<ul>
<li>eg. no process in the batch queue, could run unless the queues for real-time processes, system processes, and interactive processes were all empty. </li>
</ul></li>
<li><strong>Time-slice among queues</strong>(队列之间划分时间片): each queue gets a certain portion of the CPU time.
<ul>
<li>eg. the foreground queue can be given 80 percent of the CPU time for RR scheduling among its processes, while the background queue receives 20 percent of the CPU to give to its processes on an FCFS basis.</li>
</ul></li>
</ul>

<h3 id="toc_12">3.6 Multilevel Feedback-Queue Scheduling</h3>

<p>The <strong>multilevel feedback queue scheduling</strong>(多级反馈队列调度) algorithm allows a process to move between queues.</p>

<ul>
<li>If a process uses too much CPU time, it will be moved to a lower-priority queue.
<ul>
<li>It leaves I/O-bound and interactive processes—which are typically characterized by short CPU bursts —in the higher-priority queues. </li>
</ul></li>
<li>A process that waits too long in a lower-priority queue may be moved to a higher-priority queue.
<ul>
<li>This form of aging prevent starvation.</li>
</ul></li>
</ul>

<p>In general, a multilevel feedback queue scheduler is defined by the following parameters:</p>

<ul>
<li>The number of queues</li>
<li>The scheduling algorithm for each queue</li>
<li>The method used to determine when to upgrade a process to a higher priority queue</li>
<li>The method used to determine when to demote a process to a lower priority queue</li>
<li>The method used to determine which queue a process will enter when that process needs service</li>
</ul>

<h2 id="toc_13">4 Thread Scheduling</h2>

<h3 id="toc_14">4.1 Contention Scope</h3>

<p><strong>Process contention scope</strong> (PCS，进程竞争范围), occurs when competition for the CPU takes place among threads belonging to the same process.</p>

<ul>
<li>the thread library schedules user-level threads to run on an available LWP, on systems implementing the many-to-one and many-to-many models.</li>
</ul>

<p>To decide which kernel-level thread to schedule onto a CPU, the kernel uses <strong>system-contention scope</strong> (SCS, 系统竞争范围).</p>

<ul>
<li>Systems using the one-to-one model, such as Windows and Linux schedule threads using only SCS.</li>
</ul>

<h3 id="toc_15">4.2 Pthread Scheduling</h3>

<p><strong>Pthreads</strong> identifies the following contention scope values:</p>

<ul>
<li><code>PTHREAD_SCOPE_PROCESS</code> schedules threads using PCS scheduling.</li>
<li><code>PTHREAD_SCOPE_SYSTEM</code> schedules threads using SCS scheduling.</li>
</ul>

<p>The Pthread IPC (Interprocess Communication) provides two functions for setting—and getting—the contention scope policy:</p>

<ul>
<li><code>pthread_attr_setscope(pthread_attr_t *attr, int scope)</code></li>
<li><code>pthread_attr_getscope(pthread_attr_t *attr, int *scope)</code></li>
</ul>

<pre><code class="language-c">#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;
#define NUM_THREADS 5

/* the thread runs in this function */
void *runner(void *param); 

int main(int argc, char *argv[])
{
    int i, scope;
    pthread_t tid[NUM_THREADS];     /* the thread identifier */
    pthread_attr_t attr;        /* set of attributes for the thread */

    /* get the default attributes */
    pthread_attr_init(&amp;attr);

    /* first inquire on the current scope */
    if (pthread_attr_getscope(&amp;attr,&amp;scope) != 0)
        fprintf(stderr, &quot;Unable to get scheduling scope.\n&quot;);
    else {
        if (scope == PTHREAD_SCOPE_PROCESS)
            printf(&quot;PTHREAD_SCOPE_PROCESS\n&quot;);
        else if (scope == PTHREAD_SCOPE_SYSTEM)
            printf(&quot;PTHREAD_SCOPE_SYSTEM\n&quot;);
        else 
            fprintf(stderr,&quot;Illegal scope value.\n&quot;);
    }
    
    /* set the scheduling algorithm to PCS or SCS */
    if (pthread_attr_setscope(&amp;attr, PTHREAD_SCOPE_SYSTEM) != 0)
        printf(&quot;unable to set scheduling policy.\n&quot;);

    /* create the threads */
    for (i = 0; i &lt; NUM_THREADS; i++) 
        pthread_create(&amp;tid[i],&amp;attr,runner,NULL); 

    /**
     * Now join on each thread
     */
    for (i = 0; i &lt; NUM_THREADS; i++) 
        pthread_join(tid[i], NULL);
}

/**
 * The thread will begin control in this function.
 */
void *runner(void *param) 
{
    /* do some work ... */

    pthread_exit(0);
}
</code></pre>

<h2 id="toc_16">5 Multi-Processor Scheduling</h2>

<h3 id="toc_17">5.1 Approaches to Multiple-Processor Scheduling</h3>

<p><strong>Asymmetric multiprocessing</strong> (AMP，非对称多处理)</p>

<ul>
<li>all scheduling decisions, I/O processing, and other system activities handled by a single processor -- the master server; the other processors execute only user code.  让一个处理器（主服务器）处理所有的调度决定、I/O处理以及其他系统活动，其他的处理器只执行用户代码。</li>
<li>it is simple because only one core accesses the system data structures, reducing the need for data sharing. 简单，因为只有一个处理器访问系统数据结构，减轻了数据共享的需要。</li>
<li>the master server becomes a potential bottleneck where overall system performance may be reduced.</li>
</ul>

<p><strong>Symmetric multiprocessing</strong> (SMP， 对称多处理)</p>

<ul>
<li>each processor is self-scheduling</li>
<li>it provides two possible strategies for organizing the threads eligible to be scheduled:
<ul>
<li>All threads may be in a _common ready queue_.
<ul>
<li>use some form of locking to protect the common ready queue from race condition</li>
<li>all accesses to the queue would require lock ownership, it would be a performance bottleneck.</li>
</ul></li>
<li>Each processor may have its own <u>private queue</u> of threads.
<ul>
<li>most common approach on systems supporting SMP</li>
<li>more efficient use of cache memory.</li>
</ul></li>
</ul></li>
</ul>

<p><img src="media/15326899337167/organization%20of%20ready%20queues.png" alt="organization of ready queues"/></p>

<h3 id="toc_18">5.2 Multicore Processors</h3>

<p><u>Issue</u> : memory stalls occurs when a processor accesses memory, it spends a significant amount of time waiting for the data to become available.</p>

<ul>
<li>occurs primarily because modern processors operate at much faster speeds than memory</li>
<li>occur because of a cache miss</li>
</ul>

<p><img src="media/15326899337167/memory%20stall.png" alt="memory stall"/><br/>
<u>Solution</u> : many recent hardware designs have implemented multithreaded processing cores in which two (or more) <strong>hardware threads</strong>(硬件线程) are assigned to each core.</p>

<ul>
<li>If one hardware thread stalls while waiting for memory, the core can switch to another thread.</li>
<li>From an operating system perspective, each hardware thread maintains its architectural state, such as instruction pointer and register set, and thus appears as a logical CPU that is available to run a software thread. This technique is known as <strong>chip multithreading</strong> (CMT, 芯片多线程). Intel use the term <strong>hyper-threading</strong>(超线程).</li>
<li><strong>NOTE</strong>: the resources of the physical core (such as caches and pipelines) are shared among its hardware threads, and a processing core can only execute one hardware thread at a time.</li>
</ul>

<p><img src="media/15326899337167/Chip%20multithreading.png" alt="Chip multithreading"/></p>

<p>Two levels of scheduling needed:</p>

<ul>
<li>It chooses which software thread to run on each hardware thread.
<ul>
<li>It may choose any scheduling algorithm. </li>
</ul></li>
<li>It chooses which hardware thread to run on CPU.
<ul>
<li>Use a simple round-robin algorithm</li>
<li>assigned to each hardware thread a dynamic urgency value ranging from 0 to 7, with 0 representing the lowest urgency and 7 the highest. </li>
</ul></li>
</ul>

<p><img src="media/15326899337167/two%20levels%20of%20scheduling.png" alt="two levels of scheduling"/></p>

<h3 id="toc_19">5.3 Load Balancing</h3>

<p><strong>Load balancing</strong>(负载均衡) attempts to keep the workload evenly distributed across all processors in an SMP system.</p>

<p>Two general approaches to load balancing:</p>

<ul>
<li><strong>Push migration</strong>: a specific task periodically checks the load on each processor and -- if it finds an imbalance -- evenly distributes the load by moving (or pushing) threads from overloaded to idle or less-busy processors.</li>
<li><strong>Pull migration</strong>: an idle processor pulls a waiting task from a busy processor.</li>
<li>They are not mutually exclusive and are, in fact, often implemented in parallel on load-balancing systems.</li>
</ul>

<h3 id="toc_20">5.4 Processor Afﬁnity</h3>

<p>Because of the high cost of invalidating and repopulating caches, most operating systems with SMP support try to <u>avoid migrating</u> a thread from one processor to another and instead attempt to keep a thread running on the same processor and take advantage of a warm cache. This is known as <strong>processor affinity</strong>(处理器亲和性)。</p>

<p>Common ready queue and per-processor ready queue(section 5.1):</p>

<ul>
<li>If we adopt the approach of a common ready queue, a thread may be selected for execution by any processor. Thus, if a thread is scheduled on a new processor, that processor’s cache must be repopulated.</li>
<li>With private, per-processor ready queues, a thread is always scheduled on the same processor and can therefore benefit from the contents of a warm cache.</li>
</ul>

<p>The main-memory architecture of a system can affect processor affinity issues as well. <strong>Non-uniform memory access</strong>(NUMA, 非一致性内存访问) where there are two physical processor chips each with their own CPU and local memory. A CPU has faster access to its local memory than to memory local to another CPU.</p>

<p><img src="media/15326899337167/numa%20and%20CPU%20scheduling.png" alt="numa and CPU scheduling"/></p>

<p>Interestingly, load balancing often <strong>counteracts</strong> the benefits of processor affinity.</p>

<h2 id="toc_21">6 Real-Time CPU Scheduling</h2>

<p>[to be continued]</p>

<h2 id="toc_22">7 Linux Scheduling</h2>

<p>The <strong><em>Completely Fair Scheduler</em></strong>（CFS，完全公平调度算法) is the default Linux scheduling algorithm.</p>

<ul>
<li>Each task has a <strong>virtual runtime</strong> value, which is its actual runtime normalized to the number of ready tasks.</li>
<li>Task priority is incorporated as a <strong>decay factor</strong> into this<br/>
formula. 
<ul>
<li>Lower-priority tasks have higher rates of decay than higher-priority tasks.</li>
</ul></li>
<li>The CPU is allocated to the task with the smallest virtual<br/>
runtime value.</li>
</ul>

<p>Standard Linux kernels implement two <strong>scheduling classes</strong>(调度类): </p>

<ul>
<li>a default scheduling class using the CFS scheduling algorithm </li>
<li>a real-time scheduling class.</li>
</ul>

<p>Each runnable task is placed in a <strong>red-black tree</strong> - a balanced binary search tree whose key is based on the value of virtual runtime <code>vruntime</code>.</p>

<ul>
<li>discover the leftmost node will require \(O(\log N)\) operations.</li>
<li>Linux scheduler caches the leftmost node in the variable <code>rb_leftmost</code>, and requires only retrieving the cached value.</li>
</ul>

<p><img src="media/15326899337167/15327413379278.gif" alt=""/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/3/27</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Computer%20System.html'>Computer System</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="os_concepts_synchronization_tools.html">
                
                  <h1>Operating System Concepts 6 - Synchronization Tools</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">1 Background</a>
</li>
<li>
<a href="#toc_1">2 The Critical-Section problem</a>
</li>
<li>
<a href="#toc_2">3 Peterson&#39;s Solution</a>
</li>
<li>
<a href="#toc_3">4 Hardware support for Synchronization</a>
<ul>
<li>
<a href="#toc_4">4.1 Memory barriers</a>
</li>
<li>
<a href="#toc_5">4.2 Hardware instructions</a>
</li>
<li>
<a href="#toc_6">4.3 Atomic variables</a>
</li>
</ul>
</li>
<li>
<a href="#toc_7">5 Mutex locks</a>
</li>
<li>
<a href="#toc_8">6 Semaphores</a>
</li>
<li>
<a href="#toc_9">7 Monitors</a>
</li>
<li>
<a href="#toc_10">8 Liveness</a>
<ul>
<li>
<a href="#toc_11">8.1 Deadlock</a>
</li>
<li>
<a href="#toc_12">8.2 Priority Inversion</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">9 Evaluation</a>
</li>
</ul>


<h2 id="toc_0">1 Background</h2>

<p>A <strong>race condition</strong>(竞争条件) occurs when several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place.</p>

<p>多个进程并发访问和操作同一数据，且执行结果与访问发生的特定顺序有关，称之为竞争条件。</p>

<h2 id="toc_1">2 The Critical-Section problem</h2>

<p>A <strong>critical section</strong>（临界区） is a section of code, in which the process may be accessing and updating data that is shared with at least one other process.</p>

<ul>
<li>When one process is executing in its critical section, no other process is allowed to execute in its critical section.</li>
</ul>

<p>The <strong>critical-section problem</strong>（临界区问题） is to design a protocol that the processes can use to synchronize their activity so as to cooperatively share data.</p>

<ul>
<li>Each process must request permission to enter its critical section.</li>
<li>The section of code implementing this request is the <strong>entry section</strong>（进入区）</li>
<li>The critical section may be followed by an <strong>exit section</strong> (退出区)。</li>
<li>The remaining code is the **remainder section **（剩余区)。</li>
</ul>

<p><img src="media/15326525991493/general%20structure%20of%20a%20typical%20process.png" alt="general structure of a typical process"/></p>

<p>A solution to the critical-section problem must satisfy the following three requirements:</p>

<ol>
<li><strong>Mutual exclusion</strong> (互斥): If process \(P_i\) is executing in its critical section, then no other processes can be executing in their critical sections. 如果进程\(P_i\)在其临界区内执行，那么其他进程都不能在其临界区内执行；</li>
<li><p><strong>Progress</strong> (前进): If no process is executing in its critical section and some processes wish to enter their critical sections, then only those processes that are not executing in their remainder sections can participate in deciding which will enter its critical section next, and this selection cannot be postponed indefinitely. 如果没有进程在其临界区内执行且有进程需进入临界区，那么只有那么不在剩余区内执行的进程可参加选择，以确定谁能下一个进入临界区，且这种选择不能无限推迟；</p></li>
<li><p><strong>Bounded waiting</strong> (有限等待): There exists a bound, or limit, on the number of times that other processes are allowed to enter their critical sections after a process has made a request to enter its critical section and before that request is granted. 从一个进程做出进入临界区的请求，直到该请求允许为止，其他进程允许进入其临界区内的次数有上限。</p></li>
</ol>

<p>Two general approaches are used to handle critical sections in operating systems: <strong>preemptive kernels</strong>（抢占内核） and <strong>nonpreemptive kernels</strong>（非抢占内核）.</p>

<ul>
<li>A preemptive kernel allows a process to be preempted while it is running in kernel mode. 抢占内核允许处于内核模式的进程被抢占。</li>
<li>A nonpreemptive kernel does not allow a process running in kernel mode to be preempted.A kernel-model process will run until it exists kernel mode, blocks, or voluntarily yields control of the CPU.非抢占内核不允许内核模式的进程被抢占。</li>
<li>A nonpreemptive kernel is essentially free from race conditions on kernel data structures, as only on process is active in the kernel at at time. 非抢占内核的内核从根本上不会导致竞争条件，因为在内核中一次只有一个进程是活跃的。</li>
<li>Preemptive kernels must be carefully designed to ensure that shared kernel data are free from race conditions. 对于抢占内核需要认真设计以确保共享内和数据免于竞争条件。</li>
<li>A preemptive kernel may be more responsive, since there is less risk that a kernel-model process will run for an arbitrarily long period before relinquishing the processor to waiting process. 抢占内核的响应更快，因为处于内核模式的进程在释放CPU之前不会运行过久。</li>
<li>A preemptive kernel is more suitable for real-time programming, as it will allow a real-time process to preemptive a process currently running in the kernel. 抢占内核更适合实时编程，因为它能允许实时进程抢占处于内核模式运行的其他进程。</li>
</ul>

<h2 id="toc_2">3 Peterson&#39;s Solution</h2>

<p><strong>Peterson’s solution</strong>(Peterson 算法) is restricted to two processes that alternate execution between their critical sections and remainder sections. The processes are numbered \(P_0\) and \(P_1\). For convenience, when presenting \(P_i\), we use \(P_j\) to denote the other process; that is \(j\) equals \(1-i\).</p>

<p>Peterson&#39;s solution requires the two processes to share two data items:</p>

<pre><code class="language-c">int turn;
boolean flag[2];
</code></pre>

<p>The structure of process \(P_i\) in Peterson&#39;s solution.</p>

<pre><code class="language-c">while (true) {
    flag[i] = true; 
    turn = j; 
    while (flag[j] &amp;&amp; turn == j) 
        ;
    /* critical section */
    flag[i] = false;
    /*remainder section */
}
</code></pre>

<ul>
<li>The variable <code>turn</code> indicates whose turn it is to enter its critical section.</li>
<li>The <code>flag</code> array is used to indicate if a process is ready to enter its critical section.</li>
</ul>

<p><strong>Note</strong>:  Peterson’s solution is <strong>not guaranteed</strong> to work on modern computer architectures for the primary reason that, to improve system performance, <strong>processors and/or compilers may reorder read and write operations that have no dependencies</strong>.</p>

<p>If the assignments of the first two statements that appear in the entry section of Peterson&#39;s solution are reordered. It is possible that both threads may be active in their critical sections at the same time.<br/>
<img src="media/15326525991493/the%20effects%20of%20instruction%20reordering%20in%20Peterson&#x27;s%20solution.png" alt="the effects of instruction reordering in Peterson&#39;s solution"/></p>

<h2 id="toc_3">4 Hardware support for Synchronization</h2>

<p>Hardware support for the critical-section problem includes, </p>

<ul>
<li>memory barriers</li>
<li>hardware instructions</li>
<li>atomic variables</li>
</ul>

<h3 id="toc_4">4.1 Memory barriers</h3>

<p>How a computer architecture determines what memory guarantees it will provide to an application program is known as its <strong>memory model</strong>(内存模型). In general, a memory model falls into one of two categories:</p>

<ol>
<li><strong>Strongly ordered</strong>, where a memory modification on one processor is immediately visible to all other processors.</li>
<li><strong>Weakly ordered</strong>, where modifications to memory on one processor may not be immediately visible to other processors.</li>
</ol>

<p>Computer architectures provide instructions that can <em>force</em> any changes in memory to be propagated to all other processors, thereby ensuring that memory modifications are visible to threads running on other processors. Such instruction are known as <strong>memory barriers</strong>(内存屏障).</p>

<ul>
<li>When a memory barrier instruction is performed, the system ensures that all loads and stores are completed before any subsequent load or store operations are performed.</li>
</ul>

<h3 id="toc_5">4.2 Hardware instructions</h3>

<p>Many modern computer systems provide special hardware instructions that allow either to test and modify the content of a word or to swap the contents of two words atomically - that is, one uninterruptible unit.</p>

<p>The definition of the atomic <code>test_and_set()</code> instruction:</p>

<pre><code class="language-c">boolean test and set(boolean *target) { 
    boolean rv = *target; 
    *target = true;
    return rv;
}
</code></pre>

<p>Mutual-exclusion implementation with <code>test_and_set()</code>:</p>

<pre><code class="language-text">do {
    while (test and set(&amp;lock)) 
        ; /* do nothing */
    /* critical section */
    lock = false;
    /* remainder section */ } 
while (true);
</code></pre>

<p>The definition of the atomic <code>compare_and_swap()</code>（CAS）instruction:</p>

<pre><code class="language-c">int compare and swap(int *value, int expected, int new value) { 
    int temp = *value;
    if (*value == expected) 
        *value = new value;
    return temp;
}
</code></pre>

<p>Mutual exclusion with the <code>compare_and_swap()</code> instruction:</p>

<pre><code class="language-c">while (true) {
    while (compare and swap(&amp;lock, 0, 1) != 0) 
        ; /* do nothing */
    /* critical section */
    lock = 0;
    /* remainder section */
}
</code></pre>

<h3 id="toc_6">4.3 Atomic variables</h3>

<p><strong>Atomic variables</strong> (原子变量) provides atomic operations on basic data types such as integers and booleans. Their use is often limited to single updates of shared data such as counters and sequence generators.</p>

<h2 id="toc_7">5 Mutex locks</h2>

<p>ISSUE: The hardware-based solutions are complicated as well as generally inaccessible to application programmers.</p>

<p>SOLUTION: Operating-system designers build higher-level software tools. The simplest of these tools is the <strong>mutex lock</strong>(互斥锁)。</p>

<ul>
<li>A process must <strong>acquire</strong> the lock before entering a critical section; </li>
<li>A process <strong>releases</strong> the lock when it exists the critical section.</li>
<li>A mutex lock has a boolean variable <strong>available</strong>, whose value indicates if the lock is available or not.</li>
<li>Calls to either <code>acquire()</code> or <code>release()</code> must be performed atomically. Thus mutex locks can be implemented using the CAS operation.</li>
</ul>

<p>Solution to the critical-section problem using mutex locks:<br/>
<img src="media/15326525991493/mutex%20lock.png" alt="mutex lock"/></p>

<p>The definition of <code>acquire()</code> is as follows:</p>

<pre><code class="language-c">acquire() { 
    while (!available) ;
        /* busy wait */ 
    available = false; 
}
</code></pre>

<p>The definition of <code>release()</code> is as follows:</p>

<pre><code class="language-c">release(){
    available = true;
}
</code></pre>

<p>The main disadvantage of the implementation is that it requires <strong>busy waiting</strong>.</p>

<ul>
<li>while  a process is in its critical section, any other process that tries enter its critical section must loop continuously in the call to <code>acquire()</code>.</li>
<li>it wastes CPU cycles.</li>
</ul>

<p>Because the process &quot;spins&quot; while waiting for the lock to become available, this type of mutex lock is also called a <code>spinlock</code>（自旋锁）。</p>

<ul>
<li>advantage: no context switch is required</li>
</ul>

<p>Spinlocks are not appropriate for single-processor systems yet are often used in multiprocessor systems.</p>

<p>在UNIX中，自旋锁相关的API：</p>

<pre><code class="language-c">// 初始化自旋锁： 用来申请使用自旋锁所需要的资源并且将它初始化为非锁定状态
int pthread_spin_init(pthread_spinlock_t *, int);
// 获得一个自旋锁：如果该自旋锁当前没有被其它线程所持有，则调用该函数的线程获得该自旋锁.
// 否则该函数在获得自旋锁之前不会返回。
int pthread_spin_lock(pthread_spinlock_t *);
//释放指定的自旋锁
int pthread_spin_unlock(pthread_spinlock_t *);
// 销毁一个自旋锁
int pthread_spin_destroy(pthread_spinlock_t *);
</code></pre>

<h2 id="toc_8">6 Semaphores</h2>

<p>A <strong>semaphore</strong>(信号量) S is an integer variable that, apart from initialization, is accessed only through two standard atomic operations: <code>wait()</code> and <code>signal()</code>. 信号量S是个整数变量，除了初始化外，它只能通过两个标准原子操作：<code>wait()</code>和<code>signal()</code>来访问。</p>

<p>The definition of <code>wait()</code> is as follows:</p>

<pre><code class="language-c">wait(S){
    while (S &lt;= 0)
        ;// busy wait
    S--;
{
</code></pre>

<p>The definition of <code>signal()</code> is as follows:</p>

<pre><code class="language-c">signal(S){
    S++;
}
</code></pre>

<p>All modifications to the integer value of the semaphore in the <code>wait()</code> and <code>signal()</code> operations must be executed atomically.  在<code>wait()</code>和<code>signal()</code>操作中，对信号量整型值的修改必须不可分地执行。</p>

<p>Operating systems often distinguish between counting and binary semaphores.通常操作系统区分计数信号量和二进制信号量。</p>

<ul>
<li>The value of a <strong>counting semaphore</strong>(计数信号量) can range over an unrestricted domain.计数信号量的值域不受限制。</li>
<li>The value of a <strong>binary semaphore</strong>(二进制信号量) can range only between 0 and 1. 二进制信号量的值只能为0或1。</li>
</ul>

<p>Counting semaphores can be used to control access to  a given resource consisting of a finite number of instances.</p>

<ul>
<li>The semaphore is initialized to the number of resources available. </li>
<li>Each process that wishes to use a resource performs a <code>wait()</code>operation on the semaphore (thereby decrementing the count). </li>
<li>When a process releases a resource, it performs a <code>signal()</code> operation (incrementing the count). </li>
<li>When the count for the semaphore goes to 0, all resources are being used. After that, processes that wish to use a resource will block until the count becomes greater than 0.</li>
</ul>

<h2 id="toc_9">7 Monitors</h2>

<p>Issues: various types of errors can be generated easily when programmers use semaphores or mutex locks incorrectly to solve the critical-section problem.</p>

<ul>
<li>interchanges the order of <code>wait()</code> and <code>signal()</code></li>
<li>replaces <code>signal()</code> with <code>wait()</code></li>
<li>omits <code>wait()</code> or <code>signal()</code></li>
</ul>

<p>Solution: An abstract data type, <strong>monitor</strong>(管程), includes a set of programmer-defined operation related to mutual exclusion within the monitor. A monitor uses <strong>condition variables</strong> that allow processes to wait for certain conditions to become true and to signal one another when conditions have been set to true.</p>

<p>Pseudocode syntax of a monitor:</p>

<pre><code class="language-c">monitor monitor name { /* shared variable declarations */
    function P1 ( . . . ) { . . .}
    function P2 ( . . . ) { . . .}
        .
        .
    function Pn ( . . . ) { . . .}
    initialization code ( . . . ) { . . .}
}
</code></pre>

<p><img src="media/15326525991493/monitor%20with%20condition%20variables.png" alt="monitor with condition variables"/></p>

<h2 id="toc_10">8 Liveness</h2>

<h3 id="toc_11">8.1 Deadlock</h3>

<p><strong>deadlocked</strong>(死锁): two or more processes are waiting indefinitely for an event.</p>

<p>A set of processes is in a deadlocked state when every process in the set is waiting for an event that can be caused only by another process in the set.</p>

<h3 id="toc_12">8.2 Priority Inversion</h3>

<p>A scheduling challenge arises when a higher-priority process needs to read or modify kernel data that are currently being accessed by a lower-priority process—or a chain of lower-priority processes. </p>

<ul>
<li>Since kernel data are typically protected with a lock, the higher-priority process will have to wait for a lower-priority one to finish with the resource. </li>
<li>The situation becomes more complicated if the lower-priority process is preempted in favor of another process with a higher priority.</li>
</ul>

<p>As an example, assume we have three processes—\(L\), \(M\), and \(H\)—whose priorities follow the order \(L &lt; M &lt; H\). </p>

<ul>
<li>Assume that process \(H\) requires a semaphore \(S\), which is currently being accessed by process \(L\). </li>
<li>Ordinarily, process \(H\) would wait for \(L\) to finish using resource S. </li>
<li>However, now suppose that process \(M\) becomes runnable, thereby preempting process \(L\). </li>
<li>Indirectly, a process with a lower priority—process \(M\)—has affected how long process \(H\) must wait for \(L\) to relinquish resource \(S\).</li>
</ul>

<p>This liveness problem is known as <strong>priority inversion</strong>（优先级反转）, and it can occur only in systems with more than two priorities. </p>

<p>Solution：  priority-inheritance protocol(优先级继承协议)：</p>

<ul>
<li>All processes that are accessing resources needed by a higher-priority process inherit the higher priority until they are finished with the resources. </li>
<li>When they are finished, priorities revert to original values.</li>
</ul>

<h2 id="toc_13">9 Evaluation</h2>

<p>Performance differences between CAS-based synchronization and traditional synchronization (such as mutex locks and semaphores) under varying contention loads:</p>

<ul>
<li><strong>Uncontended</strong>： Although both options are generally fast, CAS protection will be somewhat faster than traditional synchronization.</li>
<li><strong>Moderate contention</strong>： CAS protection will be faster—possibly much faster —than traditional synchronization.</li>
<li><strong>High contention</strong>： Under very highly contended loads, traditional synchronization will ultimately be faster than CAS-based synchronization.</li>
</ul>

<p>Higher-level tools such as monitors and condition variables may have significant overhead, and may be less likely to scale in highly contended situations.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/3/27</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Computer%20System.html'>Computer System</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="diagrammatize_TCP_IP.html">
                
                  <h1>图解TCP/IP</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>在计算机通信中，事先达成一个详细的约定，并遵循这一约定进行处理，这种约定其实就是<strong>协议</strong>。</p>

<p><strong>计算机网络体系结构</strong>将网络协议进行了系统的归纳。TCP/IP就是IP、TCP、HTTP等协议的集合。除此之外，还有很多其他类型的网络体系结构。</p>

<p><img src="media/15166008990128/15322220890961.jpg" alt=""/></p>

<h2 id="toc_0">OSI模型</h2>

<p><em><u>开放式系统互联通信参考模型</u></em>(Open System Interconnection Reference Model， 简称<strong>OSI模型</strong>)。在这一模型中，每个分层都接收由它下一层所提供的特定服务，并且负责为自己的上一层提供特定的服务。上下层之间进行交互时所遵循的约定叫做<strong>接口</strong>。统一层之间的交互所遵循的约定叫做<strong>协议</strong>。</p>

<ul>
<li>分层可以将每个独立使用，即使系统中某些分层发生变化，也不会波及整个系统</li>
<li>分层能够细分通信功能，更易于单独实现每个分层的协议，并界定各个分层的具体责任和义务。</li>
</ul>

<p><img src="media/15166008990128/15322230855436.jpg" alt=""/></p>

<ul>
<li>应⽤层：为应⽤程序提供服务并规定应⽤程序中通信相关的细节。包括⽂件传输、电⼦邮件、远程登录（虚拟终端）等协议。 </li>
<li>表⽰层：将应⽤处理的信息转换为适合⽹络传输的格式，或将来⾃下⼀层的数据转换为上层能够处理的格式。因此它主要负责数据格式的转换。具体来说，就是将设备固有的数据格式转换为⽹络标准传输格式。不同设备对同⼀⽐特流解释的结果可能会不同。因此，使它们保持⼀致是这 ⼀层的主要作⽤。 </li>
<li>会话层： 负责建⽴和断开通信连接（数据流动的逻辑通路），以及数据的分割等数据传输相关的管理。 </li>
<li>传输层： 起着可靠传输的作⽤。只在通信双⽅节点上进⾏处理，⽽⽆需在路由器上处理。 </li>
<li>⽹络层：将数据传输到⽬标地址。⽬标地址可以是多个⽹络通过路由器连接⽽成的某⼀个地址。因此这⼀层主要负责寻址和路由选择。 </li>
<li>数据链路层：负责物理层⾯上互连的、节点之间的通信传输。例如与1个以太⽹相连的2个节点之间的通信。 将0、1序列划分为具有意义的数据帧传送给对端（数据帧的⽣成与接收）。 </li>
<li>物理层：负责0、1⽐特流（0、1序列）与电压的⾼低、光的闪灭之间的互换。</li>
</ul>

<h2 id="toc_1">TCP/IP协议</h2>

<p><img src="media/15166008990128/15322239071091.jpg" alt=""/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/1/22</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Computer%20System.html'>Computer System</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="csapp-internet-programming.html">
                
                  <h1>CSAPP - 网络编程</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">1 客户端-服务器编程模型</a>
</li>
<li>
<a href="#toc_1">2 网络</a>
<ul>
<li>
<a href="#toc_2">2.1 网络层次系统</a>
</li>
<li>
<a href="#toc_3">2.2 网络协议</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">3 全球IP因特网</a>
<ul>
<li>
<a href="#toc_5">3.1 IP地址</a>
</li>
<li>
<a href="#toc_6">3.2 域名</a>
</li>
<li>
<a href="#toc_7">3.3 因特网连接</a>
</li>
</ul>
</li>
<li>
<a href="#toc_8">4 套接字接口</a>
<ul>
<li>
<a href="#toc_9">4.1 Echo客户端和服务器示例</a>
</li>
</ul>
</li>
<li>
<a href="#toc_10">5 Web服务器</a>
<ul>
<li>
<a href="#toc_11">5.1 Web基础</a>
</li>
<li>
<a href="#toc_12">5.2 Web内容</a>
</li>
<li>
<a href="#toc_13">5.3 HTTP事务</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">1 客户端-服务器编程模型</h2>

<p>每个网络应用都是基于<strong>客户端-服务器</strong>模型的。</p>

<p>客户端-服务器模型中的基本操作是<strong>事务</strong>(transaction)。一个客户端-服务器事务由以下四步组成：</p>

<ol>
<li>当一个客户端需要服务时，它向服务器发送一个请求，发起一个事务。</li>
<li>服务器收到请求后，解释它，并以适当的方式操作它的资源。</li>
<li>服务器给客户端发送一个响应，并等待下一个请求。</li>
<li>客户端收到响应并处理它。</li>
</ol>

<p><img src="media/15320197756867/client-server%20model.png" alt="client-server mode"/></p>

<p><strong>客户端和服务器是进程</strong>，而不是常提到的机器或者主机。</p>

<ul>
<li>一台主机可以同时运行许多不同的客户端和服务器</li>
<li>一个客户端和服务器的事务可以在同一台或是不同的主机上。</li>
</ul>

<h2 id="toc_1">2 网络</h2>

<p>对主机而言，网络只是又一种I/O设备，是数据源和数据接收方。物理上而言，网络是一个按照地理远近组成的层次系统。</p>

<h3 id="toc_2">2.1 网络层次系统</h3>

<p>(1) 最底层：以太网段<br/>
<strong>局域网</strong>(LAN, Local Area Network)的范围一般限制在一个建筑或者校园内。最流行的局域网技术是<strong>以太网</strong>(Ethernet)，由电缆和集线器(hub)组成一个以太网段。</p>

<p><img src="media/15320197756867/ethernet%20segment.png" alt="ethernet segment"/></p>

<p>(2) 桥接以太网<br/>
<strong>桥接以太网</strong>(bridged Ethernet)是将以太网段用电缆和网桥(bridge)连接成的较大的局域网。<br/>
<img src="media/15320197756867/bridged%20ethernet%20segment.png" alt="bridged ethernet segment"/></p>

<p>(3) 互联网络<br/>
多个不兼容的局域网可以通过路由器(routers)连接成互联网络(internets)。</p>

<p><img src="media/15320197756867/last_level_internets.png" alt="last_level_internets"/></p>

<h3 id="toc_3">2.2 网络协议</h3>

<p>互联网络是由各种局域网和广域网组成，它们采用完全不同且不兼容的技术。那么如何能让某台主机跨过所有不兼容的网络发送数据位到另一台目的主机呢？</p>

<p>解决方法：一层运行在每台主机和路由器上的<strong>协议</strong>软件，它消除了不同网络之间的差异。协议提供了两种基本能力：</p>

<ul>
<li>提供了命名机制
<ul>
<li>定义一致的<strong>主机地址</strong>(host adress)格式</li>
<li>每台主机会被分配至少一个<strong>互联网络地址</strong>(internet address)，地址唯一地标识了主机</li>
</ul></li>
<li><p>提供了传送机制</p>
<ul>
<li>定义了统一的基本传送单位-<strong>包</strong>(packet)</li>
<li>包由<strong>包头</strong>(header)和<strong>有效载荷</strong>(payload)组成
<ul>
<li>包头包括包的大小以及源主机和目的主机的地址</li>
<li>有效载荷包括从源主机发出的数据位</li>
</ul></li>
</ul>
<p><img src="media/15320197756867/Transferring%20Internet%20Data%20Via%20Encapsulation.png" alt="Transferring Internet Data Via Encapsulation"/></p></li>
</ul>

<p>PH: Internet packet header, 互联网络包头<br/>
FH: LAN frame header, 局域网帧头</p>

<h2 id="toc_4">3 全球IP因特网</h2>

<p>全球IP因特网(Global IP Internet)是最著名和最成功的互联网络(internet)实现。每台因特网主机都运行实现TCP/IP协议的软件，使用套接字接口(sockets interface)函数和Unix I/O函数来通信。</p>

<p><img src="media/15320197756867/hardware%20and%20software%20organization%20of%20an%20internet%20application.png" alt="hardware and software organization of an internet application"/></p>

<p>从程序员的角度：</p>

<ul>
<li>主机被映射为一组32位的<strong>IP地址</strong>(IP addresses)
<ul>
<li>128.2.203.179</li>
</ul></li>
<li>IP地址被映射为一组标识符，叫做<strong>域名</strong>(domain name)</li>
<li>因特网主机上的进程能够通过<strong>连接</strong>和任何其他因特网主机上的进程通信。</li>
</ul>

<h3 id="toc_5">3.1 IP地址</h3>

<p>32位IP地址存在一个IP地址结构(<code>in_addr</code>)中</p>

<ul>
<li>IP地址在内存中是以<strong>网络字节顺序</strong>(network byte order, 大端法)存放的</li>
</ul>

<pre><code class="language-c">/* Internet address structure */ 
struct in_addr { 
    uint32_t s_addr; /* network byte order (big-endian) */ 
};
</code></pre>

<h3 id="toc_6">3.2 域名</h3>

<p><strong>域名</strong>(domain names)是一串用句点分隔的单词(字母、数字和破折号)。域名集合形成了一个层次结构，可以表示为一棵树。</p>

<p><img src="media/15320197756867/Internet%20Domain%20Names.png" alt="Internet Domain Names"/></p>

<p><strong>域名系统</strong>(Domain Naming System, DNS)是映射IP地址和域名的数据库。可以把DNS数据库视为上百万的<strong>主机条目结构</strong>(host entry structure)的集合，其中每条定义了一组域名和一组IP地址之间的映射。</p>

<ul>
<li>DNS映射，可以通过<code>nsloopup</code>查看</li>
<li>在最简单的情况中，一个域名和一个IP地址之间是一一映射
<ul>
<li><code>nslookup whaleshark.ics.cs.cmu.edu</code> - <code>Address: 128.2.210.175</code></li>
</ul></li>
<li>然而，在某些情况下，多个域名可以映射为同一个IP地址
<ul>
<li><code>nslookup cs.mit.edu/ nslookup eecs.mit.edu</code> - <code>Address: 18.62.1.6</code></li>
</ul></li>
<li>在最通常的情况下，多个域名可以映射到同一组的多个IP地址
<ul>
<li><code>nslookup www.twitter.com</code> - <code>Address: 199.16.156.6</code>, <code>Address:199.16.156.70</code></li>
</ul></li>
</ul>

<h3 id="toc_7">3.3 因特网连接</h3>

<p>客户端和服务器通过<strong>连接</strong>(connections)发送字节流来通信，每一个连接都有如下特点：</p>

<ul>
<li>点对点(point-to-point)：连接一对进程</li>
<li>全双工(full-duplex)：数据可以同时在两个方向传送</li>
<li>可靠性(reliable)：发送和接收的字节流顺序相同</li>
</ul>

<p>套接字(sockets)是连接的端点，套接字地址用 “地址：端口”来表示。</p>

<ul>
<li>端口(port)是一个16位整数，标识了一个进程。
<ul>
<li>临时端口：当可会淡发起连接请求时，内核自动分配的端口</li>
<li>知名端口：和服务器提供的服务有短的端口 (
<ul>
<li>Web服务器使用端口80</li>
<li>ssh服务器使用端口22</li>
<li>email服务器使用端口25</li>
</ul></li>
</ul></li>
</ul>

<p>一个连接是由它两端的套接字地址唯一确定的（套接字对, socket pair）。</p>

<p><img src="media/15320197756867/socket%20pair.png" alt="socket pai"/></p>

<p>使用端口来识别服务</p>

<p><img src="media/15320197756867/using%20ports%20to%20identify%20services.png" alt="using ports to identify services"/></p>

<h2 id="toc_8">4 套接字接口</h2>

<p>什么是套接字？</p>

<ul>
<li>对于内核来说，套接字是通信的端点。 To the kernel, a socket is an endpoint of communication</li>
<li>对于应用来说，套接字是让应用从网络读写的文件描述符。 To an application, a socket is a file descriptor that lets the application read/write from/to the network.</li>
</ul>

<p>客户端和服务器通过对套接字描述符读写进行通信：</p>

<p><img src="media/15320197756867/client%20and%20servers%20communicate%20%20via%20socket%20descriptors.png" alt="client and servers communicate  via socket descriptors"/></p>

<p>(1) 通用套接字地址(generic socket address)：</p>

<ul>
<li>以套接字地址作为<code>connect()</code>, <code>bind()</code>, <code>accept</code>的实参</li>
<li>仅仅因为那时的C不存在<code>void *</code>指针，所以套接字接口被设计成这样。</li>
</ul>

<pre><code class="language-c">struct sockaddr { 
    uint16_t sa_family; /* Protocol family */
    char sa_data[14]; }; /* Address data. */
</code></pre>

<p>(2) 因特网的套接字地址</p>

<ul>
<li>必须将<code>struct sockaddr_in *</code> 转换为 <code>struct sockaddr *</code>才能以套接字地址作为函数实参</li>
</ul>

<pre><code class="language-c">struct sockaddr_in {
    uint16_t sin_family;
    uint16_t sin_port;
    struct in_addr sin_addr;
    unsigned char sin_zero[8];
    };
</code></pre>

<p><img src="media/15320197756867/sockets%20interface.png" alt="sockets interface"/></p>

<ol>
<li>开启服务器(start server)
<ul>
<li><code>getaddrinfo</code>: 把主机名(hostname）、主机地址(host addresses)、端口(ports)和服务名(service names)转换为套接字地址结构。</li>
<li><code>socket</code>: 创建一个套接字描述符(socket descriptor)，也就是之后用来读写的 file descriptor</li>
<li><code>bind</code>: 请求内核把套接字地址和套接字描述符绑定</li>
<li><code>listen</code>: 将套接字描述符从一个主动套接字转换为监听套接字(listening socket)，该套接字可以接受来自客户端的连接请求</li>
<li><code>accept</code>: 等待来自客户端的连接请求</li>
</ul></li>
<li>开启客户端(start client)
<ul>
<li><code>getaddrinfo</code>, <code>socket</code>与开启服务器相同</li>
<li><code>connect</code>: 试图与服务器建立连接</li>
</ul></li>
</ol>

<h3 id="toc_9">4.1 Echo客户端和服务器示例</h3>

<p>在和服务器建立连接之后，客户端进入一个循环，反复从标准输入读取文本行，发送文本行给服务器，从服务器读取回送的行，并输出结果到接准输出。</p>

<pre><code class="language-c">#include &quot;csapp.h&quot;
int main (int argc, char **argv) {
    int clientfd;
    char *host, *port, buf[MAXLINE];
    rio_t rio;
    
    host = argv[1];
    port = argv[2];
    
    //和服务器建立连接
    clientfd = Open_clientfd(host, port);
    Rio_readinitb(&amp;rio, clientfd);
    
    while (Fgets(buf, MAXLINE, stdin) != NULL) {
        // 写入，也就是向服务器发送信息
        Rio_writen(clientfd, buf, strlen(buf));
        // 读取，也就是从服务器接收信息
        Rio_readlineb(&amp;rio, buf, MAXLINE);
        // 把从服务器接收的信息显示在输出中
        Fputs(buf, stdout);
    }
    Close(clientfd);
    exit(0);
}
</code></pre>

<p>服务器在打开监听描述符后，进入一个无限循环。每次循环都等待一个来自客户端的连接请求，输出已连接客户端的域名和IP地址，并调用echo函数为这些客户端服务。在echo程序返回后，主程序关闭已连接描述符。</p>

<pre><code class="language-c">#include &quot;csapp.h&quot;
void echo(int connfd);
int main(int argc, char **argv){
    int listenfd, connfd;
    socklen_t clientlen;
    struct sockaddr_storage clientaddr; // Enough room for any addr
    char client_hostname[MAXLINE], client_port[MAXLINE];
    
    // 开启监听端口，注意只开这么一次
    listenfd = Open_listenfd(argv[1]);
    while (1) {
        // 需要具体的大小
        clientlen = sizeof(struct sockaddr_storage); // Important!
        // 等待连接
        connfd = Accept(listenfd, (SA *)&amp;clientaddr, &amp;clientlen);
        // 获取客户端相关信息
        Getnameinfo((SA *) &amp;clientaddr, clientlen, client_hostname,
                     MAXLINE, client_port, MAXLINE, 0);
        printf(&quot;Connected to (%s, %s)\n&quot;, client_hostname, client_port);
        // 服务器具体完成的工作
        echo(coonfd);
        Close(connfd);
    }
    exit(0);
}
void echo(int connfd) {
    size_t n;
    char buf[MAXLINE];
    rio_t rio;
    
    // 读取从客户端传输过来的数据
    Rio_readinitb(&amp;rio, connfd);
    while((n = Rio_readlineb(&amp;rio, buf, MAXLINE)) != 0) {
        printf(&quot;server received %d bytes\n&quot;, (int)n);
        // 把从 client 接收到的信息再写回去
        Rio_writen(connfd, buf, n);
    }
}
</code></pre>

<h2 id="toc_10">5 Web服务器</h2>

<h3 id="toc_11">5.1 Web基础</h3>

<p>Web客户端和服务器之间的交互用的是<strong>HTTP协议</strong>(超文本传输协议)，交互的基本过程为：</p>

<ul>
<li>客户端和服务器建立TCP连接</li>
<li>客户端请求内容</li>
<li>服务器响应请求的内容</li>
<li>服务器和客户端最终关闭 连接</li>
</ul>

<p><img src="media/15320197756867/web%20server%20basics.png" alt="web server basics"/></p>

<h3 id="toc_12">5.2 Web内容</h3>

<p>Web服务器返回内容给客户端，内容是与一个<strong>MIME</strong>类型相关的字节序列。(MIME -  Multipurpose Internet Mail Extensions)</p>

<p>HTTP响应返回的类型可以是静态的，也可以是动态的：</p>

<ul>
<li>静态内容：内容存储在文件中，响应HTTP请求后返回给客户端
<ul>
<li>例如HTML文件，图片，声音</li>
</ul></li>
<li>动态内容：运行一个可执行文件产生输出，返回给客户端</li>
</ul>

<h3 id="toc_13">5.3 HTTP事务</h3>

<p>一个<strong>HTTP请求</strong>(request)是一个<strong>请求行</strong>(request line)，后面跟随着零个或多个<strong>请求报头</strong>(request header)，再跟随一个终止报头的空行。</p>

<p><strong>请求行</strong>的格式是<code>&lt;method&gt; &lt;uri&gt; &lt;version&gt;</code>。</p>

<ul>
<li><code>&lt;method&gt;</code>可以是GET, POST, OPTIONS, HEAD, PUT, DELETE, TRAXE</li>
<li><code>&lt;uri&gt;</code>是响应的URL的后缀，包括文件名和可选的参数</li>
<li><code>&lt;version&gt;</code>是该请求遵循的HTTP的版本(HTTP/1.0或者HTTP/1.1)</li>
</ul>

<p><strong>请求报头</strong>的格式是<code>&lt;header name&gt;:&lt;header data&gt;</code></p>

<ul>
<li>为服务器提供额外信息，例如浏览器的商标名</li>
</ul>

<p><strong>HTTP响应</strong>与HTTP请求类似，是一个<strong>响应行</strong>(response line)，后面跟着零个或者多个<strong>响应报头</strong>(response header)，再跟随一个终止报头的空行，再跟随一个响应主体(response body)。</p>

<p><strong>响应行</strong>的格式是<code>&lt;version&gt; &lt;status code&gt; &lt;status message&gt;</code></p>

<ul>
<li><code>&lt;version&gt;</code>是响应所遵循的HTTP版本</li>
<li><code>&lt;status-code&gt;</code>是一个3位的正整数，指明对请求的处理</li>
<li><code>&lt;status-message&gt;</code> 英文描述</li>
</ul>

<p><strong>响应报头</strong>的格式是<code>&lt;header name&gt;:&lt;header data&gt;</code></p>

<p>下面是HTTP请求的一个实例<br/>
<img src="media/15320197756867/example%20HTTP%20transaction.png" alt="example HTTP transaction"/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/1/25</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Computer%20System.html'>Computer System</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15323640524161.html">
                
                  <h1>Machine Learning with large datasets</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">1 overview</h2>

<p>Why use big data?</p>

<ul>
<li>Simple learning methods with large data sets can outperform complex learners with smaller datasets</li>
<li>The ordering of learning methods, best-to-worst, can be different for small datasets than from large datasets</li>
<li>The best way to improve performance for a learning system is often to collect more data</li>
<li>Large datasets often imply large classifiers</li>
</ul>

<p>Asymptotic analysis</p>

<ul>
<li>It measures number of operations as function of problem size</li>
<li>Different operations (eg disk seeking, scanning, memory access) can have very very different costs</li>
<li>Disk access is cheapest when you scan sequentially</li>
</ul>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/7/24</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Computer%20System.html'>Computer System</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="Computer System.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="Computer System_2.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">他山之石，可以攻玉</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Leetcode.html"><strong>Leetcode</strong></a>
        
            <a href="programming_language.html"><strong>编程语言</strong></a>
        
            <a href="data_structure_and_algorithm.html"><strong>数据结构和算法</strong></a>
        
            <a href="Python%E7%89%B9%E6%80%A7.html"><strong>Python特性</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="English.html"><strong>English</strong></a>
        
            <a href="Computer%20System.html"><strong>Computer System</strong></a>
        
            <a href="Deep%20Learning.html"><strong>Deep Learning</strong></a>
        
            <a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html"><strong>Linux 系统编程</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html"><strong>数据库</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="Big%20Data.html"><strong>Big Data</strong></a>
        
            <a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html"><strong>文献阅读</strong></a>
        
            <a href="Tools.html"><strong>Tools</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15046649572570.html">Pandas</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="exceptional_control_flow.html">CSAPP - 异常控制流</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="introduction_to_computer_system_CMU.html">CMU 15-213 Introduction to Computer Systems</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os-concepts-os-structures.html">Operating System Concepts 2 - Operating System structures</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os-concets-processes.html">Operating System Concepts 3 - Processes</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
