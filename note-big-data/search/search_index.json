{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Zhenhua's Notes This site documents reading and learning notes. Other Note Site Note - Java/OS Note - Algorithm Search (function() { var cx = '011299089536274713055:ppqfpivtvxy'; var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true; gcse.src = 'https://cse.google.com/cse.js?cx=' + cx; var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s); })(); TOC See Table of Contents . Books and Materials See Books and Materials . Roadmap See Roadmap .","title":"Home"},{"location":"#zhenhuas-notes","text":"This site documents reading and learning notes.","title":"Zhenhua's Notes"},{"location":"#other-note-site","text":"Note - Java/OS Note - Algorithm","title":"Other Note Site"},{"location":"#search","text":"(function() { var cx = '011299089536274713055:ppqfpivtvxy'; var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true; gcse.src = 'https://cse.google.com/cse.js?cx=' + cx; var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s); })();","title":"Search"},{"location":"#toc","text":"See Table of Contents .","title":"TOC"},{"location":"#books-and-materials","text":"See Books and Materials .","title":"Books and Materials"},{"location":"#roadmap","text":"See Roadmap .","title":"Roadmap"},{"location":"books/","text":"Books and Materials Books The following is the primary reading list of books. Each chapter is organized as a single page; the included sections are noted with major concepts, and summary. Hadoop Hadoop: The Definitive Guide, 4th Edition by Tom White Spark Learning Spark: Lighting-Fast Data Analysis by Tom White GDM A Programmer's Guide to Data Mining by Ron Zacharski","title":"Books"},{"location":"books/#books-and-materials","text":"","title":"Books and Materials"},{"location":"books/#books","text":"The following is the primary reading list of books. Each chapter is organized as a single page; the included sections are noted with major concepts, and summary.","title":"Books"},{"location":"books/#hadoop","text":"Hadoop: The Definitive Guide, 4th Edition by Tom White","title":"Hadoop"},{"location":"books/#spark","text":"Learning Spark: Lighting-Fast Data Analysis by Tom White","title":"Spark"},{"location":"books/#gdm","text":"A Programmer's Guide to Data Mining by Ron Zacharski","title":"GDM"},{"location":"toc/","text":"Table of Contents Shortcuts APUE - x86 assembly - Bash - BD - C - CLRS - CNAPP - CNSPP - CSN - DDA - DevOps - x86 disassembly - Golang - GOPL - HTAE - ICND1 - ICND2 - iptables - LKD - LSP - Nginx - PER - PIC - Python - Ruby - SPEC - TCPIP - TCPv1 - TLPI - TWGR - UNP - UTLK - Vim Contents Name Status Progress APUE Backlog Almost Done x86 assembly Backlog Almost Done Bash Future Started BD Discontinued First Half C Stub - CLRS Discontinued Started CNAPP Future Started CNSPP Backlog Started CSN Active First Half DDA Active Started DevOps Future Second Half x86 disassembly Future Started Golang Discontinued First Half GOPL Active Almost Done HTAE Backlog Started ICND1 Future Started ICND2 Future Started iptables Stub - LKD Active Second Half LSP Backlog Started Nginx Stub - PER Future Started PIC Future Started Python Stub - Ruby Discontinued First Half SPEC Backlog First Half TCPIP Backlog Started TCPv1 Backlog First Half TLPI Backlog Started TWGR Backlog Started UNP Backlog First Half UTLK Backlog First Half Vim Stub - Note: Active items are of P0 priority per OKR , and are in bold. All other items in the OKR are in backlog . Future items are not in the OKR but are likely to be revisited in the future. Stub items are not intended as notes. For progress, started : 15%; first half : 15% ~ 50%; second half : 50% ~ 85%; almost done : 85% ~ 100%; complete : 100%. For full titles, see Books and Materials .","title":"Toc"},{"location":"toc/#table-of-contents","text":"","title":"Table of Contents"},{"location":"toc/#shortcuts","text":"APUE - x86 assembly - Bash - BD - C - CLRS - CNAPP - CNSPP - CSN - DDA - DevOps - x86 disassembly - Golang - GOPL - HTAE - ICND1 - ICND2 - iptables - LKD - LSP - Nginx - PER - PIC - Python - Ruby - SPEC - TCPIP - TCPv1 - TLPI - TWGR - UNP - UTLK - Vim","title":"Shortcuts"},{"location":"toc/#contents","text":"Name Status Progress APUE Backlog Almost Done x86 assembly Backlog Almost Done Bash Future Started BD Discontinued First Half C Stub - CLRS Discontinued Started CNAPP Future Started CNSPP Backlog Started CSN Active First Half DDA Active Started DevOps Future Second Half x86 disassembly Future Started Golang Discontinued First Half GOPL Active Almost Done HTAE Backlog Started ICND1 Future Started ICND2 Future Started iptables Stub - LKD Active Second Half LSP Backlog Started Nginx Stub - PER Future Started PIC Future Started Python Stub - Ruby Discontinued First Half SPEC Backlog First Half TCPIP Backlog Started TCPv1 Backlog First Half TLPI Backlog Started TWGR Backlog Started UNP Backlog First Half UTLK Backlog First Half Vim Stub - Note: Active items are of P0 priority per OKR , and are in bold. All other items in the OKR are in backlog . Future items are not in the OKR but are likely to be revisited in the future. Stub items are not intended as notes. For progress, started : 15%; first half : 15% ~ 50%; second half : 50% ~ 85%; almost done : 85% ~ 100%; complete : 100%. For full titles, see Books and Materials .","title":"Contents"},{"location":"MLA/","text":"MLA Chapter 1: \u673a\u5668\u5b66\u4e60\u57fa\u7840","title":"Home"},{"location":"MLA/#mla","text":"Chapter 1: \u673a\u5668\u5b66\u4e60\u57fa\u7840","title":"MLA"},{"location":"MLA/ch1/","text":"\u673a\u5668\u5b66\u4e60\u5b9e\u6218 1 - \u673a\u5668\u5b66\u4e60\u57fa\u7840","title":"Ch1"},{"location":"MLA/ch1/#1-","text":"","title":"\u673a\u5668\u5b66\u4e60\u5b9e\u6218 1 - \u673a\u5668\u5b66\u4e60\u57fa\u7840"},{"location":"gdm/","text":"GDM Chapter 1: \u63a8\u8350\u7cfb\u7edf\u5165\u95e8 Chapter 2: \u9690\u5f0f\u8bc4\u4ef7\u548c\u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\u7b97\u6cd5 Chapter 3: \u5206\u7c7b Chapter 4: \u8fdb\u4e00\u6b65\u63a2\u7d22\u5206\u7c7b Chapter 5: \u6982\u7387\u548c\u6734\u7d20\u8d1d\u53f6\u65af Chapter 6: \u6734\u7d20\u8d1d\u53f6\u65af\u548c\u6587\u672c\u6570\u636e Chapter 7: \u805a\u7c7b","title":"Contents"},{"location":"gdm/#gdm","text":"Chapter 1: \u63a8\u8350\u7cfb\u7edf\u5165\u95e8 Chapter 2: \u9690\u5f0f\u8bc4\u4ef7\u548c\u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\u7b97\u6cd5 Chapter 3: \u5206\u7c7b Chapter 4: \u8fdb\u4e00\u6b65\u63a2\u7d22\u5206\u7c7b Chapter 5: \u6982\u7387\u548c\u6734\u7d20\u8d1d\u53f6\u65af Chapter 6: \u6734\u7d20\u8d1d\u53f6\u65af\u548c\u6587\u672c\u6570\u636e Chapter 7: \u805a\u7c7b","title":"GDM"},{"location":"gdm/ch1/","text":"\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 1 \u63a8\u8350\u7cfb\u7edf\u5165\u95e8 \u672c\u7ae0\u5c06\u4ecb\u7ecd\u534f\u540c\u8fc7\u6ee4\uff0c\u57fa\u672c\u7684\u8ddd\u79bb\u7b97\u6cd5\uff0c\u6700\u540e\u4f7f\u7528Python\u5b9e\u73b0\u4e00\u4e2a\u7b80\u5355\u7684\u63a8\u8350\u7b97\u6cd5\u3002 \u534f\u540c\u8fc7\u6ee4\uff0c\u987e\u540d\u601d\u4e49\uff0c\u662f\u5229\u7528\u4ed6\u4eba\u7684\u559c\u597d\u6765\u8fdb\u884c\u63a8\u8350\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u662f\u5927\u5bb6\u4e00\u8d77\u4ea7\u751f\u7684\u63a8\u8350\u3002\u5b83\u7684\u5de5\u4f5c\u539f\u7406\u662f\uff0c\u5728\u7f51\u7ad9\u4e0a\u67e5\u627e\u4e00\u4e2a\u548c\u4f60\u7c7b\u4f3c\u7684\u7528\u6237\uff0c\u7136\u540e\u5c06\u5b83\u559c\u6b22\u7684\u4e66\u7c4d\u63a8\u8350\u7ed9\u4f60\u3002 \u5982\u4f55\u627e\u5230\u76f8\u4f3c\u7684\u7528\u6237\uff1f \u66fc\u54c8\u987f\u8ddd\u79bb \u987e\u540d\u601d\u4e49\uff0c\u5728\u66fc\u54c8\u987f\u8857\u533a\u8981\u4ece\u4e00\u4e2a\u5341\u5b57\u8def\u53e3\u5f00\u8f66\u5230\u53e6\u4e00\u4e2a\u5341\u5b57\u8def\u53e3\uff0c\u5b9e\u9645\u9a7e\u9a76\u8ddd\u79bb\u5c31\u662f\u201c\u66fc\u54c8\u987f\u8ddd\u79bb\u201d\u3002 \u6700\u7b80\u5355\u7684\u8ddd\u79bb\u8ba1\u7b97\u65b9\u5f0f\u662f\u66fc\u54c8\u987f\u8ddd\u79bb\u3002\u5728\u4e8c\u7ef4\u6a21\u578b\u4e2d\uff0c\u6bcf\u4e2a\u4eba\u90fd\u53ef\u4ee5\u7528 (x, y) (x, y) \u7684\u70b9\u6765\u8868\u793a\uff0c\u8fd9\u91cc\u7528\u4e0b\u6807\u6765\u8868\u793a\u4e0d\u540c\u7684\u4eba\uff0c (x_1, y_1) (x_1, y_1) \u8868\u793a\u827e\u7c73\uff0c (x_2, y_2) (x_2, y_2) \u8868\u793a\u795e\u79d8\u7684X\u5148\u751f\uff0c\u90a3\u4e48\u4ed6\u4eec\u4e4b\u95f4\u7684\u66fc\u54c8\u987f\u8ddd\u79bb\u5c31\u662f\uff1a |x_1-x_2|+|y_1-y_2| |x_1-x_2|+|y_1-y_2| \u66fc\u54c8\u987f\u8ddd\u79bb\u7684\u4f18\u70b9\u4e4b\u4e00\u662f\u8ba1\u7b97\u901f\u5ea6\u5feb\uff0c\u5bf9\u4e8eFacebook\u8fd9\u6837\u9700\u8981\u8ba1\u7b97\u767e\u4e07\u7528\u6237\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\u65f6\u5c31\u975e\u5e38\u6709\u5229\u3002 def manhattan ( rating1 , rating2 ): Computes the Manhattan distance. Both rating1 and rating2 are dictionaries of the form { The Strokes : 3.0, Slightly Stoopid : 2.5} distance = 0 commonRatings = False for key in rating1 : if key in rating2 : distance += abs ( rating1 [ key ] - rating2 [ key ]) commonRatings = True if commonRatings : return distance else : return - 1 #Indicates no ratings in common \u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb \u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5c31\u662f\u4e24\u70b9\u4e4b\u95f4\u7684\u76f4\u7ebf\u8ddd\u79bb\u3002 \u4e0b\u9762\u7684\u659c\u7ebf\u5c31\u662f\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\uff0c\u516c\u5f0f\u662f\uff1a \\sqrt{(x_1-x_2)^2+(y_1-y_2)^2} \\sqrt{(x_1-x_2)^2+(y_1-y_2)^2} \u66fc\u54c8\u987f\u8ddd\u79bb\u548c\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5728\u6570\u636e\u5b8c\u6574\u7684\u60c5\u51b5\u4e0b\u6548\u679c\u6700\u597d\u3002 def euclidean ( rating1 , rating2 ): Computes the Euclidean Distance :param rating1: rating :param rating2: rating :return: distance if common ratings exists, or -1 distance = 0 commonRatings = False for key in rating1 : if key in rating2 : distance += pow ( rating1 [ key ] - rating2 [ key ], 2 ) commonRatings = True if commonRatings : return distance else : return - 1 # Indicates no ratings in common \u95f5\u53ef\u592b\u65af\u57fa\u8ddd\u79bb \u6211\u4eec\u53ef\u4ee5\u5c06\u66fc\u54c8\u987f\u8ddd\u79bb\u548c\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5f52\u7eb3\u6210\u4e00\u4e2a\u516c\u5f0f\uff0c\u8fd9\u4e2a\u516c\u5f0f\u79f0\u4e3a\u95f5\u53ef\u592b\u65af\u57fa\u8ddd\u79bb(Minkowski Distance)\uff1a d(x,y) = (\\sum_{k=1}^{n}|x_k-y_k|^r)^{\\frac{1}{r}} d(x,y) = (\\sum_{k=1}^{n}|x_k-y_k|^r)^{\\frac{1}{r}} \u5176\u4e2d\uff1a r = 1 r = 1 , \u8be5\u516c\u5f0f\u5373\u66fc\u54c8\u987f\u8ddd\u79bb r = 2 r = 2 , \u8be5\u516c\u5f0f\u5373\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb r = \\infty r = \\infty , \u5207\u6bd4\u96ea\u592b\u8ddd\u79bb Note r r \u503c\u8d8a\u5927\uff0c\u5355\u4e2a\u7ef4\u5ea6\u7684\u5dee\u503c\u5927\u5c0f\u4f1a\u5bf9\u6574\u4f53\u8ddd\u79bb\u6709\u66f4\u5927\u7684\u5f71\u54cd\u3002 \u5207\u6bd4\u96ea\u592b\u8ddd\u79bb \u5207\u6bd4\u96ea\u592b\u8ddd\u79bb(Chebyshev Distance)\u662f\u5b9a\u4e49\u4e3a\u5176\u5404\u5750\u6807\u6570\u503c\u5dee\u7684\u6700\u5927\u503c\u3002 D_{\\rm Chebyshev}(x,y) = \\max_i(|x_i - y_i|)=\\lim_{k \\to \\infty} \\bigg( \\sum_{i=1}^n \\left| x_i - y_i \\right|^k \\bigg)^{1/k} D_{\\rm Chebyshev}(x,y) = \\max_i(|x_i - y_i|)=\\lim_{k \\to \\infty} \\bigg( \\sum_{i=1}^n \\left| x_i - y_i \\right|^k \\bigg)^{1/k} def chebyshev ( rating1 , rating2 ): Computes the Chebyshev Distance :param rating1: rating :param rating2: rating :return: distance if common ratings exists, or -1 distance = 0 commonRatings = False for key in rating1 : if key in rating2 : distance = max ( distance , abs ( rating1 [ key ] - rating2 [ key ])) commonRatings = True if commonRatings : return distance else : return - 1 # Indicates no ratings in common \u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570 \u8ba9\u6211\u4eec\u4ed4\u7ec6\u770b\u770b\u7528\u6237\u5bf9\u4e50\u961f\u7684\u8bc4\u5206\uff0c\u53ef\u4ee5\u53d1\u73b0\u6bcf\u4e2a\u7528\u6237\u7684\u6253\u5206\u6807\u51c6\u975e\u5e38\u4e0d\u540c\uff1a Bill\u6ca1\u6709\u6253\u51fa\u6781\u7aef\u7684\u5206\u6570\uff0c\u90fd\u57282\u81f34\u5206\u4e4b\u95f4\uff1b Jordyn\u4f3c\u4e4e\u559c\u6b22\u6240\u6709\u7684\u4e50\u961f\uff0c\u6253\u5206\u90fd\u57284\u81f35\u4e4b\u95f4\uff1b Hailey\u662f\u4e00\u4e2a\u6709\u8da3\u7684\u4eba\uff0c\u4ed6\u7684\u5206\u6570\u4e0d\u662f1\u5c31\u662f4\u3002 \u90a3\u4e48\uff0c\u5982\u4f55\u6bd4\u8f83\u8fd9\u4e9b\u7528\u6237\u5462\uff1f\u6bd4\u5982Hailey\u76844\u5206\u76f8\u5f53\u4e8eJordan\u76844\u5206\u8fd8\u662f5\u5206\u5462\uff1f\u6211\u89c9\u5f97\u66f4\u63a5\u8fd15\u5206\u3002\u8fd9\u6837\u4e00\u6765\u5c31\u4f1a\u5f71\u54cd\u5230\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u4e86\u3002Clara\u6700\u4f4e\u7ed9\u4e864\u5206\u2014\u2014\u5979\u6240\u6709\u7684\u6253\u5206\u90fd\u57284\u81f35\u5206\u4e4b\u95f4\uff0c\u8fd9\u79cd\u73b0\u8c61\u5728\u6570\u636e\u6316\u6398\u9886\u57df\u79f0\u4e3a \u5206\u6570\u81a8\u80c0 \u3002 \u89e3\u51b3\u65b9\u6cd5\u4e4b\u4e00\u662f\u4f7f\u7528\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570, \u7528\u4e8e\u5ea6\u91cf\u4e24\u4e2a\u53d8\u91cfX\u548cY\u4e4b\u95f4\u7684\u76f8\u5173(\u7ebf\u6027\u76f8\u5173)\uff0c\u5176\u503c\u4ecb\u4e8e-1\u4e0e1\u4e4b\u95f4, 1\u8868\u793a\u5b8c\u5168\u543b\u5408\uff0c-1\u8868\u793a\u5b8c\u5168\u76f8\u6096\u3002\u4e0b\u9762\u662f\u5e38\u89c1\u7684\u51e0\u7ec4 (x, y) (x, y) \u70b9\u96c6\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u3002 \u4e24\u4e2a\u53d8\u91cf\u4e4b\u95f4\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u5b9a\u4e49\u4e3a\u4e24\u4e2a\u53d8\u91cf\u4e4b\u95f4\u7684\u534f\u65b9\u5dee( \\text{cov}(X,Y) \\text{cov}(X,Y) )\u548c\u6807\u51c6\u5dee( \\sigma_X \\sigma_X )\u7684\u5546\uff1a \\rho_{X,Y}={\\mathrm{cov}(X,Y) \\over \\sigma_X \\sigma_Y} ={E[(X-\\mu_X)(Y-\\mu_Y)] \\over \\sigma_X\\sigma_Y} \\rho_{X,Y}={\\mathrm{cov}(X,Y) \\over \\sigma_X \\sigma_Y} ={E[(X-\\mu_X)(Y-\\mu_Y)] \\over \\sigma_X\\sigma_Y} \u5bf9\u4e8e\u6837\u672c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570: r_{xy}=\\frac{\\sum x_iy_i-n \\bar{x} \\bar{y}}{(n-1) s_x s_y}=\\frac{n\\sum x_iy_i-\\sum x_i\\sum y_i}{\\sqrt{n\\sum x_i^2-(\\sum x_i)^2}~\\sqrt{n\\sum y_i^2-(\\sum y_i)^2}}. r_{xy}=\\frac{\\sum x_iy_i-n \\bar{x} \\bar{y}}{(n-1) s_x s_y}=\\frac{n\\sum x_iy_i-\\sum x_i\\sum y_i}{\\sqrt{n\\sum x_i^2-(\\sum x_i)^2}~\\sqrt{n\\sum y_i^2-(\\sum y_i)^2}}. \u4ee5\u4e0a\u65b9\u7a0b\u7ed9\u51fa\u4e86\u8ba1\u7b97\u6837\u672c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u7b80\u5355\u7684\u5355\u6d41\u7a0b\u7b97\u6cd5\uff0c\u4f46\u662f\u5176\u4f9d\u8d56\u4e8e\u6d89\u53ca\u5230\u7684\u6570\u636e\uff0c\u6709\u65f6\u5b83\u53ef\u80fd\u662f\u6570\u503c\u4e0d\u7a33\u5b9a\u7684\u3002\u4f46\u5b83\u6700\u5927\u7684\u4f18\u70b9\u662f\uff0c\u7528\u4ee3\u7801\u5b9e\u73b0\u7684\u65f6\u5019\u53ef\u4ee5\u53ea\u904d\u5386\u4e00\u6b21\u6570\u636e\u3002 def pearson ( rating1 , rating2 ): Compute pearson coefficient :param rating1: a dictionary :param rating2: a dictionary :return: pearson coefficient sum_xy = 0 sum_x = 0 sum_y = 0 sum_x2 = 0 sum_y2 = 0 n = 0 commonRatings = False for key in rating1 : if key in rating2 : n += 1 x = rating1 [ key ] y = rating2 [ key ] sum_xy += x * y sum_x += x sum_y += y sum_x2 += pow ( x , 2 ) sum_y2 += pow ( y , 2 ) commonRatings = True if not commonRatings : return - 1 # now compute denominator denominator = math . sqrt ( sum_x2 - pow ( sum_x , 2 ) / n ) * math . sqrt ( sum_y2 - pow ( sum_y , 2 ) / n ) if denominator == 0 : return 0 else : return ( sum_xy - ( sum_x * sum_y ) / n ) / denominator \u4f59\u5f26\u76f8\u4f3c\u5ea6 \u5f53\u6211\u4eec\u75281500\u4e07\u9996\u6b4c\u66f2\u6765\u6bd4\u8f83\u4e24\u4e2a\u7528\u6237\u65f6\uff0c\u5f88\u6709\u53ef\u80fd\u4ed6\u4eec\u4e4b\u95f4\u6ca1\u6709\u4efb\u4f55\u4ea4\u96c6\uff0c\u8fd9\u6837\u4e00\u6765\u5c31\u65e0\u4ece\u8ba1\u7b97\u4ed6\u4eec\u4e4b\u95f4\u7684\u8ddd\u79bb\u4e86\u3002\u7c7b\u4f3c\u7684\u60c5\u51b5\u662f\u5728\u8ba1\u7b97\u4e24\u7bc7\u6587\u7ae0\u7684\u76f8\u4f3c\u5ea6\u65f6\u3002\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u8ba1\u7b97\u4e2d\u4f1a\u7565\u8fc7\u8fd9\u4e9b\u975e\u96f6\u503c\u3002\u5b83\u7684\u8ba1\u7b97\u516c\u5f0f\u662f\uff1a \\cos(x,y) = \\frac{x\\cdot y}{||x||\\times||y||} \\cos(x,y) = \\frac{x\\cdot y}{||x||\\times||y||} \u5176\u4e2d\uff0c \\cdot \\cdot \u53f7\u8868\u793a\u6570\u91cf\u79ef\u3002 ||x|| ||x|| \u8868\u793a\u5411\u91cf x x \u7684\u6a21\u3002 \u4f59\u5f26\u76f8\u4f3c\u5ea6\u5728\u6587\u672c\u6316\u6398\u4e2d\u5e94\u7528\u5f97\u8f83\u591a\uff0c\u5728\u534f\u540c\u8fc7\u6ee4\u4e2d\u4e5f\u4f1a\u4f7f\u7528\u5230\u3002 \u5e94\u8be5\u4f7f\u7528\u54ea\u79cd\u76f8\u4f3c\u5ea6\uff1f \u5982\u679c\u6570\u636e\u5b58\u5728\u201c\u5206\u6570\u81a8\u80c0\u201d\u95ee\u9898\uff0c\u5c31\u4f7f\u7528\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u3002 \u5982\u679c\u6570\u636e\u6bd4\u8f83\u201c\u5bc6\u96c6\u201d\uff0c\u53d8\u91cf\u4e4b\u95f4\u57fa\u672c\u90fd\u5b58\u5728\u516c\u6709\u503c\uff0c\u4e14\u8fd9\u4e9b\u8ddd\u79bb\u6570\u636e\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u90a3\u5c31\u4f7f\u7528\u6b27\u51e0\u91cc\u5f97\u6216\u66fc\u54c8\u987f\u8ddd\u79bb\u3002 \u5982\u679c\u6570\u636e\u662f\u7a00\u758f\u7684\uff0c\u5219\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3002 Note \u5728\u6570\u636e\u6807\u51c6\u5316( \\mu=0,\\sigma=1 \\mu=0,\\sigma=1 \uff09\u540e\uff0cPearson\u76f8\u5173\u6027\u7cfb\u6570\u3001\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9\u53ef\u8ba4\u4e3a\u662f\u7b49\u4ef7\u7684[ 1 ]\u3002 kNN \u4e0a\u9762\u7684\u505a\u6cd5\u4e2d\uff0c\u6211\u4eec\u53ea\u4f9d\u9760\u6700\u76f8\u4f3c\u7684\u4e00\u4e2a\u7528\u6237\u6765\u505a\u63a8\u8350\uff0c\u5982\u679c\u8fd9\u4e2a\u7528\u6237\u6709\u4e9b\u7279\u6b8a\u7684\u504f\u597d\uff0c\u5c31\u4f1a\u76f4\u63a5\u53cd\u6620\u5728\u63a8\u8350\u5185\u5bb9\u91cc\u3002\u89e3\u51b3\u65b9\u6cd5\u4e4b\u4e00\u662f\u627e\u5bfb\u591a\u4e2a\u76f8\u4f3c\u7684\u7528\u6237\uff0c\u8fd9\u91cc\u5c31\u8981\u7528\u5230K\u6700\u90bb\u8fd1\u7b97\u6cd5\u4e86\u3002 \u5728\u534f\u540c\u8fc7\u6ee4\u4e2d\u53ef\u4ee5\u4f7f\u7528K\u6700\u90bb\u8fd1\u7b97\u6cd5\u6765\u627e\u51faK\u4e2a\u6700\u76f8\u4f3c\u7684\u7528\u6237\uff0c\u4ee5\u6b64\u4f5c\u4e3a\u63a8\u8350\u7684\u57fa\u7840\u3002\u4e0d\u540c\u7684 \u5e94\u7528\u6709\u4e0d\u540c\u7684K\u503c\uff0c\u9700\u8981\u505a\u4e00\u4e9b\u5b9e\u9a8c\u6765\u5f97\u51fa\u3002\u4ee5\u4e0b\u7ed9\u5230\u8bfb\u8005\u4e00\u4e2a\u57fa\u672c\u7684\u601d\u8def\u3002 \u5047\u8bbe\u6211\u8981\u4e3aAnn\u505a\u63a8\u8350\uff0c\u5e76\u4ee4K=3\u3002\u4f7f\u7528\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u5f97\u5230\u7684\u7ed3\u679c\u662f\uff1a Person Pearson Sally 0.8 Eric 0.7 Amanda 0.5 \u8fd9\u4e09\u4e2a\u4eba\u90fd\u4f1a\u5bf9\u63a8\u8350\u7ed3\u679c\u6709\u6240\u8d21\u732e\uff0c\u95ee\u9898\u5728\u4e8e\u6211\u4eec\u5982\u4f55\u786e\u5b9a\u4ed6\u4eec\u7684\u6bd4\u91cd\u5462\uff1f \u6211\u4eec\u76f4\u63a5\u7528\u76f8\u5173\u7cfb\u6570\u7684\u6bd4\u91cd\u6765\u63cf\u8ff0\uff0cSally\u7684\u6bd4\u91cd\u662f0.8/2=40%\uff0cEric\u662f0.7/2=35%\uff0cAmanda \u5219\u662f25%\uff1a \u5047\u8bbe\u4ed6\u4eec\u4e09\u4eba\u5bf9Grey Wardens\u7684\u8bc4\u5206\u4ee5\u53ca\u52a0\u6743\u540e\u7684\u7ed3\u679c\u5982\u4e0b\uff1a Person Grey Wardens Rating Influence Sally 4.5 25% Eric 5 35% Amanda 3.5 40% \u6700\u540e\u8ba1\u7b97\u5f97\u5230\u7684\u5206\u6570\u4e3a\u4e3a\u52a0\u6743\u548c 4.5\\times 25\\% + 5\\times 35\\% + 3.5 \\times 40\\% 4.5\\times 25\\% + 5\\times 35\\% + 3.5 \\times 40\\% \u3002 Python\u63a8\u8350\u6a21\u5757 Cai-Nicolas Zeigler\u4ece\u56fe\u4e66\u6f02\u6d41\u7ad9\u6536\u96c6\u4e86\u8d85\u8fc7100\u4e07\u6761\u8bc4\u4ef7\u6570\u636e\u2014\u2014278,858\u4f4d\u7528\u6237\u4e3a271,379\u672c\u4e66\u6253\u4e86\u5206\u3002\u6570\u636e\u53ef\u4ee5\u4ece\u8fd9\u4e2a \u5730\u5740 \u83b7\u5f97\u3002 CSV\u6587\u4ef6\u5305\u542b\u4e86\u4e09\u5f20\u8868\uff1a \u7528\u6237\u8868\uff0c\u5305\u62ec\u7528\u6237ID\u3001\u4f4d\u7f6e\u3001\u5e74\u9f84\u7b49\u4fe1\u606f\u3002\u5176\u4e2d\u7528\u6237\u7684\u59d3\u540d\u5df2\u7ecf\u9690\u53bb\uff1b \u4e66\u7c4d\u8868\uff0c\u5305\u62ecISBN\u53f7\u3001\u6807\u9898\u3001\u4f5c\u8005\u3001\u51fa\u7248\u65e5\u671f\u3001\u51fa\u7248\u793e\u7b49\uff1b \u8bc4\u5206\u8868\uff0c\u5305\u62ec\u7528\u6237ID\u3001\u4e66\u7c4dISBN\u53f7\u3001\u4ee5\u53ca\u8bc4\u5206\uff080-10\u5206\uff09\u3002 Recommender class Recommender : def __init__ ( self , books , users , user_ratings , book_ratings ): initialize basic data :param books: a dictionary of books, whose key is book id :param users: a dictionary of users, whose key is user id :param book_ratings: a dictionary of book ratings, whose key is book id :param user_ratings: a dictionary of user ratings, whose key is user id self . books = books self . users = users self . book_ratings = book_ratings self . user_ratings = user_ratings def recommend ( self , user_to_recommend_int , k = 1 ): Recommend user books :param user_to_recommend_int: int, user id :param k : int, for nearest k neighbors :return: a list of books user_to_recommend = str ( user_to_recommend_int ) if user_to_recommend not in self . users : raise Exception ( user does not exist!! ) # find the user having min distances from user_to_recommend distances = [] find_user = False for user in self . users : if user_to_recommend == user : continue # extract user ratings based on user ids, # and compute the distance between them distance = Distance . pearson ( self . user_ratings [ user_to_recommend ], self . user_ratings [ user ]) if distance != - 1 : distances . append ([ user , distance ]) find_user = True if not find_user : return [] # sort user based on their distances # pearson \u7cfb\u6570\u8d8a\u5927\uff0c\u8ddd\u79bb\u8d8a\u8fd1\uff0c\u6240\u4ee5\u7528reverse distances . sort ( key = lambda x : x [ 1 ], reverse = True ) # compute weight based on distances distances = distances [ 0 : k ] sum_distance = sum ([ distance for user , distance in distances ]) for i in range ( len ( distances )): distances [ i ][ 1 ] /= sum_distance # recommend books books_to_recommend = {} for user_id , weight in distances : for book_id in self . user_ratings [ user_id ]: if book_id not in self . user_ratings [ user_to_recommend ]: # the user haven t seen if book_id not in books_to_recommend : # haven t recommend books_to_recommend [ book_id ] = self . user_ratings [ user_id ][ book_id ] * weight else : books_to_recommend [ book_id ] = books_to_recommend [ book_id ] \\ + self . user_ratings [ user_id ][ book_id ] * weight # transform to a list of tuple books_to_recommend = [( book_id , project_rating ) for book_id , project_rating in books_to_recommend . items ()] # sort based on project_rating books_to_recommend . sort ( key = lambda x : x [ 1 ], reverse = True ) # extract book title books_to_recommend = [ self . books [ book_id ][ title ] for book_id , project_rating in books_to_recommend ] return books_to_recommend if __name__ == __main__ : ratings = BooksImport () books , users , user_ratings , book_ratings = ratings . recommender_import () test = Recommender ( books , users , user_ratings , book_ratings ) print ( test . recommend ( 171118 )) Distance import math class Distance : Compute distance of two users, having different ratings. Both rating1 and rating2 are dictionaries of the form { The Strokes : 3.0, Slightly Stoopid : 2.5} def __init__ ( self ): pass @staticmethod def manhattan ( rating1 , rating2 ): Computes the Manhattan distance. distance = 0 common_ratings = False for key in rating1 : if key in rating2 : distance += abs ( rating1 [ key ] - rating2 [ key ]) common_ratings = True if common_ratings : return distance else : return - 1 # Indicates no ratings in common @staticmethod def euclidean ( rating1 , rating2 ): Computes the Euclidean Distance :param rating1: rating :param rating2: rating :return: distance if common ratings exists, or -1 distance = 0 commonRatings = False for key in rating1 : if key in rating2 : distance += pow ( rating1 [ key ] - rating2 [ key ], 2 ) commonRatings = True if commonRatings : return distance else : return - 1 # Indicates no ratings in common @staticmethod def chebyshev ( rating1 , rating2 ): Computes the Chebyshev Distance :param rating1: rating :param rating2: rating :return: distance if common ratings exists, or -1 distance = 0 commonRatings = False for key in rating1 : if key in rating2 : distance = max ( distance , abs ( rating1 [ key ] - rating2 [ key ])) commonRatings = True if commonRatings : return distance else : return - 1 # Indicates no ratings in common @staticmethod def pearson ( rating1 , rating2 ): Compute pearson coefficient :param rating1: a dictionary :param rating2: a dictionary :return: pearson coefficient sum_xy = 0 sum_x = 0 sum_y = 0 sum_x2 = 0 sum_y2 = 0 n = 0 commonRatings = False for key in rating1 : if key in rating2 : n += 1 x = rating1 [ key ] y = rating2 [ key ] sum_xy += x * y sum_x += x sum_y += y sum_x2 += pow ( x , 2 ) sum_y2 += pow ( y , 2 ) commonRatings = True if not commonRatings : return - 1 # now compute denominator denominator = math . sqrt ( sum_x2 - pow ( sum_x , 2 ) / n ) \\ * math . sqrt ( sum_y2 - pow ( sum_y , 2 ) / n ) if denominator == 0 : return 0 else : return ( sum_xy - ( sum_x * sum_y ) / n ) / denominator Books_import class BooksImport : def __init__ ( self ): self . books = {} self . users = {} self . book_ratings = {} self . user_ratings = {} self . bx_books_import () self . bx_users_import () self . bx_ratings_import () def bx_books_import ( self ): import books meta information try : booksfile = codecs . open ( BX-Dump/BX-Books.csv , r , utf-8 ) for line in booksfile : props = line . split ( ; ) isbn = props [ 0 ] . strip ( ) title = props [ 1 ] . strip ( ) author = props [ 2 ] . strip ( ) year = props [ 3 ] . strip ( ) self . books [ isbn ] = { title : title , author : author , year : year } booksfile . close () except IOError as e : error = Failed to load: {0} . format ( e ) print ( error ) def bx_users_import ( self ): import user meta information user is a dictionary, whose key is user_id try : users_file = codecs . open ( BX-Dump/BX-Users.csv , r , utf--8 ) for line in users_file : props = line . split ( ; ) user_id = props [ 0 ] . strip ( ) location = props [ 1 ] . strip ( ) self . users [ user_id ] = location self . user_ratings [ user_id ] = {} users_file . close () except IOError as e : error = Failed to load: {0} . format ( e ) print ( error ) def bx_ratings_import ( self ): try : ratings_file = codecs . open ( BX-Dump/BX-Book-Ratings.csv , r , utf--8 ) for line in ratings_file : props = line . split ( ; ) user_id = props [ 0 ] . strip ( ) book_id = props [ 1 ] . strip ( ) rating = int ( props [ 2 ] . strip () . strip ( )) if book_id in self . book_ratings : self . book_ratings [ book_id ] . append ( rating ) else : self . book_ratings [ book_id ] = [ rating ] self . user_ratings [ user_id ][ book_id ] = rating ratings_file . close () except IOError as e : error = Failed to load: {0} . format ( e ) print ( error ) def get_books ( self ): return self . books def get_users ( self ): return self . users def get_user_ratings ( self ): return self . user_ratings def get_book_ratings ( self ): return self . book_ratings def recommender_import ( self ): return self . books , self . users , self . user_ratings , self . book_ratings","title":"Chapter 1: \u63a8\u8350\u7cfb\u7edf\u5165\u95e8"},{"location":"gdm/ch1/#-1","text":"\u672c\u7ae0\u5c06\u4ecb\u7ecd\u534f\u540c\u8fc7\u6ee4\uff0c\u57fa\u672c\u7684\u8ddd\u79bb\u7b97\u6cd5\uff0c\u6700\u540e\u4f7f\u7528Python\u5b9e\u73b0\u4e00\u4e2a\u7b80\u5355\u7684\u63a8\u8350\u7b97\u6cd5\u3002 \u534f\u540c\u8fc7\u6ee4\uff0c\u987e\u540d\u601d\u4e49\uff0c\u662f\u5229\u7528\u4ed6\u4eba\u7684\u559c\u597d\u6765\u8fdb\u884c\u63a8\u8350\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u662f\u5927\u5bb6\u4e00\u8d77\u4ea7\u751f\u7684\u63a8\u8350\u3002\u5b83\u7684\u5de5\u4f5c\u539f\u7406\u662f\uff0c\u5728\u7f51\u7ad9\u4e0a\u67e5\u627e\u4e00\u4e2a\u548c\u4f60\u7c7b\u4f3c\u7684\u7528\u6237\uff0c\u7136\u540e\u5c06\u5b83\u559c\u6b22\u7684\u4e66\u7c4d\u63a8\u8350\u7ed9\u4f60\u3002 \u5982\u4f55\u627e\u5230\u76f8\u4f3c\u7684\u7528\u6237\uff1f","title":"\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 1 \u63a8\u8350\u7cfb\u7edf\u5165\u95e8"},{"location":"gdm/ch1/#_1","text":"\u987e\u540d\u601d\u4e49\uff0c\u5728\u66fc\u54c8\u987f\u8857\u533a\u8981\u4ece\u4e00\u4e2a\u5341\u5b57\u8def\u53e3\u5f00\u8f66\u5230\u53e6\u4e00\u4e2a\u5341\u5b57\u8def\u53e3\uff0c\u5b9e\u9645\u9a7e\u9a76\u8ddd\u79bb\u5c31\u662f\u201c\u66fc\u54c8\u987f\u8ddd\u79bb\u201d\u3002 \u6700\u7b80\u5355\u7684\u8ddd\u79bb\u8ba1\u7b97\u65b9\u5f0f\u662f\u66fc\u54c8\u987f\u8ddd\u79bb\u3002\u5728\u4e8c\u7ef4\u6a21\u578b\u4e2d\uff0c\u6bcf\u4e2a\u4eba\u90fd\u53ef\u4ee5\u7528 (x, y) (x, y) \u7684\u70b9\u6765\u8868\u793a\uff0c\u8fd9\u91cc\u7528\u4e0b\u6807\u6765\u8868\u793a\u4e0d\u540c\u7684\u4eba\uff0c (x_1, y_1) (x_1, y_1) \u8868\u793a\u827e\u7c73\uff0c (x_2, y_2) (x_2, y_2) \u8868\u793a\u795e\u79d8\u7684X\u5148\u751f\uff0c\u90a3\u4e48\u4ed6\u4eec\u4e4b\u95f4\u7684\u66fc\u54c8\u987f\u8ddd\u79bb\u5c31\u662f\uff1a |x_1-x_2|+|y_1-y_2| |x_1-x_2|+|y_1-y_2| \u66fc\u54c8\u987f\u8ddd\u79bb\u7684\u4f18\u70b9\u4e4b\u4e00\u662f\u8ba1\u7b97\u901f\u5ea6\u5feb\uff0c\u5bf9\u4e8eFacebook\u8fd9\u6837\u9700\u8981\u8ba1\u7b97\u767e\u4e07\u7528\u6237\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\u65f6\u5c31\u975e\u5e38\u6709\u5229\u3002 def manhattan ( rating1 , rating2 ): Computes the Manhattan distance. Both rating1 and rating2 are dictionaries of the form { The Strokes : 3.0, Slightly Stoopid : 2.5} distance = 0 commonRatings = False for key in rating1 : if key in rating2 : distance += abs ( rating1 [ key ] - rating2 [ key ]) commonRatings = True if commonRatings : return distance else : return - 1 #Indicates no ratings in common","title":"\u66fc\u54c8\u987f\u8ddd\u79bb"},{"location":"gdm/ch1/#_2","text":"\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5c31\u662f\u4e24\u70b9\u4e4b\u95f4\u7684\u76f4\u7ebf\u8ddd\u79bb\u3002 \u4e0b\u9762\u7684\u659c\u7ebf\u5c31\u662f\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\uff0c\u516c\u5f0f\u662f\uff1a \\sqrt{(x_1-x_2)^2+(y_1-y_2)^2} \\sqrt{(x_1-x_2)^2+(y_1-y_2)^2} \u66fc\u54c8\u987f\u8ddd\u79bb\u548c\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5728\u6570\u636e\u5b8c\u6574\u7684\u60c5\u51b5\u4e0b\u6548\u679c\u6700\u597d\u3002 def euclidean ( rating1 , rating2 ): Computes the Euclidean Distance :param rating1: rating :param rating2: rating :return: distance if common ratings exists, or -1 distance = 0 commonRatings = False for key in rating1 : if key in rating2 : distance += pow ( rating1 [ key ] - rating2 [ key ], 2 ) commonRatings = True if commonRatings : return distance else : return - 1 # Indicates no ratings in common","title":"\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb"},{"location":"gdm/ch1/#_3","text":"\u6211\u4eec\u53ef\u4ee5\u5c06\u66fc\u54c8\u987f\u8ddd\u79bb\u548c\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5f52\u7eb3\u6210\u4e00\u4e2a\u516c\u5f0f\uff0c\u8fd9\u4e2a\u516c\u5f0f\u79f0\u4e3a\u95f5\u53ef\u592b\u65af\u57fa\u8ddd\u79bb(Minkowski Distance)\uff1a d(x,y) = (\\sum_{k=1}^{n}|x_k-y_k|^r)^{\\frac{1}{r}} d(x,y) = (\\sum_{k=1}^{n}|x_k-y_k|^r)^{\\frac{1}{r}} \u5176\u4e2d\uff1a r = 1 r = 1 , \u8be5\u516c\u5f0f\u5373\u66fc\u54c8\u987f\u8ddd\u79bb r = 2 r = 2 , \u8be5\u516c\u5f0f\u5373\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb r = \\infty r = \\infty , \u5207\u6bd4\u96ea\u592b\u8ddd\u79bb Note r r \u503c\u8d8a\u5927\uff0c\u5355\u4e2a\u7ef4\u5ea6\u7684\u5dee\u503c\u5927\u5c0f\u4f1a\u5bf9\u6574\u4f53\u8ddd\u79bb\u6709\u66f4\u5927\u7684\u5f71\u54cd\u3002","title":"\u95f5\u53ef\u592b\u65af\u57fa\u8ddd\u79bb"},{"location":"gdm/ch1/#_4","text":"\u5207\u6bd4\u96ea\u592b\u8ddd\u79bb(Chebyshev Distance)\u662f\u5b9a\u4e49\u4e3a\u5176\u5404\u5750\u6807\u6570\u503c\u5dee\u7684\u6700\u5927\u503c\u3002 D_{\\rm Chebyshev}(x,y) = \\max_i(|x_i - y_i|)=\\lim_{k \\to \\infty} \\bigg( \\sum_{i=1}^n \\left| x_i - y_i \\right|^k \\bigg)^{1/k} D_{\\rm Chebyshev}(x,y) = \\max_i(|x_i - y_i|)=\\lim_{k \\to \\infty} \\bigg( \\sum_{i=1}^n \\left| x_i - y_i \\right|^k \\bigg)^{1/k} def chebyshev ( rating1 , rating2 ): Computes the Chebyshev Distance :param rating1: rating :param rating2: rating :return: distance if common ratings exists, or -1 distance = 0 commonRatings = False for key in rating1 : if key in rating2 : distance = max ( distance , abs ( rating1 [ key ] - rating2 [ key ])) commonRatings = True if commonRatings : return distance else : return - 1 # Indicates no ratings in common","title":"\u5207\u6bd4\u96ea\u592b\u8ddd\u79bb"},{"location":"gdm/ch1/#_5","text":"\u8ba9\u6211\u4eec\u4ed4\u7ec6\u770b\u770b\u7528\u6237\u5bf9\u4e50\u961f\u7684\u8bc4\u5206\uff0c\u53ef\u4ee5\u53d1\u73b0\u6bcf\u4e2a\u7528\u6237\u7684\u6253\u5206\u6807\u51c6\u975e\u5e38\u4e0d\u540c\uff1a Bill\u6ca1\u6709\u6253\u51fa\u6781\u7aef\u7684\u5206\u6570\uff0c\u90fd\u57282\u81f34\u5206\u4e4b\u95f4\uff1b Jordyn\u4f3c\u4e4e\u559c\u6b22\u6240\u6709\u7684\u4e50\u961f\uff0c\u6253\u5206\u90fd\u57284\u81f35\u4e4b\u95f4\uff1b Hailey\u662f\u4e00\u4e2a\u6709\u8da3\u7684\u4eba\uff0c\u4ed6\u7684\u5206\u6570\u4e0d\u662f1\u5c31\u662f4\u3002 \u90a3\u4e48\uff0c\u5982\u4f55\u6bd4\u8f83\u8fd9\u4e9b\u7528\u6237\u5462\uff1f\u6bd4\u5982Hailey\u76844\u5206\u76f8\u5f53\u4e8eJordan\u76844\u5206\u8fd8\u662f5\u5206\u5462\uff1f\u6211\u89c9\u5f97\u66f4\u63a5\u8fd15\u5206\u3002\u8fd9\u6837\u4e00\u6765\u5c31\u4f1a\u5f71\u54cd\u5230\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u4e86\u3002Clara\u6700\u4f4e\u7ed9\u4e864\u5206\u2014\u2014\u5979\u6240\u6709\u7684\u6253\u5206\u90fd\u57284\u81f35\u5206\u4e4b\u95f4\uff0c\u8fd9\u79cd\u73b0\u8c61\u5728\u6570\u636e\u6316\u6398\u9886\u57df\u79f0\u4e3a \u5206\u6570\u81a8\u80c0 \u3002 \u89e3\u51b3\u65b9\u6cd5\u4e4b\u4e00\u662f\u4f7f\u7528\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570, \u7528\u4e8e\u5ea6\u91cf\u4e24\u4e2a\u53d8\u91cfX\u548cY\u4e4b\u95f4\u7684\u76f8\u5173(\u7ebf\u6027\u76f8\u5173)\uff0c\u5176\u503c\u4ecb\u4e8e-1\u4e0e1\u4e4b\u95f4, 1\u8868\u793a\u5b8c\u5168\u543b\u5408\uff0c-1\u8868\u793a\u5b8c\u5168\u76f8\u6096\u3002\u4e0b\u9762\u662f\u5e38\u89c1\u7684\u51e0\u7ec4 (x, y) (x, y) \u70b9\u96c6\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u3002 \u4e24\u4e2a\u53d8\u91cf\u4e4b\u95f4\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u5b9a\u4e49\u4e3a\u4e24\u4e2a\u53d8\u91cf\u4e4b\u95f4\u7684\u534f\u65b9\u5dee( \\text{cov}(X,Y) \\text{cov}(X,Y) )\u548c\u6807\u51c6\u5dee( \\sigma_X \\sigma_X )\u7684\u5546\uff1a \\rho_{X,Y}={\\mathrm{cov}(X,Y) \\over \\sigma_X \\sigma_Y} ={E[(X-\\mu_X)(Y-\\mu_Y)] \\over \\sigma_X\\sigma_Y} \\rho_{X,Y}={\\mathrm{cov}(X,Y) \\over \\sigma_X \\sigma_Y} ={E[(X-\\mu_X)(Y-\\mu_Y)] \\over \\sigma_X\\sigma_Y} \u5bf9\u4e8e\u6837\u672c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570: r_{xy}=\\frac{\\sum x_iy_i-n \\bar{x} \\bar{y}}{(n-1) s_x s_y}=\\frac{n\\sum x_iy_i-\\sum x_i\\sum y_i}{\\sqrt{n\\sum x_i^2-(\\sum x_i)^2}~\\sqrt{n\\sum y_i^2-(\\sum y_i)^2}}. r_{xy}=\\frac{\\sum x_iy_i-n \\bar{x} \\bar{y}}{(n-1) s_x s_y}=\\frac{n\\sum x_iy_i-\\sum x_i\\sum y_i}{\\sqrt{n\\sum x_i^2-(\\sum x_i)^2}~\\sqrt{n\\sum y_i^2-(\\sum y_i)^2}}. \u4ee5\u4e0a\u65b9\u7a0b\u7ed9\u51fa\u4e86\u8ba1\u7b97\u6837\u672c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u7b80\u5355\u7684\u5355\u6d41\u7a0b\u7b97\u6cd5\uff0c\u4f46\u662f\u5176\u4f9d\u8d56\u4e8e\u6d89\u53ca\u5230\u7684\u6570\u636e\uff0c\u6709\u65f6\u5b83\u53ef\u80fd\u662f\u6570\u503c\u4e0d\u7a33\u5b9a\u7684\u3002\u4f46\u5b83\u6700\u5927\u7684\u4f18\u70b9\u662f\uff0c\u7528\u4ee3\u7801\u5b9e\u73b0\u7684\u65f6\u5019\u53ef\u4ee5\u53ea\u904d\u5386\u4e00\u6b21\u6570\u636e\u3002 def pearson ( rating1 , rating2 ): Compute pearson coefficient :param rating1: a dictionary :param rating2: a dictionary :return: pearson coefficient sum_xy = 0 sum_x = 0 sum_y = 0 sum_x2 = 0 sum_y2 = 0 n = 0 commonRatings = False for key in rating1 : if key in rating2 : n += 1 x = rating1 [ key ] y = rating2 [ key ] sum_xy += x * y sum_x += x sum_y += y sum_x2 += pow ( x , 2 ) sum_y2 += pow ( y , 2 ) commonRatings = True if not commonRatings : return - 1 # now compute denominator denominator = math . sqrt ( sum_x2 - pow ( sum_x , 2 ) / n ) * math . sqrt ( sum_y2 - pow ( sum_y , 2 ) / n ) if denominator == 0 : return 0 else : return ( sum_xy - ( sum_x * sum_y ) / n ) / denominator","title":"\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570"},{"location":"gdm/ch1/#_6","text":"\u5f53\u6211\u4eec\u75281500\u4e07\u9996\u6b4c\u66f2\u6765\u6bd4\u8f83\u4e24\u4e2a\u7528\u6237\u65f6\uff0c\u5f88\u6709\u53ef\u80fd\u4ed6\u4eec\u4e4b\u95f4\u6ca1\u6709\u4efb\u4f55\u4ea4\u96c6\uff0c\u8fd9\u6837\u4e00\u6765\u5c31\u65e0\u4ece\u8ba1\u7b97\u4ed6\u4eec\u4e4b\u95f4\u7684\u8ddd\u79bb\u4e86\u3002\u7c7b\u4f3c\u7684\u60c5\u51b5\u662f\u5728\u8ba1\u7b97\u4e24\u7bc7\u6587\u7ae0\u7684\u76f8\u4f3c\u5ea6\u65f6\u3002\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u8ba1\u7b97\u4e2d\u4f1a\u7565\u8fc7\u8fd9\u4e9b\u975e\u96f6\u503c\u3002\u5b83\u7684\u8ba1\u7b97\u516c\u5f0f\u662f\uff1a \\cos(x,y) = \\frac{x\\cdot y}{||x||\\times||y||} \\cos(x,y) = \\frac{x\\cdot y}{||x||\\times||y||} \u5176\u4e2d\uff0c \\cdot \\cdot \u53f7\u8868\u793a\u6570\u91cf\u79ef\u3002 ||x|| ||x|| \u8868\u793a\u5411\u91cf x x \u7684\u6a21\u3002 \u4f59\u5f26\u76f8\u4f3c\u5ea6\u5728\u6587\u672c\u6316\u6398\u4e2d\u5e94\u7528\u5f97\u8f83\u591a\uff0c\u5728\u534f\u540c\u8fc7\u6ee4\u4e2d\u4e5f\u4f1a\u4f7f\u7528\u5230\u3002","title":"\u4f59\u5f26\u76f8\u4f3c\u5ea6"},{"location":"gdm/ch1/#_7","text":"\u5982\u679c\u6570\u636e\u5b58\u5728\u201c\u5206\u6570\u81a8\u80c0\u201d\u95ee\u9898\uff0c\u5c31\u4f7f\u7528\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u3002 \u5982\u679c\u6570\u636e\u6bd4\u8f83\u201c\u5bc6\u96c6\u201d\uff0c\u53d8\u91cf\u4e4b\u95f4\u57fa\u672c\u90fd\u5b58\u5728\u516c\u6709\u503c\uff0c\u4e14\u8fd9\u4e9b\u8ddd\u79bb\u6570\u636e\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u90a3\u5c31\u4f7f\u7528\u6b27\u51e0\u91cc\u5f97\u6216\u66fc\u54c8\u987f\u8ddd\u79bb\u3002 \u5982\u679c\u6570\u636e\u662f\u7a00\u758f\u7684\uff0c\u5219\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3002 Note \u5728\u6570\u636e\u6807\u51c6\u5316( \\mu=0,\\sigma=1 \\mu=0,\\sigma=1 \uff09\u540e\uff0cPearson\u76f8\u5173\u6027\u7cfb\u6570\u3001\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9\u53ef\u8ba4\u4e3a\u662f\u7b49\u4ef7\u7684[ 1 ]\u3002","title":"\u5e94\u8be5\u4f7f\u7528\u54ea\u79cd\u76f8\u4f3c\u5ea6\uff1f"},{"location":"gdm/ch1/#knn","text":"\u4e0a\u9762\u7684\u505a\u6cd5\u4e2d\uff0c\u6211\u4eec\u53ea\u4f9d\u9760\u6700\u76f8\u4f3c\u7684\u4e00\u4e2a\u7528\u6237\u6765\u505a\u63a8\u8350\uff0c\u5982\u679c\u8fd9\u4e2a\u7528\u6237\u6709\u4e9b\u7279\u6b8a\u7684\u504f\u597d\uff0c\u5c31\u4f1a\u76f4\u63a5\u53cd\u6620\u5728\u63a8\u8350\u5185\u5bb9\u91cc\u3002\u89e3\u51b3\u65b9\u6cd5\u4e4b\u4e00\u662f\u627e\u5bfb\u591a\u4e2a\u76f8\u4f3c\u7684\u7528\u6237\uff0c\u8fd9\u91cc\u5c31\u8981\u7528\u5230K\u6700\u90bb\u8fd1\u7b97\u6cd5\u4e86\u3002 \u5728\u534f\u540c\u8fc7\u6ee4\u4e2d\u53ef\u4ee5\u4f7f\u7528K\u6700\u90bb\u8fd1\u7b97\u6cd5\u6765\u627e\u51faK\u4e2a\u6700\u76f8\u4f3c\u7684\u7528\u6237\uff0c\u4ee5\u6b64\u4f5c\u4e3a\u63a8\u8350\u7684\u57fa\u7840\u3002\u4e0d\u540c\u7684 \u5e94\u7528\u6709\u4e0d\u540c\u7684K\u503c\uff0c\u9700\u8981\u505a\u4e00\u4e9b\u5b9e\u9a8c\u6765\u5f97\u51fa\u3002\u4ee5\u4e0b\u7ed9\u5230\u8bfb\u8005\u4e00\u4e2a\u57fa\u672c\u7684\u601d\u8def\u3002 \u5047\u8bbe\u6211\u8981\u4e3aAnn\u505a\u63a8\u8350\uff0c\u5e76\u4ee4K=3\u3002\u4f7f\u7528\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u5f97\u5230\u7684\u7ed3\u679c\u662f\uff1a Person Pearson Sally 0.8 Eric 0.7 Amanda 0.5 \u8fd9\u4e09\u4e2a\u4eba\u90fd\u4f1a\u5bf9\u63a8\u8350\u7ed3\u679c\u6709\u6240\u8d21\u732e\uff0c\u95ee\u9898\u5728\u4e8e\u6211\u4eec\u5982\u4f55\u786e\u5b9a\u4ed6\u4eec\u7684\u6bd4\u91cd\u5462\uff1f \u6211\u4eec\u76f4\u63a5\u7528\u76f8\u5173\u7cfb\u6570\u7684\u6bd4\u91cd\u6765\u63cf\u8ff0\uff0cSally\u7684\u6bd4\u91cd\u662f0.8/2=40%\uff0cEric\u662f0.7/2=35%\uff0cAmanda \u5219\u662f25%\uff1a \u5047\u8bbe\u4ed6\u4eec\u4e09\u4eba\u5bf9Grey Wardens\u7684\u8bc4\u5206\u4ee5\u53ca\u52a0\u6743\u540e\u7684\u7ed3\u679c\u5982\u4e0b\uff1a Person Grey Wardens Rating Influence Sally 4.5 25% Eric 5 35% Amanda 3.5 40% \u6700\u540e\u8ba1\u7b97\u5f97\u5230\u7684\u5206\u6570\u4e3a\u4e3a\u52a0\u6743\u548c 4.5\\times 25\\% + 5\\times 35\\% + 3.5 \\times 40\\% 4.5\\times 25\\% + 5\\times 35\\% + 3.5 \\times 40\\% \u3002","title":"kNN"},{"location":"gdm/ch1/#python","text":"Cai-Nicolas Zeigler\u4ece\u56fe\u4e66\u6f02\u6d41\u7ad9\u6536\u96c6\u4e86\u8d85\u8fc7100\u4e07\u6761\u8bc4\u4ef7\u6570\u636e\u2014\u2014278,858\u4f4d\u7528\u6237\u4e3a271,379\u672c\u4e66\u6253\u4e86\u5206\u3002\u6570\u636e\u53ef\u4ee5\u4ece\u8fd9\u4e2a \u5730\u5740 \u83b7\u5f97\u3002 CSV\u6587\u4ef6\u5305\u542b\u4e86\u4e09\u5f20\u8868\uff1a \u7528\u6237\u8868\uff0c\u5305\u62ec\u7528\u6237ID\u3001\u4f4d\u7f6e\u3001\u5e74\u9f84\u7b49\u4fe1\u606f\u3002\u5176\u4e2d\u7528\u6237\u7684\u59d3\u540d\u5df2\u7ecf\u9690\u53bb\uff1b \u4e66\u7c4d\u8868\uff0c\u5305\u62ecISBN\u53f7\u3001\u6807\u9898\u3001\u4f5c\u8005\u3001\u51fa\u7248\u65e5\u671f\u3001\u51fa\u7248\u793e\u7b49\uff1b \u8bc4\u5206\u8868\uff0c\u5305\u62ec\u7528\u6237ID\u3001\u4e66\u7c4dISBN\u53f7\u3001\u4ee5\u53ca\u8bc4\u5206\uff080-10\u5206\uff09\u3002 Recommender class Recommender : def __init__ ( self , books , users , user_ratings , book_ratings ): initialize basic data :param books: a dictionary of books, whose key is book id :param users: a dictionary of users, whose key is user id :param book_ratings: a dictionary of book ratings, whose key is book id :param user_ratings: a dictionary of user ratings, whose key is user id self . books = books self . users = users self . book_ratings = book_ratings self . user_ratings = user_ratings def recommend ( self , user_to_recommend_int , k = 1 ): Recommend user books :param user_to_recommend_int: int, user id :param k : int, for nearest k neighbors :return: a list of books user_to_recommend = str ( user_to_recommend_int ) if user_to_recommend not in self . users : raise Exception ( user does not exist!! ) # find the user having min distances from user_to_recommend distances = [] find_user = False for user in self . users : if user_to_recommend == user : continue # extract user ratings based on user ids, # and compute the distance between them distance = Distance . pearson ( self . user_ratings [ user_to_recommend ], self . user_ratings [ user ]) if distance != - 1 : distances . append ([ user , distance ]) find_user = True if not find_user : return [] # sort user based on their distances # pearson \u7cfb\u6570\u8d8a\u5927\uff0c\u8ddd\u79bb\u8d8a\u8fd1\uff0c\u6240\u4ee5\u7528reverse distances . sort ( key = lambda x : x [ 1 ], reverse = True ) # compute weight based on distances distances = distances [ 0 : k ] sum_distance = sum ([ distance for user , distance in distances ]) for i in range ( len ( distances )): distances [ i ][ 1 ] /= sum_distance # recommend books books_to_recommend = {} for user_id , weight in distances : for book_id in self . user_ratings [ user_id ]: if book_id not in self . user_ratings [ user_to_recommend ]: # the user haven t seen if book_id not in books_to_recommend : # haven t recommend books_to_recommend [ book_id ] = self . user_ratings [ user_id ][ book_id ] * weight else : books_to_recommend [ book_id ] = books_to_recommend [ book_id ] \\ + self . user_ratings [ user_id ][ book_id ] * weight # transform to a list of tuple books_to_recommend = [( book_id , project_rating ) for book_id , project_rating in books_to_recommend . items ()] # sort based on project_rating books_to_recommend . sort ( key = lambda x : x [ 1 ], reverse = True ) # extract book title books_to_recommend = [ self . books [ book_id ][ title ] for book_id , project_rating in books_to_recommend ] return books_to_recommend if __name__ == __main__ : ratings = BooksImport () books , users , user_ratings , book_ratings = ratings . recommender_import () test = Recommender ( books , users , user_ratings , book_ratings ) print ( test . recommend ( 171118 )) Distance import math class Distance : Compute distance of two users, having different ratings. Both rating1 and rating2 are dictionaries of the form { The Strokes : 3.0, Slightly Stoopid : 2.5} def __init__ ( self ): pass @staticmethod def manhattan ( rating1 , rating2 ): Computes the Manhattan distance. distance = 0 common_ratings = False for key in rating1 : if key in rating2 : distance += abs ( rating1 [ key ] - rating2 [ key ]) common_ratings = True if common_ratings : return distance else : return - 1 # Indicates no ratings in common @staticmethod def euclidean ( rating1 , rating2 ): Computes the Euclidean Distance :param rating1: rating :param rating2: rating :return: distance if common ratings exists, or -1 distance = 0 commonRatings = False for key in rating1 : if key in rating2 : distance += pow ( rating1 [ key ] - rating2 [ key ], 2 ) commonRatings = True if commonRatings : return distance else : return - 1 # Indicates no ratings in common @staticmethod def chebyshev ( rating1 , rating2 ): Computes the Chebyshev Distance :param rating1: rating :param rating2: rating :return: distance if common ratings exists, or -1 distance = 0 commonRatings = False for key in rating1 : if key in rating2 : distance = max ( distance , abs ( rating1 [ key ] - rating2 [ key ])) commonRatings = True if commonRatings : return distance else : return - 1 # Indicates no ratings in common @staticmethod def pearson ( rating1 , rating2 ): Compute pearson coefficient :param rating1: a dictionary :param rating2: a dictionary :return: pearson coefficient sum_xy = 0 sum_x = 0 sum_y = 0 sum_x2 = 0 sum_y2 = 0 n = 0 commonRatings = False for key in rating1 : if key in rating2 : n += 1 x = rating1 [ key ] y = rating2 [ key ] sum_xy += x * y sum_x += x sum_y += y sum_x2 += pow ( x , 2 ) sum_y2 += pow ( y , 2 ) commonRatings = True if not commonRatings : return - 1 # now compute denominator denominator = math . sqrt ( sum_x2 - pow ( sum_x , 2 ) / n ) \\ * math . sqrt ( sum_y2 - pow ( sum_y , 2 ) / n ) if denominator == 0 : return 0 else : return ( sum_xy - ( sum_x * sum_y ) / n ) / denominator Books_import class BooksImport : def __init__ ( self ): self . books = {} self . users = {} self . book_ratings = {} self . user_ratings = {} self . bx_books_import () self . bx_users_import () self . bx_ratings_import () def bx_books_import ( self ): import books meta information try : booksfile = codecs . open ( BX-Dump/BX-Books.csv , r , utf-8 ) for line in booksfile : props = line . split ( ; ) isbn = props [ 0 ] . strip ( ) title = props [ 1 ] . strip ( ) author = props [ 2 ] . strip ( ) year = props [ 3 ] . strip ( ) self . books [ isbn ] = { title : title , author : author , year : year } booksfile . close () except IOError as e : error = Failed to load: {0} . format ( e ) print ( error ) def bx_users_import ( self ): import user meta information user is a dictionary, whose key is user_id try : users_file = codecs . open ( BX-Dump/BX-Users.csv , r , utf--8 ) for line in users_file : props = line . split ( ; ) user_id = props [ 0 ] . strip ( ) location = props [ 1 ] . strip ( ) self . users [ user_id ] = location self . user_ratings [ user_id ] = {} users_file . close () except IOError as e : error = Failed to load: {0} . format ( e ) print ( error ) def bx_ratings_import ( self ): try : ratings_file = codecs . open ( BX-Dump/BX-Book-Ratings.csv , r , utf--8 ) for line in ratings_file : props = line . split ( ; ) user_id = props [ 0 ] . strip ( ) book_id = props [ 1 ] . strip ( ) rating = int ( props [ 2 ] . strip () . strip ( )) if book_id in self . book_ratings : self . book_ratings [ book_id ] . append ( rating ) else : self . book_ratings [ book_id ] = [ rating ] self . user_ratings [ user_id ][ book_id ] = rating ratings_file . close () except IOError as e : error = Failed to load: {0} . format ( e ) print ( error ) def get_books ( self ): return self . books def get_users ( self ): return self . users def get_user_ratings ( self ): return self . user_ratings def get_book_ratings ( self ): return self . book_ratings def recommender_import ( self ): return self . books , self . users , self . user_ratings , self . book_ratings","title":"Python\u63a8\u8350\u6a21\u5757"},{"location":"gdm/ch2/","text":"\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 2 \u9690\u5f0f\u8bc4\u4ef7\u548c\u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\u7b97\u6cd5 \u9690\u5f0f\u8bc4\u4ef7 \u7528\u6237\u7684\u8bc4\u4ef7\u7c7b\u578b\u53ef\u4ee5\u5206\u4e3a\u663e\u5f0f\u8bc4\u4ef7\u548c\u9690\u5f0f\u8bc4\u4ef7\u3002 \u663e\u5f0f\u8bc4\u4ef7 \u6307\u7684\u662f\u7528\u6237\u660e\u786e\u5730\u7ed9\u51fa\u5bf9\u7269\u54c1\u7684\u8bc4\u4ef7\u3002\u6700\u5e38\u89c1\u7684\u4f8b\u5b50\u662fYouTube\u4e0a\u7684\u201c\u559c\u6b22\u201d\u548c\u201c\u4e0d\u559c\u6b22\u201d\u6309\u94ae\uff0c\u4ee5\u53ca\u4e9a\u9a6c\u900a\u7684\u661f\u7ea7\u8bc4\u4ef7\u7cfb\u7edf\u3002 \u9690\u5f0f\u8bc4\u4ef7 \uff0c\u5c31\u662f\u6211\u4eec\u4e0d\u8ba9\u7528\u6237\u660e\u786e\u7ed9\u51fa\u5bf9\u7269\u54c1\u7684\u8bc4\u4ef7\uff0c\u800c\u662f\u901a\u8fc7\u89c2\u5bdf\u4ed6\u4eec\u7684\u884c\u4e3a\u6765\u83b7\u5f97\u504f\u597d\u4fe1\u606f\u3002\u793a\u4f8b\u4e4b\u4e00\u662f\u8bb0\u5f55\u7528\u6237\u5728\u7ebd\u7ea6\u65f6\u62a5\u7f51\u4e0a\u7684\u70b9\u51fb\u8bb0\u5f55\uff0c\u4e9a\u9a6c\u900a\u4e0a\u7528\u6237\u7684\u5b9e\u9645\u8d2d\u4e70\u8bb0\u5f55\u3002 \u6211\u4eec\u53ef\u4ee5\u6536\u96c6\u5230\u54ea\u4e9b\u9690\u5f0f\u8bc4\u4ef7\u5462\uff1f e.g. \u7f51\u9875\u65b9\u9762\uff1a\u9875\u9762\u70b9\u51fb\u3001\u505c\u7559\u65f6\u95f4\u3001\u91cd\u590d\u8bbf\u95ee\u6b21\u6570\u3001\u5f15\u7528\u7387\u3001Hulu\u4e0a\u89c2\u770b\u89c6\u9891\u7684\u6b21\u6570\uff1b\u97f3\u4e50\u64ad\u653e\u5668\uff1a\u64ad\u653e\u7684\u66f2\u76ee\u3001\u8df3\u8fc7\u7684\u66f2\u76ee\u3001\u64ad\u653e\u6b21\u6570\uff1b \u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\u7b97\u6cd5 \u76ee\u524d\u4e3a\u6b62\u6211\u4eec\u63cf\u8ff0\u7684\u90fd\u662f\u57fa\u4e8e\u7528\u6237\u7684\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\uff1a\u5c06\u4e00\u4e2a\u7528\u6237\u548c\u5176\u4ed6 \u6240\u6709 \u7528\u6237\u8fdb\u884c\u5bf9\u6bd4\uff0c\u627e\u5230\u76f8\u4f3c\u7684\u4eba\u3002\u8fd9\u79cd\u7b97\u6cd5\u6709\u4e24\u4e2a\u5f0a\u7aef\uff1a \u6269\u5c55\u6027 \uff1a\u968f\u7740\u7528\u6237\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5176\u8ba1\u7b97\u91cf\u4e5f\u4f1a\u589e\u52a0\u3002\u8fd9\u79cd\u7b97\u6cd5\u5728\u53ea\u6709\u51e0\u5343\u4e2a\u7528\u6237\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u5de5\u4f5c\u5f97\u5f88\u597d\uff0c\u4f46\u8fbe\u5230\u4e00\u767e\u4e07\u4e2a\u7528\u6237\u65f6\u5c31\u4f1a\u51fa\u73b0\u74f6\u9888\u3002 \u7a00\u758f\u6027 \uff1a\u5927\u591a\u6570\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u7528\u6237\u4ec5\u4ec5\u5bf9\u4e00\u5c0f\u90e8\u5206\u7269\u54c1\u8fdb\u884c\u4e86\u8bc4\u4ef7\uff0c\u8fd9\u5c31\u9020\u6210\u4e86\u6570\u636e\u7684\u7a00\u758f\u6027\u3002\u6bd4\u5982\u4e9a\u9a6c\u900a\u6709\u4e0a\u767e\u4e07\u672c\u4e66\uff0c\u4f46\u7528\u6237\u53ea\u8bc4\u8bba\u4e86\u5f88\u5c11\u4e00\u90e8\u5206\uff0c\u4e8e\u662f\u5c31\u5f88\u96be\u627e\u5230\u4e24\u4e2a\u76f8\u4f3c\u7684\u7528\u6237\u4e86\u3002 \u4fee\u6b63\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6 \u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6765\u8ba1\u7b97\u4e24\u4e2a\u7269\u54c1\u7684\u8ddd\u79bb\u3002\u7531\u4e8e\u201c\u5206\u6570\u81a8\u80c0\u201d\u73b0\u8c61\uff0c\u9700\u8981\u4ece\u7528\u6237\u7684\u8bc4\u4ef7\u4e2d\u51cf\u53bb\u4ed6\u6240\u6709\u8bc4\u4ef7\u7684\u5747\u503c\uff0c\u8fd9\u5c31\u662f\u4fee\u6b63\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6(Adjusted Cosine Similarity)\u3002\u8fd9\u4e2a\u516c\u5f0f\u6765\u81ea\u4e8e\u4e00\u7bc7\u5f71\u54cd\u6df1\u8fdc\u7684\u8bba\u6587\u300a \u57fa\u4e8e\u7269\u54c1\u7684\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5 \u300b\u3002 s(i,j) =\\frac{\\sum_{u\\in U}(R_{u,i}-\\bar R_u)(R_{u,j}-\\bar R_u)}{\\sqrt{\\sum_{u\\in U}(R_{u,i}-\\bar R_u)^2}\\sqrt{\\sum_{u\\in U}(R_{u,j}-\\bar R_u)^2}} s(i,j) =\\frac{\\sum_{u\\in U}(R_{u,i}-\\bar R_u)(R_{u,j}-\\bar R_u)}{\\sqrt{\\sum_{u\\in U}(R_{u,i}-\\bar R_u)^2}\\sqrt{\\sum_{u\\in U}(R_{u,j}-\\bar R_u)^2}} U U \u8868\u793a\u540c\u65f6\u8bc4\u4ef7\u8fc7\u7269\u54c1 i i \u548c j j \u7684\u7528\u6237\u96c6\u5408\uff0c \\bar R_u \\bar R_u \u8868\u793a\u7528\u6237 u u \u5bf9\u6240\u6709\u7269\u54c1\u7684\u8bc4\u4ef7\u5747\u503c\uff0c s(i,j) s(i,j) \u8868\u793a\u7269\u54c1 i i \u548c j j \u7684\u76f8\u4f3c\u5ea6\u3002 def cosinesimilarity ( item1 , item2 , userRatings ): averages = {} for user , ratings in userRatings . items (): averages [ user ] = ( float ( sum ( ratings . values ())) / len ( ratings . values ())) num = 0 # \u5206\u5b50 dem1 = 0 # \u5206\u6bcd\u7684\u7b2c\u4e00\u90e8\u5206 dem2 = 0 for ( user , ratings ) in userRatings . items (): if item1 in ratings and item2 in ratings : avg = averages [ user ] num += ( ratings [ item1 ] - avg ) * ( ratings [ item2 ] - avg ) dem1 += ( ratings [ item1 ] - avg ) ** 2 dem2 += ( ratings [ item1 ] - avg ) ** 2 return num / ( math . sqrt ( dem1 ) * math . sqrt ( dem2 )) \u9884\u6d4b \u90a3\u4e0b\u9762\u8be5\u5982\u4f55\u4f7f\u7528\u5b83\u6765\u505a\u9884\u6d4b\u5462\uff1f\u6bd4\u5982\u6211\u60f3\u77e5\u9053David\u6709\u591a\u559c\u6b22Kacey Musgraves\uff1f\u5728\u8a08\u7b97\u5b8c similarity \u4e4b\u5f8c\uff0c\u4e0b\u4e00\u6b65\u9a5f\u5c31\u662f\u8981\u9032\u884c\u67d0\u500b item \u7684\u9810\u6e2c\uff0c\u9019\u88e1\u6709\u5169\u7a2e\u65b9\u6cd5\uff0c\u5206\u5225\u662f\uff1a weighted-sum \u548c regression\u3002 weighted-sum p(u,i) = \\frac{{\\sum_{N\\in \\text{similarTo}(i)}(S_{i,N}\\times R_{u,N})}}{\\sum_{N\\in \\text{similarTo}(i)}|S_{i,N}|} p(u,i) = \\frac{{\\sum_{N\\in \\text{similarTo}(i)}(S_{i,N}\\times R_{u,N})}}{\\sum_{N\\in \\text{similarTo}(i)}|S_{i,N}|} \u5176\u4e2d p(u,i) p(u,i) \u8868\u793a\u9884\u6d4b\u7684\u7528\u6237 u u \u5bf9\u7269\u54c1 i i \u7684\u8bc4\u5206\uff0c S_{i,N} S_{i,N} \u8868\u793a\u7269\u54c1 i i \u548c N N \u7684\u76f8\u4f3c\u5ea6\uff0c R_{u,N} R_{u,N} \u8868\u793a\u7528\u6237 u u \u5bf9\u7269\u54c1 N N \u7684\u8bc4\u5206\u3002 N N \u662f\u4e00\u4e2a\u7269\u54c1\u7684\u96c6\u5408\uff0c\u6709\u5982\u4e0b\u7279\u6027\uff1a\u7528\u6237 u u \u5bf9\u96c6\u5408\u4e2d\u7684\u7269\u54c1\u6253\u8fc7\u5206\uff0c\u7269\u54c1 i i \u548c\u96c6\u5408\u4e2d\u7684\u7269\u54c1\u6709\u76f8\u4f3c\u5ea6\u6570\u636e\u3002 \u4f7f\u7528Python\u5b9e\u73b0\u4fee\u6b63\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7b97\u6cd5 class ItemBasedCF : \u4f7f\u7528\u4fee\u6b63\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5b9e\u73b0\u7269\u54c1\u63a8\u8350 def __init__ ( self , data ): initialize data :param data: a dict of (user, ratings) For instance, users2 = { Amy : { Taylor Swift : 4, PSY : 3, Whitney Houston : 4}, Clara : { PSY : 3.5, Whitney Houston : 4}, Daisy : { Taylor Swift : 5, Whitney Houston : 3}} self . data = data self . items = set () # a set of items # compute average user ratings of given data self . avg_user_rating = {} for user , ratings in data . items (): self . avg_user_rating [ user ] = np . average ( list ( ratings . values ())) for item in ratings . keys (): self . items . add ( item ) def adjusted_cosine_similarity ( self , item1 , item2 ): Compute adjusted cosine similarity :param item1: an item :param item2: an item :return: similarity if ( item1 not in self . items ) or ( item2 not in self . items ): raise ( Exception ( Input Item NOT FOUND! )) num = 0 # numerator den1 = 0 # denominator1 den2 = 0 # denominator2 for user , ratings in self . data . items (): if item1 in ratings and item2 in ratings : x = ratings [ item1 ] - self . avg_user_rating [ user ] y = ratings [ item2 ] - self . avg_user_rating [ user ] num += x * y den1 += x * x den2 += y * y den = np . sqrt ( den1 * den2 ) # denominator if den == 0 : return None return num / den def predict ( self , user , item ): predict rating of given user on given item :param user: an user :param item: an item :return: rating if user not in self . data : raise ( Exception ( Input User NOT FOUND! )) if item not in self . items : raise ( Exception ( Input Item NOT FOUND! )) num = 0 # numerator den = 0 # denominator for another_item , rating in self . data [ user ] . items (): if item != another_item : similarity = self . adjusted_cosine_similarity ( item , another_item ) if similarity is None : continue num += rating * similarity den += abs ( similarity ) if den == 0 : return None return num / den def recommend ( self , user ): recommend items to the user :param user: an user :return: a list of items(up to 10) recommendations = [] for item in self . items : if item not in self . data [ user ]: predict = self . predict ( user , item ) if predict is not None : recommendations . append (( predict , item )) recommendations . sort ( reverse = True ) return list ( map ( lambda x : x [ 1 ], recommendations )) Slope One\u7b97\u6cd5 Slope One\u662f\u53e6\u4e00\u79cd\u6bd4\u8f83\u6d41\u884c\u7684\u57fa\u4e8e\u7269\u54c1\u7684\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\u3002\u5b83\u6700\u5927\u7684\u4f18\u52bf\u662f\u7b80\u5355\uff0c\u6613\u4e8e\u5b9e\u73b0\u3002Slope One\u7b97\u6cd5\u662f\u5728\u4e00\u7bc7\u540d\u4e3a\u300a Slope One Predictors for Online Rating-Based Collaborative Filtering \u300b\u7684\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\uff0c\u7531Lemire\u548cMachlachlan\u5408\u8457\u3002\u8fd9\u7bc7\u8bba\u6587\u975e\u5e38\u503c\u5f97\u4e00\u8bfb\u3002 \u6211\u4eec\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\u6765\u4e86\u89e3\u8fd9\u4e2a\u7b97\u6cd5\u3002\u5047\u8bbeA\u7ed9Item I\u6253\u4e861\u5206\uff0c\u7ed9Item J\u6253\u4e861.5\u5206\uff1bB\u7ed9Item I\u6253\u4e862\u5206\u3002\u6211\u4eec\u53ef\u4ee5\u7528\u4ee5\u4e0b\u903b\u8f91\u6765\u9884\u6d4bB\u5bf9Item J\u7684\u8bc4\u5206\uff1a\u7531\u4e8eA\u7ed9Item J\u6253\u7684\u5206\u6570\u8981\u6bd4Item I\u7684\u9ad80.5\u5206\uff0c\u6240\u4ee5\u6211\u4eec\u9884\u6d4bB\u4e5f\u4f1a\u7ed9\u9ad80.5\u5206\uff0c\u53732.5\u5206\u3002 \u53ef\u4ee5\u5c06Slope One\u5206\u4e3a\u4e24\u4e2a\u6b65\u9aa4\uff1a \u8ba1\u7b97\u51fa\u7269\u54c1\u4e4b\u95f4\u7684\u4e24\u4e24\u5dee\u503c(\u53ef\u4ee5\u5728\u591c\u95f4\u6279\u91cf\u8ba1\u7b97) \u8fdb\u884c\u9884\u6d4b\uff0c\u53ef\u4ee5\u4f7f\u7528\u52a0\u6743\u7684Slope One\u7b97\u6cd5 \u8ba1\u7b97\u5dee\u503c \u7269\u54c1 i i \u4e0e\u7269\u54c1 j j \u4e4b\u95f4\u7684\u5e73\u5747\u5dee\u5f02\u4e3a\uff1a \\text{dev}_{i,j}=\\sum_{u\\in S_{i,j}(X)}\\frac{u_i-u_j}{\\text{card}(S_{i,j}(X))} \\text{dev}_{i,j}=\\sum_{u\\in S_{i,j}(X)}\\frac{u_i-u_j}{\\text{card}(S_{i,j}(X))} \u5176\u4e2d S_{i,j}(X) S_{i,j}(X) \u8868\u793a\u540c\u65f6\u8bc4\u4ef7\u8fc7 i,j i,j \u7684\u7528\u6237\u96c6\u5408\uff0c \\text{card}(S) \\text{card}(S) \u8868\u793a S S \u4e2d\u6709\u591a\u5c11\u4e2a\u5143\u7d20\uff0c X X \u8868\u793a\u6240\u6709\u8bc4\u5206\u503c\u7684\u96c6\u5408\uff0c \\text{card}(S_{j,i}(X)) \\text{card}(S_{j,i}(X)) \u5219\u8868\u793a\u540c\u65f6\u8bc4\u4ef7\u8fc7\u7269\u54c1 i i \u548c j j \u7684\u7528\u6237\u6570\uff0c u_i u_i \u8868\u793a\u7528\u6237 u u \u5bf9\u7269\u54c1 i i \u7684\u8bc4\u5206\u3002 Question \u5982\u679c\u6709\u4e00\u4e2a\u65b0\u8fdb\u7684\u7528\u6237\u5bf910\u4e2a\u6b4c\u624b\u505a\u4e86\u8bc4\u4ef7\uff0c\u6211\u4eec\u662f\u5426\u9700\u8981\u91cd\u65b0\u8ba1\u7b9720\u4e07\u00d720\u4e07\u7684\u5dee\u5f02\u6570\u636e\uff0c\u6216\u662f\u6709\u5176\u4ed6\u66f4\u7b80\u5355\u7684\u65b9\u6cd5\uff1f \u7b54\u6848\u662f\u4f60\u4e0d\u9700\u8981\u8ba1\u7b97\u6574\u4e2a\u6570\u636e\u96c6\uff0c\u8fd9\u6b63\u662fSlope One\u7684\u7f8e\u5999\u4e4b\u5904\u3002\u5bf9\u4e8e\u4e24\u4e2a\u7269\u54c1\uff0c\u6211\u4eec\u53ea\u9700\u8bb0\u5f55\u540c\u65f6\u8bc4\u4ef7\u8fc7\u8fd9\u5bf9\u7269\u54c1\u7684\u7528\u6237\u6570\u5c31\u53ef\u4ee5\u4e86\u3002 \u4f7f\u7528\u52a0\u6743\u7684Slope One\u7b97\u6cd5\u8fdb\u884c\u9884\u6d4b \u4f7f\u7528\u52a0\u6743\u7684Slope One\u7b97\u6cd5(Weighted Slope One, WS1)\u6765\u8fdb\u884c\u9884\u6d4b\uff0c\u7528 P^{WS1} P^{WS1} \u6765\u8868\u793a\u9884\u6d4b\u7528\u6237 u u \u5bf9\u7269\u54c1 j j \u7684\u8bc4\u5206\uff1a P^{WS1}(u)_j=\\frac{\\sum_{i\\in S(u) - \\{j\\}}(\\text{dev}_{i,j}+u_i)c_{j,i}}{\\sum_{i\\in S(u) - \\{j\\}} c_{j,i}} P^{WS1}(u)_j=\\frac{\\sum_{i\\in S(u) - \\{j\\}}(\\text{dev}_{i,j}+u_i)c_{j,i}}{\\sum_{i\\in S(u) - \\{j\\}} c_{j,i}} \u5176\u4e2d\uff1a c_{j,i}=\\text{card}(S_{j,i}(\\chi)) c_{j,i}=\\text{card}(S_{j,i}(\\chi)) \u3002\u5f0f\u4e2d \\sum_{i\\in S(u) - \\{j\\}} \\sum_{i\\in S(u) - \\{j\\}} \u8868\u793a\u7528\u6237 u u \u8bc4\u4ef7\u7684\u9664 j j \u9664\u5916\u7684\u7269\u54c1\uff0c\u5176\u4ed6\u7b26\u53f7\u4e0e\u4e0a\u4e00\u8282\u7684\u542b\u4e49\u76f8\u540c\u3002\u8fd9\u4e2a\u516c\u5f0f\u5176\u5b9e\u5f88\u597d\u7406\u89e3\uff0c \\text{dev}_{i,j}+u_i \\text{dev}_{i,j}+u_i \u8868\u793a\u6839\u636e\u7269\u54c1 i i \u9884\u6d4b\u5f97\u5230\u7684\u7528\u6237 u u \u5bf9\u7269\u54c1 j j \u7684\u8bc4\u5206\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u52a0\u6743\u5e73\u5747\u5c31\u5f97\u5230 P^{WS1} P^{WS1} \u3002 \u4f7f\u7528Python\u5b9e\u73b0Slope One\u7b97\u6cd5 \u5176\u5b9e\u4ee3\u7801\u6574\u4f53\u601d\u8def\u6bd4\u8f83\u7b80\u5355\uff0c\u628a\u516c\u5f0f\u8f6c\u5316\u4e3a\u4ee3\u7801\u5c31\u53ef\u4ee5\u4e86\uff0c\u6ca1\u6709\u4ec0\u4e48\u7279\u6b8a\u7684\u6280\u5de7\u3002\u552f\u4e00\u9700\u8981\u6ce8\u610f\u7684\u662f\u5c3d\u53ef\u80fd\u7684\u6d4b\u8bd5\u8be5\u7a0b\u5e8f\uff0c\u4fdd\u8bc1\u5176\u6b63\u786e\u6027\u3002\u7531\u4e8e\u7f51\u4e0a\u6709\u4e0d\u5c11SlopeOne\u4ee3\u7801\uff0c\u6211\u81ea\u5df1\u5199\u5b8c\u4ee5\u540e\u548c\u4ed6\u4eec\u7684\u7ed3\u679c\u6bd4\u5bf9\uff0c\u53d1\u73b0\u5b8c\u5168\u4e00\u81f4\u3002 class SlopeOne : def __init__ ( self , data ): initialize :param data: a dictionary, whose key is an item, and value is a rating. self . data = data # frequencies, a dictionary, whose key is an item, # and value is a dictionary of (item, frequency) self . frequencies = {} # deviations, a dictionary, whose key is an item, # and value is a dictionary of (item, deviation) self . deviations = {} def computeDeviations ( self ): compute deviations between items :return: # \u83b7\u53d6\u6bcf\u4f4d\u7528\u6237\u7684\u8bc4\u5206\u6570\u636eratings for ratings in self . data . values (): # \u5bf9\u4e8e\u8be5\u7528\u6237\u7684\u6bcf\u4e2a\u8bc4\u5206\u9879(\u6b4c\u624b\u3001\u5206\u6570) for ( item1 , rating1 ) in ratings . items (): self . frequencies . setdefault ( item1 , {}) self . deviations . setdefault ( item1 , {}) # \u518d\u6b21\u904d\u5386\u8be5\u7528\u6237\u7684\u6bcf\u4e2a\u8bc4\u5206\u9879 for ( item2 , rating2 ) in ratings . items (): if item1 != item2 : # \u5c06\u8bc4\u5206\u7684\u5dee\u5f02\u4fdd\u5b58\u5230\u53d8\u91cf\u4e2d self . frequencies [ item1 ] . setdefault ( item2 , 0 ) self . deviations [ item1 ] . setdefault ( item2 , 0.0 ) self . frequencies [ item1 ][ item2 ] += 1 self . deviations [ item1 ][ item2 ] += ( rating1 - rating2 ) # \u8ba1\u7b97deviations for item1 , deviations in self . deviations . items (): for item2 in deviations : deviations [ item2 ] /= self . frequencies [ item1 ][ item2 ] def predict ( self , user , item ): predict the ratings of the user regard to the item Using Weighted Slope One (WSO) :param user: an user :param item: an item :return: a prediction, double if user not in self . data : raise Exception # predictions predictions = 0 frequency = 0 # \u7528\u6237user \u8bc4\u4ef7\u7684\u9664 item \u9664\u5916\u7684\u7269\u54c1 for diff_item , ratings in self . data [ user ] . items (): if ( item == diff_item ) or ( diff_item not in self . deviations [ item ]): continue predictions += ( self . deviations [ item ][ diff_item ] + self . data [ user ][ diff_item ]) \\ * self . frequencies [ item ][ diff_item ] frequency += self . frequencies [ item ][ diff_item ] predictions /= frequency return predictions def recommendation ( self , user ): recommend items to user :param user: an user :return: a list of items recommended if user not in self . data : raise Exception if self . deviations == {}: self . computeDeviations () recommendations = [] # \u6240\u6709\u7684item for item in self . deviations . keys (): # \u7528\u6237\u672a\u8bc4\u4ef7\u8fc7\u7684item if item not in self . data [ user ]: recommendations . append (( item , self . predict ( user , item ))) recommendations . sort ( key = lambda x : x [ 1 ], reverse = True ) return list ( map ( lambda x : x [ 0 ], recommendations )) Example: MovieLens MovieLens\u6570\u636e\u96c6\u662f\u7531\u660e\u5c3c\u82cf\u8fbe\u5dde\u5927\u5b66\u7684GroupLens\u7814\u7a76\u9879\u76ee\u6536\u96c6\u7684\uff0c\u662f\u7528\u6237\u5bf9\u7535\u5f71\u7684\u8bc4\u5206\u3002 \u8fd9\u4e2a\u6570\u636e\u96c6\u53ef\u4ee5\u5728 www.grouplens.org \u4e0b\u8f7d\u3002\u5176\u4e2d100K\u6570\u636e\u96c6\u5305\u542b\u4e86943\u4f4d\u7528\u6237\u5bf91682\u90e8\u7535\u5f71\u7684\u8bc4\u4ef7\uff0c\u7ea610\u4e07\u6761\u8bb0\u5f55\u3002 \u4f7f\u7528MovieLens 100K\u6570\u636e\u96c6\u5904\u7406\u7684\u8fc7\u7a0b\u5982\u4e0b\uff1a \u6839\u636eREADME\u4e2d\u63cf\u8ff0\u7684\u6587\u4ef6\u683c\u5f0f\uff0c\u5c06\u7535\u5f71\u3001\u7528\u6237\u3001\u8bc4\u5206\u6570\u636e\u5bfc\u5165\u5e76\u8f6c\u5316\u4e3a\u5408\u9002\u7684\u6570\u636e\u683c\u5f0f \u5229\u7528SlopeOne\u8fdb\u884c\u63a8\u8350 \u5c06\u63a8\u8350\u7ed3\u679c\u5c55\u793a \u4ee3\u7801\u6574\u4f53\u975e\u5e38\u7b80\u6d01\u7684\uff0c\u56e0\u4e3a\u53ef\u4ee5\u76f4\u63a5\u7ee7\u627f\u4e0a\u9762\u5199\u7684SlopeOne\u7b97\u6cd5\u3002 class MovieRecommendation ( SlopeOne ): def __init__ ( self , data , movie ): super ( MovieRecommendation , self ) . __init__ ( data ) self . movie = movie def recommend_movie ( self , user ): movies = self . recommendation ( user )[: 10 ] return list ( map ( lambda x : self . movie [ x ][ title ], movies )) \u8fdb\u4e00\u6b65\u9605\u8bfb http://www.diva-portal.se/smash/get/diva2:811049/FULLTEXT01.pdf https://dzone.com/articles/slope-one-recommender https://www.slideshare.net/irecsys/slope-one-recommender-on-hadoop-15199798?from_action=save#","title":"Chapter 2: \u9690\u5f0f\u8bc4\u4ef7\u548c\u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\u7b97\u6cd5"},{"location":"gdm/ch2/#-2","text":"","title":"\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 2 \u9690\u5f0f\u8bc4\u4ef7\u548c\u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\u7b97\u6cd5"},{"location":"gdm/ch2/#_1","text":"\u7528\u6237\u7684\u8bc4\u4ef7\u7c7b\u578b\u53ef\u4ee5\u5206\u4e3a\u663e\u5f0f\u8bc4\u4ef7\u548c\u9690\u5f0f\u8bc4\u4ef7\u3002 \u663e\u5f0f\u8bc4\u4ef7 \u6307\u7684\u662f\u7528\u6237\u660e\u786e\u5730\u7ed9\u51fa\u5bf9\u7269\u54c1\u7684\u8bc4\u4ef7\u3002\u6700\u5e38\u89c1\u7684\u4f8b\u5b50\u662fYouTube\u4e0a\u7684\u201c\u559c\u6b22\u201d\u548c\u201c\u4e0d\u559c\u6b22\u201d\u6309\u94ae\uff0c\u4ee5\u53ca\u4e9a\u9a6c\u900a\u7684\u661f\u7ea7\u8bc4\u4ef7\u7cfb\u7edf\u3002 \u9690\u5f0f\u8bc4\u4ef7 \uff0c\u5c31\u662f\u6211\u4eec\u4e0d\u8ba9\u7528\u6237\u660e\u786e\u7ed9\u51fa\u5bf9\u7269\u54c1\u7684\u8bc4\u4ef7\uff0c\u800c\u662f\u901a\u8fc7\u89c2\u5bdf\u4ed6\u4eec\u7684\u884c\u4e3a\u6765\u83b7\u5f97\u504f\u597d\u4fe1\u606f\u3002\u793a\u4f8b\u4e4b\u4e00\u662f\u8bb0\u5f55\u7528\u6237\u5728\u7ebd\u7ea6\u65f6\u62a5\u7f51\u4e0a\u7684\u70b9\u51fb\u8bb0\u5f55\uff0c\u4e9a\u9a6c\u900a\u4e0a\u7528\u6237\u7684\u5b9e\u9645\u8d2d\u4e70\u8bb0\u5f55\u3002 \u6211\u4eec\u53ef\u4ee5\u6536\u96c6\u5230\u54ea\u4e9b\u9690\u5f0f\u8bc4\u4ef7\u5462\uff1f e.g. \u7f51\u9875\u65b9\u9762\uff1a\u9875\u9762\u70b9\u51fb\u3001\u505c\u7559\u65f6\u95f4\u3001\u91cd\u590d\u8bbf\u95ee\u6b21\u6570\u3001\u5f15\u7528\u7387\u3001Hulu\u4e0a\u89c2\u770b\u89c6\u9891\u7684\u6b21\u6570\uff1b\u97f3\u4e50\u64ad\u653e\u5668\uff1a\u64ad\u653e\u7684\u66f2\u76ee\u3001\u8df3\u8fc7\u7684\u66f2\u76ee\u3001\u64ad\u653e\u6b21\u6570\uff1b","title":"\u9690\u5f0f\u8bc4\u4ef7"},{"location":"gdm/ch2/#_2","text":"\u76ee\u524d\u4e3a\u6b62\u6211\u4eec\u63cf\u8ff0\u7684\u90fd\u662f\u57fa\u4e8e\u7528\u6237\u7684\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\uff1a\u5c06\u4e00\u4e2a\u7528\u6237\u548c\u5176\u4ed6 \u6240\u6709 \u7528\u6237\u8fdb\u884c\u5bf9\u6bd4\uff0c\u627e\u5230\u76f8\u4f3c\u7684\u4eba\u3002\u8fd9\u79cd\u7b97\u6cd5\u6709\u4e24\u4e2a\u5f0a\u7aef\uff1a \u6269\u5c55\u6027 \uff1a\u968f\u7740\u7528\u6237\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5176\u8ba1\u7b97\u91cf\u4e5f\u4f1a\u589e\u52a0\u3002\u8fd9\u79cd\u7b97\u6cd5\u5728\u53ea\u6709\u51e0\u5343\u4e2a\u7528\u6237\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u5de5\u4f5c\u5f97\u5f88\u597d\uff0c\u4f46\u8fbe\u5230\u4e00\u767e\u4e07\u4e2a\u7528\u6237\u65f6\u5c31\u4f1a\u51fa\u73b0\u74f6\u9888\u3002 \u7a00\u758f\u6027 \uff1a\u5927\u591a\u6570\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u7528\u6237\u4ec5\u4ec5\u5bf9\u4e00\u5c0f\u90e8\u5206\u7269\u54c1\u8fdb\u884c\u4e86\u8bc4\u4ef7\uff0c\u8fd9\u5c31\u9020\u6210\u4e86\u6570\u636e\u7684\u7a00\u758f\u6027\u3002\u6bd4\u5982\u4e9a\u9a6c\u900a\u6709\u4e0a\u767e\u4e07\u672c\u4e66\uff0c\u4f46\u7528\u6237\u53ea\u8bc4\u8bba\u4e86\u5f88\u5c11\u4e00\u90e8\u5206\uff0c\u4e8e\u662f\u5c31\u5f88\u96be\u627e\u5230\u4e24\u4e2a\u76f8\u4f3c\u7684\u7528\u6237\u4e86\u3002","title":"\u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\u7b97\u6cd5"},{"location":"gdm/ch2/#_3","text":"\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6765\u8ba1\u7b97\u4e24\u4e2a\u7269\u54c1\u7684\u8ddd\u79bb\u3002\u7531\u4e8e\u201c\u5206\u6570\u81a8\u80c0\u201d\u73b0\u8c61\uff0c\u9700\u8981\u4ece\u7528\u6237\u7684\u8bc4\u4ef7\u4e2d\u51cf\u53bb\u4ed6\u6240\u6709\u8bc4\u4ef7\u7684\u5747\u503c\uff0c\u8fd9\u5c31\u662f\u4fee\u6b63\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6(Adjusted Cosine Similarity)\u3002\u8fd9\u4e2a\u516c\u5f0f\u6765\u81ea\u4e8e\u4e00\u7bc7\u5f71\u54cd\u6df1\u8fdc\u7684\u8bba\u6587\u300a \u57fa\u4e8e\u7269\u54c1\u7684\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5 \u300b\u3002 s(i,j) =\\frac{\\sum_{u\\in U}(R_{u,i}-\\bar R_u)(R_{u,j}-\\bar R_u)}{\\sqrt{\\sum_{u\\in U}(R_{u,i}-\\bar R_u)^2}\\sqrt{\\sum_{u\\in U}(R_{u,j}-\\bar R_u)^2}} s(i,j) =\\frac{\\sum_{u\\in U}(R_{u,i}-\\bar R_u)(R_{u,j}-\\bar R_u)}{\\sqrt{\\sum_{u\\in U}(R_{u,i}-\\bar R_u)^2}\\sqrt{\\sum_{u\\in U}(R_{u,j}-\\bar R_u)^2}} U U \u8868\u793a\u540c\u65f6\u8bc4\u4ef7\u8fc7\u7269\u54c1 i i \u548c j j \u7684\u7528\u6237\u96c6\u5408\uff0c \\bar R_u \\bar R_u \u8868\u793a\u7528\u6237 u u \u5bf9\u6240\u6709\u7269\u54c1\u7684\u8bc4\u4ef7\u5747\u503c\uff0c s(i,j) s(i,j) \u8868\u793a\u7269\u54c1 i i \u548c j j \u7684\u76f8\u4f3c\u5ea6\u3002 def cosinesimilarity ( item1 , item2 , userRatings ): averages = {} for user , ratings in userRatings . items (): averages [ user ] = ( float ( sum ( ratings . values ())) / len ( ratings . values ())) num = 0 # \u5206\u5b50 dem1 = 0 # \u5206\u6bcd\u7684\u7b2c\u4e00\u90e8\u5206 dem2 = 0 for ( user , ratings ) in userRatings . items (): if item1 in ratings and item2 in ratings : avg = averages [ user ] num += ( ratings [ item1 ] - avg ) * ( ratings [ item2 ] - avg ) dem1 += ( ratings [ item1 ] - avg ) ** 2 dem2 += ( ratings [ item1 ] - avg ) ** 2 return num / ( math . sqrt ( dem1 ) * math . sqrt ( dem2 ))","title":"\u4fee\u6b63\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6"},{"location":"gdm/ch2/#_4","text":"\u90a3\u4e0b\u9762\u8be5\u5982\u4f55\u4f7f\u7528\u5b83\u6765\u505a\u9884\u6d4b\u5462\uff1f\u6bd4\u5982\u6211\u60f3\u77e5\u9053David\u6709\u591a\u559c\u6b22Kacey Musgraves\uff1f\u5728\u8a08\u7b97\u5b8c similarity \u4e4b\u5f8c\uff0c\u4e0b\u4e00\u6b65\u9a5f\u5c31\u662f\u8981\u9032\u884c\u67d0\u500b item \u7684\u9810\u6e2c\uff0c\u9019\u88e1\u6709\u5169\u7a2e\u65b9\u6cd5\uff0c\u5206\u5225\u662f\uff1a weighted-sum \u548c regression\u3002 weighted-sum p(u,i) = \\frac{{\\sum_{N\\in \\text{similarTo}(i)}(S_{i,N}\\times R_{u,N})}}{\\sum_{N\\in \\text{similarTo}(i)}|S_{i,N}|} p(u,i) = \\frac{{\\sum_{N\\in \\text{similarTo}(i)}(S_{i,N}\\times R_{u,N})}}{\\sum_{N\\in \\text{similarTo}(i)}|S_{i,N}|} \u5176\u4e2d p(u,i) p(u,i) \u8868\u793a\u9884\u6d4b\u7684\u7528\u6237 u u \u5bf9\u7269\u54c1 i i \u7684\u8bc4\u5206\uff0c S_{i,N} S_{i,N} \u8868\u793a\u7269\u54c1 i i \u548c N N \u7684\u76f8\u4f3c\u5ea6\uff0c R_{u,N} R_{u,N} \u8868\u793a\u7528\u6237 u u \u5bf9\u7269\u54c1 N N \u7684\u8bc4\u5206\u3002 N N \u662f\u4e00\u4e2a\u7269\u54c1\u7684\u96c6\u5408\uff0c\u6709\u5982\u4e0b\u7279\u6027\uff1a\u7528\u6237 u u \u5bf9\u96c6\u5408\u4e2d\u7684\u7269\u54c1\u6253\u8fc7\u5206\uff0c\u7269\u54c1 i i \u548c\u96c6\u5408\u4e2d\u7684\u7269\u54c1\u6709\u76f8\u4f3c\u5ea6\u6570\u636e\u3002","title":"\u9884\u6d4b"},{"location":"gdm/ch2/#python","text":"class ItemBasedCF : \u4f7f\u7528\u4fee\u6b63\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5b9e\u73b0\u7269\u54c1\u63a8\u8350 def __init__ ( self , data ): initialize data :param data: a dict of (user, ratings) For instance, users2 = { Amy : { Taylor Swift : 4, PSY : 3, Whitney Houston : 4}, Clara : { PSY : 3.5, Whitney Houston : 4}, Daisy : { Taylor Swift : 5, Whitney Houston : 3}} self . data = data self . items = set () # a set of items # compute average user ratings of given data self . avg_user_rating = {} for user , ratings in data . items (): self . avg_user_rating [ user ] = np . average ( list ( ratings . values ())) for item in ratings . keys (): self . items . add ( item ) def adjusted_cosine_similarity ( self , item1 , item2 ): Compute adjusted cosine similarity :param item1: an item :param item2: an item :return: similarity if ( item1 not in self . items ) or ( item2 not in self . items ): raise ( Exception ( Input Item NOT FOUND! )) num = 0 # numerator den1 = 0 # denominator1 den2 = 0 # denominator2 for user , ratings in self . data . items (): if item1 in ratings and item2 in ratings : x = ratings [ item1 ] - self . avg_user_rating [ user ] y = ratings [ item2 ] - self . avg_user_rating [ user ] num += x * y den1 += x * x den2 += y * y den = np . sqrt ( den1 * den2 ) # denominator if den == 0 : return None return num / den def predict ( self , user , item ): predict rating of given user on given item :param user: an user :param item: an item :return: rating if user not in self . data : raise ( Exception ( Input User NOT FOUND! )) if item not in self . items : raise ( Exception ( Input Item NOT FOUND! )) num = 0 # numerator den = 0 # denominator for another_item , rating in self . data [ user ] . items (): if item != another_item : similarity = self . adjusted_cosine_similarity ( item , another_item ) if similarity is None : continue num += rating * similarity den += abs ( similarity ) if den == 0 : return None return num / den def recommend ( self , user ): recommend items to the user :param user: an user :return: a list of items(up to 10) recommendations = [] for item in self . items : if item not in self . data [ user ]: predict = self . predict ( user , item ) if predict is not None : recommendations . append (( predict , item )) recommendations . sort ( reverse = True ) return list ( map ( lambda x : x [ 1 ], recommendations ))","title":"\u4f7f\u7528Python\u5b9e\u73b0\u4fee\u6b63\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7b97\u6cd5"},{"location":"gdm/ch2/#slope-one","text":"Slope One\u662f\u53e6\u4e00\u79cd\u6bd4\u8f83\u6d41\u884c\u7684\u57fa\u4e8e\u7269\u54c1\u7684\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\u3002\u5b83\u6700\u5927\u7684\u4f18\u52bf\u662f\u7b80\u5355\uff0c\u6613\u4e8e\u5b9e\u73b0\u3002Slope One\u7b97\u6cd5\u662f\u5728\u4e00\u7bc7\u540d\u4e3a\u300a Slope One Predictors for Online Rating-Based Collaborative Filtering \u300b\u7684\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\uff0c\u7531Lemire\u548cMachlachlan\u5408\u8457\u3002\u8fd9\u7bc7\u8bba\u6587\u975e\u5e38\u503c\u5f97\u4e00\u8bfb\u3002 \u6211\u4eec\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\u6765\u4e86\u89e3\u8fd9\u4e2a\u7b97\u6cd5\u3002\u5047\u8bbeA\u7ed9Item I\u6253\u4e861\u5206\uff0c\u7ed9Item J\u6253\u4e861.5\u5206\uff1bB\u7ed9Item I\u6253\u4e862\u5206\u3002\u6211\u4eec\u53ef\u4ee5\u7528\u4ee5\u4e0b\u903b\u8f91\u6765\u9884\u6d4bB\u5bf9Item J\u7684\u8bc4\u5206\uff1a\u7531\u4e8eA\u7ed9Item J\u6253\u7684\u5206\u6570\u8981\u6bd4Item I\u7684\u9ad80.5\u5206\uff0c\u6240\u4ee5\u6211\u4eec\u9884\u6d4bB\u4e5f\u4f1a\u7ed9\u9ad80.5\u5206\uff0c\u53732.5\u5206\u3002 \u53ef\u4ee5\u5c06Slope One\u5206\u4e3a\u4e24\u4e2a\u6b65\u9aa4\uff1a \u8ba1\u7b97\u51fa\u7269\u54c1\u4e4b\u95f4\u7684\u4e24\u4e24\u5dee\u503c(\u53ef\u4ee5\u5728\u591c\u95f4\u6279\u91cf\u8ba1\u7b97) \u8fdb\u884c\u9884\u6d4b\uff0c\u53ef\u4ee5\u4f7f\u7528\u52a0\u6743\u7684Slope One\u7b97\u6cd5","title":"Slope One\u7b97\u6cd5"},{"location":"gdm/ch2/#_5","text":"\u7269\u54c1 i i \u4e0e\u7269\u54c1 j j \u4e4b\u95f4\u7684\u5e73\u5747\u5dee\u5f02\u4e3a\uff1a \\text{dev}_{i,j}=\\sum_{u\\in S_{i,j}(X)}\\frac{u_i-u_j}{\\text{card}(S_{i,j}(X))} \\text{dev}_{i,j}=\\sum_{u\\in S_{i,j}(X)}\\frac{u_i-u_j}{\\text{card}(S_{i,j}(X))} \u5176\u4e2d S_{i,j}(X) S_{i,j}(X) \u8868\u793a\u540c\u65f6\u8bc4\u4ef7\u8fc7 i,j i,j \u7684\u7528\u6237\u96c6\u5408\uff0c \\text{card}(S) \\text{card}(S) \u8868\u793a S S \u4e2d\u6709\u591a\u5c11\u4e2a\u5143\u7d20\uff0c X X \u8868\u793a\u6240\u6709\u8bc4\u5206\u503c\u7684\u96c6\u5408\uff0c \\text{card}(S_{j,i}(X)) \\text{card}(S_{j,i}(X)) \u5219\u8868\u793a\u540c\u65f6\u8bc4\u4ef7\u8fc7\u7269\u54c1 i i \u548c j j \u7684\u7528\u6237\u6570\uff0c u_i u_i \u8868\u793a\u7528\u6237 u u \u5bf9\u7269\u54c1 i i \u7684\u8bc4\u5206\u3002 Question \u5982\u679c\u6709\u4e00\u4e2a\u65b0\u8fdb\u7684\u7528\u6237\u5bf910\u4e2a\u6b4c\u624b\u505a\u4e86\u8bc4\u4ef7\uff0c\u6211\u4eec\u662f\u5426\u9700\u8981\u91cd\u65b0\u8ba1\u7b9720\u4e07\u00d720\u4e07\u7684\u5dee\u5f02\u6570\u636e\uff0c\u6216\u662f\u6709\u5176\u4ed6\u66f4\u7b80\u5355\u7684\u65b9\u6cd5\uff1f \u7b54\u6848\u662f\u4f60\u4e0d\u9700\u8981\u8ba1\u7b97\u6574\u4e2a\u6570\u636e\u96c6\uff0c\u8fd9\u6b63\u662fSlope One\u7684\u7f8e\u5999\u4e4b\u5904\u3002\u5bf9\u4e8e\u4e24\u4e2a\u7269\u54c1\uff0c\u6211\u4eec\u53ea\u9700\u8bb0\u5f55\u540c\u65f6\u8bc4\u4ef7\u8fc7\u8fd9\u5bf9\u7269\u54c1\u7684\u7528\u6237\u6570\u5c31\u53ef\u4ee5\u4e86\u3002","title":"\u8ba1\u7b97\u5dee\u503c"},{"location":"gdm/ch2/#slope-one_1","text":"\u4f7f\u7528\u52a0\u6743\u7684Slope One\u7b97\u6cd5(Weighted Slope One, WS1)\u6765\u8fdb\u884c\u9884\u6d4b\uff0c\u7528 P^{WS1} P^{WS1} \u6765\u8868\u793a\u9884\u6d4b\u7528\u6237 u u \u5bf9\u7269\u54c1 j j \u7684\u8bc4\u5206\uff1a P^{WS1}(u)_j=\\frac{\\sum_{i\\in S(u) - \\{j\\}}(\\text{dev}_{i,j}+u_i)c_{j,i}}{\\sum_{i\\in S(u) - \\{j\\}} c_{j,i}} P^{WS1}(u)_j=\\frac{\\sum_{i\\in S(u) - \\{j\\}}(\\text{dev}_{i,j}+u_i)c_{j,i}}{\\sum_{i\\in S(u) - \\{j\\}} c_{j,i}} \u5176\u4e2d\uff1a c_{j,i}=\\text{card}(S_{j,i}(\\chi)) c_{j,i}=\\text{card}(S_{j,i}(\\chi)) \u3002\u5f0f\u4e2d \\sum_{i\\in S(u) - \\{j\\}} \\sum_{i\\in S(u) - \\{j\\}} \u8868\u793a\u7528\u6237 u u \u8bc4\u4ef7\u7684\u9664 j j \u9664\u5916\u7684\u7269\u54c1\uff0c\u5176\u4ed6\u7b26\u53f7\u4e0e\u4e0a\u4e00\u8282\u7684\u542b\u4e49\u76f8\u540c\u3002\u8fd9\u4e2a\u516c\u5f0f\u5176\u5b9e\u5f88\u597d\u7406\u89e3\uff0c \\text{dev}_{i,j}+u_i \\text{dev}_{i,j}+u_i \u8868\u793a\u6839\u636e\u7269\u54c1 i i \u9884\u6d4b\u5f97\u5230\u7684\u7528\u6237 u u \u5bf9\u7269\u54c1 j j \u7684\u8bc4\u5206\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u52a0\u6743\u5e73\u5747\u5c31\u5f97\u5230 P^{WS1} P^{WS1} \u3002","title":"\u4f7f\u7528\u52a0\u6743\u7684Slope One\u7b97\u6cd5\u8fdb\u884c\u9884\u6d4b"},{"location":"gdm/ch2/#pythonslope-one","text":"\u5176\u5b9e\u4ee3\u7801\u6574\u4f53\u601d\u8def\u6bd4\u8f83\u7b80\u5355\uff0c\u628a\u516c\u5f0f\u8f6c\u5316\u4e3a\u4ee3\u7801\u5c31\u53ef\u4ee5\u4e86\uff0c\u6ca1\u6709\u4ec0\u4e48\u7279\u6b8a\u7684\u6280\u5de7\u3002\u552f\u4e00\u9700\u8981\u6ce8\u610f\u7684\u662f\u5c3d\u53ef\u80fd\u7684\u6d4b\u8bd5\u8be5\u7a0b\u5e8f\uff0c\u4fdd\u8bc1\u5176\u6b63\u786e\u6027\u3002\u7531\u4e8e\u7f51\u4e0a\u6709\u4e0d\u5c11SlopeOne\u4ee3\u7801\uff0c\u6211\u81ea\u5df1\u5199\u5b8c\u4ee5\u540e\u548c\u4ed6\u4eec\u7684\u7ed3\u679c\u6bd4\u5bf9\uff0c\u53d1\u73b0\u5b8c\u5168\u4e00\u81f4\u3002 class SlopeOne : def __init__ ( self , data ): initialize :param data: a dictionary, whose key is an item, and value is a rating. self . data = data # frequencies, a dictionary, whose key is an item, # and value is a dictionary of (item, frequency) self . frequencies = {} # deviations, a dictionary, whose key is an item, # and value is a dictionary of (item, deviation) self . deviations = {} def computeDeviations ( self ): compute deviations between items :return: # \u83b7\u53d6\u6bcf\u4f4d\u7528\u6237\u7684\u8bc4\u5206\u6570\u636eratings for ratings in self . data . values (): # \u5bf9\u4e8e\u8be5\u7528\u6237\u7684\u6bcf\u4e2a\u8bc4\u5206\u9879(\u6b4c\u624b\u3001\u5206\u6570) for ( item1 , rating1 ) in ratings . items (): self . frequencies . setdefault ( item1 , {}) self . deviations . setdefault ( item1 , {}) # \u518d\u6b21\u904d\u5386\u8be5\u7528\u6237\u7684\u6bcf\u4e2a\u8bc4\u5206\u9879 for ( item2 , rating2 ) in ratings . items (): if item1 != item2 : # \u5c06\u8bc4\u5206\u7684\u5dee\u5f02\u4fdd\u5b58\u5230\u53d8\u91cf\u4e2d self . frequencies [ item1 ] . setdefault ( item2 , 0 ) self . deviations [ item1 ] . setdefault ( item2 , 0.0 ) self . frequencies [ item1 ][ item2 ] += 1 self . deviations [ item1 ][ item2 ] += ( rating1 - rating2 ) # \u8ba1\u7b97deviations for item1 , deviations in self . deviations . items (): for item2 in deviations : deviations [ item2 ] /= self . frequencies [ item1 ][ item2 ] def predict ( self , user , item ): predict the ratings of the user regard to the item Using Weighted Slope One (WSO) :param user: an user :param item: an item :return: a prediction, double if user not in self . data : raise Exception # predictions predictions = 0 frequency = 0 # \u7528\u6237user \u8bc4\u4ef7\u7684\u9664 item \u9664\u5916\u7684\u7269\u54c1 for diff_item , ratings in self . data [ user ] . items (): if ( item == diff_item ) or ( diff_item not in self . deviations [ item ]): continue predictions += ( self . deviations [ item ][ diff_item ] + self . data [ user ][ diff_item ]) \\ * self . frequencies [ item ][ diff_item ] frequency += self . frequencies [ item ][ diff_item ] predictions /= frequency return predictions def recommendation ( self , user ): recommend items to user :param user: an user :return: a list of items recommended if user not in self . data : raise Exception if self . deviations == {}: self . computeDeviations () recommendations = [] # \u6240\u6709\u7684item for item in self . deviations . keys (): # \u7528\u6237\u672a\u8bc4\u4ef7\u8fc7\u7684item if item not in self . data [ user ]: recommendations . append (( item , self . predict ( user , item ))) recommendations . sort ( key = lambda x : x [ 1 ], reverse = True ) return list ( map ( lambda x : x [ 0 ], recommendations ))","title":"\u4f7f\u7528Python\u5b9e\u73b0Slope One\u7b97\u6cd5"},{"location":"gdm/ch2/#example-movielens","text":"MovieLens\u6570\u636e\u96c6\u662f\u7531\u660e\u5c3c\u82cf\u8fbe\u5dde\u5927\u5b66\u7684GroupLens\u7814\u7a76\u9879\u76ee\u6536\u96c6\u7684\uff0c\u662f\u7528\u6237\u5bf9\u7535\u5f71\u7684\u8bc4\u5206\u3002 \u8fd9\u4e2a\u6570\u636e\u96c6\u53ef\u4ee5\u5728 www.grouplens.org \u4e0b\u8f7d\u3002\u5176\u4e2d100K\u6570\u636e\u96c6\u5305\u542b\u4e86943\u4f4d\u7528\u6237\u5bf91682\u90e8\u7535\u5f71\u7684\u8bc4\u4ef7\uff0c\u7ea610\u4e07\u6761\u8bb0\u5f55\u3002 \u4f7f\u7528MovieLens 100K\u6570\u636e\u96c6\u5904\u7406\u7684\u8fc7\u7a0b\u5982\u4e0b\uff1a \u6839\u636eREADME\u4e2d\u63cf\u8ff0\u7684\u6587\u4ef6\u683c\u5f0f\uff0c\u5c06\u7535\u5f71\u3001\u7528\u6237\u3001\u8bc4\u5206\u6570\u636e\u5bfc\u5165\u5e76\u8f6c\u5316\u4e3a\u5408\u9002\u7684\u6570\u636e\u683c\u5f0f \u5229\u7528SlopeOne\u8fdb\u884c\u63a8\u8350 \u5c06\u63a8\u8350\u7ed3\u679c\u5c55\u793a \u4ee3\u7801\u6574\u4f53\u975e\u5e38\u7b80\u6d01\u7684\uff0c\u56e0\u4e3a\u53ef\u4ee5\u76f4\u63a5\u7ee7\u627f\u4e0a\u9762\u5199\u7684SlopeOne\u7b97\u6cd5\u3002 class MovieRecommendation ( SlopeOne ): def __init__ ( self , data , movie ): super ( MovieRecommendation , self ) . __init__ ( data ) self . movie = movie def recommend_movie ( self , user ): movies = self . recommendation ( user )[: 10 ] return list ( map ( lambda x : self . movie [ x ][ title ], movies ))","title":"Example: MovieLens"},{"location":"gdm/ch2/#_6","text":"http://www.diva-portal.se/smash/get/diva2:811049/FULLTEXT01.pdf https://dzone.com/articles/slope-one-recommender https://www.slideshare.net/irecsys/slope-one-recommender-on-hadoop-15199798?from_action=save#","title":"\u8fdb\u4e00\u6b65\u9605\u8bfb"},{"location":"gdm/ch3/","text":"\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 3 \u5206\u7c7b \u8bd5\u60f3\u4e00\u4e2a\u6781\u7aef\u7684\u4f8b\u5b50\uff1a\u4e00\u4e2a\u65b0\u4e50\u961f\u53d1\u5e03\u4e86\u4e13\u8f91\uff0c\u8fd9\u5f20\u4e13\u8f91\u8fd8\u6ca1\u6709\u88ab\u4efb\u4f55\u7528\u6237\u8bc4\u4ef7\u6216\u8d2d\u4e70\u8fc7\uff0c\u5982\u679c\u662f\u7528\u534f\u540c\u8fc7\u6ee4\u7684\u8bdd\uff0c\u90a3\u5b83\u5c06\u6c38\u8fdc\u4e0d\u4f1a\u51fa\u73b0\u5728\u63a8\u8350\u5217\u8868\u4e2d\u3002 \u8fd9\u65f6\u6211\u4eec\u53ef\u4ee5 \u6839\u636e\u7269\u54c1\u7279\u5f81\u8fdb\u884c\u5206\u7c7b \u3002 \u6765\u770b\u4e00\u4e0b \u6f58\u591a\u62c9\u97f3\u4e50\u76d2 \u7684\u4f8b\u5b50\u3002\u5728\u8fd9\u4e2a\u7ad9\u70b9\u4e0a\u4f60\u53ef\u4ee5\u8bbe\u7acb\u5404\u79cd\u97f3\u4e50\u9891\u9053\uff0c\u53ea\u9700\u4e3a\u8fd9\u4e2a\u9891\u9053\u6dfb\u52a0\u4e00\u4e2a\u6b4c\u624b\uff0c\u6f58\u591a\u62c9\u5c31\u4f1a\u64ad\u653e\u548c\u8fd9\u4e2a\u6b4c\u624b\u98ce\u683c\u76f8\u7c7b\u4f3c\u7684\u6b4c\u66f2\u3002\u539f\u56e0\u5728\u4e8e\u6f58\u591a\u62c9\u7f51\u7ad9\u7684\u63a8\u8350\u7cfb\u7edf\u662f\u57fa\u4e8e\u4e00\u4e2a\u540d\u4e3a\u97f3\u4e50\u57fa\u56e0\u7684\u9879\u76ee\u3002 \u4ed6\u4eec\u96c7\u4f63\u4e86\u4e13\u4e1a\u7684\u97f3\u4e50\u5bb6\u5bf9\u6b4c\u66f2\u8fdb\u884c\u5206\u7c7b\uff08\u63d0\u53d6\u5b83\u4eec\u7684\u201c\u57fa\u56e0\u201d\uff09\uff1b\u8fd9\u4e9b\u4e13\u5bb6\u8981\u7504\u522b400\u591a\u79cd\u97f3\u4e50\u7279\u5f81\u3002 \u5177\u4f53\u65b9\u6cd5\u662f\u5c06\u6bcf\u79cd\u6b4c\u66f2\u7c7b\u578b\u62c6\u5206\u6210\u5355\u72ec\u7684\u7279\u5f81\uff0c\u5e76\u5bf9\u6b64\u8fdb\u884c\u6253\u5206\u3002\u6bcf\u4e2a\u7279\u5f81\u90fd\u662f1\u52305\u5206\u7684\u5c3a\u5ea6\uff0c0.5\u5206\u4e3a\u4e00\u6863\u30021\u5206\u8868\u793a\u5b8c\u5168\u4e0d\u7b26\u5408\uff0c5\u5206\u5219\u8868\u793a\u5f88\u76f8\u7b26\u3002 \u4e0b\u8868\u4f7f\u7528\u4e00\u4e9b\u97f3\u4e50\u7279\u5f81(\u4f7f\u7528\u94a2\u7434\u7684\u7a0b\u5ea6\u3001\u4f7f\u7528\u7f8e\u58f0\u7684\u7a0b\u5ea6\u3001\u8282\u594f\u7b49)\u5bf9\u6b4c\u66f2\u8fdb\u884c\u8bc4\u5206\uff1a \u53ef\u4ee5\u5c06\u4e0a\u8868\u5b58\u6210\u4e8c\u7ef4HashTable\u7684\u5f62\u5f0f\uff0c\u53ef\u4ee5\u76f4\u63a5\u5229\u7528\u7b2c\u4e00\u7ae0\u4e2d\u7684\u5404\u79cd\u8ddd\u79bb\u8ba1\u7b97\u516c\u5f0f\u8ba1\u7b97\u6b4c\u66f2\u4e4b\u95f4\u7684\u8ddd\u79bb\u6765\u8fdb\u884c\u63a8\u8350\u3002\u6309\u6211\u7684\u7406\u89e3\uff0c\u53ef\u4ee5\u8fd9\u4e48\u770b\u5f85\u95ee\u9898\uff0c\u6216\u8bb8\u66f4\u52a0\u5bb9\u6613\u7406\u89e3\uff1a\u628a\u8fd9\u4e9b\u7279\u5f81(piano, vocals, driving beat)\u770b\u6210\u662f\u7528\u6237\uff0c\u5404\u4e2a\u7279\u5f81\u4e0a\u7684\u8bc4\u5206\u53ef\u4ee5\u770b\u6210\u662f\u7528\u6237\u8bc4\u5206\uff0c\u628a\u57fa\u4e8e\u7528\u6237\u7684\u534f\u540c\u8fc7\u6ee4\u770b\u6210\u662f\u57fa\u4e8e\u7279\u5f81\u7684\u534f\u540c\u8fc7\u6ee4\u3002 \u8bc4\u5206\u6807\u51c6 \u5047\u5982\u6211\u60f3\u589e\u52a0\u4e00\u79cd\u97f3\u4e50\u7279\u5f81\u2014\u2014\u6bcf\u5206\u949f\u7684\u9f13\u70b9\u6570\uff08bpm\uff09\uff0c\u7528\u6765\u5224\u65ad\u8fd9\u662f\u4e00\u9996\u5feb\u6b4c\u8fd8\u662f\u6162\u6b4c\u3002\u7531\u4e8ebpm\u53d6\u503c\u8303\u56f4(100~200\u5de6\u53f3)\u4e0e\u524d\u9762(1~5)\u7684\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4bpm\u57fa\u672c\u51b3\u5b9a\u4e86\u4e24\u9996\u6b4c\u7684\u8ddd\u79bb\u3002 \u8bc4\u5206\u6807\u51c6\u4e0d\u4e00\u662f\u6240\u6709\u63a8\u8350\u7cfb\u7edf\u7684\u5927\u654c\uff01\u8ba9\u6570\u636e\u53d8\u5f97\u53ef\u7528\u6211\u4eec\u53ef\u4ee5\u5bf9\u5176\u8fdb\u884c \u6807\u51c6\u5316 (normalization)\u3002 Rescaling/Min-max normalization \u6700\u5e38\u7528\u7684\u65b9\u6cd5\u662f\u5c06\u6240\u6709\u6570\u636e\u90fd\u8f6c\u5316\u4e3a0\u52301\u4e4b\u95f4\u7684\u503c\uff1a \u5c06\u6bcf\u4e2a\u503c\u51cf\u53bb\u6700\u5c0f\u503c\uff0c\u5e76\u9664\u4ee5\u8303\u56f4\uff1a \\frac{x_i-\\min}{\\max-\\min} \\frac{x_i-\\min}{\\max-\\min} z-score normalization/Standardization \u8fd8\u53ef\u4ee5\u4f7f\u7528 \u6807\u51c6\u5206 (z-score or standard score)\uff1a\u5206\u503c\u504f\u79bb\u5747\u503c\u7684\u7a0b\u5ea6\uff0c \\text{standard score} = \\frac{x_i-\\bar x}{\\sigma} \\text{standard score} = \\frac{x_i-\\bar x}{\\sigma} Modified Z-Score \u6807\u51c6\u5206\u7684\u95ee\u9898\u5728\u4e8e\u5b83\u4f1a\u53d7\u5f02\u5e38\u503c\u7684\u5f71\u54cd\u3002 \u6bd4\u5982\u8bf4\u4e00\u5bb6\u516c\u53f8\u6709100\u540d\u5458\u5de5\uff0c\u666e\u901a\u5458\u5de5\u6bcf\u5c0f\u65f6\u8d5a10\u7f8e\u5143\uff0c\u800cCEO\u4e00\u5e74\u80fd\u8d5a600\u4e07\uff0c\u90a3\u5168\u516c\u53f8\u7684\u5e73\u5747\u65f6\u85aa\u4e3a38\u7f8e\u5143\u3002\u5728\u4fee\u6b63\u7684\u6807\u51c6\u5206(Modified Z-Score)\u4e2d\uff0c\u5c06\u6807\u51c6\u5206\u516c\u5f0f\u4e2d\u7684\u5747\u503c\u6539\u4e3a\u4e2d\u4f4d\u6570(median)\uff0c\u5c06\u6807\u51c6\u5dee\u6539\u4e3a \u5e73\u5747\u7edd\u5bf9\u504f\u5dee (MAD, Mean Absolute Deviation[ wiki ])\u3002 \\text{modified z-score}= \\frac{x_i-\\text{median}}{\\text{MAD}} \\text{modified z-score}= \\frac{x_i-\\text{median}}{\\text{MAD}} \\text{mean absolute deviation(MAD)} = \\frac{\\sum_i |x_i-\\text{median}|}{\\text{card}(x)} \\text{mean absolute deviation(MAD)} = \\frac{\\sum_i |x_i-\\text{median}|}{\\text{card}(x)} \u662f\u5426\u9700\u8981\u6807\u51c6\u5316\uff1f \u9700\u8981\u8fdb\u884c\u6807\u51c6\u5316\u7684\u60c5\u5f62\uff1a \u6211\u4eec\u9700\u8981\u901a\u8fc7\u7269\u54c1\u7279\u6027\u6765\u8ba1\u7b97\u8ddd\u79bb\uff1b \u4e0d\u540c\u7279\u6027\u4e4b\u95f4\u7684\u5c3a\u5ea6\u76f8\u5dee\u5f88\u5927\u3002 Python\u6570\u636e\u683c\u5f0f \u4e0a\u9762\u6211\u4eec\u7528\u4e8c\u7ef4HashTable\u7684\u5f62\u5f0f\u6765\u5b58\u50a8\u6b4c\u66f2\u7684\u7279\u5f81\u6570\u636e\uff0c\u8fd9\u6837\u505a\u867d\u7136\u53ef\u884c\uff0c\u4f46\u5374\u6bd4\u8f83\u7e41\u7410\uff0cpiano\u3001vocals\u8fd9\u6837\u7684\u952e\u540d\u9700\u8981\u91cd\u590d\u5f88\u591a\u6b21\u3002\u6211\u4eec\u53ef\u4ee5\u5c06\u5176\u7b80\u5316\u4e3a\u5411\u91cf\uff0c\u5373Python\u4e2d\u7684\u6570\u7ec4\u7c7b\u578b\uff1a # \u7269\u54c1\u5411\u91cf\u4e2d\u7684\u7279\u5f81\u4f9d\u6b21\u4e3a\uff1apiano, vocals, beat, blues, guitar, backup vocals, rap items = { Dr Dog/Fate : [ 2.5 , 4 , 3.5 , 3 , 5 , 4 , 1 ], Phoenix/Lisztomania : [ 2 , 5 , 5 , 3 , 2 , 1 , 1 ], Heartless Bastards/Out : [ 1 , 5 , 4 , 2 , 4 , 1 , 1 ], Black Eyed Peas/Rock That Body : [ 2 , 5 , 5 , 1 , 2 , 2 , 4 ], Lady Gaga/Alejandro : [ 1 , 5 , 3 , 2 , 1 , 2 , 1 ] } \u90a3\u4e48\u7528\u6237\u201c\u8d5e\u201d\u548c\u201c\u8e29\u201d\u7684\u6570\u636e\u7528\u4ec0\u4e48\u8868\u793a\u5462\uff1f\u7531\u4e8e\u7528\u6237\u5e76\u4e0d\u4f1a\u5bf9\u6240\u6709\u7684\u6b4c\u66f2\u90fd\u505a\u8fd9\u4e9b\u64cd\u4f5c\uff0c\u6240\u4ee5\u7528\u5d4c\u5957\u7684\u5b57\u5178\u6765\u8868\u793a\u6bd4\u8f83\u65b9\u4fbf\uff1a users = { Angelica : { Dr Dog/Fate : L , Phoenix/Lisztomania : L , Mike Posner : D , Black Eyed Peas/Rock That Body : D , Lady Gaga/Alejandro : L }, Bill : { Dr Dog/Fate : L , Phoenix/Lisztomania : L , Heartless Bastards/Out at Sea : L , Black Eyed Peas/Rock That Body : D , Lady Gaga/Alejandro : D }} \u8fd9\u91cc\u4f7f\u7528L\u548cD\u4e24\u4e2a\u5b57\u6bcd\u6765\u8868\u793a\u559c\u6b22\u548c\u4e0d\u559c\u6b22\u3002 \u5206\u7c7b\u5668 \u5206\u7c7b\u5668\u662f\u6307\u901a\u8fc7\u7269\u54c1\u7279\u5f81\u6765\u5224\u65ad\u5b83\u5e94\u8be5\u5c5e\u4e8e\u54ea\u4e2a\u7ec4\u6216\u7c7b\u522b\u7684\u7a0b\u5e8f\u3002\u5206\u7c7b\u5668\u7a0b\u5e8f\u4f1a\u57fa\u4e8e\u4e00\u7ec4\u5df2\u7ecf\u505a\u8fc7\u5206\u7c7b\u7684\u7269\u54c1\u8fdb\u884c\u5b66\u4e60\uff0c\u4ece\u800c\u5224\u65ad\u65b0\u7269\u54c1\u7684\u6240\u5c5e\u7c7b\u522b\u3002 \u5efa\u7acb\u4e00\u4e2a\u5206\u7c7b\u51fd\u6570\uff0c\u7528\u6765\u9884\u6d4b\u7528\u6237\u5bf9\u4e00\u4e2a\u65b0\u7269\u54c1\u7684\u559c\u597d;\u8fd9\u4e2a\u51fd\u6570\u4f1a\u5148\u8ba1\u7b97\u51fa\u4e0e\u8fd9\u4e2a\u7269\u54c1\u8ddd\u79bb\u6700\u8fd1\u7684\u7269\u54c1\uff0c\u7136\u540e\u627e\u5230\u7528\u6237\u5bf9\u8fd9\u4e2a\u6700\u8fd1\u7269\u54c1\u7684\u8bc4\u4ef7\uff0c\u4ee5\u6b64\u4f5c\u4e3a\u65b0\u7269\u54c1\u7684\u9884\u6d4b\u503c\u3002 class simple_classifier : def __init__ ( self , users , items ): self . users = users self . items = items def computeNearestNeighbor ( self , itemName , itemVector , items ): \u6309\u7167\u8ddd\u79bb\u6392\u5e8f\uff0c\u8fd4\u56de\u90bb\u8fd1\u7269\u54c1\u5217\u8868 :param itemName :param itemVector :param items distances = [] for otherItem in items : if otherItem != itemName : distance = DistanceVector . manhattan ( itemVector , items [ otherItem ]) distances . append (( distance , otherItem )) # \u6700\u8fd1\u7684\u6392\u5728\u524d\u9762 distances . sort () return distances def classify ( self , user , itemName , itemVector ): nearest = self . computeNearestNeighbor ( itemName , itemVector , self . items )[ 0 ][ 1 ] rating = self . users [ user ][ nearest ] return rating \u8ba9\u6211\u4eec\u8bd5\u7528\u4e00\u4e0b\uff1a classify ( Angelica , Chris Cagle/I Breathe In. I Breathe Out , [ 1 , 5 , 2.5 , 1 , 1 , 5 , 1 ]) L \u5176\u5b9e\u6211\u4eec\u505a\u7684\u662f\u4e00\u4e2a\u5206\u7c7b\u5668\uff0c\u5c06\u6b4c\u66f2\u5206\u4e3a\u4e86\u7528\u6237\u559c\u6b22\u548c\u4e0d\u559c\u6b22\u4e24\u4e2a\u7c7b\u522b\u3002 Example:\u8fd0\u52a8\u9879\u76ee \u5148\u770b\u4e00\u4e2a\u8f83\u4e3a\u7b80\u5355\u7684\u4f8b\u5b50\u2014\u2014\u6839\u636e\u5973\u8fd0\u52a8\u5458\u7684\u8eab\u9ad8\u548c\u4f53\u91cd\u6765\u5224\u65ad\u5979\u4eec\u662f\u4ece\u4e8b\u4ec0\u4e48\u8fd0\u52a8\u9879\u76ee\u7684\u3002\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e\u5206\u522b\u6765\u81eaathletesTrainingSet.txt\u548cathletesTestSet.txt\u3002\u6587\u4ef6\u683c\u5f0f\u5982\u4e0b\uff1a comment class num num Asuka Teramoto Gymnastics 54 66 Brittainey Raven Basketball 72 162 \u6570\u636e\u683c\u5f0f \u5728Python\u4e2d\u5e94\u8be5\u5982\u4f55\u5408\u9002\u7684\u8868\u793a\u8fd9\u4e9b\u6570\u636e\u5462\uff1f\u7531\u4e8e\u5206\u7c7b\u5668\u7a0b\u5e8f\u6839\u672c\u4e0d\u4f1a\u4f7f\u7528\u5230\u59d3\u540d\uff0c\u6240\u4ee5\u7528\u59d3\u540d\u4f5c\u4e3a\u952e\u503c\u7684\u5b57\u5178\u662f\u4e0d\u5408\u9002\u7684\u3002\u7531\u4e8e\u6211\u4eec\u9700\u8981\u904d\u5386\u6587\u4ef6\u7684\u6570\u636e\uff0c\u6240\u4ee5\u4f7f\u7528list\u7c7b\u578b\u662f\u5408\u7406\u7684\u3002\u5982\u4e0b\u662f\u6700\u5408\u7406\u7684\uff1a [( Gymnastics , [ 54 , 66 ], [ Asuka Termoto ]), ( Basketball , [ 72 , 162 ], [ Brittainey Raven ]), ... ] \u5b83\u5c06\u4e0d\u540c\u7c7b\u578b\u7684\u6570\u636e\u533a\u522b\u5f00\u6765\u4e86\uff0c\u4f9d\u6b21\u662f\u5206\u7c7b\u3001\u7279\u5f81\u3001\u5907\u6ce8\u3002 Python\u4ee3\u7801 class AthleteClassifier : def __init__ ( self , filename = None , format = None , header = True ): initialize :param format: num, comment, class :param filename: filename for training data :param header: whether header line exist # format self . format = format # self.data format: class, fields, comments self . data = None # a list of median self . medians = [] # mean absolute deviation self . mad = [] # test result self . test_result = None # classes self . classes = set () # training data if a file is provided if filename and format : # data self . data = self . pre_processing ( filename , header ) # mad self . mean_absolute_deviation () # normalize self . normalizeColumn () def pre_processing ( self , filename , header = True ): read file and transform to data format :param filename: filename for training data :param header: whether header line exist file = codecs . open ( filename , mode = r , encoding = utf-8 ) # skip header if header : file . readline () data = [] n = len ( self . format ) for line in file . readlines (): fields = line . strip ( \\n ) . split ( \\t ) if len ( fields ) != n : raise ( Exception ( Input File Format Error )) vector , comment , classification = ([], , None ) for i in range ( n ): field , field_format = fields [ i ], self . format [ i ] if field_format == num : vector . append ( float ( field )) elif field_format == class : classification = field self . classes . add ( field ) elif field_format == comment : comment += ( field + ) data . append ([ classification , vector , comment . strip ()]) return data def mean_absolute_deviation ( self ): calculate mean absolute deviation of each fields for data for i in range ( len ( self . data [ 0 ][ 1 ])): # calculate median field = list ( map ( lambda x : x [ 1 ][ i ], self . data )) self . medians . append ( np . median ( field )) self . mad . append ( sum ( abs ( np . array ( field ) - self . medians [ i ])) / len ( field )) def normalizeColumn ( self ): given a column number, normalize that column in self.data using the Modified Standard Score Modified Standard Score = score - median /(maxScore - minScore) for item in self . data : for j in range ( len ( item [ 1 ])): item [ 1 ][ j ] = ( item [ 1 ][ j ] - self . medians [ j ]) / self . mad [ j ] def normalizeVector ( self , itemVector ): given a vector, having the same format as the vector in the self.data[0][1], normalize it using a modified z-score :param itemVector: a vector, size of fields in self.data :return: a vector normazalied return [( itemVector [ j ] - self . medians [ j ]) / self . mad [ j ] for j in range ( len ( itemVector ))] def nearest_neighbor ( self , itemVector ): find nearest neighbor :param itemVector: a vector :return: nearest neighbor and the distance between them distance = [( DistanceVector . manhattan ( np . array ( itemVector ), np . array ( item [ 1 ])), item ) for item in self . data ] return min ( distance ) def classify ( self , itemVector ): \u9884\u6d4bitemVector\u7684\u5206\u7c7b :return: the result of the classification return self . nearest_neighbor ( self . normalizeVector ( itemVector ))[ 1 ][ 0 ] def initialize_test_result ( self ): initialize test result self . test_result = {} classes = list ( self . classes ) classes . sort () for class_name in classes : self . test_result . setdefault ( class_name , {}) for another_class_name in classes : self . test_result [ class_name ][ another_class_name ] = 0 def test ( self , testfile , header = True ): test file, return accuracy :param testfile: a file :param header: whether header line exist :return: double, accuracy accurate = 0 # check if test result is initialized if not self . test_result : self . initialize_test_result () test_data = self . pre_processing ( testfile , header ) for item in test_data : classify_result = self . classify ( item [ 1 ]) self . test_result [ item [ 0 ]][ classify_result ] += 1 if classify_result == item [ 0 ]: accurate += 1 #else: #print(item, not equals , classify_result) return round ( accurate / float ( len ( test_data )) * 100 , 1 ) \u7ed3\u679c\u6d4b\u8bd5\u51c6\u786e\u7387\u4e3a84%\u3002 classifier = AthleteClassifier ( training_file , [ comment , class , num , num ], header = True ) print ( classifier . test ( test_file , header = True )) #84.2 Example: \u9e22\u5c3e\u82b1 \u9e22\u5c3e\u82b1\u6570\u636e\u96c6\u5728\u6570\u636e\u6316\u6398\u9886\u57df\u662f\u6bd4\u8f83\u6709\u540d\u7684\u3002 \u5b83\u662f20\u4e16\u7eaa30\u5e74\u4ee3Ronald Fisher\u5bf9\u4e09\u79cd\u9e22\u5c3e\u82b1\u768450\u4e2a\u6837\u672c\u505a\u7684\u6d4b\u91cf\u6570\u636e(\u843c\u7247\u548c\u82b1\u74e3)\u3002\u8bad\u7ec3\u96c6\u4e2d\u6709120\u6761\u6570\u636e\uff0c\u6d4b\u8bd5\u96c6\u4e2d\u670930\u6761\uff0c\u4e24\u8005\u6ca1\u6709\u4ea4\u96c6\u3002 \u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\u4f7f\u7528\u540c\u6837\u7684\u5206\u7c7b\u5668\u670993.1\u7684\u6b63\u786e\u7387\u3002\u5982\u679c\u8ba1\u7b97\u8ddd\u79bb\u65f6\uff0c\u4f7f\u7528\u6b27\u5f0f\u8ddd\u79bb\uff0c\u5206\u7c7b\u5668\u670996.6%\u7684\u6b63\u786e\u7387\u3002 classifier = AthleteClassifier ( training_file , [ num , num , num , num , class ], header = True ) classifier . test ( test_file , header = False ) # 93.1% correct Example: \u6bcf\u52a0\u4ed1\u71c3\u6cb9\u516c\u91cc\u6570 \u5361\u5185\u57fa\u6885\u9686\u5927\u5b66\u7edf\u8ba1\u7684\u6c7d\u8f66\u71c3\u6cb9\u6d88\u8017\u548c\u516c\u91cc\u6570\u6570\u636e\u4e5f\u662f\u4e00\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u3002\u5927\u81f4\u683c\u5f0f\u5982\u4e0b\uff1a \u6839\u636e\u6c7d\u7f38\u6570\u3001\u6392\u6c14\u91cf\u3001\u9a6c\u529b\u3001\u91cd\u91cf\u3001 \u52a0\u901f\u5ea6\u7b49\u6570\u636e\u9884\u6d4b\u6bcf\u52a0\u4ed1\u71c3\u6cb9\u516c\u91cc\u6570(mpg)\u3002\u5982\u679c\u4e0d\u8fdb\u884c\u6807\u51c6\u5316\uff0c\u51c6\u786e\u7387\u5c06\u53ea\u670932%\u3002\u6807\u51c6\u5316\u4e4b\u540e\u670955%\u51c6\u786e\u7387\u3002","title":"Chapter 3: \u5206\u7c7b"},{"location":"gdm/ch3/#-3","text":"\u8bd5\u60f3\u4e00\u4e2a\u6781\u7aef\u7684\u4f8b\u5b50\uff1a\u4e00\u4e2a\u65b0\u4e50\u961f\u53d1\u5e03\u4e86\u4e13\u8f91\uff0c\u8fd9\u5f20\u4e13\u8f91\u8fd8\u6ca1\u6709\u88ab\u4efb\u4f55\u7528\u6237\u8bc4\u4ef7\u6216\u8d2d\u4e70\u8fc7\uff0c\u5982\u679c\u662f\u7528\u534f\u540c\u8fc7\u6ee4\u7684\u8bdd\uff0c\u90a3\u5b83\u5c06\u6c38\u8fdc\u4e0d\u4f1a\u51fa\u73b0\u5728\u63a8\u8350\u5217\u8868\u4e2d\u3002 \u8fd9\u65f6\u6211\u4eec\u53ef\u4ee5 \u6839\u636e\u7269\u54c1\u7279\u5f81\u8fdb\u884c\u5206\u7c7b \u3002 \u6765\u770b\u4e00\u4e0b \u6f58\u591a\u62c9\u97f3\u4e50\u76d2 \u7684\u4f8b\u5b50\u3002\u5728\u8fd9\u4e2a\u7ad9\u70b9\u4e0a\u4f60\u53ef\u4ee5\u8bbe\u7acb\u5404\u79cd\u97f3\u4e50\u9891\u9053\uff0c\u53ea\u9700\u4e3a\u8fd9\u4e2a\u9891\u9053\u6dfb\u52a0\u4e00\u4e2a\u6b4c\u624b\uff0c\u6f58\u591a\u62c9\u5c31\u4f1a\u64ad\u653e\u548c\u8fd9\u4e2a\u6b4c\u624b\u98ce\u683c\u76f8\u7c7b\u4f3c\u7684\u6b4c\u66f2\u3002\u539f\u56e0\u5728\u4e8e\u6f58\u591a\u62c9\u7f51\u7ad9\u7684\u63a8\u8350\u7cfb\u7edf\u662f\u57fa\u4e8e\u4e00\u4e2a\u540d\u4e3a\u97f3\u4e50\u57fa\u56e0\u7684\u9879\u76ee\u3002 \u4ed6\u4eec\u96c7\u4f63\u4e86\u4e13\u4e1a\u7684\u97f3\u4e50\u5bb6\u5bf9\u6b4c\u66f2\u8fdb\u884c\u5206\u7c7b\uff08\u63d0\u53d6\u5b83\u4eec\u7684\u201c\u57fa\u56e0\u201d\uff09\uff1b\u8fd9\u4e9b\u4e13\u5bb6\u8981\u7504\u522b400\u591a\u79cd\u97f3\u4e50\u7279\u5f81\u3002 \u5177\u4f53\u65b9\u6cd5\u662f\u5c06\u6bcf\u79cd\u6b4c\u66f2\u7c7b\u578b\u62c6\u5206\u6210\u5355\u72ec\u7684\u7279\u5f81\uff0c\u5e76\u5bf9\u6b64\u8fdb\u884c\u6253\u5206\u3002\u6bcf\u4e2a\u7279\u5f81\u90fd\u662f1\u52305\u5206\u7684\u5c3a\u5ea6\uff0c0.5\u5206\u4e3a\u4e00\u6863\u30021\u5206\u8868\u793a\u5b8c\u5168\u4e0d\u7b26\u5408\uff0c5\u5206\u5219\u8868\u793a\u5f88\u76f8\u7b26\u3002 \u4e0b\u8868\u4f7f\u7528\u4e00\u4e9b\u97f3\u4e50\u7279\u5f81(\u4f7f\u7528\u94a2\u7434\u7684\u7a0b\u5ea6\u3001\u4f7f\u7528\u7f8e\u58f0\u7684\u7a0b\u5ea6\u3001\u8282\u594f\u7b49)\u5bf9\u6b4c\u66f2\u8fdb\u884c\u8bc4\u5206\uff1a \u53ef\u4ee5\u5c06\u4e0a\u8868\u5b58\u6210\u4e8c\u7ef4HashTable\u7684\u5f62\u5f0f\uff0c\u53ef\u4ee5\u76f4\u63a5\u5229\u7528\u7b2c\u4e00\u7ae0\u4e2d\u7684\u5404\u79cd\u8ddd\u79bb\u8ba1\u7b97\u516c\u5f0f\u8ba1\u7b97\u6b4c\u66f2\u4e4b\u95f4\u7684\u8ddd\u79bb\u6765\u8fdb\u884c\u63a8\u8350\u3002\u6309\u6211\u7684\u7406\u89e3\uff0c\u53ef\u4ee5\u8fd9\u4e48\u770b\u5f85\u95ee\u9898\uff0c\u6216\u8bb8\u66f4\u52a0\u5bb9\u6613\u7406\u89e3\uff1a\u628a\u8fd9\u4e9b\u7279\u5f81(piano, vocals, driving beat)\u770b\u6210\u662f\u7528\u6237\uff0c\u5404\u4e2a\u7279\u5f81\u4e0a\u7684\u8bc4\u5206\u53ef\u4ee5\u770b\u6210\u662f\u7528\u6237\u8bc4\u5206\uff0c\u628a\u57fa\u4e8e\u7528\u6237\u7684\u534f\u540c\u8fc7\u6ee4\u770b\u6210\u662f\u57fa\u4e8e\u7279\u5f81\u7684\u534f\u540c\u8fc7\u6ee4\u3002","title":"\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 3 \u5206\u7c7b"},{"location":"gdm/ch3/#_1","text":"\u5047\u5982\u6211\u60f3\u589e\u52a0\u4e00\u79cd\u97f3\u4e50\u7279\u5f81\u2014\u2014\u6bcf\u5206\u949f\u7684\u9f13\u70b9\u6570\uff08bpm\uff09\uff0c\u7528\u6765\u5224\u65ad\u8fd9\u662f\u4e00\u9996\u5feb\u6b4c\u8fd8\u662f\u6162\u6b4c\u3002\u7531\u4e8ebpm\u53d6\u503c\u8303\u56f4(100~200\u5de6\u53f3)\u4e0e\u524d\u9762(1~5)\u7684\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4bpm\u57fa\u672c\u51b3\u5b9a\u4e86\u4e24\u9996\u6b4c\u7684\u8ddd\u79bb\u3002 \u8bc4\u5206\u6807\u51c6\u4e0d\u4e00\u662f\u6240\u6709\u63a8\u8350\u7cfb\u7edf\u7684\u5927\u654c\uff01\u8ba9\u6570\u636e\u53d8\u5f97\u53ef\u7528\u6211\u4eec\u53ef\u4ee5\u5bf9\u5176\u8fdb\u884c \u6807\u51c6\u5316 (normalization)\u3002","title":"\u8bc4\u5206\u6807\u51c6"},{"location":"gdm/ch3/#rescalingmin-max-normalization","text":"\u6700\u5e38\u7528\u7684\u65b9\u6cd5\u662f\u5c06\u6240\u6709\u6570\u636e\u90fd\u8f6c\u5316\u4e3a0\u52301\u4e4b\u95f4\u7684\u503c\uff1a \u5c06\u6bcf\u4e2a\u503c\u51cf\u53bb\u6700\u5c0f\u503c\uff0c\u5e76\u9664\u4ee5\u8303\u56f4\uff1a \\frac{x_i-\\min}{\\max-\\min} \\frac{x_i-\\min}{\\max-\\min}","title":"Rescaling/Min-max normalization"},{"location":"gdm/ch3/#z-score-normalizationstandardization","text":"\u8fd8\u53ef\u4ee5\u4f7f\u7528 \u6807\u51c6\u5206 (z-score or standard score)\uff1a\u5206\u503c\u504f\u79bb\u5747\u503c\u7684\u7a0b\u5ea6\uff0c \\text{standard score} = \\frac{x_i-\\bar x}{\\sigma} \\text{standard score} = \\frac{x_i-\\bar x}{\\sigma}","title":"z-score normalization/Standardization"},{"location":"gdm/ch3/#modified-z-score","text":"\u6807\u51c6\u5206\u7684\u95ee\u9898\u5728\u4e8e\u5b83\u4f1a\u53d7\u5f02\u5e38\u503c\u7684\u5f71\u54cd\u3002 \u6bd4\u5982\u8bf4\u4e00\u5bb6\u516c\u53f8\u6709100\u540d\u5458\u5de5\uff0c\u666e\u901a\u5458\u5de5\u6bcf\u5c0f\u65f6\u8d5a10\u7f8e\u5143\uff0c\u800cCEO\u4e00\u5e74\u80fd\u8d5a600\u4e07\uff0c\u90a3\u5168\u516c\u53f8\u7684\u5e73\u5747\u65f6\u85aa\u4e3a38\u7f8e\u5143\u3002\u5728\u4fee\u6b63\u7684\u6807\u51c6\u5206(Modified Z-Score)\u4e2d\uff0c\u5c06\u6807\u51c6\u5206\u516c\u5f0f\u4e2d\u7684\u5747\u503c\u6539\u4e3a\u4e2d\u4f4d\u6570(median)\uff0c\u5c06\u6807\u51c6\u5dee\u6539\u4e3a \u5e73\u5747\u7edd\u5bf9\u504f\u5dee (MAD, Mean Absolute Deviation[ wiki ])\u3002 \\text{modified z-score}= \\frac{x_i-\\text{median}}{\\text{MAD}} \\text{modified z-score}= \\frac{x_i-\\text{median}}{\\text{MAD}} \\text{mean absolute deviation(MAD)} = \\frac{\\sum_i |x_i-\\text{median}|}{\\text{card}(x)} \\text{mean absolute deviation(MAD)} = \\frac{\\sum_i |x_i-\\text{median}|}{\\text{card}(x)}","title":"Modified Z-Score"},{"location":"gdm/ch3/#_2","text":"\u9700\u8981\u8fdb\u884c\u6807\u51c6\u5316\u7684\u60c5\u5f62\uff1a \u6211\u4eec\u9700\u8981\u901a\u8fc7\u7269\u54c1\u7279\u6027\u6765\u8ba1\u7b97\u8ddd\u79bb\uff1b \u4e0d\u540c\u7279\u6027\u4e4b\u95f4\u7684\u5c3a\u5ea6\u76f8\u5dee\u5f88\u5927\u3002","title":"\u662f\u5426\u9700\u8981\u6807\u51c6\u5316\uff1f"},{"location":"gdm/ch3/#python","text":"\u4e0a\u9762\u6211\u4eec\u7528\u4e8c\u7ef4HashTable\u7684\u5f62\u5f0f\u6765\u5b58\u50a8\u6b4c\u66f2\u7684\u7279\u5f81\u6570\u636e\uff0c\u8fd9\u6837\u505a\u867d\u7136\u53ef\u884c\uff0c\u4f46\u5374\u6bd4\u8f83\u7e41\u7410\uff0cpiano\u3001vocals\u8fd9\u6837\u7684\u952e\u540d\u9700\u8981\u91cd\u590d\u5f88\u591a\u6b21\u3002\u6211\u4eec\u53ef\u4ee5\u5c06\u5176\u7b80\u5316\u4e3a\u5411\u91cf\uff0c\u5373Python\u4e2d\u7684\u6570\u7ec4\u7c7b\u578b\uff1a # \u7269\u54c1\u5411\u91cf\u4e2d\u7684\u7279\u5f81\u4f9d\u6b21\u4e3a\uff1apiano, vocals, beat, blues, guitar, backup vocals, rap items = { Dr Dog/Fate : [ 2.5 , 4 , 3.5 , 3 , 5 , 4 , 1 ], Phoenix/Lisztomania : [ 2 , 5 , 5 , 3 , 2 , 1 , 1 ], Heartless Bastards/Out : [ 1 , 5 , 4 , 2 , 4 , 1 , 1 ], Black Eyed Peas/Rock That Body : [ 2 , 5 , 5 , 1 , 2 , 2 , 4 ], Lady Gaga/Alejandro : [ 1 , 5 , 3 , 2 , 1 , 2 , 1 ] } \u90a3\u4e48\u7528\u6237\u201c\u8d5e\u201d\u548c\u201c\u8e29\u201d\u7684\u6570\u636e\u7528\u4ec0\u4e48\u8868\u793a\u5462\uff1f\u7531\u4e8e\u7528\u6237\u5e76\u4e0d\u4f1a\u5bf9\u6240\u6709\u7684\u6b4c\u66f2\u90fd\u505a\u8fd9\u4e9b\u64cd\u4f5c\uff0c\u6240\u4ee5\u7528\u5d4c\u5957\u7684\u5b57\u5178\u6765\u8868\u793a\u6bd4\u8f83\u65b9\u4fbf\uff1a users = { Angelica : { Dr Dog/Fate : L , Phoenix/Lisztomania : L , Mike Posner : D , Black Eyed Peas/Rock That Body : D , Lady Gaga/Alejandro : L }, Bill : { Dr Dog/Fate : L , Phoenix/Lisztomania : L , Heartless Bastards/Out at Sea : L , Black Eyed Peas/Rock That Body : D , Lady Gaga/Alejandro : D }} \u8fd9\u91cc\u4f7f\u7528L\u548cD\u4e24\u4e2a\u5b57\u6bcd\u6765\u8868\u793a\u559c\u6b22\u548c\u4e0d\u559c\u6b22\u3002","title":"Python\u6570\u636e\u683c\u5f0f"},{"location":"gdm/ch3/#_3","text":"\u5206\u7c7b\u5668\u662f\u6307\u901a\u8fc7\u7269\u54c1\u7279\u5f81\u6765\u5224\u65ad\u5b83\u5e94\u8be5\u5c5e\u4e8e\u54ea\u4e2a\u7ec4\u6216\u7c7b\u522b\u7684\u7a0b\u5e8f\u3002\u5206\u7c7b\u5668\u7a0b\u5e8f\u4f1a\u57fa\u4e8e\u4e00\u7ec4\u5df2\u7ecf\u505a\u8fc7\u5206\u7c7b\u7684\u7269\u54c1\u8fdb\u884c\u5b66\u4e60\uff0c\u4ece\u800c\u5224\u65ad\u65b0\u7269\u54c1\u7684\u6240\u5c5e\u7c7b\u522b\u3002 \u5efa\u7acb\u4e00\u4e2a\u5206\u7c7b\u51fd\u6570\uff0c\u7528\u6765\u9884\u6d4b\u7528\u6237\u5bf9\u4e00\u4e2a\u65b0\u7269\u54c1\u7684\u559c\u597d;\u8fd9\u4e2a\u51fd\u6570\u4f1a\u5148\u8ba1\u7b97\u51fa\u4e0e\u8fd9\u4e2a\u7269\u54c1\u8ddd\u79bb\u6700\u8fd1\u7684\u7269\u54c1\uff0c\u7136\u540e\u627e\u5230\u7528\u6237\u5bf9\u8fd9\u4e2a\u6700\u8fd1\u7269\u54c1\u7684\u8bc4\u4ef7\uff0c\u4ee5\u6b64\u4f5c\u4e3a\u65b0\u7269\u54c1\u7684\u9884\u6d4b\u503c\u3002 class simple_classifier : def __init__ ( self , users , items ): self . users = users self . items = items def computeNearestNeighbor ( self , itemName , itemVector , items ): \u6309\u7167\u8ddd\u79bb\u6392\u5e8f\uff0c\u8fd4\u56de\u90bb\u8fd1\u7269\u54c1\u5217\u8868 :param itemName :param itemVector :param items distances = [] for otherItem in items : if otherItem != itemName : distance = DistanceVector . manhattan ( itemVector , items [ otherItem ]) distances . append (( distance , otherItem )) # \u6700\u8fd1\u7684\u6392\u5728\u524d\u9762 distances . sort () return distances def classify ( self , user , itemName , itemVector ): nearest = self . computeNearestNeighbor ( itemName , itemVector , self . items )[ 0 ][ 1 ] rating = self . users [ user ][ nearest ] return rating \u8ba9\u6211\u4eec\u8bd5\u7528\u4e00\u4e0b\uff1a classify ( Angelica , Chris Cagle/I Breathe In. I Breathe Out , [ 1 , 5 , 2.5 , 1 , 1 , 5 , 1 ]) L \u5176\u5b9e\u6211\u4eec\u505a\u7684\u662f\u4e00\u4e2a\u5206\u7c7b\u5668\uff0c\u5c06\u6b4c\u66f2\u5206\u4e3a\u4e86\u7528\u6237\u559c\u6b22\u548c\u4e0d\u559c\u6b22\u4e24\u4e2a\u7c7b\u522b\u3002","title":"\u5206\u7c7b\u5668"},{"location":"gdm/ch3/#example","text":"\u5148\u770b\u4e00\u4e2a\u8f83\u4e3a\u7b80\u5355\u7684\u4f8b\u5b50\u2014\u2014\u6839\u636e\u5973\u8fd0\u52a8\u5458\u7684\u8eab\u9ad8\u548c\u4f53\u91cd\u6765\u5224\u65ad\u5979\u4eec\u662f\u4ece\u4e8b\u4ec0\u4e48\u8fd0\u52a8\u9879\u76ee\u7684\u3002\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e\u5206\u522b\u6765\u81eaathletesTrainingSet.txt\u548cathletesTestSet.txt\u3002\u6587\u4ef6\u683c\u5f0f\u5982\u4e0b\uff1a comment class num num Asuka Teramoto Gymnastics 54 66 Brittainey Raven Basketball 72 162","title":"Example:\u8fd0\u52a8\u9879\u76ee"},{"location":"gdm/ch3/#_4","text":"\u5728Python\u4e2d\u5e94\u8be5\u5982\u4f55\u5408\u9002\u7684\u8868\u793a\u8fd9\u4e9b\u6570\u636e\u5462\uff1f\u7531\u4e8e\u5206\u7c7b\u5668\u7a0b\u5e8f\u6839\u672c\u4e0d\u4f1a\u4f7f\u7528\u5230\u59d3\u540d\uff0c\u6240\u4ee5\u7528\u59d3\u540d\u4f5c\u4e3a\u952e\u503c\u7684\u5b57\u5178\u662f\u4e0d\u5408\u9002\u7684\u3002\u7531\u4e8e\u6211\u4eec\u9700\u8981\u904d\u5386\u6587\u4ef6\u7684\u6570\u636e\uff0c\u6240\u4ee5\u4f7f\u7528list\u7c7b\u578b\u662f\u5408\u7406\u7684\u3002\u5982\u4e0b\u662f\u6700\u5408\u7406\u7684\uff1a [( Gymnastics , [ 54 , 66 ], [ Asuka Termoto ]), ( Basketball , [ 72 , 162 ], [ Brittainey Raven ]), ... ] \u5b83\u5c06\u4e0d\u540c\u7c7b\u578b\u7684\u6570\u636e\u533a\u522b\u5f00\u6765\u4e86\uff0c\u4f9d\u6b21\u662f\u5206\u7c7b\u3001\u7279\u5f81\u3001\u5907\u6ce8\u3002","title":"\u6570\u636e\u683c\u5f0f"},{"location":"gdm/ch3/#python_1","text":"class AthleteClassifier : def __init__ ( self , filename = None , format = None , header = True ): initialize :param format: num, comment, class :param filename: filename for training data :param header: whether header line exist # format self . format = format # self.data format: class, fields, comments self . data = None # a list of median self . medians = [] # mean absolute deviation self . mad = [] # test result self . test_result = None # classes self . classes = set () # training data if a file is provided if filename and format : # data self . data = self . pre_processing ( filename , header ) # mad self . mean_absolute_deviation () # normalize self . normalizeColumn () def pre_processing ( self , filename , header = True ): read file and transform to data format :param filename: filename for training data :param header: whether header line exist file = codecs . open ( filename , mode = r , encoding = utf-8 ) # skip header if header : file . readline () data = [] n = len ( self . format ) for line in file . readlines (): fields = line . strip ( \\n ) . split ( \\t ) if len ( fields ) != n : raise ( Exception ( Input File Format Error )) vector , comment , classification = ([], , None ) for i in range ( n ): field , field_format = fields [ i ], self . format [ i ] if field_format == num : vector . append ( float ( field )) elif field_format == class : classification = field self . classes . add ( field ) elif field_format == comment : comment += ( field + ) data . append ([ classification , vector , comment . strip ()]) return data def mean_absolute_deviation ( self ): calculate mean absolute deviation of each fields for data for i in range ( len ( self . data [ 0 ][ 1 ])): # calculate median field = list ( map ( lambda x : x [ 1 ][ i ], self . data )) self . medians . append ( np . median ( field )) self . mad . append ( sum ( abs ( np . array ( field ) - self . medians [ i ])) / len ( field )) def normalizeColumn ( self ): given a column number, normalize that column in self.data using the Modified Standard Score Modified Standard Score = score - median /(maxScore - minScore) for item in self . data : for j in range ( len ( item [ 1 ])): item [ 1 ][ j ] = ( item [ 1 ][ j ] - self . medians [ j ]) / self . mad [ j ] def normalizeVector ( self , itemVector ): given a vector, having the same format as the vector in the self.data[0][1], normalize it using a modified z-score :param itemVector: a vector, size of fields in self.data :return: a vector normazalied return [( itemVector [ j ] - self . medians [ j ]) / self . mad [ j ] for j in range ( len ( itemVector ))] def nearest_neighbor ( self , itemVector ): find nearest neighbor :param itemVector: a vector :return: nearest neighbor and the distance between them distance = [( DistanceVector . manhattan ( np . array ( itemVector ), np . array ( item [ 1 ])), item ) for item in self . data ] return min ( distance ) def classify ( self , itemVector ): \u9884\u6d4bitemVector\u7684\u5206\u7c7b :return: the result of the classification return self . nearest_neighbor ( self . normalizeVector ( itemVector ))[ 1 ][ 0 ] def initialize_test_result ( self ): initialize test result self . test_result = {} classes = list ( self . classes ) classes . sort () for class_name in classes : self . test_result . setdefault ( class_name , {}) for another_class_name in classes : self . test_result [ class_name ][ another_class_name ] = 0 def test ( self , testfile , header = True ): test file, return accuracy :param testfile: a file :param header: whether header line exist :return: double, accuracy accurate = 0 # check if test result is initialized if not self . test_result : self . initialize_test_result () test_data = self . pre_processing ( testfile , header ) for item in test_data : classify_result = self . classify ( item [ 1 ]) self . test_result [ item [ 0 ]][ classify_result ] += 1 if classify_result == item [ 0 ]: accurate += 1 #else: #print(item, not equals , classify_result) return round ( accurate / float ( len ( test_data )) * 100 , 1 ) \u7ed3\u679c\u6d4b\u8bd5\u51c6\u786e\u7387\u4e3a84%\u3002 classifier = AthleteClassifier ( training_file , [ comment , class , num , num ], header = True ) print ( classifier . test ( test_file , header = True )) #84.2","title":"Python\u4ee3\u7801"},{"location":"gdm/ch3/#example_1","text":"\u9e22\u5c3e\u82b1\u6570\u636e\u96c6\u5728\u6570\u636e\u6316\u6398\u9886\u57df\u662f\u6bd4\u8f83\u6709\u540d\u7684\u3002 \u5b83\u662f20\u4e16\u7eaa30\u5e74\u4ee3Ronald Fisher\u5bf9\u4e09\u79cd\u9e22\u5c3e\u82b1\u768450\u4e2a\u6837\u672c\u505a\u7684\u6d4b\u91cf\u6570\u636e(\u843c\u7247\u548c\u82b1\u74e3)\u3002\u8bad\u7ec3\u96c6\u4e2d\u6709120\u6761\u6570\u636e\uff0c\u6d4b\u8bd5\u96c6\u4e2d\u670930\u6761\uff0c\u4e24\u8005\u6ca1\u6709\u4ea4\u96c6\u3002 \u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\u4f7f\u7528\u540c\u6837\u7684\u5206\u7c7b\u5668\u670993.1\u7684\u6b63\u786e\u7387\u3002\u5982\u679c\u8ba1\u7b97\u8ddd\u79bb\u65f6\uff0c\u4f7f\u7528\u6b27\u5f0f\u8ddd\u79bb\uff0c\u5206\u7c7b\u5668\u670996.6%\u7684\u6b63\u786e\u7387\u3002 classifier = AthleteClassifier ( training_file , [ num , num , num , num , class ], header = True ) classifier . test ( test_file , header = False ) # 93.1% correct","title":"Example: \u9e22\u5c3e\u82b1"},{"location":"gdm/ch3/#example_2","text":"\u5361\u5185\u57fa\u6885\u9686\u5927\u5b66\u7edf\u8ba1\u7684\u6c7d\u8f66\u71c3\u6cb9\u6d88\u8017\u548c\u516c\u91cc\u6570\u6570\u636e\u4e5f\u662f\u4e00\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u3002\u5927\u81f4\u683c\u5f0f\u5982\u4e0b\uff1a \u6839\u636e\u6c7d\u7f38\u6570\u3001\u6392\u6c14\u91cf\u3001\u9a6c\u529b\u3001\u91cd\u91cf\u3001 \u52a0\u901f\u5ea6\u7b49\u6570\u636e\u9884\u6d4b\u6bcf\u52a0\u4ed1\u71c3\u6cb9\u516c\u91cc\u6570(mpg)\u3002\u5982\u679c\u4e0d\u8fdb\u884c\u6807\u51c6\u5316\uff0c\u51c6\u786e\u7387\u5c06\u53ea\u670932%\u3002\u6807\u51c6\u5316\u4e4b\u540e\u670955%\u51c6\u786e\u7387\u3002","title":"Example: \u6bcf\u52a0\u4ed1\u71c3\u6cb9\u516c\u91cc\u6570"},{"location":"gdm/ch4/","text":"\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 4 \u8fdb\u4e00\u6b65\u63a2\u7d22\u5206\u7c7b \u6548\u679c\u8bc4\u4f30\u7b97\u6cd5\u548ckNN \u5f53\u6211\u4eec\u6784\u5efa\u5b8c\u4e00\u4e2a\u5206\u7c7b\u5668\u540e\uff0c\u5e94\u8be5\u95ee\u4ee5\u4e0b\u95ee\u9898\uff1a\u5206\u7c7b\u5668\u7684\u51c6\u786e\u5ea6\u5982\u4f55\uff1f \u7ed3\u679c\u7406\u60f3\u5417\uff1f \u5982\u4f55\u4e0e\u5176\u5b83\u5206\u7c7b\u5668\u505a\u6bd4\u8f83\uff1f \u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6 \u6570\u636e\u96c6\u53ef\u4ee5\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\uff0c\u7b2c\u4e00\u90e8\u5206\u7528\u6765\u6784\u9020\u5206\u7c7b\u5668\uff0c\u79f0\u4e3a \u8bad\u7ec3\u96c6 \uff1b\u53e6\u4e00\u90e8\u5206\u7528\u6765\u8bc4\u4f30\u5206\u7c7b\u5668\u7684\u7ed3\u679c\uff0c\u79f0\u4e3a \u6d4b\u8bd5\u96c6 \u3002 k\u6298\u4ea4\u53c9\u9a8c\u8bc1 \u4e0d\u8fc7\uff0c\u8fd9\u79cd\u4e0a\u9762\u8fd9\u79cd\u7b80\u5355\u3001\u76f4\u63a5\u7684\u505a\u6cd5\u4f3c\u4e4e\u6709\u95ee\u9898\uff1a\u5982\u679c\u5206\u5272\u7684\u65f6\u5019\u4e0d\u51d1\u5de7\uff0c\u5c31\u4f1a\u5f15\u53d1\u5f02\u5e38\u3002\u6bd4\u5982\uff0c\u82e5\u6d4b\u8bd5\u96c6\u4e2d\u7684\u7bee\u7403\u8fd0\u52a8\u5458\u6070\u5de7\u90fd\u5f88\u77ee\uff0c\u5979\u4eec\u5c31\u4f1a\u88ab\u5f52\u4e3a\u9a6c\u62c9\u677e\u8fd0\u52a8\u5458\u3002\u89e3\u51b3\u65b9\u6cd5\u4e4b\u4e00\u662f\u5c06\u6570\u636e\u96c6\u6309\u4e0d\u540c\u7684\u65b9\u5f0f\u62c6\u5206\uff0c\u6d4b\u8bd5\u591a\u6b21\uff0c\u53d6\u7ed3\u679c\u7684\u5e73\u5747\u503c\u3002 \u5982\u679c\u5c06\u6570\u636e\u96c6\u5206\u6210 k k \u4efd\uff0c\u7528 k-1 k-1 \u4efd\u6765\u505a\u8bad\u7ec3\u96c6\uff0c\u53e6\u4e00\u4efd\u6765\u505a\u6d4b\u8bd5\u96c6\uff0c\u5e76\u8fed\u4ee3 k k \u6b21\uff0c\u53eb\u505a k k \u6298\u4ea4\u53c9\u9a8c\u8bc1 ( k k -fold cross validation)\u3002\u5728\u6570\u636e\u6316\u6398\u4e2d\uff0c\u901a\u5e38\u7684\u505a\u6cd5\u662f\u5c06\u6570\u636e\u96c6\u62c6\u5206\u6210\u5341\u4efd\uff0c\u79f0\u4e3a \u5341\u6298\u4ea4\u53c9\u9a8c\u8bc1 \u3002 \u6211\u4eec\u6765\u770b\u4e00\u4e2a\u793a\u4f8b\uff1a\u5047\u8bbe\u6709\u4e00\u4e2a\u5206\u7c7b\u5668\u80fd\u5224\u65ad\u67d0\u4e2a\u4eba\u662f\u5426\u662f\u7bee\u7403\u8fd0\u52a8\u5458\uff0c\u6570\u636e\u96c6\u5305\u542b500\u4e2a\u8fd0\u52a8\u5458\u548c500\u4e2a\u666e\u901a\u4eba\u3002 \u7b2c\u4e00\u6b65\uff1a\u5c06\u6570\u636e\u5206\u621010\u4efd\uff0c\u6bcf\u4efd\u5305\u542b50\u4e2a\u7bee\u7403\u8fd0\u52a8\u5458\uff0c50\u4e2a\u666e\u901a\u4eba \u7b2c\u4e8c\u6b65\uff1a\u91cd\u590d\u4ee5\u4e0b\u6b65\u9aa410\u6b21 \u6bcf\u6b21\u8fed\u4ee3\u6211\u4eec\u4fdd\u7559\u4e00\u4efd\uff0c\u6bd4\u5982\u7b2c\u4e00\u6b21\u8fed\u4ee3\u4fdd\u7559\u7b2c1\u4efd\uff0c\u7b2c\u4e8c\u6b21\u4fdd\u7559\u7b2c2\u4efd\u3002 \u6211\u4eec\u4f7f\u7528\u5269\u4f59\u76849\u4efd\u6765\u8bad\u7ec3\u5206\u7c7b\u5668\uff0c\u6bd4\u5982\u7b2c\u4e00\u6b21\u8fed\u4ee3\u4f7f\u7528\u7b2c2\u81f310\u4efd\u6765\u8bad\u7ec3\u3002 \u6211\u4eec\u7528\u521a\u624d\u4fdd\u7559\u7684\u4e00\u4efd\u6765\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u8bb0\u5f55\u7ed3\u679c\uff0c\u6bd4\u5982\uff1a35\u4e2a\u7bee\u7403\u8fd0\u52a8\u5458\u5206\u7c7b\u6b63\u786e\uff0c 29\u4e2a\u666e\u901a\u4eba\u5206\u7c7b\u6b63\u786e\u3002 \u7b2c\u4e09\u6b65\uff1a\u5408\u5e76\u7ed3\u679c \u6211\u4eec\u53ef\u4ee5\u7528\u4e00\u5f20\u8868\u683c\u6765\u5c55\u793a\u7ed3\u679c(\u5c31\u662f\u9a6c\u4e0a\u8981\u63d0\u5230\u7684\u6df7\u6dc6\u77e9\u9635)\uff1a \u901a\u8fc7\u5341\u6298\u4ea4\u53c9\u9a8c\u8bc1\u5f97\u5230\u7684\u8bc4\u4ef7\u7ed3\u679c\u80af\u5b9a\u4f1a\u6bd4\u4e8c\u6298\u6216\u8005\u4e09\u6298\u6765\u5f97\u51c6\u786e\uff0c\u56e0\u4e3a\u4f7f\u7528\u4e86\u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u3002 \u7559\u4e00\u6cd5 \u65e2\u7136\u5341\u6298\u4ea4\u53c9\u9a8c\u8bc1\u6548\u679c\u90a3\u4e48\u597d\uff0c\u6211\u4eec\u4e3a\u4f55\u4e0d\u505a\u4e00\u4e2a N N \u6298\u4ea4\u53c9\u9a8c\u8bc1\uff1f N N \u5373\u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u91cf\u3002\u5728\u6570\u636e\u6316\u6398\u9886\u57df\uff0c N N \u6298\u4ea4\u53c9\u9a8c\u8bc1\u53c8\u79f0\u4e3a \u7559\u4e00\u6cd5 \u3002 \u4f18\u70b9\uff1a \u51e0\u4e4e\u6240\u6709\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u7136\u540e\u7528\u4e00\u4e2a\u6570\u636e\u8fdb\u884c\u6d4b\u8bd5\u3002 \u786e\u5b9a\u6027\u3002 \u7f3a\u70b9\uff1a \u6700\u5927\u7684\u7f3a\u70b9\u662f\u8ba1\u7b97\u65f6\u95f4\u5f88\u957f\u3002 \u5206\u5c42\u95ee\u9898 \u5728\u7559\u4e00\u6cd5\u4e2d\uff0c\u6240\u6709\u7684\u6d4b\u8bd5\u96c6\u90fd\u53ea\u5305\u542b\u4e00\u4e2a\u6570\u636e(\u5206\u5c42\u95ee\u9898)\u3002\u6240\u4ee5\u8bf4\uff0c\u7559\u4e00\u6cd5\u5bf9\u5c0f\u6570\u636e\u96c6\u662f\u5408\u9002\u7684\uff0c\u4f46\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6211\u4eec\u4f1a\u9009\u62e9\u5341\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3002 \u6df7\u6dc6\u77e9\u9635 \u6df7\u6dc6\u77e9\u9635(confusion matrix)\u53ef\u4ee5\u5c55\u73b0\u66f4\u4e3a\u8be6\u7ec6\u7684\u8bc4\u4ef7\u7ed3\u679c\u3002\u6df7\u6dc6\u77e9\u9635\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u5feb\u901f\u8bc6\u522b\u51fa\u5206\u7c7b\u5668\u5230\u5e95\u5728\u54ea\u4e9b\u7c7b\u522b\u4e0a\u53d1\u751f\u4e86\u6df7\u6dc6\uff0c\u56e0\u6b64\u5f97\u540d\u3002\u8868\u683c\u7684\u884c\u8868\u793a\u6d4b\u8bd5\u7528\u4f8b\u5b9e\u9645\u6240\u5c5e\u7684\u7c7b\u522b\uff0c\u5217\u5219\u8868\u793a\u5206\u7c7b\u5668\u7684\u5224\u65ad\u7ed3\u679c\u3002 \u5728\u8fd9\u4e2a\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u7684\u5206\u7c7b\u5668\u53ef\u4ee5\u5f88\u597d\u5730\u533a\u5206\u4f53\u64cd\u8fd0\u52a8\u5458\u548c\u7bee\u7403\u8fd0\u52a8\u5458\uff0c\u800c\u9a6c\u62c9\u677e\u8fd0\u52a8\u5458\u5219\u6bd4\u8f83\u5bb9\u6613\u548c\u5176\u4ed6\u4e24\u4e2a\u7c7b\u522b\u53d1\u751f\u6df7\u6dc6\u3002 Application: \u6bcf\u52a0\u4ed1\u71c3\u6cb9\u516c\u91cc\u6570 \u5361\u5185\u57fa\u6885\u9686\u5927\u5b66\u7edf\u8ba1\u7684\u6c7d\u8f66\u71c3\u6cb9\u6d88\u8017\u548c\u516c\u91cc\u6570\u6570\u636e\u4e5f\u662f\u4e00\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u3002\u5728 \u524d\u9762\u4e00\u7ae0 \u4e2d\u5df2\u7ecf\u4ecb\u7ecd\u8fc7\u3002 \u6839\u636e\u6c7d\u7f38\u6570\u3001\u6392\u6c14\u91cf\u3001\u9a6c\u529b\u3001\u91cd\u91cf\u3001 \u52a0\u901f\u5ea6\u7b49\u6570\u636e\u9884\u6d4b\u6bcf\u52a0\u4ed1\u71c3\u6cb9\u516c\u91cc\u6570(mpg)\u3002392\u6761\u6570\u636e\u90fd\u5b58\u653e\u5728mpgData.txt\u6587\u4ef6\u4e2d\uff0c\u5e76\u7528\u4e0b\u9762\u8fd9\u6bb5Python\u4ee3\u7801\u5c06\u8fd9\u4e9b\u6570\u636e\u6309\u5c42\u6b21\u7b49\u5206\u6210\u5341\u4efd\uff1a # divide data into 10 buckets import random def buckets ( filename , bucket_name , separator , class_column ): @param filename: file name of the original data. @param bucket_name: the prefix for all the bucket names. @param separator: the character that divides the columns. @param class_column: column that indicates the class, start from 0 # put the data in 10 buckets number_of_buckets = 10 data = {} # first read in the data and divide by category with open ( filename ) as f : lines = f . readlines () for line in lines : if separator != \\t : line = line . replace ( separator , \\t ) # first get the category category = line . split ()[ class_column ] data . setdefault ( category , []) data [ category ] . append ( line ) # initialize the buckets buckets = [] for i in range ( number_of_buckets ): buckets . append ([]) # now for each category put the data into the buckets for k in data . keys (): # randomize order of instances for each class random . shuffle ( data [ k ]) bNum = 0 # divide into buckets for item in data [ k ]: buckets [ bNum ] . append ( item ) bNum = ( bNum + 1 ) % number_of_buckets # write to file for bNum in range ( number_of_buckets ): f = open ( %s - %02i % ( bucket_name , bNum + 1 ), w ) for item in buckets [ bNum ]: f . write ( item ) f . close () # example of how to use this code buckets ( mpgTrainingSet , mpgData , \\t , 1 ) \u6267\u884c\u8fd9\u4e2a\u7a0b\u5e8f\u540e\u4f1a\u751f\u621010\u4e2a\u6587\u4ef6\uff1ampgData01\u3001mpgData02\u7b49\u3002\u7136\u540e\u9700\u8981\u4fee\u6539\u4e0a\u4e00\u7ae0\u7684\u8fd1\u90bb\u7b97\u6cd5\u7a0b\u5e8f\uff0c\u8ba9AthleteClassifier.test()\u51fd\u6570\u80fd\u591f\u6267\u884c\u5341\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3002 def ten_fold_cross_validation ( self , bucket_name , separator , format , header = True ): 10-fold cross validation :param bucket_name: the prefix for all the bucket names. :param separator: the character that divides the columns. :param format: num, comment, class :param header: whether header line exist :return: accuracy self . format = format self . test_result = {} accuracy = [] # repeat 10 times, select test bucket for test for test_bucket_number in range ( 10 ): # select remaining 9 bucket for training training_data = [] for bucket_number in range ( 10 ): if test_bucket_number != bucket_number : training_filename = %s%s%02i % ( bucket_name , separator , bucket_number + 1 ) training_data . extend ( self . pre_processing ( training_filename , header )) self . data = training_data # normalize self . mean_absolute_deviation () self . normalizeColumn () # test test_filename = %s%s%02i % ( bucket_name , separator , test_bucket_number + 1 ) accuracy . append ( self . test ( test_filename , header )) # print output print ( , end = ) for actual in self . test_result . keys (): print ( %5s % actual , end = ) print ( \\n , +---- * len ( self . classes ), + ) for actual , test_results in self . test_result . items (): print ( %3s % actual , end = | ) for accuracy_count in test_results . values (): print ( %3s % accuracy_count , end = | ) print ( \\n ) print ( , +---- * len ( self . classes ), + ) avg_accuracy = np . average ( accuracy ) print ( accuray is , avg_accuracy ) return avg_accuracy Kappa\u6307\u6807 Kappa\u6307\u6807\u53ef\u4ee5\u7528\u6765\u8bc4\u4ef7\u5206\u7c7b\u5668\u7684\u6548\u679c\u6bd4\u968f\u673a\u5206\u7c7b\u8981\u597d\u591a\u5c11\uff0c\u516c\u5f0f\u4e3a \\kappa = \\frac{P(c)-P(r)}{1-P(r)} \\kappa = \\frac{P(c)-P(r)}{1-P(r)} P(c) P(c) \u8868\u793a\u5206\u7c7b\u5668\u7684\u51c6\u786e\u7387\uff0c P(r) P(r) \u8868\u793a\u968f\u673a\u5206\u7c7b\u5668\u7684\u51c6\u786e\u7387\u3002\u53ef\u4ee5\u53c2\u8003\u4e0b\u5217\u7ecf\u9a8c\u7ed3\u679c\uff1a kNN\u7b97\u6cd5 \u5176\u5b9ekNN\u7b97\u6cd5\u5df2\u7ecf\u5728 \u7b2c\u4e00\u7ae0 \u91cc\u53d9\u8ff0\u5e76\u4f7f\u7528\u4e86\u3002\u5f53\u65f6\u8ba1\u7b97\u8ddd\u79bb\u65f6\uff0c\u4f7f\u7528\u7684\u662f\u76ae\u5c14\u68ee\u76f8\u5173\u7cfb\u6570\u3002\u90a3\u4e48\uff0c\u5982\u679c\u662f\u66fc\u54c8\u987f\u8ddd\u79bb\u5462\uff1f\u9996\u5148\uff0c\u5c06\u8ddd\u79bb\u53d6\u5012\u6570\uff0c\u7136\u540e\u628a\u6240\u6709\u7684\u8ddd\u79bb\u5012\u6570\u9664\u4ee5\u8ddd\u79bb\u5012\u6570\u7684\u548c\uff0c\u4ece\u800c\u5f97\u5230\u8bc4\u5206\u7684\u6743\u91cd\uff0c\u6700\u540e\u52a0\u6743\u5e73\u5747\u5f97\u5230\u8bc4\u5206\u3002 \u5176\u5b9e\u628a\u8ddd\u79bb\u7684\u5012\u6570\u770b\u6210\u662f\u76f8\u4f3c\u5ea6\uff0c\u5c31\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u4e86\u3002 Example: \u6bd4\u9a6c\u5370\u7b2c\u5b89\u4eba\u7cd6\u5c3f\u75c5 \u8fd9\u4e00\u8282\u5206\u6790\u7531\u7f8e\u56fd\u56fd\u5bb6\u7cd6\u5c3f\u75c5\u3001\u6d88\u5316\u548c\u80be\u810f\u75be\u75c5\u7814\u7a76\u6240\u63d0\u4f9b\u7684\u6bd4\u9a6c\u5370\u7b2c\u5b89\u4eba\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6( \u4e0b\u8f7d )\u3002\u6570\u636e\u96c6\u4e2d\u7684\u4e00\u6761\u8bb0\u5f55\u4ee3\u8868\u4e00\u540d21\u5c81\u4ee5\u4e0a\u7684\u6bd4\u9a6c\u5973\u6027\uff0c\u5979\u4eec\u5206\u7c7b\u4e24\u7c7b\uff1a\u4e94\u5e74\u5185\u67e5\u51fa\u60a3\u6709\u7cd6\u5c3f\u75c5\uff0c\u4ee5\u53ca\u6ca1\u6709\u5f97\u75c5\u3002\u6570\u636e\u96c6\u4e2d\u5305\u542b\u4e868\u4e2a\u7279\u5f81\uff0c\u5982\u6000\u5b55\u6b21\u6570\uff0c\u8212\u5f20\u538b\u7b49\u3002","title":"Chapter 4: \u8fdb\u4e00\u6b65\u63a2\u7d22\u5206\u7c7b"},{"location":"gdm/ch4/#-4","text":"","title":"\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 4 \u8fdb\u4e00\u6b65\u63a2\u7d22\u5206\u7c7b"},{"location":"gdm/ch4/#knn","text":"\u5f53\u6211\u4eec\u6784\u5efa\u5b8c\u4e00\u4e2a\u5206\u7c7b\u5668\u540e\uff0c\u5e94\u8be5\u95ee\u4ee5\u4e0b\u95ee\u9898\uff1a\u5206\u7c7b\u5668\u7684\u51c6\u786e\u5ea6\u5982\u4f55\uff1f \u7ed3\u679c\u7406\u60f3\u5417\uff1f \u5982\u4f55\u4e0e\u5176\u5b83\u5206\u7c7b\u5668\u505a\u6bd4\u8f83\uff1f","title":"\u6548\u679c\u8bc4\u4f30\u7b97\u6cd5\u548ckNN"},{"location":"gdm/ch4/#_1","text":"\u6570\u636e\u96c6\u53ef\u4ee5\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\uff0c\u7b2c\u4e00\u90e8\u5206\u7528\u6765\u6784\u9020\u5206\u7c7b\u5668\uff0c\u79f0\u4e3a \u8bad\u7ec3\u96c6 \uff1b\u53e6\u4e00\u90e8\u5206\u7528\u6765\u8bc4\u4f30\u5206\u7c7b\u5668\u7684\u7ed3\u679c\uff0c\u79f0\u4e3a \u6d4b\u8bd5\u96c6 \u3002","title":"\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6"},{"location":"gdm/ch4/#k","text":"\u4e0d\u8fc7\uff0c\u8fd9\u79cd\u4e0a\u9762\u8fd9\u79cd\u7b80\u5355\u3001\u76f4\u63a5\u7684\u505a\u6cd5\u4f3c\u4e4e\u6709\u95ee\u9898\uff1a\u5982\u679c\u5206\u5272\u7684\u65f6\u5019\u4e0d\u51d1\u5de7\uff0c\u5c31\u4f1a\u5f15\u53d1\u5f02\u5e38\u3002\u6bd4\u5982\uff0c\u82e5\u6d4b\u8bd5\u96c6\u4e2d\u7684\u7bee\u7403\u8fd0\u52a8\u5458\u6070\u5de7\u90fd\u5f88\u77ee\uff0c\u5979\u4eec\u5c31\u4f1a\u88ab\u5f52\u4e3a\u9a6c\u62c9\u677e\u8fd0\u52a8\u5458\u3002\u89e3\u51b3\u65b9\u6cd5\u4e4b\u4e00\u662f\u5c06\u6570\u636e\u96c6\u6309\u4e0d\u540c\u7684\u65b9\u5f0f\u62c6\u5206\uff0c\u6d4b\u8bd5\u591a\u6b21\uff0c\u53d6\u7ed3\u679c\u7684\u5e73\u5747\u503c\u3002 \u5982\u679c\u5c06\u6570\u636e\u96c6\u5206\u6210 k k \u4efd\uff0c\u7528 k-1 k-1 \u4efd\u6765\u505a\u8bad\u7ec3\u96c6\uff0c\u53e6\u4e00\u4efd\u6765\u505a\u6d4b\u8bd5\u96c6\uff0c\u5e76\u8fed\u4ee3 k k \u6b21\uff0c\u53eb\u505a k k \u6298\u4ea4\u53c9\u9a8c\u8bc1 ( k k -fold cross validation)\u3002\u5728\u6570\u636e\u6316\u6398\u4e2d\uff0c\u901a\u5e38\u7684\u505a\u6cd5\u662f\u5c06\u6570\u636e\u96c6\u62c6\u5206\u6210\u5341\u4efd\uff0c\u79f0\u4e3a \u5341\u6298\u4ea4\u53c9\u9a8c\u8bc1 \u3002 \u6211\u4eec\u6765\u770b\u4e00\u4e2a\u793a\u4f8b\uff1a\u5047\u8bbe\u6709\u4e00\u4e2a\u5206\u7c7b\u5668\u80fd\u5224\u65ad\u67d0\u4e2a\u4eba\u662f\u5426\u662f\u7bee\u7403\u8fd0\u52a8\u5458\uff0c\u6570\u636e\u96c6\u5305\u542b500\u4e2a\u8fd0\u52a8\u5458\u548c500\u4e2a\u666e\u901a\u4eba\u3002 \u7b2c\u4e00\u6b65\uff1a\u5c06\u6570\u636e\u5206\u621010\u4efd\uff0c\u6bcf\u4efd\u5305\u542b50\u4e2a\u7bee\u7403\u8fd0\u52a8\u5458\uff0c50\u4e2a\u666e\u901a\u4eba \u7b2c\u4e8c\u6b65\uff1a\u91cd\u590d\u4ee5\u4e0b\u6b65\u9aa410\u6b21 \u6bcf\u6b21\u8fed\u4ee3\u6211\u4eec\u4fdd\u7559\u4e00\u4efd\uff0c\u6bd4\u5982\u7b2c\u4e00\u6b21\u8fed\u4ee3\u4fdd\u7559\u7b2c1\u4efd\uff0c\u7b2c\u4e8c\u6b21\u4fdd\u7559\u7b2c2\u4efd\u3002 \u6211\u4eec\u4f7f\u7528\u5269\u4f59\u76849\u4efd\u6765\u8bad\u7ec3\u5206\u7c7b\u5668\uff0c\u6bd4\u5982\u7b2c\u4e00\u6b21\u8fed\u4ee3\u4f7f\u7528\u7b2c2\u81f310\u4efd\u6765\u8bad\u7ec3\u3002 \u6211\u4eec\u7528\u521a\u624d\u4fdd\u7559\u7684\u4e00\u4efd\u6765\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u8bb0\u5f55\u7ed3\u679c\uff0c\u6bd4\u5982\uff1a35\u4e2a\u7bee\u7403\u8fd0\u52a8\u5458\u5206\u7c7b\u6b63\u786e\uff0c 29\u4e2a\u666e\u901a\u4eba\u5206\u7c7b\u6b63\u786e\u3002 \u7b2c\u4e09\u6b65\uff1a\u5408\u5e76\u7ed3\u679c \u6211\u4eec\u53ef\u4ee5\u7528\u4e00\u5f20\u8868\u683c\u6765\u5c55\u793a\u7ed3\u679c(\u5c31\u662f\u9a6c\u4e0a\u8981\u63d0\u5230\u7684\u6df7\u6dc6\u77e9\u9635)\uff1a \u901a\u8fc7\u5341\u6298\u4ea4\u53c9\u9a8c\u8bc1\u5f97\u5230\u7684\u8bc4\u4ef7\u7ed3\u679c\u80af\u5b9a\u4f1a\u6bd4\u4e8c\u6298\u6216\u8005\u4e09\u6298\u6765\u5f97\u51c6\u786e\uff0c\u56e0\u4e3a\u4f7f\u7528\u4e86\u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u3002","title":"k\u6298\u4ea4\u53c9\u9a8c\u8bc1"},{"location":"gdm/ch4/#_2","text":"\u65e2\u7136\u5341\u6298\u4ea4\u53c9\u9a8c\u8bc1\u6548\u679c\u90a3\u4e48\u597d\uff0c\u6211\u4eec\u4e3a\u4f55\u4e0d\u505a\u4e00\u4e2a N N \u6298\u4ea4\u53c9\u9a8c\u8bc1\uff1f N N \u5373\u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u91cf\u3002\u5728\u6570\u636e\u6316\u6398\u9886\u57df\uff0c N N \u6298\u4ea4\u53c9\u9a8c\u8bc1\u53c8\u79f0\u4e3a \u7559\u4e00\u6cd5 \u3002 \u4f18\u70b9\uff1a \u51e0\u4e4e\u6240\u6709\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u7136\u540e\u7528\u4e00\u4e2a\u6570\u636e\u8fdb\u884c\u6d4b\u8bd5\u3002 \u786e\u5b9a\u6027\u3002 \u7f3a\u70b9\uff1a \u6700\u5927\u7684\u7f3a\u70b9\u662f\u8ba1\u7b97\u65f6\u95f4\u5f88\u957f\u3002 \u5206\u5c42\u95ee\u9898 \u5728\u7559\u4e00\u6cd5\u4e2d\uff0c\u6240\u6709\u7684\u6d4b\u8bd5\u96c6\u90fd\u53ea\u5305\u542b\u4e00\u4e2a\u6570\u636e(\u5206\u5c42\u95ee\u9898)\u3002\u6240\u4ee5\u8bf4\uff0c\u7559\u4e00\u6cd5\u5bf9\u5c0f\u6570\u636e\u96c6\u662f\u5408\u9002\u7684\uff0c\u4f46\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6211\u4eec\u4f1a\u9009\u62e9\u5341\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3002","title":"\u7559\u4e00\u6cd5"},{"location":"gdm/ch4/#_3","text":"\u6df7\u6dc6\u77e9\u9635(confusion matrix)\u53ef\u4ee5\u5c55\u73b0\u66f4\u4e3a\u8be6\u7ec6\u7684\u8bc4\u4ef7\u7ed3\u679c\u3002\u6df7\u6dc6\u77e9\u9635\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u5feb\u901f\u8bc6\u522b\u51fa\u5206\u7c7b\u5668\u5230\u5e95\u5728\u54ea\u4e9b\u7c7b\u522b\u4e0a\u53d1\u751f\u4e86\u6df7\u6dc6\uff0c\u56e0\u6b64\u5f97\u540d\u3002\u8868\u683c\u7684\u884c\u8868\u793a\u6d4b\u8bd5\u7528\u4f8b\u5b9e\u9645\u6240\u5c5e\u7684\u7c7b\u522b\uff0c\u5217\u5219\u8868\u793a\u5206\u7c7b\u5668\u7684\u5224\u65ad\u7ed3\u679c\u3002 \u5728\u8fd9\u4e2a\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u7684\u5206\u7c7b\u5668\u53ef\u4ee5\u5f88\u597d\u5730\u533a\u5206\u4f53\u64cd\u8fd0\u52a8\u5458\u548c\u7bee\u7403\u8fd0\u52a8\u5458\uff0c\u800c\u9a6c\u62c9\u677e\u8fd0\u52a8\u5458\u5219\u6bd4\u8f83\u5bb9\u6613\u548c\u5176\u4ed6\u4e24\u4e2a\u7c7b\u522b\u53d1\u751f\u6df7\u6dc6\u3002","title":"\u6df7\u6dc6\u77e9\u9635"},{"location":"gdm/ch4/#application","text":"\u5361\u5185\u57fa\u6885\u9686\u5927\u5b66\u7edf\u8ba1\u7684\u6c7d\u8f66\u71c3\u6cb9\u6d88\u8017\u548c\u516c\u91cc\u6570\u6570\u636e\u4e5f\u662f\u4e00\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u3002\u5728 \u524d\u9762\u4e00\u7ae0 \u4e2d\u5df2\u7ecf\u4ecb\u7ecd\u8fc7\u3002 \u6839\u636e\u6c7d\u7f38\u6570\u3001\u6392\u6c14\u91cf\u3001\u9a6c\u529b\u3001\u91cd\u91cf\u3001 \u52a0\u901f\u5ea6\u7b49\u6570\u636e\u9884\u6d4b\u6bcf\u52a0\u4ed1\u71c3\u6cb9\u516c\u91cc\u6570(mpg)\u3002392\u6761\u6570\u636e\u90fd\u5b58\u653e\u5728mpgData.txt\u6587\u4ef6\u4e2d\uff0c\u5e76\u7528\u4e0b\u9762\u8fd9\u6bb5Python\u4ee3\u7801\u5c06\u8fd9\u4e9b\u6570\u636e\u6309\u5c42\u6b21\u7b49\u5206\u6210\u5341\u4efd\uff1a # divide data into 10 buckets import random def buckets ( filename , bucket_name , separator , class_column ): @param filename: file name of the original data. @param bucket_name: the prefix for all the bucket names. @param separator: the character that divides the columns. @param class_column: column that indicates the class, start from 0 # put the data in 10 buckets number_of_buckets = 10 data = {} # first read in the data and divide by category with open ( filename ) as f : lines = f . readlines () for line in lines : if separator != \\t : line = line . replace ( separator , \\t ) # first get the category category = line . split ()[ class_column ] data . setdefault ( category , []) data [ category ] . append ( line ) # initialize the buckets buckets = [] for i in range ( number_of_buckets ): buckets . append ([]) # now for each category put the data into the buckets for k in data . keys (): # randomize order of instances for each class random . shuffle ( data [ k ]) bNum = 0 # divide into buckets for item in data [ k ]: buckets [ bNum ] . append ( item ) bNum = ( bNum + 1 ) % number_of_buckets # write to file for bNum in range ( number_of_buckets ): f = open ( %s - %02i % ( bucket_name , bNum + 1 ), w ) for item in buckets [ bNum ]: f . write ( item ) f . close () # example of how to use this code buckets ( mpgTrainingSet , mpgData , \\t , 1 ) \u6267\u884c\u8fd9\u4e2a\u7a0b\u5e8f\u540e\u4f1a\u751f\u621010\u4e2a\u6587\u4ef6\uff1ampgData01\u3001mpgData02\u7b49\u3002\u7136\u540e\u9700\u8981\u4fee\u6539\u4e0a\u4e00\u7ae0\u7684\u8fd1\u90bb\u7b97\u6cd5\u7a0b\u5e8f\uff0c\u8ba9AthleteClassifier.test()\u51fd\u6570\u80fd\u591f\u6267\u884c\u5341\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3002 def ten_fold_cross_validation ( self , bucket_name , separator , format , header = True ): 10-fold cross validation :param bucket_name: the prefix for all the bucket names. :param separator: the character that divides the columns. :param format: num, comment, class :param header: whether header line exist :return: accuracy self . format = format self . test_result = {} accuracy = [] # repeat 10 times, select test bucket for test for test_bucket_number in range ( 10 ): # select remaining 9 bucket for training training_data = [] for bucket_number in range ( 10 ): if test_bucket_number != bucket_number : training_filename = %s%s%02i % ( bucket_name , separator , bucket_number + 1 ) training_data . extend ( self . pre_processing ( training_filename , header )) self . data = training_data # normalize self . mean_absolute_deviation () self . normalizeColumn () # test test_filename = %s%s%02i % ( bucket_name , separator , test_bucket_number + 1 ) accuracy . append ( self . test ( test_filename , header )) # print output print ( , end = ) for actual in self . test_result . keys (): print ( %5s % actual , end = ) print ( \\n , +---- * len ( self . classes ), + ) for actual , test_results in self . test_result . items (): print ( %3s % actual , end = | ) for accuracy_count in test_results . values (): print ( %3s % accuracy_count , end = | ) print ( \\n ) print ( , +---- * len ( self . classes ), + ) avg_accuracy = np . average ( accuracy ) print ( accuray is , avg_accuracy ) return avg_accuracy","title":"Application: \u6bcf\u52a0\u4ed1\u71c3\u6cb9\u516c\u91cc\u6570"},{"location":"gdm/ch4/#kappa","text":"Kappa\u6307\u6807\u53ef\u4ee5\u7528\u6765\u8bc4\u4ef7\u5206\u7c7b\u5668\u7684\u6548\u679c\u6bd4\u968f\u673a\u5206\u7c7b\u8981\u597d\u591a\u5c11\uff0c\u516c\u5f0f\u4e3a \\kappa = \\frac{P(c)-P(r)}{1-P(r)} \\kappa = \\frac{P(c)-P(r)}{1-P(r)} P(c) P(c) \u8868\u793a\u5206\u7c7b\u5668\u7684\u51c6\u786e\u7387\uff0c P(r) P(r) \u8868\u793a\u968f\u673a\u5206\u7c7b\u5668\u7684\u51c6\u786e\u7387\u3002\u53ef\u4ee5\u53c2\u8003\u4e0b\u5217\u7ecf\u9a8c\u7ed3\u679c\uff1a","title":"Kappa\u6307\u6807"},{"location":"gdm/ch4/#knn_1","text":"\u5176\u5b9ekNN\u7b97\u6cd5\u5df2\u7ecf\u5728 \u7b2c\u4e00\u7ae0 \u91cc\u53d9\u8ff0\u5e76\u4f7f\u7528\u4e86\u3002\u5f53\u65f6\u8ba1\u7b97\u8ddd\u79bb\u65f6\uff0c\u4f7f\u7528\u7684\u662f\u76ae\u5c14\u68ee\u76f8\u5173\u7cfb\u6570\u3002\u90a3\u4e48\uff0c\u5982\u679c\u662f\u66fc\u54c8\u987f\u8ddd\u79bb\u5462\uff1f\u9996\u5148\uff0c\u5c06\u8ddd\u79bb\u53d6\u5012\u6570\uff0c\u7136\u540e\u628a\u6240\u6709\u7684\u8ddd\u79bb\u5012\u6570\u9664\u4ee5\u8ddd\u79bb\u5012\u6570\u7684\u548c\uff0c\u4ece\u800c\u5f97\u5230\u8bc4\u5206\u7684\u6743\u91cd\uff0c\u6700\u540e\u52a0\u6743\u5e73\u5747\u5f97\u5230\u8bc4\u5206\u3002 \u5176\u5b9e\u628a\u8ddd\u79bb\u7684\u5012\u6570\u770b\u6210\u662f\u76f8\u4f3c\u5ea6\uff0c\u5c31\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u4e86\u3002","title":"kNN\u7b97\u6cd5"},{"location":"gdm/ch4/#example","text":"\u8fd9\u4e00\u8282\u5206\u6790\u7531\u7f8e\u56fd\u56fd\u5bb6\u7cd6\u5c3f\u75c5\u3001\u6d88\u5316\u548c\u80be\u810f\u75be\u75c5\u7814\u7a76\u6240\u63d0\u4f9b\u7684\u6bd4\u9a6c\u5370\u7b2c\u5b89\u4eba\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6( \u4e0b\u8f7d )\u3002\u6570\u636e\u96c6\u4e2d\u7684\u4e00\u6761\u8bb0\u5f55\u4ee3\u8868\u4e00\u540d21\u5c81\u4ee5\u4e0a\u7684\u6bd4\u9a6c\u5973\u6027\uff0c\u5979\u4eec\u5206\u7c7b\u4e24\u7c7b\uff1a\u4e94\u5e74\u5185\u67e5\u51fa\u60a3\u6709\u7cd6\u5c3f\u75c5\uff0c\u4ee5\u53ca\u6ca1\u6709\u5f97\u75c5\u3002\u6570\u636e\u96c6\u4e2d\u5305\u542b\u4e868\u4e2a\u7279\u5f81\uff0c\u5982\u6000\u5b55\u6b21\u6570\uff0c\u8212\u5f20\u538b\u7b49\u3002","title":"Example: \u6bd4\u9a6c\u5370\u7b2c\u5b89\u4eba\u7cd6\u5c3f\u75c5"},{"location":"gdm/ch5/","text":"\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 5 \u6734\u7d20\u8d1d\u53f6\u65af 1 \u6734\u7d20\u8d1d\u53f6\u65af \u4f7f\u7528\u8fd1\u90bb\u7b97\u6cd5\u65f6\uff0c\u6211\u4eec\u5f88\u96be\u5bf9\u5206\u7c7b\u7ed3\u679c\u7684\u7f6e\u4fe1\u5ea6\u8fdb\u884c\u91cf\u5316\u3002\u4f46\u5982\u679c\u4f7f\u7528\u7684\u662f\u57fa\u4e8e\u6982\u7387\u7684\u5206\u7c7b\u7b97\u6cd5\u2014\u2014\u8d1d\u53f6\u65af\u7b97\u6cd5\u2014\u2014\u90a3\u5c31\u53ef\u4ee5\u7ed9\u51fa\u5206\u7c7b\u7ed3\u679c\u7684\u53ef\u80fd\u6027\u4e86\uff1a\u8fd9\u540d\u8fd0\u52a8\u5458\u670980%\u7684\u51e0\u7387\u662f\u7bee\u7403\u8fd0\u52a8\u5458\u3002 \u8fd1\u90bb\u7b97\u6cd5\u53c8\u79f0\u4e3a \u88ab\u52a8\u5b66\u4e60\u7b97\u6cd5 \u3002\u8fd9\u79cd\u7b97\u6cd5\u53ea\u662f\u5c06\u8bad\u7ec3\u96c6\u7684\u6570\u636e\u4fdd\u5b58\u8d77\u6765\uff0c\u5728\u6536\u5230\u6d4b\u8bd5\u6570\u636e\u65f6\u624d\u4f1a\u8fdb\u884c\u8ba1\u7b97\u3002\u5982\u679c\u6211\u4eec\u670910\u4e07\u9996\u97f3\u4e50\uff0c\u90a3\u6bcf\u8fdb\u884c\u4e00\u6b21\u5206\u7c7b\uff0c\u90fd\u9700\u8981\u904d\u5386\u8fd910\u4e07\u6761\u8bb0\u5f55\u624d\u884c\u3002 \u8d1d\u53f6\u65af\u7b97\u6cd5\u5219\u662f\u4e00\u79cd \u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5 \u3002\u5b83\u4f1a\u6839\u636e\u8bad\u7ec3\u96c6\u6784\u5efa\u8d77\u4e00\u4e2a\u6a21\u578b\uff0c\u5e76\u7528\u8fd9\u4e2a\u6a21\u578b\u6765\u5bf9\u65b0\u7684\u8bb0\u5f55\u8fdb\u884c\u5206\u7c7b\uff0c\u56e0\u6b64\u901f\u5ea6\u4f1a\u5feb\u5f88\u591a\u3002 \u8d1d\u53f6\u65af\u7b97\u6cd5\u7684\u4e24\u4e2a\u4f18\u70b9\u5373\uff1a \u80fd\u591f\u7ed9\u51fa\u5206\u7c7b\u7ed3\u679c\u7684\u7f6e\u4fe1\u5ea6\uff1b \u5b83\u662f\u4e00\u79cd\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\u3002 \u6982\u7387 \u6211\u4eec\u7528 P(h|D) P(h|D) \u6765\u8868\u793a D D \u6761\u4ef6\u4e0b\u4e8b\u4ef6 h h \u53d1\u751f\u7684\u6982\u7387\u3002 P(h) P(h) \u8868\u793a\u4e8b\u4ef6 h h \u53d1\u751f\u7684\u6982\u7387\uff0c\u79f0\u4e3ah\u7684\u5148\u9a8c\u6982\u7387\u3002 P(h|d) P(h|d) \u79f0\u4e3a\u540e\u9a8c\u6982\u7387\uff0c\u8868\u793a\u5728\u89c2\u5bdf\u4e86\u6570\u636e\u96c6 d d \u4e4b\u540e\uff0c h h \u4e8b\u4ef6\u53d1\u751f\u7684\u6982\u7387\u662f\u591a\u5c11\u3002\u540e\u9a8c\u6982\u7387\u53c8\u79f0\u4e3a\u6761\u4ef6\u6982\u7387\u3002 \u8d1d\u53f6\u65af\u6cd5\u5219 \u8d1d\u53f6\u65af\u6cd5\u5219\u63cf\u8ff0\u4e86 P(h) P(h) \u3001 P(h|D) P(h|D) \u3001 P(D) P(D) \u3001\u4ee5\u53ca P(D|h) P(D|h) \u8fd9\u56db\u4e2a\u6982\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\uff1a P(h|D) = \\frac{P(D|h)P(h)}{P(D)} P(h|D) = \\frac{P(D|h)P(h)}{P(D)} \u5982\u679c\u6211\u4eec\u6709 h_1, h_2,...h_n h_1, h_2,...h_n \u7b49\u4e8b\u4ef6\u3002\u8ba1\u7b97\u4e0d\u540c\u4e8b\u4ef6\u53d1\u751f\u7684\u6982\u7387\uff0c P(h_i|D) = \\frac{P(D|h_i)P(h_i)}{P(D)} P(h_i|D) = \\frac{P(D|h_i)P(h_i)}{P(D)} \u9009\u53d6\u6700\u5927\u7684\u6982\u7387\uff0c\u5c31\u80fd\u7528\u4f5c\u5206\u7c7b\u4e86\u3002\u8fd9\u79cd\u65b9\u6cd5\u53eb\u6700\u5927\u540e\u9a8c\u4f30\u8ba1\uff0c\u8bb0\u4e3a h_{MAP} h_{MAP} \uff1a h_{MAP} = \\arg \\max_{h\\in H} P(h|D) = \\arg \\max_{h\\in H} \\frac{P(D|h)P(h)}{P(D)} h_{MAP} = \\arg \\max_{h\\in H} P(h|D) = \\arg \\max_{h\\in H} \\frac{P(D|h)P(h)}{P(D)} H H \u8868\u793a\u6240\u6709\u7684\u4e8b\u4ef6\uff0c\u6240\u4ee5 h\\in H h\\in H \u8868\u793a\u201c\u5bf9\u4e8e\u96c6\u5408\u4e2d\u7684\u6bcf\u4e00\u4e2a\u4e8b\u4ef6\u201d\u3002\u6574\u4e2a\u516c\u5f0f\u7684\u542b\u4e49\u5c31\u662f\uff1a\u5bf9\u4e8e\u96c6\u5408\u4e2d\u7684\u6bcf\u4e00\u4e2a\u4e8b\u4ef6\uff0c\u8ba1\u7b97\u51fa P(h|D) P(h|D) \u7684\u503c\uff0c\u5e76\u53d6\u6700\u5927\u7684\u7ed3\u679c\u3002 \u53ef\u4ee5\u53d1\u73b0\u5bf9\u4e8e\u6240\u6709\u7684\u4e8b\u4ef6\uff0c\u516c\u5f0f\u4e2d\u7684\u5206\u6bcd\u90fd\u662f P(D) P(D) \uff0c\u56e0\u6b64\u5373\u4fbf\u53ea\u8ba1\u7b97 P(D|h)P(h) P(D|h)P(h) \uff0c\u4e5f\u53ef\u4ee5\u5224\u65ad\u51fa\u6700\u5927\u7684\u7ed3\u679c\u3002\u90a3\u4e48\u8fd9\u4e2a\u516c\u5f0f\u5c31\u53ef\u4ee5\u7b80\u5316\u4e3a\uff1a h_{MAP} = \\arg \\max_{h\\in H} P(D|h)P(h) h_{MAP} = \\arg \\max_{h\\in H} P(D|h)P(h) 2 Example: \u624b\u73af\u63a8\u8350 \u73b0\u5728\u6211\u4eec\u8981\u4e3aiHealth\u516c\u53f8\u53d1\u4e00\u5957\u63a8\u8350\u7cfb\u7edf\u3002iHealth\u65b0\u51fa\u4ea7\u4e86\u4e24\u4ef6\u5546\u54c1\uff1ai100\u548ci500\u3002\u4e3a\u4e86\u6536\u96c6\u6570\u636e\uff0c\u8ba9\u8d2d\u4e70\u7684\u7528\u6237\u586b\u5199\u8c03\u67e5\u95ee\u5377\uff0c\u6bcf\u4e2a\u95ee\u9898\u90fd\u5bf9\u5e94\u4e00\u4e2a\u7279\u5f81\uff1a \u5df2\u77e5\u4e00\u4f4d\u5ba2\u6237\u7684\u8fd0\u52a8\u76ee\u7684\u3001\u5f53\u524d\u8fd0\u52a8\u6c34\u5e73\u3001\u5bf9\u5065\u8eab\u7684\u70ed\u60c5\u3001\u662f\u5426\u9002\u5e94\u9ad8\u79d1\u6280\u4ea7\u54c1\uff0c\u8bf7\u7528\u6734\u7d20\u8d1d\u53f6\u65af\u6765\u63a8\u8350\u624b\u73af\u578b\u53f7\u3002 \u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u5305\u542b\u4e24\u4e2a\u90e8\u5206\uff1a\u8bad\u7ec3\u548c\u5206\u7c7b\u3002 \u8bad\u7ec3 \u8bad\u7ec3\u7684\u8f93\u51fa\u7ed3\u679c\u5e94\u8be5\u662f\uff1a \u5148\u9a8c\u6982\u7387\uff0c\u5982 P(i100) = 0.4 P(i100) = 0.4 \u6761\u4ef6\u6982\u7387\uff0c\u5982 P(\u5065\u5eb7|i100) = 0.167 P(\u5065\u5eb7|i100) = 0.167 \u6211\u4eec\u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\u8868\u793a\u5148\u9a8c\u6982\u7387\uff1a self . prior = { i500 : 0.6 , i100 : 0.4 } \u6761\u4ef6\u6982\u7387\u7684\u8868\u793a\u6709\u4e9b\u590d\u6742\uff0c\u7528\u5d4c\u5957\u7684\u5b57\u5178\u6765\u5b9e\u73b0\uff1a { i500 : { 1 : { appearance : 0.3333333333333333 , health : 0.4444444444444444 , both : 0.2222222222222222 }, 2 : { active : 0.4444444444444444 , sedentary : 0.2222222222222222 , moderate : 0.3333333333333333 }, 3 : { aggressive : 0.6666666666666666 , moderate : 0.3333333333333333 }, 4 : { yes : 0.6666666666666666 , no : 0.3333333333333333 }}, i100 : { 1 : { both : 0.5 , health : 0.16666666666666666 , appearance : 0.3333333333333333 }, 2 : { active : 0.3333333333333333 , sedentary : 0.5 , moderate : 0.16666666666666666 }, 3 : { aggressive : 0.16666666666666666 , moderate : 0.8333333333333334 }, 4 : { yes : 0.3333333333333333 , no : 0.6666666666666666 }}} 1\u30012\u30013\u30014\u8868\u793a\u7b2c\u51e0\u5217\uff0c\u6240\u4ee5\u7b2c\u4e00\u884c\u53ef\u4ee5\u89e3\u91ca\u4e3a\u8d2d\u4e70i500\u7684\u987e\u5ba2\u4e2d\u8fd0\u52a8\u76ee\u7684\u662f\u5916\u8868\u7684\u6982\u7387\u662f 0.333\u3002 \u4e3a\u4e86\u8ba1\u7b97\u6982\u7387\uff0c\u8981\u8fdb\u884c\u8ba1\u6570\uff0c\u53ef\u4ee5\u7528\u5b57\u5178\u6765\u7edf\u8ba1\u6bcf\u4e2a\u578b\u53f7\u7684\u6b21\u6570\u3002 \u4ee5\u4e0b\u662f\u8bad\u7ec3\u7528\u7684Python\u4ee3\u7801\uff1a class WristBandBayes : Recoommand Writeband Using naive bayes def __init__ ( self , bucketPrefix , testBucketNumber , dataFormat ): :param bucketPrefix: \u5206\u6876\u6570\u636e\u96c6\u6587\u4ef6\u524d\u7f00 :param testBucketNumber: \u6d4b\u8bd5\u6876\u7684\u7f16\u53f7 :param dataFormat: \u6570\u636e\u683c\u5f0f\uff0c\u5f62\u5982attr attr attr attr class # \u603b\u6761\u6570 total = 0 # \u5148\u9a8c\u6982\u7387\u8ba1\u6570 classes = {} # \u540e\u9a8c\u6982\u7387\u8ba1\u6570 counts = {} # \u4ece\u6587\u4ef6\u4e2d\u8bfb\u53d6\u6570\u636e self . format = dataFormat . strip () . split ( \\t ) # \u5148\u9a8c\u6982\u7387 self . prior = {} # \u6761\u4ef6\u6982\u7387 self . conditional = {} # \u904d\u5386\u5341\u4e2a\u6876\uff0c \u5341\u6298\u4ea4\u53c9\u9a8c\u8bc1 for i in range ( 1 , 11 ): # \u8df3\u8fc7\u6d4b\u8bd5\u6876 if i != testBucketNumber : filename = %s - %02i % ( bucketPrefix , i ) f = open ( filename ) lines = f . readlines () f . close () for line in lines : fields = line . strip () . split ( \\t ) ignore = [] vector = [] for i in range ( len ( fields )): if self . format [ i ] == num : vector . append ( float ( fields [ i ])) elif self . format [ i ] == attr : vector . append ( fields [ i ]) elif self . format [ i ] == comment : ignore . append ( fields [ i ]) elif self . format [ i ] == class : category = fields [ i ] # \u5904\u7406\u8be5\u6761\u8bb0\u5f55 total += 1 classes . setdefault ( category , 0 ) counts . setdefault ( category , {}) classes [ category ] += 1 # \u5904\u7406\u5404\u4e2a\u5c5e\u6027 col = 0 for columnValue in vector : col += 1 counts [ category ] . setdefault ( col , {}) counts [ category ][ col ] . setdefault ( columnValue , 0 ) counts [ category ][ col ][ columnValue ] += 1 # \u8ba1\u6570\u7ed3\u675f\uff0c\u5f00\u59cb\u8ba1\u7b97\u6982\u7387 # \u8ba1\u7b97\u5148\u9a8c\u6982\u7387P(h) for ( category , count ) in classes . items (): self . prior [ category ] = count / total # \u8ba1\u7b97\u6761\u4ef6\u6982\u7387P(h|D) for ( category , columns ) in counts . items (): self . conditional . setdefault ( category , {}) for ( col_id , valueCounts ) in columns . items (): self . conditional [ category ] . setdefault ( col_id , {}) for ( attrValue , count ) in valueCounts . items (): self . conditional [ category ][ col_id ][ attrValue ] = ( count / classes [ category ]) self . tmp = counts def classify ( self , itemVector ): \u8fd4\u56deitemVector\u6240\u5c5e\u7c7b\u522b results = [] for ( category , prior ) in self . prior . items (): prob = prior col = 1 for attrValue in itemVector : if attrValue not in self . conditional [ category ][ col ]: # \u5c5e\u6027\u4e0d\u5b58\u5728\uff0c\u8fd4\u56de0\u6982\u7387 prob = 0 else : prob = prob * self . conditional [ category ][ col ][ attrValue ] col += 1 results . append (( prob , category )) # \u8fd4\u56de\u6982\u7387\u6700\u9ad8\u7684\u7ed3\u679c return max ( results )[ 1 ] if __name__ == __main__ : c = WristBandBayes ( /Users/larry/datamining/DataminingGuideBook-Codes/chapter-6/iHealth/i , 10 , attr \\t attr \\t attr \\t attr \\t class ) print ( c . classify ([ health moderate , moderate , yes ])) 3 Example: \u7f8e\u56fd\u56fd\u4f1a\u6295\u7968\u6570\u636e \u7f8e\u56fd\u56fd\u4f1a\u6295\u7968\u6570\u636e \uff0c\u5176\u4e2d\u6bcf\u6761\u8bb0\u5f55\u4ee3\u8868\u4e00\u4e2a\u9009\u6c11\uff0c\u7b2c\u4e00\u5217\u662f\u5206\u7c7b\u540d\u79f0\uff08democrat, republican\uff09\uff0c\u4e4b\u540e\u662f16\u6761\u6cd5\u6848\uff0c\u7528y\u548cn\u8868\u793a\u8be5\u4eba\u662f\u5426\u652f\u6301\u3002 \u6587\u4ef6\u683c\u5f0f\u5982\u4e0b\uff1a democrat y n y n n y y y y y n n y n n y democrat y y y n n y y y y n n n n n y y republican y y n y y y n n n y n y y y n n \u5728\u8c03\u7528\u4e0a\u4e00\u8282\u7f16\u5199\u7684\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u65f6\u4f7f\u7528\u4ee5\u4e0bdataFormat\u53c2\u6570\u5c31\u53ef\u4ee5\u4e86\uff1a class\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\t attr\\tattr\\tattr \u6982\u7387\u503c\u4e3a0 \u4f46\u662f\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u6709\u4e00\u4e9b\u95ee\u9898\u3002 \u4f7f\u7528\u6734\u7d20\u8d1d\u53f6\u65af\u8ba1\u7b97\u5f97\u5230\u7684\u6982\u7387\u5176\u5b9e\u662f\u771f\u5b9e\u6982\u7387\u7684\u4e00\u79cd\u4f30\u8ba1\uff0c\u800c\u771f\u5b9e\u6982\u7387\u662f\u5bf9\u5168\u91cf\u6570\u636e\u505a\u7edf\u8ba1\u5f97\u5230\u7684\u3002\u6bd4\u5982\u8bf4\uff0c\u6211\u4eec\u9700\u8981\u5bf9\u6240\u6709\u4eba\u90fd\u505a\u8840\u6db2\u6d4b\u8bd5\uff0c\u624d\u80fd\u5f97\u5230\u5065\u5eb7\u4eba\u8fd4\u56de\u9634\u6027\u7ed3\u679c\u7684\u771f\u5b9e\u6982\u7387\u3002\u663e\u7136\uff0c\u5bf9\u5168\u91cf\u6570\u636e\u505a\u7edf\u8ba1\u662f\u4e0d\u73b0\u5b9e\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u4f1a\u9009\u53d6\u4e00\u4e2a\u6837\u672c\uff0c\u59821000\u4eba\uff0c\u5bf9\u4ed6\u4eec\u8fdb\u884c\u6d4b\u8bd5\u5e76\u8ba1\u7b97\u6982\u7387\u3002\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\uff0c\u8fd9\u79cd\u4f30\u8ba1\u90fd\u662f\u63a5\u8fd1\u4e8e\u771f\u5b9e\u6982\u7387\u7684\u3002\u4f46\u5f53\u771f\u5b9e\u6982\u7387\u975e\u5e38\u5c0f\u65f6\uff0c\u8fd9\u79cd\u62bd\u6837\u7edf\u8ba1\u7684\u505a\u6cd5\u5c31\u4f1a\u6709\u95ee\u9898\u4e86\u3002\u6bd4\u5982\u8bf4\uff0c\u6c11\u4e3b\u515a\u5bf9\u7f51\u7edc\u975e\u6cd5\u4f20\u64ad\u6cd5\u6848\u7684\u5426\u51b3\u7387\u662f0.03\uff0c\u5373 P(S=no|\u6c11\u4e3b\u515a) = 0.03 P(S=no|\u6c11\u4e3b\u515a) = 0.03 \u3002\u5982\u679c\u6211\u4eec \u5206\u522b\u9009\u53d6\u5341\u4e2a\u6c11\u4e3b\u515a\u548c\u5171\u548c\u515a\u4eba\uff0c\u770b\u4ed6\u4eec\u5bf9\u8be5\u6cd5\u6848\u7684\u6295\u7968\u60c5\u51b5\uff0c\u4f60\u89c9\u5f97\u5f97\u5230\u7684\u6982\u7387\u4f1a\u662f\u4ec0\u4e48\uff1f\u7b54\u6848\u5f88\u53ef\u80fd\u662f0\u3002 \u5728\u6734\u7d20\u8d1d\u53f6\u65af\u4e2d\uff0c\u6982\u7387\u4e3a0\u7684\u5f71\u54cd\u662f\u5f88\u5927\u7684\u3002\u5982\u679c\u5176\u4e2d\u2f00\u4e2a\u6982\u7387\u503c\u4e3a0\uff0c\u90a3\u4e48\u6700\u540e\u7684\u4e58\u79ef\u4e5f\u4e3a0\u3002 \u4e3a\u4e86\u8868\u793a\u65b9\u4fbf\uff0c\u6211\u4eec\u91c7\u7528\u4ee5\u4e0b\u516c\u5f0f\uff1a P(x|y)=\\frac{n_c}{n} P(x|y)=\\frac{n_c}{n} <span><span class=\"MathJax_Preview\">P(x|y)=\\frac{n_c}{n}</span><script type=\"math/tex\">P(x|y)=\\frac{n_c}{n} \u5176\u4e2d, n n \u8868\u793a\u8bad\u7ec3\u96c6\u4e2dy\u7c7b\u522b\u7684\u8bb0\u5f55\u6570\uff1b n_c n_c \u8868\u793a y y \u7c7b\u522b\u4e2d\u503c\u4e3a x x \u7684\u8bb0\u5f55\u6570\u3002\u6211\u4eec\u7684\u95ee\u9898\u662f n_c n_c \u53ef\u80fd\u4e3a0\u3002\u89e3\u51b3\u65b9\u6cd5\u662f\u5c06\u516c\u793a\u53d8\u4e3a\u4ee5\u4e0b\u5f62\u5f0f\uff1a P(x|y)=\\frac{n_c+mp}{n+m} P(x|y)=\\frac{n_c+mp}{n+m} m m \u662f\u4e00\u4e2a\u5e38\u6570\uff0c\u8868\u793a\u7b49\u6548\u6837\u672c\u5927\u5c0f\u3002\u51b3\u5b9a\u5e38\u6570 m m \u7684\u65b9\u6cd5\u6709\u5f88\u591a\uff0c\u6211\u4eec\u8fd9\u91cc\u4f7f\u7528\u503c\u7684\u7c7b\u522b\u6570\u76ee\u6765\u4f5c\u4e3a m m \uff0c\u6bd4\u5982\u6295\u7968\u6709\u8d5e\u6210\u548c\u5426\u51b3\u4e24\u79cd\u7c7b\u522b\uff0c\u6240\u4ee5 m m \u5c31\u4e3a2\u3002 p p \u5219\u662f\u76f8\u5e94\u7684\u5148\u9a8c\u6982\u7387\uff0c\u6bd4\u5982\u8bf4\u8d5e\u6210\u548c\u5426\u51b3\u7684\u6982\u7387\u5206\u522b\u662f0.5\uff0c\u90a3 p p \u5c31\u662f0.5\u3002 \u6570\u503c\u578b\u6570\u636e \u5728\u8d1d\u53f6\u65af\u65b9\u6cd5\u4e2d\uff0c\u4e4b\u524d\u6211\u4eec\u5bf9\u4e8b\u7269\u8fdb\u884c\u4e86\u8ba1\u6570\uff0c\u8fd9\u79cd\u8ba1\u6570\u5219\u662f\u53ef\u4ee5\u5ea6\u91cf\u7684\u3002\u5bf9\u4e8e\u6570\u503c\u578b\u7684\u6570\u636e\u8981 \u5982\u4f55\u8ba1\u6570\u5462\uff1f\u901a\u5e38\u6709\u4e24\u79cd\u505a\u6cd5\uff1a\u533a\u5206\u7c7b\u522b\u548c\u9ad8\u65af\u5206\u5e03 \u533a\u5206\u7c7b\u522b \u6211\u4eec\u53ef\u4ee5\u5212\u5b9a\u51e0\u4e2a\u8303\u56f4\u4f5c\u4e3a\u5206\u7c7b\uff0c\u5982\uff1a \u5e74\u9f84 18 18 - 22 23 - 30 31 - 40 40 \u5e74\u85aa $200,000 150,000 - 200,000 100,000 - 150,000 60,000 - 100,000 40,000 - 60,000 \u5212\u5206\u7c7b\u522b\u540e\uff0c\u5c31\u53ef\u4ee5\u5e94\u7528\u6734\u7d20\u8d1d\u53f6\u65af\u65b9\u6cd5\u4e86\u3002 \u9ad8\u65af\u5206\u5e03 \u5c5e\u4e8e\u7c7b\u522b y_i y_i \u7684\u7279\u5f81 x_i x_i \u7684\u6982\u7387\u4e3a P(x_i|y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{ij}}e^{-\\frac{(x_i-u_{ij})^2}{2\\sigma^2_{ij}}} P(x_i|y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{ij}}e^{-\\frac{(x_i-u_{ij})^2}{2\\sigma^2_{ij}}} \u4e3a\u4e86\u4e3e\u4f8b\uff0c\u6211\u4eec\u4e3a\u4e0a\u9762\u8bb2\u8ff0\u7684\u624b\u73af\u7684\u4f8b\u5b50\u589e\u52a0\u4e00\u5217\u6536\u5165\u5c5e\u6027\u3002\u5047\u8bbe\u6211\u4eec\u8981\u8ba1\u7b97 P(100k|i500) P(100k|i500) \u7684\u6982\u7387\uff0c\u5373\u8d2d\u4e70i500\u7684\u7528\u6237\u4e2d\u6536\u5165\u662f100,000\u7f8e\u5143\u7684\u6982\u7387\u3002\u90a3\u4e48 u_{ij}, \\sigma_{ij} u_{ij}, \\sigma_{ij} \u5206\u522b\u662f\u8d2d\u4e70i500\u7684\u7528\u6237\u7684\u5e73\u5747\u6536\u5165\u548c\u6536\u5165\u7684\u6807\u51c6\u5dee\u3002 \u6837\u672c\u6807\u51c6\u5dee\u7684\u8ba1\u7b97\u516c\u5f0f\u662f\uff1a \\sigma = \\sqrt{\\frac{\\sum_i(x_i-\\bar x)^2}{card(x)-1}} \\sigma = \\sqrt{\\frac{\\sum_i(x_i-\\bar x)^2}{card(x)-1}} \u5728\u8bad\u7ec3\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u65f6\uff0c\u53ef\u4ee5\u5c06\u6240\u6709\u5c5e\u6027\u7684\u5e73\u5747\u503c\u548c\u6837\u672c\u6807\u51c6\u5dee\u8ba1\u7b97\u51fa\u6765\uff0c\u800c\u5206\u7c7b\u9636\u6bb5\u4f7f\u7528\u5e73\u5747\u503c\u548c\u6837\u672c\u6807\u51c6\u5dee\u8ba1\u7b97\u6982\u7387\u5bc6\u5ea6\u5206\u5e03\uff1a def pdf ( mean , ssd , x ): \u6982\u7387\u5bc6\u5ea6\u51fd\u6570\uff0c\u8ba1\u7b97P(x|y) ePart = math . pow ( math . e , - ( x - mean ) ** 2 / ( 2 * ssd ** 2 )) return ( 1.0 / ( math . sqrt ( 2 * math . pi ) * ssd )) * ePart \u4f7f\u7528Python\u5b9e\u73b0","title":"Chapter 5: \u6982\u7387\u548c\u6734\u7d20\u8d1d\u53f6\u65af"},{"location":"gdm/ch5/#-5","text":"","title":"\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 5 \u6734\u7d20\u8d1d\u53f6\u65af"},{"location":"gdm/ch5/#1","text":"\u4f7f\u7528\u8fd1\u90bb\u7b97\u6cd5\u65f6\uff0c\u6211\u4eec\u5f88\u96be\u5bf9\u5206\u7c7b\u7ed3\u679c\u7684\u7f6e\u4fe1\u5ea6\u8fdb\u884c\u91cf\u5316\u3002\u4f46\u5982\u679c\u4f7f\u7528\u7684\u662f\u57fa\u4e8e\u6982\u7387\u7684\u5206\u7c7b\u7b97\u6cd5\u2014\u2014\u8d1d\u53f6\u65af\u7b97\u6cd5\u2014\u2014\u90a3\u5c31\u53ef\u4ee5\u7ed9\u51fa\u5206\u7c7b\u7ed3\u679c\u7684\u53ef\u80fd\u6027\u4e86\uff1a\u8fd9\u540d\u8fd0\u52a8\u5458\u670980%\u7684\u51e0\u7387\u662f\u7bee\u7403\u8fd0\u52a8\u5458\u3002 \u8fd1\u90bb\u7b97\u6cd5\u53c8\u79f0\u4e3a \u88ab\u52a8\u5b66\u4e60\u7b97\u6cd5 \u3002\u8fd9\u79cd\u7b97\u6cd5\u53ea\u662f\u5c06\u8bad\u7ec3\u96c6\u7684\u6570\u636e\u4fdd\u5b58\u8d77\u6765\uff0c\u5728\u6536\u5230\u6d4b\u8bd5\u6570\u636e\u65f6\u624d\u4f1a\u8fdb\u884c\u8ba1\u7b97\u3002\u5982\u679c\u6211\u4eec\u670910\u4e07\u9996\u97f3\u4e50\uff0c\u90a3\u6bcf\u8fdb\u884c\u4e00\u6b21\u5206\u7c7b\uff0c\u90fd\u9700\u8981\u904d\u5386\u8fd910\u4e07\u6761\u8bb0\u5f55\u624d\u884c\u3002 \u8d1d\u53f6\u65af\u7b97\u6cd5\u5219\u662f\u4e00\u79cd \u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5 \u3002\u5b83\u4f1a\u6839\u636e\u8bad\u7ec3\u96c6\u6784\u5efa\u8d77\u4e00\u4e2a\u6a21\u578b\uff0c\u5e76\u7528\u8fd9\u4e2a\u6a21\u578b\u6765\u5bf9\u65b0\u7684\u8bb0\u5f55\u8fdb\u884c\u5206\u7c7b\uff0c\u56e0\u6b64\u901f\u5ea6\u4f1a\u5feb\u5f88\u591a\u3002 \u8d1d\u53f6\u65af\u7b97\u6cd5\u7684\u4e24\u4e2a\u4f18\u70b9\u5373\uff1a \u80fd\u591f\u7ed9\u51fa\u5206\u7c7b\u7ed3\u679c\u7684\u7f6e\u4fe1\u5ea6\uff1b \u5b83\u662f\u4e00\u79cd\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\u3002","title":"1 \u6734\u7d20\u8d1d\u53f6\u65af"},{"location":"gdm/ch5/#_1","text":"\u6211\u4eec\u7528 P(h|D) P(h|D) \u6765\u8868\u793a D D \u6761\u4ef6\u4e0b\u4e8b\u4ef6 h h \u53d1\u751f\u7684\u6982\u7387\u3002 P(h) P(h) \u8868\u793a\u4e8b\u4ef6 h h \u53d1\u751f\u7684\u6982\u7387\uff0c\u79f0\u4e3ah\u7684\u5148\u9a8c\u6982\u7387\u3002 P(h|d) P(h|d) \u79f0\u4e3a\u540e\u9a8c\u6982\u7387\uff0c\u8868\u793a\u5728\u89c2\u5bdf\u4e86\u6570\u636e\u96c6 d d \u4e4b\u540e\uff0c h h \u4e8b\u4ef6\u53d1\u751f\u7684\u6982\u7387\u662f\u591a\u5c11\u3002\u540e\u9a8c\u6982\u7387\u53c8\u79f0\u4e3a\u6761\u4ef6\u6982\u7387\u3002","title":"\u6982\u7387"},{"location":"gdm/ch5/#_2","text":"\u8d1d\u53f6\u65af\u6cd5\u5219\u63cf\u8ff0\u4e86 P(h) P(h) \u3001 P(h|D) P(h|D) \u3001 P(D) P(D) \u3001\u4ee5\u53ca P(D|h) P(D|h) \u8fd9\u56db\u4e2a\u6982\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\uff1a P(h|D) = \\frac{P(D|h)P(h)}{P(D)} P(h|D) = \\frac{P(D|h)P(h)}{P(D)} \u5982\u679c\u6211\u4eec\u6709 h_1, h_2,...h_n h_1, h_2,...h_n \u7b49\u4e8b\u4ef6\u3002\u8ba1\u7b97\u4e0d\u540c\u4e8b\u4ef6\u53d1\u751f\u7684\u6982\u7387\uff0c P(h_i|D) = \\frac{P(D|h_i)P(h_i)}{P(D)} P(h_i|D) = \\frac{P(D|h_i)P(h_i)}{P(D)} \u9009\u53d6\u6700\u5927\u7684\u6982\u7387\uff0c\u5c31\u80fd\u7528\u4f5c\u5206\u7c7b\u4e86\u3002\u8fd9\u79cd\u65b9\u6cd5\u53eb\u6700\u5927\u540e\u9a8c\u4f30\u8ba1\uff0c\u8bb0\u4e3a h_{MAP} h_{MAP} \uff1a h_{MAP} = \\arg \\max_{h\\in H} P(h|D) = \\arg \\max_{h\\in H} \\frac{P(D|h)P(h)}{P(D)} h_{MAP} = \\arg \\max_{h\\in H} P(h|D) = \\arg \\max_{h\\in H} \\frac{P(D|h)P(h)}{P(D)} H H \u8868\u793a\u6240\u6709\u7684\u4e8b\u4ef6\uff0c\u6240\u4ee5 h\\in H h\\in H \u8868\u793a\u201c\u5bf9\u4e8e\u96c6\u5408\u4e2d\u7684\u6bcf\u4e00\u4e2a\u4e8b\u4ef6\u201d\u3002\u6574\u4e2a\u516c\u5f0f\u7684\u542b\u4e49\u5c31\u662f\uff1a\u5bf9\u4e8e\u96c6\u5408\u4e2d\u7684\u6bcf\u4e00\u4e2a\u4e8b\u4ef6\uff0c\u8ba1\u7b97\u51fa P(h|D) P(h|D) \u7684\u503c\uff0c\u5e76\u53d6\u6700\u5927\u7684\u7ed3\u679c\u3002 \u53ef\u4ee5\u53d1\u73b0\u5bf9\u4e8e\u6240\u6709\u7684\u4e8b\u4ef6\uff0c\u516c\u5f0f\u4e2d\u7684\u5206\u6bcd\u90fd\u662f P(D) P(D) \uff0c\u56e0\u6b64\u5373\u4fbf\u53ea\u8ba1\u7b97 P(D|h)P(h) P(D|h)P(h) \uff0c\u4e5f\u53ef\u4ee5\u5224\u65ad\u51fa\u6700\u5927\u7684\u7ed3\u679c\u3002\u90a3\u4e48\u8fd9\u4e2a\u516c\u5f0f\u5c31\u53ef\u4ee5\u7b80\u5316\u4e3a\uff1a h_{MAP} = \\arg \\max_{h\\in H} P(D|h)P(h) h_{MAP} = \\arg \\max_{h\\in H} P(D|h)P(h)","title":"\u8d1d\u53f6\u65af\u6cd5\u5219"},{"location":"gdm/ch5/#2-example","text":"\u73b0\u5728\u6211\u4eec\u8981\u4e3aiHealth\u516c\u53f8\u53d1\u4e00\u5957\u63a8\u8350\u7cfb\u7edf\u3002iHealth\u65b0\u51fa\u4ea7\u4e86\u4e24\u4ef6\u5546\u54c1\uff1ai100\u548ci500\u3002\u4e3a\u4e86\u6536\u96c6\u6570\u636e\uff0c\u8ba9\u8d2d\u4e70\u7684\u7528\u6237\u586b\u5199\u8c03\u67e5\u95ee\u5377\uff0c\u6bcf\u4e2a\u95ee\u9898\u90fd\u5bf9\u5e94\u4e00\u4e2a\u7279\u5f81\uff1a \u5df2\u77e5\u4e00\u4f4d\u5ba2\u6237\u7684\u8fd0\u52a8\u76ee\u7684\u3001\u5f53\u524d\u8fd0\u52a8\u6c34\u5e73\u3001\u5bf9\u5065\u8eab\u7684\u70ed\u60c5\u3001\u662f\u5426\u9002\u5e94\u9ad8\u79d1\u6280\u4ea7\u54c1\uff0c\u8bf7\u7528\u6734\u7d20\u8d1d\u53f6\u65af\u6765\u63a8\u8350\u624b\u73af\u578b\u53f7\u3002 \u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u5305\u542b\u4e24\u4e2a\u90e8\u5206\uff1a\u8bad\u7ec3\u548c\u5206\u7c7b\u3002","title":"2 Example: \u624b\u73af\u63a8\u8350"},{"location":"gdm/ch5/#_3","text":"\u8bad\u7ec3\u7684\u8f93\u51fa\u7ed3\u679c\u5e94\u8be5\u662f\uff1a \u5148\u9a8c\u6982\u7387\uff0c\u5982 P(i100) = 0.4 P(i100) = 0.4 \u6761\u4ef6\u6982\u7387\uff0c\u5982 P(\u5065\u5eb7|i100) = 0.167 P(\u5065\u5eb7|i100) = 0.167 \u6211\u4eec\u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\u8868\u793a\u5148\u9a8c\u6982\u7387\uff1a self . prior = { i500 : 0.6 , i100 : 0.4 } \u6761\u4ef6\u6982\u7387\u7684\u8868\u793a\u6709\u4e9b\u590d\u6742\uff0c\u7528\u5d4c\u5957\u7684\u5b57\u5178\u6765\u5b9e\u73b0\uff1a { i500 : { 1 : { appearance : 0.3333333333333333 , health : 0.4444444444444444 , both : 0.2222222222222222 }, 2 : { active : 0.4444444444444444 , sedentary : 0.2222222222222222 , moderate : 0.3333333333333333 }, 3 : { aggressive : 0.6666666666666666 , moderate : 0.3333333333333333 }, 4 : { yes : 0.6666666666666666 , no : 0.3333333333333333 }}, i100 : { 1 : { both : 0.5 , health : 0.16666666666666666 , appearance : 0.3333333333333333 }, 2 : { active : 0.3333333333333333 , sedentary : 0.5 , moderate : 0.16666666666666666 }, 3 : { aggressive : 0.16666666666666666 , moderate : 0.8333333333333334 }, 4 : { yes : 0.3333333333333333 , no : 0.6666666666666666 }}} 1\u30012\u30013\u30014\u8868\u793a\u7b2c\u51e0\u5217\uff0c\u6240\u4ee5\u7b2c\u4e00\u884c\u53ef\u4ee5\u89e3\u91ca\u4e3a\u8d2d\u4e70i500\u7684\u987e\u5ba2\u4e2d\u8fd0\u52a8\u76ee\u7684\u662f\u5916\u8868\u7684\u6982\u7387\u662f 0.333\u3002 \u4e3a\u4e86\u8ba1\u7b97\u6982\u7387\uff0c\u8981\u8fdb\u884c\u8ba1\u6570\uff0c\u53ef\u4ee5\u7528\u5b57\u5178\u6765\u7edf\u8ba1\u6bcf\u4e2a\u578b\u53f7\u7684\u6b21\u6570\u3002 \u4ee5\u4e0b\u662f\u8bad\u7ec3\u7528\u7684Python\u4ee3\u7801\uff1a class WristBandBayes : Recoommand Writeband Using naive bayes def __init__ ( self , bucketPrefix , testBucketNumber , dataFormat ): :param bucketPrefix: \u5206\u6876\u6570\u636e\u96c6\u6587\u4ef6\u524d\u7f00 :param testBucketNumber: \u6d4b\u8bd5\u6876\u7684\u7f16\u53f7 :param dataFormat: \u6570\u636e\u683c\u5f0f\uff0c\u5f62\u5982attr attr attr attr class # \u603b\u6761\u6570 total = 0 # \u5148\u9a8c\u6982\u7387\u8ba1\u6570 classes = {} # \u540e\u9a8c\u6982\u7387\u8ba1\u6570 counts = {} # \u4ece\u6587\u4ef6\u4e2d\u8bfb\u53d6\u6570\u636e self . format = dataFormat . strip () . split ( \\t ) # \u5148\u9a8c\u6982\u7387 self . prior = {} # \u6761\u4ef6\u6982\u7387 self . conditional = {} # \u904d\u5386\u5341\u4e2a\u6876\uff0c \u5341\u6298\u4ea4\u53c9\u9a8c\u8bc1 for i in range ( 1 , 11 ): # \u8df3\u8fc7\u6d4b\u8bd5\u6876 if i != testBucketNumber : filename = %s - %02i % ( bucketPrefix , i ) f = open ( filename ) lines = f . readlines () f . close () for line in lines : fields = line . strip () . split ( \\t ) ignore = [] vector = [] for i in range ( len ( fields )): if self . format [ i ] == num : vector . append ( float ( fields [ i ])) elif self . format [ i ] == attr : vector . append ( fields [ i ]) elif self . format [ i ] == comment : ignore . append ( fields [ i ]) elif self . format [ i ] == class : category = fields [ i ] # \u5904\u7406\u8be5\u6761\u8bb0\u5f55 total += 1 classes . setdefault ( category , 0 ) counts . setdefault ( category , {}) classes [ category ] += 1 # \u5904\u7406\u5404\u4e2a\u5c5e\u6027 col = 0 for columnValue in vector : col += 1 counts [ category ] . setdefault ( col , {}) counts [ category ][ col ] . setdefault ( columnValue , 0 ) counts [ category ][ col ][ columnValue ] += 1 # \u8ba1\u6570\u7ed3\u675f\uff0c\u5f00\u59cb\u8ba1\u7b97\u6982\u7387 # \u8ba1\u7b97\u5148\u9a8c\u6982\u7387P(h) for ( category , count ) in classes . items (): self . prior [ category ] = count / total # \u8ba1\u7b97\u6761\u4ef6\u6982\u7387P(h|D) for ( category , columns ) in counts . items (): self . conditional . setdefault ( category , {}) for ( col_id , valueCounts ) in columns . items (): self . conditional [ category ] . setdefault ( col_id , {}) for ( attrValue , count ) in valueCounts . items (): self . conditional [ category ][ col_id ][ attrValue ] = ( count / classes [ category ]) self . tmp = counts def classify ( self , itemVector ): \u8fd4\u56deitemVector\u6240\u5c5e\u7c7b\u522b results = [] for ( category , prior ) in self . prior . items (): prob = prior col = 1 for attrValue in itemVector : if attrValue not in self . conditional [ category ][ col ]: # \u5c5e\u6027\u4e0d\u5b58\u5728\uff0c\u8fd4\u56de0\u6982\u7387 prob = 0 else : prob = prob * self . conditional [ category ][ col ][ attrValue ] col += 1 results . append (( prob , category )) # \u8fd4\u56de\u6982\u7387\u6700\u9ad8\u7684\u7ed3\u679c return max ( results )[ 1 ] if __name__ == __main__ : c = WristBandBayes ( /Users/larry/datamining/DataminingGuideBook-Codes/chapter-6/iHealth/i , 10 , attr \\t attr \\t attr \\t attr \\t class ) print ( c . classify ([ health moderate , moderate , yes ]))","title":"\u8bad\u7ec3"},{"location":"gdm/ch5/#3-example","text":"\u7f8e\u56fd\u56fd\u4f1a\u6295\u7968\u6570\u636e \uff0c\u5176\u4e2d\u6bcf\u6761\u8bb0\u5f55\u4ee3\u8868\u4e00\u4e2a\u9009\u6c11\uff0c\u7b2c\u4e00\u5217\u662f\u5206\u7c7b\u540d\u79f0\uff08democrat, republican\uff09\uff0c\u4e4b\u540e\u662f16\u6761\u6cd5\u6848\uff0c\u7528y\u548cn\u8868\u793a\u8be5\u4eba\u662f\u5426\u652f\u6301\u3002 \u6587\u4ef6\u683c\u5f0f\u5982\u4e0b\uff1a democrat y n y n n y y y y y n n y n n y democrat y y y n n y y y y n n n n n y y republican y y n y y y n n n y n y y y n n \u5728\u8c03\u7528\u4e0a\u4e00\u8282\u7f16\u5199\u7684\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u65f6\u4f7f\u7528\u4ee5\u4e0bdataFormat\u53c2\u6570\u5c31\u53ef\u4ee5\u4e86\uff1a class\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\tattr\\t attr\\tattr\\tattr","title":"3 Example: \u7f8e\u56fd\u56fd\u4f1a\u6295\u7968\u6570\u636e"},{"location":"gdm/ch5/#0","text":"\u4f46\u662f\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u6709\u4e00\u4e9b\u95ee\u9898\u3002 \u4f7f\u7528\u6734\u7d20\u8d1d\u53f6\u65af\u8ba1\u7b97\u5f97\u5230\u7684\u6982\u7387\u5176\u5b9e\u662f\u771f\u5b9e\u6982\u7387\u7684\u4e00\u79cd\u4f30\u8ba1\uff0c\u800c\u771f\u5b9e\u6982\u7387\u662f\u5bf9\u5168\u91cf\u6570\u636e\u505a\u7edf\u8ba1\u5f97\u5230\u7684\u3002\u6bd4\u5982\u8bf4\uff0c\u6211\u4eec\u9700\u8981\u5bf9\u6240\u6709\u4eba\u90fd\u505a\u8840\u6db2\u6d4b\u8bd5\uff0c\u624d\u80fd\u5f97\u5230\u5065\u5eb7\u4eba\u8fd4\u56de\u9634\u6027\u7ed3\u679c\u7684\u771f\u5b9e\u6982\u7387\u3002\u663e\u7136\uff0c\u5bf9\u5168\u91cf\u6570\u636e\u505a\u7edf\u8ba1\u662f\u4e0d\u73b0\u5b9e\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u4f1a\u9009\u53d6\u4e00\u4e2a\u6837\u672c\uff0c\u59821000\u4eba\uff0c\u5bf9\u4ed6\u4eec\u8fdb\u884c\u6d4b\u8bd5\u5e76\u8ba1\u7b97\u6982\u7387\u3002\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\uff0c\u8fd9\u79cd\u4f30\u8ba1\u90fd\u662f\u63a5\u8fd1\u4e8e\u771f\u5b9e\u6982\u7387\u7684\u3002\u4f46\u5f53\u771f\u5b9e\u6982\u7387\u975e\u5e38\u5c0f\u65f6\uff0c\u8fd9\u79cd\u62bd\u6837\u7edf\u8ba1\u7684\u505a\u6cd5\u5c31\u4f1a\u6709\u95ee\u9898\u4e86\u3002\u6bd4\u5982\u8bf4\uff0c\u6c11\u4e3b\u515a\u5bf9\u7f51\u7edc\u975e\u6cd5\u4f20\u64ad\u6cd5\u6848\u7684\u5426\u51b3\u7387\u662f0.03\uff0c\u5373 P(S=no|\u6c11\u4e3b\u515a) = 0.03 P(S=no|\u6c11\u4e3b\u515a) = 0.03 \u3002\u5982\u679c\u6211\u4eec \u5206\u522b\u9009\u53d6\u5341\u4e2a\u6c11\u4e3b\u515a\u548c\u5171\u548c\u515a\u4eba\uff0c\u770b\u4ed6\u4eec\u5bf9\u8be5\u6cd5\u6848\u7684\u6295\u7968\u60c5\u51b5\uff0c\u4f60\u89c9\u5f97\u5f97\u5230\u7684\u6982\u7387\u4f1a\u662f\u4ec0\u4e48\uff1f\u7b54\u6848\u5f88\u53ef\u80fd\u662f0\u3002 \u5728\u6734\u7d20\u8d1d\u53f6\u65af\u4e2d\uff0c\u6982\u7387\u4e3a0\u7684\u5f71\u54cd\u662f\u5f88\u5927\u7684\u3002\u5982\u679c\u5176\u4e2d\u2f00\u4e2a\u6982\u7387\u503c\u4e3a0\uff0c\u90a3\u4e48\u6700\u540e\u7684\u4e58\u79ef\u4e5f\u4e3a0\u3002 \u4e3a\u4e86\u8868\u793a\u65b9\u4fbf\uff0c\u6211\u4eec\u91c7\u7528\u4ee5\u4e0b\u516c\u5f0f\uff1a P(x|y)=\\frac{n_c}{n} P(x|y)=\\frac{n_c}{n} <span><span class=\"MathJax_Preview\">P(x|y)=\\frac{n_c}{n}</span><script type=\"math/tex\">P(x|y)=\\frac{n_c}{n} \u5176\u4e2d, n n \u8868\u793a\u8bad\u7ec3\u96c6\u4e2dy\u7c7b\u522b\u7684\u8bb0\u5f55\u6570\uff1b n_c n_c \u8868\u793a y y \u7c7b\u522b\u4e2d\u503c\u4e3a x x \u7684\u8bb0\u5f55\u6570\u3002\u6211\u4eec\u7684\u95ee\u9898\u662f n_c n_c \u53ef\u80fd\u4e3a0\u3002\u89e3\u51b3\u65b9\u6cd5\u662f\u5c06\u516c\u793a\u53d8\u4e3a\u4ee5\u4e0b\u5f62\u5f0f\uff1a P(x|y)=\\frac{n_c+mp}{n+m} P(x|y)=\\frac{n_c+mp}{n+m} m m \u662f\u4e00\u4e2a\u5e38\u6570\uff0c\u8868\u793a\u7b49\u6548\u6837\u672c\u5927\u5c0f\u3002\u51b3\u5b9a\u5e38\u6570 m m \u7684\u65b9\u6cd5\u6709\u5f88\u591a\uff0c\u6211\u4eec\u8fd9\u91cc\u4f7f\u7528\u503c\u7684\u7c7b\u522b\u6570\u76ee\u6765\u4f5c\u4e3a m m \uff0c\u6bd4\u5982\u6295\u7968\u6709\u8d5e\u6210\u548c\u5426\u51b3\u4e24\u79cd\u7c7b\u522b\uff0c\u6240\u4ee5 m m \u5c31\u4e3a2\u3002 p p \u5219\u662f\u76f8\u5e94\u7684\u5148\u9a8c\u6982\u7387\uff0c\u6bd4\u5982\u8bf4\u8d5e\u6210\u548c\u5426\u51b3\u7684\u6982\u7387\u5206\u522b\u662f0.5\uff0c\u90a3 p p \u5c31\u662f0.5\u3002","title":"\u6982\u7387\u503c\u4e3a0"},{"location":"gdm/ch5/#_4","text":"\u5728\u8d1d\u53f6\u65af\u65b9\u6cd5\u4e2d\uff0c\u4e4b\u524d\u6211\u4eec\u5bf9\u4e8b\u7269\u8fdb\u884c\u4e86\u8ba1\u6570\uff0c\u8fd9\u79cd\u8ba1\u6570\u5219\u662f\u53ef\u4ee5\u5ea6\u91cf\u7684\u3002\u5bf9\u4e8e\u6570\u503c\u578b\u7684\u6570\u636e\u8981 \u5982\u4f55\u8ba1\u6570\u5462\uff1f\u901a\u5e38\u6709\u4e24\u79cd\u505a\u6cd5\uff1a\u533a\u5206\u7c7b\u522b\u548c\u9ad8\u65af\u5206\u5e03","title":"\u6570\u503c\u578b\u6570\u636e"},{"location":"gdm/ch5/#_5","text":"\u6211\u4eec\u53ef\u4ee5\u5212\u5b9a\u51e0\u4e2a\u8303\u56f4\u4f5c\u4e3a\u5206\u7c7b\uff0c\u5982\uff1a \u5e74\u9f84 18 18 - 22 23 - 30 31 - 40 40 \u5e74\u85aa $200,000 150,000 - 200,000 100,000 - 150,000 60,000 - 100,000 40,000 - 60,000 \u5212\u5206\u7c7b\u522b\u540e\uff0c\u5c31\u53ef\u4ee5\u5e94\u7528\u6734\u7d20\u8d1d\u53f6\u65af\u65b9\u6cd5\u4e86\u3002","title":"\u533a\u5206\u7c7b\u522b"},{"location":"gdm/ch5/#_6","text":"\u5c5e\u4e8e\u7c7b\u522b y_i y_i \u7684\u7279\u5f81 x_i x_i \u7684\u6982\u7387\u4e3a P(x_i|y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{ij}}e^{-\\frac{(x_i-u_{ij})^2}{2\\sigma^2_{ij}}} P(x_i|y_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{ij}}e^{-\\frac{(x_i-u_{ij})^2}{2\\sigma^2_{ij}}} \u4e3a\u4e86\u4e3e\u4f8b\uff0c\u6211\u4eec\u4e3a\u4e0a\u9762\u8bb2\u8ff0\u7684\u624b\u73af\u7684\u4f8b\u5b50\u589e\u52a0\u4e00\u5217\u6536\u5165\u5c5e\u6027\u3002\u5047\u8bbe\u6211\u4eec\u8981\u8ba1\u7b97 P(100k|i500) P(100k|i500) \u7684\u6982\u7387\uff0c\u5373\u8d2d\u4e70i500\u7684\u7528\u6237\u4e2d\u6536\u5165\u662f100,000\u7f8e\u5143\u7684\u6982\u7387\u3002\u90a3\u4e48 u_{ij}, \\sigma_{ij} u_{ij}, \\sigma_{ij} \u5206\u522b\u662f\u8d2d\u4e70i500\u7684\u7528\u6237\u7684\u5e73\u5747\u6536\u5165\u548c\u6536\u5165\u7684\u6807\u51c6\u5dee\u3002 \u6837\u672c\u6807\u51c6\u5dee\u7684\u8ba1\u7b97\u516c\u5f0f\u662f\uff1a \\sigma = \\sqrt{\\frac{\\sum_i(x_i-\\bar x)^2}{card(x)-1}} \\sigma = \\sqrt{\\frac{\\sum_i(x_i-\\bar x)^2}{card(x)-1}} \u5728\u8bad\u7ec3\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u65f6\uff0c\u53ef\u4ee5\u5c06\u6240\u6709\u5c5e\u6027\u7684\u5e73\u5747\u503c\u548c\u6837\u672c\u6807\u51c6\u5dee\u8ba1\u7b97\u51fa\u6765\uff0c\u800c\u5206\u7c7b\u9636\u6bb5\u4f7f\u7528\u5e73\u5747\u503c\u548c\u6837\u672c\u6807\u51c6\u5dee\u8ba1\u7b97\u6982\u7387\u5bc6\u5ea6\u5206\u5e03\uff1a def pdf ( mean , ssd , x ): \u6982\u7387\u5bc6\u5ea6\u51fd\u6570\uff0c\u8ba1\u7b97P(x|y) ePart = math . pow ( math . e , - ( x - mean ) ** 2 / ( 2 * ssd ** 2 )) return ( 1.0 / ( math . sqrt ( 2 * math . pi ) * ssd )) * ePart","title":"\u9ad8\u65af\u5206\u5e03"},{"location":"gdm/ch5/#python","text":"","title":"\u4f7f\u7528Python\u5b9e\u73b0"},{"location":"gdm/ch6/","text":"\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 6 \u6734\u7d20\u8d1d\u53f6\u65af\u7b97\u6cd5\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c","title":"Chapter 6: \u6734\u7d20\u8d1d\u53f6\u65af\u548c\u6587\u672c\u6570\u636e"},{"location":"gdm/ch6/#-6","text":"","title":"\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 6 \u6734\u7d20\u8d1d\u53f6\u65af\u7b97\u6cd5\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c"},{"location":"gdm/ch7/","text":"\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 7 \u805a\u7c7b","title":"Chapter 7: \u805a\u7c7b"},{"location":"gdm/ch7/#-7","text":"","title":"\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 7 \u805a\u7c7b"},{"location":"hadoop/","text":"HADOOP Chapter 1: Meet Hadoop Chapter 2: MapReduce Chapter 3: The Hadoop Distributed FileSystem Chapter 4: YARN Chapter 5: Hadoop I/O Chapter 6: Developing a MapReduce Application Chapter 7: How MapReduce Works Chapter 8: MapReduce Types and Formats Chapter 9: MapReduce Features Chapter 10: Setting Up a Hadoop Cluster Chapter 11: Adminstering Hadoop Chapter 12: Avro Chapter 13: Parquet Chapter 14: Flume Chapter 15: Sqoop Chapter 16: Pig Chapter 17: Hive Chapter 18: Crunch Chapter 19: Spark Chapter 20: HBase Chapter 21: ZooKeeper Chapter 22: Composable Data at Center Chapter 23: Biological Data Science: Saving Lives with Software Chapter 24: Cascading","title":"Contents"},{"location":"hadoop/#hadoop","text":"Chapter 1: Meet Hadoop Chapter 2: MapReduce Chapter 3: The Hadoop Distributed FileSystem Chapter 4: YARN Chapter 5: Hadoop I/O Chapter 6: Developing a MapReduce Application Chapter 7: How MapReduce Works Chapter 8: MapReduce Types and Formats Chapter 9: MapReduce Features Chapter 10: Setting Up a Hadoop Cluster Chapter 11: Adminstering Hadoop Chapter 12: Avro Chapter 13: Parquet Chapter 14: Flume Chapter 15: Sqoop Chapter 16: Pig Chapter 17: Hive Chapter 18: Crunch Chapter 19: Spark Chapter 20: HBase Chapter 21: ZooKeeper Chapter 22: Composable Data at Center Chapter 23: Biological Data Science: Saving Lives with Software Chapter 24: Cascading","title":"HADOOP"},{"location":"hadoop/ch1/","text":"Hadoop: The Definitive Guide 1 - Meet Hadoop 1 \u6570\u636e\uff01\u6570\u636e\uff01 \u73b0\u5728\u662f\u6570\u636e\u5927\u7206\u70b8\u65f6\u4ee3\uff0c\u5168\u7403\u6570\u636e\u603b\u91cf\u8fdc\u8fdc\u8d85\u8fc7\u4e86\u5168\u4e16\u754c\u6bcf\u4eba\u4e00\u5757\u786c\u76d8\u4e2d\u6240\u80fd\u4fdd\u5b58\u7684\u6570\u636e\u603b\u91cf\u3002 \u4e2a\u4eba\u4ea7\u751f\u7684\u6570\u636e\u6b63\u5728\u5feb\u901f\u589e\u957f \u4e2a\u4eba\u4fe1\u606f\u6863\u6848\u5c06\u65e5\u76ca\u666e\u53ca\uff08\u7535\u8bdd\u3001\u90ae\u4ef6\u3001\u6587\u4ef6\u3001\u7167\u7247\uff09 \u7269\u8054\u7f51\u7684\u673a\u5668\u8bbe\u5907\u4ea7\u751f\u7684\u6570\u636e\u53ef\u80fd\u8fdc\u8fdc\u8d85\u8fc7\u4e2a\u4eba\u4ea7\u751f\u7684\u6570\u636e \u673a\u5668\u65e5\u5fd7\u3001\u4f20\u611f\u5668\u7f51\u7edc\u3001\u96f6\u552e\u4ea4\u6613\u6570\u636e\u7b49 1.2 \u6570\u636e\u7684\u5b58\u50a8\u4e0e\u5206\u6790 \u9047\u5230\u7684\u95ee\u9898\uff1a\u786c\u76d8\u5b58\u50a8\u5bb9\u91cf\u4e0d\u65ad\u63d0\u5347\uff0c\u8bbf\u95ee\u901f\u5ea6\u6ca1\u6709\u4e0e\u65f6\u4ff1\u8fdb \u8bfb\u5199\u786c\u76d8\u4e2d\u7684\u6570\u636e\u9700\u8981\u66f4\u957f\u65f6\u95f4 \u89e3\u51b3\u65b9\u6cd5\uff1a \u540c\u65f6\u4ece\u591a\u4e2a\u786c\u76d8\u4e0a\u8bfb\u53d6\u6570\u636e\uff0c\u6bcf\u4e2a\u786c\u76d8\u5b58\u50a8\u4e00\u90e8\u5206\u6570\u636e \u867d\u7136\u6d6a\u8d39\u4e86\u786c\u76d8\u5bb9\u91cf\uff0c\u4f46\u662f\u7531\u4e8e\u7528\u6237\u7684\u5206\u6790\u5de5\u4f5c\u90fd\u662f\u5728\u4e0d\u540c\u65f6\u95f4\u70b9\u8fdb\u884c\u7684\uff0c\u6240\u4ee5\u5f7c\u6b64\u4e4b\u95f4\u7684\u5e72\u6270\u5e76\u4e0d\u592a\u5927 \u65b0\u7684\u95ee\u9898\uff1a\u8981\u5bf9\u591a\u4e2a\u786c\u76d8\u4e2d\u7684\u6570\u636e\u5e76\u884c\u8fdb\u884c\u8bfb\u5199\u6570\u636e\uff0c\u8fd8\u6709\u66f4\u591a\u95ee\u9898\u8981\u89e3\u51b3 \u786c\u4ef6\u6545\u969c\u95ee\u9898\uff1a\u6700\u5e38\u89c1\u7684\u505a\u6cd5\u662f\u7cfb\u7edf\u4fdd\u5b58\u6570\u636e\u7684\u526f\u672c(replica) \u5927\u591a\u6570\u5206\u6790\u4efb\u52a1\u9700\u8981\u4ee5\u67d0\u79cd\u65b9\u5f0f\u7ed3\u5408\u5927\u90e8\u5206\u6570\u636e\u6765\u5171\u540c\u5b8c\u6210\u5206\u6790\uff0c\u4fdd\u8bc1\u5176\u6b63\u786e\u6027\u662f\u4e00\u4e2a\u975e\u5e38\u5927\u7684\u6311\u6218\uff1aMapReduce 1.3 \u67e5\u8be2\u6240\u6709\u6570\u636e MapReduce\u662f\u4e00\u4e2a \u6279\u91cf\u67e5\u8be2\u5904\u7406\u5668 (batch processing system)\uff0c\u80fd\u591f\u5728\u5408\u7406\u7684\u65f6\u95f4\u8303\u56f4\u5185\u5904\u7406\u9488\u5bf9\u6574\u4e2a\u6570\u636e\u96c6\u7684\u52a8\u6001\u67e5\u8be2\u3002 1.4 \u4e0d\u4ec5\u4ec5\u662f\u6279\u5904\u7406 MapReduce\u57fa\u672c\u4e0a\u662f\u4e00\u4e2a\u6279\u5904\u7406\u7cfb\u7edf\uff0c\u5e76\u4e0d\u9002\u5408\u4ea4\u4e92\u5f0f\u5206\u6790: \u4f60\u4e0d\u53ef\u80fd\u6267\u884c\u4e00\u6761\u67e5\u8be2\u5e76\u5728\u51e0\u79d2\u5185\u6216\u66f4\u77ed\u65f6\u95f4\u5185\u5f97\u5230\u7ed3\u679c\uff1b\u5178\u578b\u60c5\u51b5\u4e0b\uff0c\u6267\u884c\u67e5\u8be2\u9700\u8981\u51e0\u5206\u949f\u6216\u66f4\u591a\u65f6\u95f4\u3002 MapReduce\u66f4\u9002\u5408\u6ca1\u6709\u7528\u6237\u5728\u73b0\u573a\u7b49\u5f85\u67e5\u8be2\u7ed3\u679c\u7684\u79bb\u7ebf\u4f7f\u7528\u573a\u666f\u3002 Hadoop\u7684\u53d1\u5c55\u5df2\u7ecf\u8d85\u8d8a\u4e86\u6279\u5904\u7406\u672c\u8eab\u3002 Hadoop\u6709\u65f6\u88ab\u7528\u4e8e\u6307\u4ee3\u4e00\u4e2a\u66f4\u5927\u7684\u3001\u591a\u4e2a\u9879\u76ee\u7ec4\u6210\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662fHDFS\u548cMapReduce HBase\uff1a\u4f7f\u7528HDFS\u505a\u5e95\u5c42\u5b58\u50a8\u7684\u952e\u503c\u5b58\u50a8\u6a21\u578b\u3002 YARN: \u96c6\u7fa4\u8d44\u6e90\u7ba1\u7406\u7cfb\u7edf 1.6 \u53d1\u5c55\u5386\u53f2 Hadoop\u662fDoug Cutting\u521b\u5efa\u7684\uff0c\u8d77\u6e90\u4e8e\u5f00\u6e90\u7f51\u7edc\u641c\u7d22\u5f15\u64ceApache Nutch. Apache Nutch\u662f\u8d77\u59cb\u4e8e2002\u5e74\uff0c\u5e76\u501f\u9274\u4e86\u201c\u8c37\u6b4c\u5206\u5e03\u5f0f\u6587\u4ef6\u7cfb\u7edf(GFS)\"\u548cMapReduce\u3002 2006\u5e74\uff0cDoug Cutting\u52a0\u5165\u96c5\u864e\uff0c\u96c5\u864e\u4e3a\u6b64\u7ec4\u7ec7\u4e86\u4e13\u95e8\u7684\u56e2\u961f\u548c\u8d44\u6e90\uff0c\u5c06Hadoop\u53d1\u5c55\u6210\u80fd\u591f\u4ee5Web\u89c4\u6a21\u8fd0\u884c\u7684\u7cfb\u7edf\u3002 2008\u5e74\uff0cHadoop\u5df2\u7ecf\u6210\u4e3aApache\u7684\u9876\u7ea7\u9879\u76ee\uff0c\u8bc1\u660e\u4e86\u5b83\u7684\u6210\u529f\u3001\u591a\u6837\u5316\u548c\u751f\u547d\u529b\u3002 \u76ee\u524d\uff0cHadoop\u88ab\u4e3b\u6d41\u4f01\u4e1a\u5e7f\u6cdb\u4f7f\u7528\u3002\u5728\u5de5\u4e1a\u754c\uff0cHadoop\u5df2\u7ecf\u662f \u516c\u8ba4\u7684\u5927\u6570\u636e\u901a\u7528\u5b58\u50a8\u548c\u5206\u6790\u5e73\u53f0 \u3002 1.7 Hadoop\u5bb6\u65cf","title":"Chapter 1: Meet Hadoop"},{"location":"hadoop/ch1/#hadoop-the-definitive-guide-1-meet-hadoop","text":"","title":"Hadoop: The Definitive Guide 1 - Meet Hadoop"},{"location":"hadoop/ch1/#1","text":"\u73b0\u5728\u662f\u6570\u636e\u5927\u7206\u70b8\u65f6\u4ee3\uff0c\u5168\u7403\u6570\u636e\u603b\u91cf\u8fdc\u8fdc\u8d85\u8fc7\u4e86\u5168\u4e16\u754c\u6bcf\u4eba\u4e00\u5757\u786c\u76d8\u4e2d\u6240\u80fd\u4fdd\u5b58\u7684\u6570\u636e\u603b\u91cf\u3002 \u4e2a\u4eba\u4ea7\u751f\u7684\u6570\u636e\u6b63\u5728\u5feb\u901f\u589e\u957f \u4e2a\u4eba\u4fe1\u606f\u6863\u6848\u5c06\u65e5\u76ca\u666e\u53ca\uff08\u7535\u8bdd\u3001\u90ae\u4ef6\u3001\u6587\u4ef6\u3001\u7167\u7247\uff09 \u7269\u8054\u7f51\u7684\u673a\u5668\u8bbe\u5907\u4ea7\u751f\u7684\u6570\u636e\u53ef\u80fd\u8fdc\u8fdc\u8d85\u8fc7\u4e2a\u4eba\u4ea7\u751f\u7684\u6570\u636e \u673a\u5668\u65e5\u5fd7\u3001\u4f20\u611f\u5668\u7f51\u7edc\u3001\u96f6\u552e\u4ea4\u6613\u6570\u636e\u7b49","title":"1 \u6570\u636e\uff01\u6570\u636e\uff01"},{"location":"hadoop/ch1/#12","text":"\u9047\u5230\u7684\u95ee\u9898\uff1a\u786c\u76d8\u5b58\u50a8\u5bb9\u91cf\u4e0d\u65ad\u63d0\u5347\uff0c\u8bbf\u95ee\u901f\u5ea6\u6ca1\u6709\u4e0e\u65f6\u4ff1\u8fdb \u8bfb\u5199\u786c\u76d8\u4e2d\u7684\u6570\u636e\u9700\u8981\u66f4\u957f\u65f6\u95f4 \u89e3\u51b3\u65b9\u6cd5\uff1a \u540c\u65f6\u4ece\u591a\u4e2a\u786c\u76d8\u4e0a\u8bfb\u53d6\u6570\u636e\uff0c\u6bcf\u4e2a\u786c\u76d8\u5b58\u50a8\u4e00\u90e8\u5206\u6570\u636e \u867d\u7136\u6d6a\u8d39\u4e86\u786c\u76d8\u5bb9\u91cf\uff0c\u4f46\u662f\u7531\u4e8e\u7528\u6237\u7684\u5206\u6790\u5de5\u4f5c\u90fd\u662f\u5728\u4e0d\u540c\u65f6\u95f4\u70b9\u8fdb\u884c\u7684\uff0c\u6240\u4ee5\u5f7c\u6b64\u4e4b\u95f4\u7684\u5e72\u6270\u5e76\u4e0d\u592a\u5927 \u65b0\u7684\u95ee\u9898\uff1a\u8981\u5bf9\u591a\u4e2a\u786c\u76d8\u4e2d\u7684\u6570\u636e\u5e76\u884c\u8fdb\u884c\u8bfb\u5199\u6570\u636e\uff0c\u8fd8\u6709\u66f4\u591a\u95ee\u9898\u8981\u89e3\u51b3 \u786c\u4ef6\u6545\u969c\u95ee\u9898\uff1a\u6700\u5e38\u89c1\u7684\u505a\u6cd5\u662f\u7cfb\u7edf\u4fdd\u5b58\u6570\u636e\u7684\u526f\u672c(replica) \u5927\u591a\u6570\u5206\u6790\u4efb\u52a1\u9700\u8981\u4ee5\u67d0\u79cd\u65b9\u5f0f\u7ed3\u5408\u5927\u90e8\u5206\u6570\u636e\u6765\u5171\u540c\u5b8c\u6210\u5206\u6790\uff0c\u4fdd\u8bc1\u5176\u6b63\u786e\u6027\u662f\u4e00\u4e2a\u975e\u5e38\u5927\u7684\u6311\u6218\uff1aMapReduce","title":"1.2 \u6570\u636e\u7684\u5b58\u50a8\u4e0e\u5206\u6790"},{"location":"hadoop/ch1/#13","text":"MapReduce\u662f\u4e00\u4e2a \u6279\u91cf\u67e5\u8be2\u5904\u7406\u5668 (batch processing system)\uff0c\u80fd\u591f\u5728\u5408\u7406\u7684\u65f6\u95f4\u8303\u56f4\u5185\u5904\u7406\u9488\u5bf9\u6574\u4e2a\u6570\u636e\u96c6\u7684\u52a8\u6001\u67e5\u8be2\u3002","title":"1.3 \u67e5\u8be2\u6240\u6709\u6570\u636e"},{"location":"hadoop/ch1/#14","text":"MapReduce\u57fa\u672c\u4e0a\u662f\u4e00\u4e2a\u6279\u5904\u7406\u7cfb\u7edf\uff0c\u5e76\u4e0d\u9002\u5408\u4ea4\u4e92\u5f0f\u5206\u6790: \u4f60\u4e0d\u53ef\u80fd\u6267\u884c\u4e00\u6761\u67e5\u8be2\u5e76\u5728\u51e0\u79d2\u5185\u6216\u66f4\u77ed\u65f6\u95f4\u5185\u5f97\u5230\u7ed3\u679c\uff1b\u5178\u578b\u60c5\u51b5\u4e0b\uff0c\u6267\u884c\u67e5\u8be2\u9700\u8981\u51e0\u5206\u949f\u6216\u66f4\u591a\u65f6\u95f4\u3002 MapReduce\u66f4\u9002\u5408\u6ca1\u6709\u7528\u6237\u5728\u73b0\u573a\u7b49\u5f85\u67e5\u8be2\u7ed3\u679c\u7684\u79bb\u7ebf\u4f7f\u7528\u573a\u666f\u3002 Hadoop\u7684\u53d1\u5c55\u5df2\u7ecf\u8d85\u8d8a\u4e86\u6279\u5904\u7406\u672c\u8eab\u3002 Hadoop\u6709\u65f6\u88ab\u7528\u4e8e\u6307\u4ee3\u4e00\u4e2a\u66f4\u5927\u7684\u3001\u591a\u4e2a\u9879\u76ee\u7ec4\u6210\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662fHDFS\u548cMapReduce HBase\uff1a\u4f7f\u7528HDFS\u505a\u5e95\u5c42\u5b58\u50a8\u7684\u952e\u503c\u5b58\u50a8\u6a21\u578b\u3002 YARN: \u96c6\u7fa4\u8d44\u6e90\u7ba1\u7406\u7cfb\u7edf","title":"1.4 \u4e0d\u4ec5\u4ec5\u662f\u6279\u5904\u7406"},{"location":"hadoop/ch1/#16","text":"Hadoop\u662fDoug Cutting\u521b\u5efa\u7684\uff0c\u8d77\u6e90\u4e8e\u5f00\u6e90\u7f51\u7edc\u641c\u7d22\u5f15\u64ceApache Nutch. Apache Nutch\u662f\u8d77\u59cb\u4e8e2002\u5e74\uff0c\u5e76\u501f\u9274\u4e86\u201c\u8c37\u6b4c\u5206\u5e03\u5f0f\u6587\u4ef6\u7cfb\u7edf(GFS)\"\u548cMapReduce\u3002 2006\u5e74\uff0cDoug Cutting\u52a0\u5165\u96c5\u864e\uff0c\u96c5\u864e\u4e3a\u6b64\u7ec4\u7ec7\u4e86\u4e13\u95e8\u7684\u56e2\u961f\u548c\u8d44\u6e90\uff0c\u5c06Hadoop\u53d1\u5c55\u6210\u80fd\u591f\u4ee5Web\u89c4\u6a21\u8fd0\u884c\u7684\u7cfb\u7edf\u3002 2008\u5e74\uff0cHadoop\u5df2\u7ecf\u6210\u4e3aApache\u7684\u9876\u7ea7\u9879\u76ee\uff0c\u8bc1\u660e\u4e86\u5b83\u7684\u6210\u529f\u3001\u591a\u6837\u5316\u548c\u751f\u547d\u529b\u3002 \u76ee\u524d\uff0cHadoop\u88ab\u4e3b\u6d41\u4f01\u4e1a\u5e7f\u6cdb\u4f7f\u7528\u3002\u5728\u5de5\u4e1a\u754c\uff0cHadoop\u5df2\u7ecf\u662f \u516c\u8ba4\u7684\u5927\u6570\u636e\u901a\u7528\u5b58\u50a8\u548c\u5206\u6790\u5e73\u53f0 \u3002","title":"1.6 \u53d1\u5c55\u5386\u53f2"},{"location":"hadoop/ch1/#17-hadoop","text":"","title":"1.7 Hadoop\u5bb6\u65cf"},{"location":"hadoop/ch10/","text":"Hadoop: The Definitive Guide 10 - Setting Up a Hadoop Cluster","title":"Chapter 10: Setting Up a Hadoop Cluster"},{"location":"hadoop/ch10/#hadoop-the-definitive-guide-10-setting-up-a-hadoop-cluster","text":"","title":"Hadoop: The Definitive Guide 10 - Setting Up a Hadoop Cluster"},{"location":"hadoop/ch11/","text":"Hadoop: The Definitive Guide 11 - Adminstering Hadoop","title":"Chapter 11: Adminstering Hadoop"},{"location":"hadoop/ch11/#hadoop-the-definitive-guide-11-adminstering-hadoop","text":"","title":"Hadoop: The Definitive Guide 11 - Adminstering Hadoop"},{"location":"hadoop/ch12/","text":"Hadoop: The Definitive Guide 12 - Avro Apache Avro is a language-neutral data serialization system. The project was created by Doug Cutting to address the major downside of Hadoop Writables : lack of language portability [see Chapter5 ]. Having a data format that can be processed by many languages makes it easier to share datasets with a wider audience. Avro data is described using a language-independent schema (\u6a21\u5f0f). Schemas are usually written in JSON, and data is usually encoded using a binary format. Geting start This section based on Apache Avro\u2122 Getting Started (Java) . Defining a schema Avro schemas are defined using JSON. Schemas are composed of primitive types (null, boolean, int, long, float, double, bytes, and string) and complex types (record, enum, array, map, union, and fixed). Let's start with a simple schema example, user.avsc : { namespace : example.avro , type : record , name : User , fields : [ { name : name , type : string }, { name : favorite_number , type : [ int , null ]}, { name : favorite_color , type : [ string , null ]} ] } This schema defines a record representing a hypothetical user. (Note that a schema file can only contain a single schema definition.) At minimum, a record definition must include its type, a name, and fields. Fields are defined via an array of objects, each of which defines a name and type; The type attribute of a field is another schema object, which can be either a primitive or complex type. Serializing and deserializing Let's create some users, serialize them to a data file on disk, and then read back the file and deserialize the users objects. Creating users First, we use a Parser to read our schema definition and create a Schema object. Schema schema = new Schema . Parser (). parse ( new File ( user.avsc )); Using this schema, let's create some users. GenericRecord user1 = new GenericData . Record ( schema ); user1 . put ( name , Alyssa ); user1 . put ( favorite_number , 256 ); // Leave favorite color null GenericRecord user2 = new GenericData . Record ( schema ); user2 . put ( name , Ben ); user2 . put ( favorite_number , 7 ); user2 . put ( favorite_color , red ); We use GenericRecords to represent users. GenericRecords uses the schema to verify that we only specify valid fields. If we try to set a non-existent field (e.g., user1.put(\"favorite_animal\", \"cat\")), we'll get an AvroRuntimeException when we run the program. Note that we do not set user1's favorite color. Since that record is of type [\"string\", \"null\"], we can either set it to a string or leave it null; it is essentially optional. Serializing Now that we've created our user objects, we use generic readers and writers to serialize and deserialize them. First we'll serialize our users to a data file on disk. // Serialize user1 and user2 to disk File file = new File ( users.avro ); DatumWriter GenericRecord datumWriter = new GenericDatumWriter GenericRecord ( schema ); DataFileWriter GenericRecord dataFileWriter = new DataFileWriter GenericRecord ( datumWriter ); dataFileWriter . create ( schema , file ); dataFileWriter . append ( user1 ); dataFileWriter . append ( user2 ); dataFileWriter . close (); We create a DatumWriter , which converts Java objects into an in-memory serialized format. GenericDatumWriter requires the schema both to determine how to write the GenericRecords and to verify that all non-nullable fields are present. We also create a DataFileWriter , which writes the serialized records, as well as the schema, to the file specified in the dataFileWriter.create call. We write our users to the file via calls to the dataFileWriter.append method. When we are done writing, we close the data file. Deserializing Finally, we'll deserialize the data file we just created. // Deserialize users from disk DatumReader GenericRecord datumReader = new GenericDatumReader GenericRecord ( schema ); DataFileReader GenericRecord dataFileReader = new DataFileReader GenericRecord ( file , datumReader ); GenericRecord user = null ; while ( dataFileReader . hasNext ()) { // Reuse user object by passing it to next(). This saves us from // allocating and garbage collecting many objects for files with // many items. user = dataFileReader . next ( user ); System . out . println ( user ); } This outputs: { name : Alyssa , favorite_number : 256, favorite_color : null} { name : Ben , favorite_number : 7, favorite_color : red } Deserializing is very similar to serializing. We create a GenericDatumReader , analogous to the GenericDatumWriter we used in serialization, which converts in-memory serialized items into GenericRecords . We pass the DatumReader and the previously created File to a DataFileReader , analogous to the DataFileWriter , which reads the data file on disk. Next, we use the DataFileReader to iterate through the serialized users and print the deserialized object to stdout. Note how we perform the iteration: we create a single GenericRecord object which we store the current deserialized user in, and pass this record object to every call of dataFileReader.next . This is a performance optimization that allows the DataFileReader to reuse the same record object rather than allocating a new GenericRecord for every iteration, which can be very expensive in terms of object allocation and garbage collection if we deserialize a large data file. While this technique is the standard way to iterate through a data file, it's also possible to use for ( GenericRecord user : dataFileReader ) if performance is not a concern. Sort Order Avro defines a sort order for objects. All types except record have preordained rules for their sort order, as described in the Avro specification, that cannot be overridden by the user. For records, however, you can control the sort order by specifying the order attribute for a field. It takes one of three values: ascending (the default), descending (to reverse the order), or ignore (so the field is skipped for comparison purposes) For example, the following schema ( SortedStringPair.avsc ) defines an ordering of StringPair records by the right field in descending order. The left field is ignored for the purposes of ordering, but it is still present in the projection: { type : record , name : StringPair , doc : A pair of strings, sorted by right field descending. , fields : [ { name : left , type : string , order : ignore }, { name : right , type : string , order : descending } ] } Avro implements efficient binary comparisons. That is to say, Avro does not have to deserialize binary data into objects to perform the comparison, because it can instead work directly on the byte streams. Avro provides the comparator for us. Avro MapReduce Avro provides a number of classes for making it easy to run MapReduce programs on Avro data. Let\u2019s rework the MapReduce program for finding the maximum temperature for each year in the weather dataset, this time using the Avro MapReduce API. We will represent weather records using the following schema: { type : record , name : WeatherRecord , doc : A weather reading. , fields : [ { name : year , type : int }, { name : temperature , type : int }, { name : stationId , type : string } ] } There are a couple of differences from the regular Hadoop MapReduce API. The first is the use of wrappers around Avro Java types. The second major difference from regular MapReduce is the use of AvroJob for configuring the job . AvroJob is a convenience class for specifying the Avro schemas for the input, map output, and final output data. Avro Mapreduce //vv AvroGenericMaxTemperature public class AvroGenericMaxTemperature extends Configured implements Tool { private static final Schema SCHEMA = new Schema . Parser (). parse ( { + \\ type\\ : \\ record\\ , + \\ name\\ : \\ WeatherRecord\\ , + \\ doc\\ : \\ A weather reading.\\ , + \\ fields\\ : [ + {\\ name\\ : \\ year\\ , \\ type\\ : \\ int\\ }, + {\\ name\\ : \\ temperature\\ , \\ type\\ : \\ int\\ }, + {\\ name\\ : \\ stationId\\ , \\ type\\ : \\ string\\ } + ] + } ); public static class MaxTemperatureMapper extends Mapper LongWritable , Text , AvroKey Integer , AvroValue GenericRecord { private NcdcRecordParser parser = new NcdcRecordParser (); private GenericRecord record = new GenericData . Record ( SCHEMA ); @Override protected void map ( LongWritable key , Text value , Context context ) throws IOException , InterruptedException { parser . parse ( value . toString ()); if ( parser . isValidTemperature ()) { record . put ( year , parser . getYearInt ()); record . put ( temperature , parser . getAirTemperature ()); record . put ( stationId , parser . getStationId ()); context . write ( new AvroKey Integer ( parser . getYearInt ()), new AvroValue GenericRecord ( record )); } } } public static class MaxTemperatureReducer extends Reducer AvroKey Integer , AvroValue GenericRecord , AvroKey GenericRecord , NullWritable { @Override protected void reduce ( AvroKey Integer key , Iterable AvroValue GenericRecord values , Context context ) throws IOException , InterruptedException { GenericRecord max = null ; for ( AvroValue GenericRecord value : values ) { GenericRecord record = value . datum (); if ( max == null || ( Integer ) record . get ( temperature ) ( Integer ) max . get ( temperature )) { max = newWeatherRecord ( record ); } } context . write ( new AvroKey ( max ), NullWritable . get ()); } private GenericRecord newWeatherRecord ( GenericRecord value ) { GenericRecord record = new GenericData . Record ( SCHEMA ); record . put ( year , value . get ( year )); record . put ( temperature , value . get ( temperature )); record . put ( stationId , value . get ( stationId )); return record ; } } @Override public int run ( String [] args ) throws Exception { if ( args . length != 2 ) { System . err . printf ( Usage: %s [generic options] input output \\n , getClass (). getSimpleName ()); ToolRunner . printGenericCommandUsage ( System . err ); return - 1 ; } Job job = new Job ( getConf (), Max temperature ); job . setJarByClass ( getClass ()); job . getConfiguration (). setBoolean ( Job . MAPREDUCE_JOB_USER_CLASSPATH_FIRST , true ); FileInputFormat . addInputPath ( job , new Path ( args [ 0 ])); FileOutputFormat . setOutputPath ( job , new Path ( args [ 1 ])); AvroJob . setMapOutputKeySchema ( job , Schema . create ( Schema . Type . INT )); AvroJob . setMapOutputValueSchema ( job , SCHEMA ); AvroJob . setOutputKeySchema ( job , SCHEMA ); job . setInputFormatClass ( TextInputFormat . class ); job . setOutputFormatClass ( AvroKeyOutputFormat . class ); job . setMapperClass ( MaxTemperatureMapper . class ); job . setReducerClass ( MaxTemperatureReducer . class ); return job . waitForCompletion ( true ) ? 0 : 1 ; } public static void main ( String [] args ) throws Exception { int exitCode = ToolRunner . run ( new AvroGenericMaxTemperature (), args ); System . exit ( exitCode ); } } Sorting Using Avro MapReduce To sort an Avro datafile, it is simple. The mapper simply emits the input key wrapped in an AvroKey and an AvroValue . The reducer acts as an identity, passing the values through as output keys, which will get written to an Avro datafile. The sorting happens in the MapReduce shuffle, and the sort function is determined by the Avro schema that is passed to the program. See Chapter 7, section Shuffle and Sort for details. Useful resources Apache Avro project page CDH usage page for Avro Avro Specification","title":"Chapter 12: Avro"},{"location":"hadoop/ch12/#hadoop-the-definitive-guide-12-avro","text":"Apache Avro is a language-neutral data serialization system. The project was created by Doug Cutting to address the major downside of Hadoop Writables : lack of language portability [see Chapter5 ]. Having a data format that can be processed by many languages makes it easier to share datasets with a wider audience. Avro data is described using a language-independent schema (\u6a21\u5f0f). Schemas are usually written in JSON, and data is usually encoded using a binary format.","title":"Hadoop: The Definitive Guide 12 - Avro"},{"location":"hadoop/ch12/#geting-start","text":"This section based on Apache Avro\u2122 Getting Started (Java) .","title":"Geting start"},{"location":"hadoop/ch12/#defining-a-schema","text":"Avro schemas are defined using JSON. Schemas are composed of primitive types (null, boolean, int, long, float, double, bytes, and string) and complex types (record, enum, array, map, union, and fixed). Let's start with a simple schema example, user.avsc : { namespace : example.avro , type : record , name : User , fields : [ { name : name , type : string }, { name : favorite_number , type : [ int , null ]}, { name : favorite_color , type : [ string , null ]} ] } This schema defines a record representing a hypothetical user. (Note that a schema file can only contain a single schema definition.) At minimum, a record definition must include its type, a name, and fields. Fields are defined via an array of objects, each of which defines a name and type; The type attribute of a field is another schema object, which can be either a primitive or complex type.","title":"Defining a schema"},{"location":"hadoop/ch12/#serializing-and-deserializing","text":"Let's create some users, serialize them to a data file on disk, and then read back the file and deserialize the users objects. Creating users First, we use a Parser to read our schema definition and create a Schema object. Schema schema = new Schema . Parser (). parse ( new File ( user.avsc )); Using this schema, let's create some users. GenericRecord user1 = new GenericData . Record ( schema ); user1 . put ( name , Alyssa ); user1 . put ( favorite_number , 256 ); // Leave favorite color null GenericRecord user2 = new GenericData . Record ( schema ); user2 . put ( name , Ben ); user2 . put ( favorite_number , 7 ); user2 . put ( favorite_color , red ); We use GenericRecords to represent users. GenericRecords uses the schema to verify that we only specify valid fields. If we try to set a non-existent field (e.g., user1.put(\"favorite_animal\", \"cat\")), we'll get an AvroRuntimeException when we run the program. Note that we do not set user1's favorite color. Since that record is of type [\"string\", \"null\"], we can either set it to a string or leave it null; it is essentially optional. Serializing Now that we've created our user objects, we use generic readers and writers to serialize and deserialize them. First we'll serialize our users to a data file on disk. // Serialize user1 and user2 to disk File file = new File ( users.avro ); DatumWriter GenericRecord datumWriter = new GenericDatumWriter GenericRecord ( schema ); DataFileWriter GenericRecord dataFileWriter = new DataFileWriter GenericRecord ( datumWriter ); dataFileWriter . create ( schema , file ); dataFileWriter . append ( user1 ); dataFileWriter . append ( user2 ); dataFileWriter . close (); We create a DatumWriter , which converts Java objects into an in-memory serialized format. GenericDatumWriter requires the schema both to determine how to write the GenericRecords and to verify that all non-nullable fields are present. We also create a DataFileWriter , which writes the serialized records, as well as the schema, to the file specified in the dataFileWriter.create call. We write our users to the file via calls to the dataFileWriter.append method. When we are done writing, we close the data file. Deserializing Finally, we'll deserialize the data file we just created. // Deserialize users from disk DatumReader GenericRecord datumReader = new GenericDatumReader GenericRecord ( schema ); DataFileReader GenericRecord dataFileReader = new DataFileReader GenericRecord ( file , datumReader ); GenericRecord user = null ; while ( dataFileReader . hasNext ()) { // Reuse user object by passing it to next(). This saves us from // allocating and garbage collecting many objects for files with // many items. user = dataFileReader . next ( user ); System . out . println ( user ); } This outputs: { name : Alyssa , favorite_number : 256, favorite_color : null} { name : Ben , favorite_number : 7, favorite_color : red } Deserializing is very similar to serializing. We create a GenericDatumReader , analogous to the GenericDatumWriter we used in serialization, which converts in-memory serialized items into GenericRecords . We pass the DatumReader and the previously created File to a DataFileReader , analogous to the DataFileWriter , which reads the data file on disk. Next, we use the DataFileReader to iterate through the serialized users and print the deserialized object to stdout. Note how we perform the iteration: we create a single GenericRecord object which we store the current deserialized user in, and pass this record object to every call of dataFileReader.next . This is a performance optimization that allows the DataFileReader to reuse the same record object rather than allocating a new GenericRecord for every iteration, which can be very expensive in terms of object allocation and garbage collection if we deserialize a large data file. While this technique is the standard way to iterate through a data file, it's also possible to use for ( GenericRecord user : dataFileReader ) if performance is not a concern.","title":"Serializing and deserializing"},{"location":"hadoop/ch12/#sort-order","text":"Avro defines a sort order for objects. All types except record have preordained rules for their sort order, as described in the Avro specification, that cannot be overridden by the user. For records, however, you can control the sort order by specifying the order attribute for a field. It takes one of three values: ascending (the default), descending (to reverse the order), or ignore (so the field is skipped for comparison purposes) For example, the following schema ( SortedStringPair.avsc ) defines an ordering of StringPair records by the right field in descending order. The left field is ignored for the purposes of ordering, but it is still present in the projection: { type : record , name : StringPair , doc : A pair of strings, sorted by right field descending. , fields : [ { name : left , type : string , order : ignore }, { name : right , type : string , order : descending } ] } Avro implements efficient binary comparisons. That is to say, Avro does not have to deserialize binary data into objects to perform the comparison, because it can instead work directly on the byte streams. Avro provides the comparator for us.","title":"Sort Order"},{"location":"hadoop/ch12/#avro-mapreduce","text":"Avro provides a number of classes for making it easy to run MapReduce programs on Avro data. Let\u2019s rework the MapReduce program for finding the maximum temperature for each year in the weather dataset, this time using the Avro MapReduce API. We will represent weather records using the following schema: { type : record , name : WeatherRecord , doc : A weather reading. , fields : [ { name : year , type : int }, { name : temperature , type : int }, { name : stationId , type : string } ] } There are a couple of differences from the regular Hadoop MapReduce API. The first is the use of wrappers around Avro Java types. The second major difference from regular MapReduce is the use of AvroJob for configuring the job . AvroJob is a convenience class for specifying the Avro schemas for the input, map output, and final output data. Avro Mapreduce //vv AvroGenericMaxTemperature public class AvroGenericMaxTemperature extends Configured implements Tool { private static final Schema SCHEMA = new Schema . Parser (). parse ( { + \\ type\\ : \\ record\\ , + \\ name\\ : \\ WeatherRecord\\ , + \\ doc\\ : \\ A weather reading.\\ , + \\ fields\\ : [ + {\\ name\\ : \\ year\\ , \\ type\\ : \\ int\\ }, + {\\ name\\ : \\ temperature\\ , \\ type\\ : \\ int\\ }, + {\\ name\\ : \\ stationId\\ , \\ type\\ : \\ string\\ } + ] + } ); public static class MaxTemperatureMapper extends Mapper LongWritable , Text , AvroKey Integer , AvroValue GenericRecord { private NcdcRecordParser parser = new NcdcRecordParser (); private GenericRecord record = new GenericData . Record ( SCHEMA ); @Override protected void map ( LongWritable key , Text value , Context context ) throws IOException , InterruptedException { parser . parse ( value . toString ()); if ( parser . isValidTemperature ()) { record . put ( year , parser . getYearInt ()); record . put ( temperature , parser . getAirTemperature ()); record . put ( stationId , parser . getStationId ()); context . write ( new AvroKey Integer ( parser . getYearInt ()), new AvroValue GenericRecord ( record )); } } } public static class MaxTemperatureReducer extends Reducer AvroKey Integer , AvroValue GenericRecord , AvroKey GenericRecord , NullWritable { @Override protected void reduce ( AvroKey Integer key , Iterable AvroValue GenericRecord values , Context context ) throws IOException , InterruptedException { GenericRecord max = null ; for ( AvroValue GenericRecord value : values ) { GenericRecord record = value . datum (); if ( max == null || ( Integer ) record . get ( temperature ) ( Integer ) max . get ( temperature )) { max = newWeatherRecord ( record ); } } context . write ( new AvroKey ( max ), NullWritable . get ()); } private GenericRecord newWeatherRecord ( GenericRecord value ) { GenericRecord record = new GenericData . Record ( SCHEMA ); record . put ( year , value . get ( year )); record . put ( temperature , value . get ( temperature )); record . put ( stationId , value . get ( stationId )); return record ; } } @Override public int run ( String [] args ) throws Exception { if ( args . length != 2 ) { System . err . printf ( Usage: %s [generic options] input output \\n , getClass (). getSimpleName ()); ToolRunner . printGenericCommandUsage ( System . err ); return - 1 ; } Job job = new Job ( getConf (), Max temperature ); job . setJarByClass ( getClass ()); job . getConfiguration (). setBoolean ( Job . MAPREDUCE_JOB_USER_CLASSPATH_FIRST , true ); FileInputFormat . addInputPath ( job , new Path ( args [ 0 ])); FileOutputFormat . setOutputPath ( job , new Path ( args [ 1 ])); AvroJob . setMapOutputKeySchema ( job , Schema . create ( Schema . Type . INT )); AvroJob . setMapOutputValueSchema ( job , SCHEMA ); AvroJob . setOutputKeySchema ( job , SCHEMA ); job . setInputFormatClass ( TextInputFormat . class ); job . setOutputFormatClass ( AvroKeyOutputFormat . class ); job . setMapperClass ( MaxTemperatureMapper . class ); job . setReducerClass ( MaxTemperatureReducer . class ); return job . waitForCompletion ( true ) ? 0 : 1 ; } public static void main ( String [] args ) throws Exception { int exitCode = ToolRunner . run ( new AvroGenericMaxTemperature (), args ); System . exit ( exitCode ); } }","title":"Avro MapReduce"},{"location":"hadoop/ch12/#sorting-using-avro-mapreduce","text":"To sort an Avro datafile, it is simple. The mapper simply emits the input key wrapped in an AvroKey and an AvroValue . The reducer acts as an identity, passing the values through as output keys, which will get written to an Avro datafile. The sorting happens in the MapReduce shuffle, and the sort function is determined by the Avro schema that is passed to the program. See Chapter 7, section Shuffle and Sort for details.","title":"Sorting Using Avro MapReduce"},{"location":"hadoop/ch12/#useful-resources","text":"Apache Avro project page CDH usage page for Avro Avro Specification","title":"Useful resources"},{"location":"hadoop/ch13/","text":"Hadoop: The Definitive Guide 13 - Parquet Apache Parquet is a columnar storage format that can efficiently store nested data.","title":"Chapter 13: Parquet"},{"location":"hadoop/ch13/#hadoop-the-definitive-guide-13-parquet","text":"Apache Parquet is a columnar storage format that can efficiently store nested data.","title":"Hadoop: The Definitive Guide 13 - Parquet"},{"location":"hadoop/ch14/","text":"Hadoop: The Definitive Guide 14 - Flume Flume is designed for high-volume ingestion into Hadoop of event-based data. The canonical example is using Flume to collect logfiles from a bank of web servers, then moving the log events from those files into new aggregated files in HDFS for processing. To use Flume, we need to run a Flume agent , which is a long-lived Java process that runs source s and sink s, connected by channel s. A source in Flume produces events and delivers them to the channel, which stores the events until they are forwarded to the sink. 1. An Example To show how Flume works, let\u2019s start with a setup that: Watches a local directory for new text files Sends each line of each file to the console as files are added Flume configuration using a spooling directory source and a logger sink agent1.sources = source1 agent1.sinks = sink1 agent1.channels = channel1 agent1.sources.source1.channels = channel1 agent1.sinks.sink1.channel = channel1 agent1.sources.source1.type = spooldir agent1.sources.source1.spoolDir = /tmp/spooldir agent1.sinks.sink1.type = logger agent1.channels.channel1.type = file Here, a spooldir is a spooling directory source that monitors a spooling directory for new files; a logger sink is a sink for logging events to the console. Source and sink must be connected to channel( agent1.sources.source1.channels = channel1 ). # create the spooling directory on the local filesystem: $ mkdir /tmp/spooldir # start the Flume agent using the flume-ng command: $ flume-ng agent \\ --conf-file spool-to-logger.properties \\ --name agent1 \\ --conf $F LUME_HOME/conf \\ -Dflume.root.logger = INFO,console # on another terminal, create a file in the spooling directory. $ echo Hello Flume /tmp/spooldir/.file1.txt $ ~ mv /tmp/spooldir/.file1.txt /tmp/spooldir/file1.txt 2. Transactions and Reliability Flume uses separate transactions to guarantee delivery from the source to the channel and from the channel to the sink. In the example in the previous section, the spooling directory source creates an event for each line in the file. The source will only mark the file as completed once the transactions encapsulating the delivery of the events to the channel have been successfully committed. Similarly, a transaction is used for the delivery of the events from the channel to the sink. If for some unlikely reason the events could not be logged, the transaction would be rolled back and the events would remain in the channel for later redelivery. 3. The HDFS Sink Events may delivered to the HDFS sink and written to a file. Files in the process of being written to have a .tmp in-use suffix (default, set by hdfs.inUsePrefix , see below) added to their name to indicate that they are not yet complete. Flume configuration using a spooling directory source and an HDFS sink: agent1.sources = source1 agent1.sinks = sink1 agent1.channels = channel1 agent1.sources.source1.channels = channel1 agent1.sinks.sink1.channel = channel1 agent1.sources.source1.type = spooldir agent1.sources.source1.spoolDir = /tmp/spooldir agent1.sinks.sink1.type = hdfs agent1.sinks.sink1.hdfs.path = /tmp/flume agent1.sinks.sink1.hdfs.filePrefix = events agent1.sinks.sink1.hdfs.fileSuffix = .log agent1.sinks.sink1.hdfs.inUsePrefix = _ agent1.sinks.sink1.hdfs.fileType = DataStream agent1.channels.channel1.type = file Partitioning and Interceptors 4 Fan Out Fan out is the term for delivering events from one source to multiple channels, so they reach multiple sinks. Flume configuration using a spooling directory source, fanning out to an HDFS sink and a logger sink: agent1.sources = source1 agent1.sinks = sink1a sink1b agent1.channels = channel1a channel1b agent1.sources.source1.channels = channel1a channel1b agent1.sinks.sink1a.channel = channel1a agent1.sinks.sink1b.channel = channel1b agent1.sources.source1.type = spooldir agent1.sources.source1.spoolDir = /tmp/spooldir agent1.sinks.sink1a.type = hdfs agent1.sinks.sink1a.hdfs.path = /tmp/flume agent1.sinks.sink1a.hdfs.filePrefix = events agent1.sinks.sink1a.hdfs.fileSuffix = .log agent1.sinks.sink1a.hdfs.fileType = DataStream agent1.sinks.sink1b.type = logger agent1.channels.channel1a.type = file agent1.channels.channel1b.type = memory Delivery Guarantees Flume uses a separate transaction to deliver each batch of events from the spooling directory source to each channel. If either of these transactions fails (if a channel is full, for example), then the events will not be removed from the source, and will be retried later. 3 Distribution: Agent Tiers Aggregating Flume events is achieved by having tiers of Flume agents. The first tier collects events from the original sources (such as web servers) and sends them to a smaller set of agents in the second tier, which aggregate events from the first tier before writing them to HDFS. Further tiers may be warranted for very large numbers of source nodes. 4 Sink Groups A sink group allows multiple sinks to be treated as one, for failover(\u6545\u969c\u8f6c\u79fb) or load-balancing purposes. If a second-tier agent is unavailable, then events will be delivered to another second-tier agent and on to HDFS without disruption. 5 Useful resources Flume main page Flume user guide Flume Getting Started guide","title":"Chapter 14: Flume"},{"location":"hadoop/ch14/#hadoop-the-definitive-guide-14-flume","text":"Flume is designed for high-volume ingestion into Hadoop of event-based data. The canonical example is using Flume to collect logfiles from a bank of web servers, then moving the log events from those files into new aggregated files in HDFS for processing. To use Flume, we need to run a Flume agent , which is a long-lived Java process that runs source s and sink s, connected by channel s. A source in Flume produces events and delivers them to the channel, which stores the events until they are forwarded to the sink.","title":"Hadoop: The Definitive Guide 14 - Flume"},{"location":"hadoop/ch14/#1-an-example","text":"To show how Flume works, let\u2019s start with a setup that: Watches a local directory for new text files Sends each line of each file to the console as files are added Flume configuration using a spooling directory source and a logger sink agent1.sources = source1 agent1.sinks = sink1 agent1.channels = channel1 agent1.sources.source1.channels = channel1 agent1.sinks.sink1.channel = channel1 agent1.sources.source1.type = spooldir agent1.sources.source1.spoolDir = /tmp/spooldir agent1.sinks.sink1.type = logger agent1.channels.channel1.type = file Here, a spooldir is a spooling directory source that monitors a spooling directory for new files; a logger sink is a sink for logging events to the console. Source and sink must be connected to channel( agent1.sources.source1.channels = channel1 ). # create the spooling directory on the local filesystem: $ mkdir /tmp/spooldir # start the Flume agent using the flume-ng command: $ flume-ng agent \\ --conf-file spool-to-logger.properties \\ --name agent1 \\ --conf $F LUME_HOME/conf \\ -Dflume.root.logger = INFO,console # on another terminal, create a file in the spooling directory. $ echo Hello Flume /tmp/spooldir/.file1.txt $ ~ mv /tmp/spooldir/.file1.txt /tmp/spooldir/file1.txt","title":"1. An Example"},{"location":"hadoop/ch14/#2-transactions-and-reliability","text":"Flume uses separate transactions to guarantee delivery from the source to the channel and from the channel to the sink. In the example in the previous section, the spooling directory source creates an event for each line in the file. The source will only mark the file as completed once the transactions encapsulating the delivery of the events to the channel have been successfully committed. Similarly, a transaction is used for the delivery of the events from the channel to the sink. If for some unlikely reason the events could not be logged, the transaction would be rolled back and the events would remain in the channel for later redelivery.","title":"2. Transactions and Reliability"},{"location":"hadoop/ch14/#3-the-hdfs-sink","text":"Events may delivered to the HDFS sink and written to a file. Files in the process of being written to have a .tmp in-use suffix (default, set by hdfs.inUsePrefix , see below) added to their name to indicate that they are not yet complete. Flume configuration using a spooling directory source and an HDFS sink: agent1.sources = source1 agent1.sinks = sink1 agent1.channels = channel1 agent1.sources.source1.channels = channel1 agent1.sinks.sink1.channel = channel1 agent1.sources.source1.type = spooldir agent1.sources.source1.spoolDir = /tmp/spooldir agent1.sinks.sink1.type = hdfs agent1.sinks.sink1.hdfs.path = /tmp/flume agent1.sinks.sink1.hdfs.filePrefix = events agent1.sinks.sink1.hdfs.fileSuffix = .log agent1.sinks.sink1.hdfs.inUsePrefix = _ agent1.sinks.sink1.hdfs.fileType = DataStream agent1.channels.channel1.type = file","title":"3. The HDFS Sink"},{"location":"hadoop/ch14/#partitioning-and-interceptors","text":"","title":"Partitioning and Interceptors"},{"location":"hadoop/ch14/#4-fan-out","text":"Fan out is the term for delivering events from one source to multiple channels, so they reach multiple sinks. Flume configuration using a spooling directory source, fanning out to an HDFS sink and a logger sink: agent1.sources = source1 agent1.sinks = sink1a sink1b agent1.channels = channel1a channel1b agent1.sources.source1.channels = channel1a channel1b agent1.sinks.sink1a.channel = channel1a agent1.sinks.sink1b.channel = channel1b agent1.sources.source1.type = spooldir agent1.sources.source1.spoolDir = /tmp/spooldir agent1.sinks.sink1a.type = hdfs agent1.sinks.sink1a.hdfs.path = /tmp/flume agent1.sinks.sink1a.hdfs.filePrefix = events agent1.sinks.sink1a.hdfs.fileSuffix = .log agent1.sinks.sink1a.hdfs.fileType = DataStream agent1.sinks.sink1b.type = logger agent1.channels.channel1a.type = file agent1.channels.channel1b.type = memory","title":"4 Fan Out"},{"location":"hadoop/ch14/#delivery-guarantees","text":"Flume uses a separate transaction to deliver each batch of events from the spooling directory source to each channel. If either of these transactions fails (if a channel is full, for example), then the events will not be removed from the source, and will be retried later.","title":"Delivery Guarantees"},{"location":"hadoop/ch14/#3-distribution-agent-tiers","text":"Aggregating Flume events is achieved by having tiers of Flume agents. The first tier collects events from the original sources (such as web servers) and sends them to a smaller set of agents in the second tier, which aggregate events from the first tier before writing them to HDFS. Further tiers may be warranted for very large numbers of source nodes.","title":"3 Distribution: Agent Tiers"},{"location":"hadoop/ch14/#4-sink-groups","text":"A sink group allows multiple sinks to be treated as one, for failover(\u6545\u969c\u8f6c\u79fb) or load-balancing purposes. If a second-tier agent is unavailable, then events will be delivered to another second-tier agent and on to HDFS without disruption.","title":"4 Sink Groups"},{"location":"hadoop/ch14/#5-useful-resources","text":"Flume main page Flume user guide Flume Getting Started guide","title":"5 Useful resources"},{"location":"hadoop/ch15/","text":"Hadoop: The Definitive Guide 15 - Sqoop","title":"Chapter 15: Sqoop"},{"location":"hadoop/ch15/#hadoop-the-definitive-guide-15-sqoop","text":"","title":"Hadoop: The Definitive Guide 15 - Sqoop"},{"location":"hadoop/ch16/","text":"Hadoop: The Definitive Guide 16 - Pig","title":"Chapter 16: Pig"},{"location":"hadoop/ch16/#hadoop-the-definitive-guide-16-pig","text":"","title":"Hadoop: The Definitive Guide 16 - Pig"},{"location":"hadoop/ch17/","text":"Hadoop: The Definitive Guide 17 - Hive Hive is a data warehousing system to store structured data on Hadoop file system. It provide an easy query these data by execution Hadoop MapReduce plans 1 . 2 Running Hive Hive Services Hive clients Hive project page Getting Started","title":"Chapter 17: Hive"},{"location":"hadoop/ch17/#hadoop-the-definitive-guide-17-hive","text":"Hive is a data warehousing system to store structured data on Hadoop file system. It provide an easy query these data by execution Hadoop MapReduce plans 1 .","title":"Hadoop: The Definitive Guide 17 - Hive"},{"location":"hadoop/ch17/#2-running-hive","text":"","title":"2 Running Hive"},{"location":"hadoop/ch17/#hive-services","text":"Hive clients Hive project page Getting Started","title":"Hive Services"},{"location":"hadoop/ch18/","text":"Hadoop: The Definitive Guide 18 - Crunch","title":"Chapter 18: Crunch"},{"location":"hadoop/ch18/#hadoop-the-definitive-guide-18-crunch","text":"","title":"Hadoop: The Definitive Guide 18 - Crunch"},{"location":"hadoop/ch19/","text":"Hadoop: The Definitive Guide 19 - Spark","title":"Chapter 19: Spark"},{"location":"hadoop/ch19/#hadoop-the-definitive-guide-19-spark","text":"","title":"Hadoop: The Definitive Guide 19 - Spark"},{"location":"hadoop/ch2/","text":"Hadoop: The Definitive Guide 2 - MapReduce MapReduce is a programming model for data processing. MapReduce programs are inherently parallel, thus putting very large-scale data analysis into the hands of anyone with enough machines at their disposal. 1 A Weather Dataset For our example, we will write a program that mines weather data. The data we will use is from the National Climatic Data Center. It is stored using a line-oriented ASCII format, in which each line is a record. Analyzing the Data with Hadoop MapReduce works by breaking the processing into two phases: the map phase and the reduce phase. Each phase has key-value pairs as input and output, the types of which may be chosen by the programmer. The programmer also specifies two functions: the map function and the reduce function. MAPINPUT: Key: the offset of the beginning of the line from the beginning of the file. (no need here, just ignore it) Value: raw NCDC data (0, 0067011990999991950051507004\u20269999999N9+00001+99999999999\u2026) (106, 0043011990999991950051512004\u20269999999N9+00221+99999999999\u2026) (212, 0043011990999991950051518004\u20269999999N9-00111+99999999999\u2026) (318, 0043012650999991949032412004\u20260500001N9+01111+99999999999\u2026) (424, 0043012650999991949032418004\u20260500001N9+00781+99999999999\u2026) MAPOUTPUT: The map function merely extracts the year and the air temperature , and emits them as output. Key: year Calue: air temperature (1950, 0) (1950, 22) (1950, \u221211) (1949, 111) (1949, 78) The output from the map function is processed by the MapReduce framework before being sent to the reduce function. This processing sorts and groups the key-value pairs by key. (1949, [111, 78]) (1950, [0, 22, \u221211]) All the reduce function has to do now is iterate through the list and pick up the maximum reading: (1949, 111) (1950, 22) Java MapReduce We need three things: a map function, a reduce function, and some code to run the job. Map The map function is represented by the Mapper class, which declares an abstract map() method. The Mapper class is a generic type, with four formal type parameters that specify the input key, input value, output key, and output value types of the map function. public class Mapper KEYIN , VALUEIN , KEYOUT , VALUEOUT {} Rather than using built-in Java types, Hadoop provides its own set of basic types that are optimized for network serialization. These are found in the org.apache.hadoop.io package. Here we use LongWritable , which corresponds to a Java Long , Text (like Java String ), and IntWritable (like Java Integer ). import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.LongWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Mapper ; import java.io.IOException ; public class MaxTemperatureMapper extends Mapper LongWritable , Text , Text , IntWritable { private static final int MISSING = 9999 ; @Override protected void map ( LongWritable key , Text value , Context context ) throws IOException , InterruptedException { String line = value . toString (); String year = line . substring ( 15 , 19 ); int airTemperature ; if ( line . charAt ( 87 ) == + ){ //parseInt doesn t like leading plus signs airTemperature = Integer . parseInt ( line . substring ( 88 , 92 )); } else { airTemperature = Integer . parseInt ( line . substring ( 87 , 92 )); } String quality = line . substring ( 92 , 93 ); if ( airTemperature != MISSING quality . matches ( [01459]] )) { context . write ( new Text ( year ), new IntWritable ( airTemperature )); } } } Reduce The reduce function is similarly defined using a Reducer . import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Reducer ; import java.io.IOException ; public class MaxTemperatureReducer extends Reducer Text , IntWritable , Text , IntWritable { @Override protected void reduce ( Text key , Iterable IntWritable values , Context context ) throws IOException , InterruptedException { int maxValue = Integer . MIN_VALUE ; for ( IntWritable value : values ) { maxValue = Math . max ( maxValue , value . get ()); } context . write ( key , new IntWritable ( maxValue )); } } MapReduce Job The third piece of code runs the MapReduce job import org.apache.hadoop.fs.Path ; import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Job ; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat ; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat ; public class MaxTemperature { public static void main ( String [] args ) throws Exception { if ( args . length != 2 ) { System . err . println ( Usage: MaxTempeature intput path output path ); System . exit (- 1 ); } Job job = new Job (); job . setJarByClass ( MaxTemperature . class ); job . setJobName ( Max Temperature ); FileInputFormat . addInputPath ( job , new Path ( args [ 0 ])); FileOutputFormat . setOutputPath ( job , new Path ( args [ 1 ])); job . setMapperClass ( MaxTemperatureMapper . class ); job . setReducerClass ( MaxTemperatureReducer . class ); job . setOutputKeyClass ( Text . class ); job . setOutputValueClass ( IntWritable . class ); System . exit ( job . waitForCompletion ( true ) ? 0 : 1 ); } } A Job object forms the specification of the job and gives you control over how the job is run. Rather than explicitly specifying the name of the JAR file, we can pass a class in the Job\u2019s setJarByClass() method, which Hadoop will use to locate the relevant JAR file by looking for the JAR file containing this class. A test run $ export HADOOP_CLASSPATH = /Users/larry/JavaProject/out/artifacts/MaxTemperature/MaxTemperature.jar $ hadoop com.definitivehadoop.weatherdata.MaxTemperature resources/HadoopBook/ncdc/sample.txt output When the hadoop command is invoked with a classname as the first argument, it launches a Java virtual machine (JVM) to run the class. The hadoop command adds the Hadoop libraries (and their dependencies) to the classpath and picks up the Hadoop configuration, too. To add the application classes to the classpath, we\u2019ve defined an environment variable called HADOOP_CLASSPATH , which the hadoop script picks up. //OUTPUT 18:20:18,944 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=148485300 FILE: Number of bytes written=150614384 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 Map-Reduce Framework Map input records=5 Map output records=0 Map output bytes=0 Map output materialized bytes=6 Input split bytes=131 Combine input records=0 Combine output records=0 Reduce input groups=0 Reduce shuffle bytes=6 Reduce input records=0 Reduce output records=0 Spilled Records=0 Shuffled Maps =1 Failed Shuffles=0 Merged Map outputs=1 GC time elapsed (ms)=5 Total committed heap usage (bytes)=406847488 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=529 File Output Format Counters Bytes Written=8 The last section of the output, titled \u201cCounters,\u201d shows the statistics that Hadoop generates for each job it runs. These are very useful for checking whether the amount of data processed is what you expected. 2 Scaling Out To scale out, we need to store the data in a distributed filesystem (typically HDFS). This allows Hadoop to move the MapReduce computation to each machine hosting a part of the data, using Hadoop\u2019s resource management system, YARN. Data Flow A MapReduce job (\u4f5c\u4e1a) is a unit of work that the client wants to be performed: it consists of the input data, the MapReduce program, and configuration information. Hadoop runs the job by dividing it into tasks (\u4efb\u52a1), of which there are two types: map tasks and reduce tasks . The tasks are scheduled using YARN and run on nodes in the cluster. Hadoop divides the input to a MapReduce job into fixed-size pieces called input splits (\u8f93\u5165\u5206\u7247), or just splits (\u5206\u7247). Hadoop creates one map task for each split, which runs the user-defined map function for each record in the split. So if we are processing the splits in parallel, the processing is better load balanced when the splits are small. On the other hand, if splits are too small, the overhead of managing the splits and map task creation begins to dominate the total job execution time. Hadoop does its best to run the map task on a node where the input data resides in HDFS, because it doesn\u2019t use valuable cluster bandwidth. This is called the data locality optimization (\u6570\u636e\u672c\u5730\u4f18\u5316). Map tasks write their output to the local disk, not to HDFS. Why is this? Map output is intermediate output: it\u2019s processed by reduce tasks to produce the final output, and once the job is complete, the map output can be thrown away. So, storing it in HDFS with replication would be overkill. The data flow for the general case of multiple reduce tasks is illustrated in figure below. This diagram makes it clear why the data flow between map and reduce tasks is colloquially known as \u201cthe shuffle,\u201d as each reduce task is fed by many map tasks. Combiner Functions Many MapReduce jobs are limited by the bandwidth available on the cluster, so it pays to minimize the data transferred between map and reduce tasks. Hadoop allows the user to specify a combiner function to be run on the map output, and the combiner function\u2019s output forms the input to the reduce function. For max temperature problem described above, the combiner function is the same implementation as the reduce function in MaxTemperatureReducer . The only change we need to make is to set the combiner class on the Job. job . setMapperClass ( MaxTemperatureMapper . class ); job . setCombinerClass ( MaxTemperatureReducer . class ); job . setReducerClass ( MaxTemperatureReducer . class ); A part of output information for running MaxTemperatureReducer is: Map input records=5 Map output records=5 Map output bytes=45 Map output materialized bytes=28 Input split bytes=131 Combine input records=5 Combine output records=2 Reduce input groups=2 Reduce shuffle bytes=28 Reduce input records=2 Reduce output records=2 3 Hadoop Streaming Hadoop provides an API to MapReduce that allows you to write your map and reduce functions in languages other than Java. Hadoop Streaming uses Unix standard streams (Unix\u6807\u51c6\u6d41) as the interface between Hadoop and your program, so you can use any language that can read standard input and write to standard output to write your MapReduce program. Streaming is naturally suited for text processing. Map input data is passed over standard input to map function. A map output key-value pair is written as a single tab-delimited line. The reduce function reads lines from standard input, which the framework guarantees are sorted by key, and writes its results to standard output. Here, We take Python as an example. Python Map #!/Users/larry/anaconda3/bin/python import re import sys for line in sys . stdin : val = line . strip () ( year , temp , q ) = ( val [ 15 : 19 ], val [ 87 : 92 ], val [ 92 : 93 ]) if ( temp != +9999 and re . match ( [01459] , q )): print ( %s \\t %s % ( year , temp )) Reduce #!/Users/larry/anaconda3/bin/python import sys ( last_key , max_val ) = ( None , - sys . maxsize ) for line in sys . stdin : ( key , val ) = line . strip () . split ( \\t ) if last_key and last_key != key : print ( %s \\t %s % ( last_key , max_val )) ( last_key , max_val ) = ( key , int ( val )) else : ( last_key , max_val ) = ( key , max ( max_val , int ( val ))) if last_key : print ( %s \\t %s % ( last_key , max_val )) For example, to run a test: $ cat sample.txt | ./max_temperature_map.py | sort | ./max_temperature_reduce.py 1949 111 1950 22 The hadoop command doesn\u2019t support a Streaming option; instead, you specify the Streaming JAR file along with the jar option. Options to the Streaming program specify the input and output paths and the map and reduce scripts. $hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.0.jar \\ -input sample.txt \\ -output output \\ -mapper max_temperature_map.py \\ -reducer max_temperature_reduce.py","title":"Chapter 2: MapReduce"},{"location":"hadoop/ch2/#hadoop-the-definitive-guide-2-mapreduce","text":"MapReduce is a programming model for data processing. MapReduce programs are inherently parallel, thus putting very large-scale data analysis into the hands of anyone with enough machines at their disposal.","title":"Hadoop: The Definitive Guide 2 - MapReduce"},{"location":"hadoop/ch2/#1-a-weather-dataset","text":"For our example, we will write a program that mines weather data. The data we will use is from the National Climatic Data Center. It is stored using a line-oriented ASCII format, in which each line is a record.","title":"1 A Weather Dataset"},{"location":"hadoop/ch2/#analyzing-the-data-with-hadoop","text":"MapReduce works by breaking the processing into two phases: the map phase and the reduce phase. Each phase has key-value pairs as input and output, the types of which may be chosen by the programmer. The programmer also specifies two functions: the map function and the reduce function. MAPINPUT: Key: the offset of the beginning of the line from the beginning of the file. (no need here, just ignore it) Value: raw NCDC data (0, 0067011990999991950051507004\u20269999999N9+00001+99999999999\u2026) (106, 0043011990999991950051512004\u20269999999N9+00221+99999999999\u2026) (212, 0043011990999991950051518004\u20269999999N9-00111+99999999999\u2026) (318, 0043012650999991949032412004\u20260500001N9+01111+99999999999\u2026) (424, 0043012650999991949032418004\u20260500001N9+00781+99999999999\u2026) MAPOUTPUT: The map function merely extracts the year and the air temperature , and emits them as output. Key: year Calue: air temperature (1950, 0) (1950, 22) (1950, \u221211) (1949, 111) (1949, 78) The output from the map function is processed by the MapReduce framework before being sent to the reduce function. This processing sorts and groups the key-value pairs by key. (1949, [111, 78]) (1950, [0, 22, \u221211]) All the reduce function has to do now is iterate through the list and pick up the maximum reading: (1949, 111) (1950, 22)","title":"Analyzing the Data with Hadoop"},{"location":"hadoop/ch2/#java-mapreduce","text":"We need three things: a map function, a reduce function, and some code to run the job. Map The map function is represented by the Mapper class, which declares an abstract map() method. The Mapper class is a generic type, with four formal type parameters that specify the input key, input value, output key, and output value types of the map function. public class Mapper KEYIN , VALUEIN , KEYOUT , VALUEOUT {} Rather than using built-in Java types, Hadoop provides its own set of basic types that are optimized for network serialization. These are found in the org.apache.hadoop.io package. Here we use LongWritable , which corresponds to a Java Long , Text (like Java String ), and IntWritable (like Java Integer ). import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.LongWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Mapper ; import java.io.IOException ; public class MaxTemperatureMapper extends Mapper LongWritable , Text , Text , IntWritable { private static final int MISSING = 9999 ; @Override protected void map ( LongWritable key , Text value , Context context ) throws IOException , InterruptedException { String line = value . toString (); String year = line . substring ( 15 , 19 ); int airTemperature ; if ( line . charAt ( 87 ) == + ){ //parseInt doesn t like leading plus signs airTemperature = Integer . parseInt ( line . substring ( 88 , 92 )); } else { airTemperature = Integer . parseInt ( line . substring ( 87 , 92 )); } String quality = line . substring ( 92 , 93 ); if ( airTemperature != MISSING quality . matches ( [01459]] )) { context . write ( new Text ( year ), new IntWritable ( airTemperature )); } } } Reduce The reduce function is similarly defined using a Reducer . import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Reducer ; import java.io.IOException ; public class MaxTemperatureReducer extends Reducer Text , IntWritable , Text , IntWritable { @Override protected void reduce ( Text key , Iterable IntWritable values , Context context ) throws IOException , InterruptedException { int maxValue = Integer . MIN_VALUE ; for ( IntWritable value : values ) { maxValue = Math . max ( maxValue , value . get ()); } context . write ( key , new IntWritable ( maxValue )); } } MapReduce Job The third piece of code runs the MapReduce job import org.apache.hadoop.fs.Path ; import org.apache.hadoop.io.IntWritable ; import org.apache.hadoop.io.Text ; import org.apache.hadoop.mapreduce.Job ; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat ; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat ; public class MaxTemperature { public static void main ( String [] args ) throws Exception { if ( args . length != 2 ) { System . err . println ( Usage: MaxTempeature intput path output path ); System . exit (- 1 ); } Job job = new Job (); job . setJarByClass ( MaxTemperature . class ); job . setJobName ( Max Temperature ); FileInputFormat . addInputPath ( job , new Path ( args [ 0 ])); FileOutputFormat . setOutputPath ( job , new Path ( args [ 1 ])); job . setMapperClass ( MaxTemperatureMapper . class ); job . setReducerClass ( MaxTemperatureReducer . class ); job . setOutputKeyClass ( Text . class ); job . setOutputValueClass ( IntWritable . class ); System . exit ( job . waitForCompletion ( true ) ? 0 : 1 ); } } A Job object forms the specification of the job and gives you control over how the job is run. Rather than explicitly specifying the name of the JAR file, we can pass a class in the Job\u2019s setJarByClass() method, which Hadoop will use to locate the relevant JAR file by looking for the JAR file containing this class. A test run $ export HADOOP_CLASSPATH = /Users/larry/JavaProject/out/artifacts/MaxTemperature/MaxTemperature.jar $ hadoop com.definitivehadoop.weatherdata.MaxTemperature resources/HadoopBook/ncdc/sample.txt output When the hadoop command is invoked with a classname as the first argument, it launches a Java virtual machine (JVM) to run the class. The hadoop command adds the Hadoop libraries (and their dependencies) to the classpath and picks up the Hadoop configuration, too. To add the application classes to the classpath, we\u2019ve defined an environment variable called HADOOP_CLASSPATH , which the hadoop script picks up. //OUTPUT 18:20:18,944 INFO mapreduce.Job: Counters: 30 File System Counters FILE: Number of bytes read=148485300 FILE: Number of bytes written=150614384 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 Map-Reduce Framework Map input records=5 Map output records=0 Map output bytes=0 Map output materialized bytes=6 Input split bytes=131 Combine input records=0 Combine output records=0 Reduce input groups=0 Reduce shuffle bytes=6 Reduce input records=0 Reduce output records=0 Spilled Records=0 Shuffled Maps =1 Failed Shuffles=0 Merged Map outputs=1 GC time elapsed (ms)=5 Total committed heap usage (bytes)=406847488 Shuffle Errors BAD_ID=0 CONNECTION=0 IO_ERROR=0 WRONG_LENGTH=0 WRONG_MAP=0 WRONG_REDUCE=0 File Input Format Counters Bytes Read=529 File Output Format Counters Bytes Written=8 The last section of the output, titled \u201cCounters,\u201d shows the statistics that Hadoop generates for each job it runs. These are very useful for checking whether the amount of data processed is what you expected.","title":"Java MapReduce"},{"location":"hadoop/ch2/#2-scaling-out","text":"To scale out, we need to store the data in a distributed filesystem (typically HDFS). This allows Hadoop to move the MapReduce computation to each machine hosting a part of the data, using Hadoop\u2019s resource management system, YARN.","title":"2 Scaling Out"},{"location":"hadoop/ch2/#data-flow","text":"A MapReduce job (\u4f5c\u4e1a) is a unit of work that the client wants to be performed: it consists of the input data, the MapReduce program, and configuration information. Hadoop runs the job by dividing it into tasks (\u4efb\u52a1), of which there are two types: map tasks and reduce tasks . The tasks are scheduled using YARN and run on nodes in the cluster. Hadoop divides the input to a MapReduce job into fixed-size pieces called input splits (\u8f93\u5165\u5206\u7247), or just splits (\u5206\u7247). Hadoop creates one map task for each split, which runs the user-defined map function for each record in the split. So if we are processing the splits in parallel, the processing is better load balanced when the splits are small. On the other hand, if splits are too small, the overhead of managing the splits and map task creation begins to dominate the total job execution time. Hadoop does its best to run the map task on a node where the input data resides in HDFS, because it doesn\u2019t use valuable cluster bandwidth. This is called the data locality optimization (\u6570\u636e\u672c\u5730\u4f18\u5316). Map tasks write their output to the local disk, not to HDFS. Why is this? Map output is intermediate output: it\u2019s processed by reduce tasks to produce the final output, and once the job is complete, the map output can be thrown away. So, storing it in HDFS with replication would be overkill. The data flow for the general case of multiple reduce tasks is illustrated in figure below. This diagram makes it clear why the data flow between map and reduce tasks is colloquially known as \u201cthe shuffle,\u201d as each reduce task is fed by many map tasks.","title":"Data Flow"},{"location":"hadoop/ch2/#combiner-functions","text":"Many MapReduce jobs are limited by the bandwidth available on the cluster, so it pays to minimize the data transferred between map and reduce tasks. Hadoop allows the user to specify a combiner function to be run on the map output, and the combiner function\u2019s output forms the input to the reduce function. For max temperature problem described above, the combiner function is the same implementation as the reduce function in MaxTemperatureReducer . The only change we need to make is to set the combiner class on the Job. job . setMapperClass ( MaxTemperatureMapper . class ); job . setCombinerClass ( MaxTemperatureReducer . class ); job . setReducerClass ( MaxTemperatureReducer . class ); A part of output information for running MaxTemperatureReducer is: Map input records=5 Map output records=5 Map output bytes=45 Map output materialized bytes=28 Input split bytes=131 Combine input records=5 Combine output records=2 Reduce input groups=2 Reduce shuffle bytes=28 Reduce input records=2 Reduce output records=2","title":"Combiner Functions"},{"location":"hadoop/ch2/#3-hadoop-streaming","text":"Hadoop provides an API to MapReduce that allows you to write your map and reduce functions in languages other than Java. Hadoop Streaming uses Unix standard streams (Unix\u6807\u51c6\u6d41) as the interface between Hadoop and your program, so you can use any language that can read standard input and write to standard output to write your MapReduce program. Streaming is naturally suited for text processing. Map input data is passed over standard input to map function. A map output key-value pair is written as a single tab-delimited line. The reduce function reads lines from standard input, which the framework guarantees are sorted by key, and writes its results to standard output. Here, We take Python as an example.","title":"3 Hadoop Streaming"},{"location":"hadoop/ch2/#python","text":"Map #!/Users/larry/anaconda3/bin/python import re import sys for line in sys . stdin : val = line . strip () ( year , temp , q ) = ( val [ 15 : 19 ], val [ 87 : 92 ], val [ 92 : 93 ]) if ( temp != +9999 and re . match ( [01459] , q )): print ( %s \\t %s % ( year , temp )) Reduce #!/Users/larry/anaconda3/bin/python import sys ( last_key , max_val ) = ( None , - sys . maxsize ) for line in sys . stdin : ( key , val ) = line . strip () . split ( \\t ) if last_key and last_key != key : print ( %s \\t %s % ( last_key , max_val )) ( last_key , max_val ) = ( key , int ( val )) else : ( last_key , max_val ) = ( key , max ( max_val , int ( val ))) if last_key : print ( %s \\t %s % ( last_key , max_val )) For example, to run a test: $ cat sample.txt | ./max_temperature_map.py | sort | ./max_temperature_reduce.py 1949 111 1950 22 The hadoop command doesn\u2019t support a Streaming option; instead, you specify the Streaming JAR file along with the jar option. Options to the Streaming program specify the input and output paths and the map and reduce scripts. $hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.0.jar \\ -input sample.txt \\ -output output \\ -mapper max_temperature_map.py \\ -reducer max_temperature_reduce.py","title":"Python"},{"location":"hadoop/ch20/","text":"Hadoop: The Definitive Guide 20 - HBase Use Apache HBase\u2122 when you need random , realtime read/write access to your Big Data. It's goal is the hosting of very large tables -- billions of rows X millions of columns -- atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google's Bigtable. [ Apache HBase ] 0 Getting Started From Apache HBase \u2122 Reference Guide The bin/start-hbase.sh script is provided as a convenient way to start HBase. Before start, make sure to start Hadoop hdfs, yarn, and zookeeper. start-all.sh #hdfs, yarn zkServer.sh start # zookeeper 1 Concepts Whirlwind Tour of the Data Model Applications store data in labeled tables. Tables are made of rows and columns. Table cells \u2014 the intersection of row and column coordinates \u2014 are versioned. By default, their version is a timestamp auto-assigned by HBase at the time of cell insertion. A cell\u2019s content is an uninterpreted array of bytes. Table row keys are also byte arrays, so theoretically anything can serve as a row key, from strings to binary representations of long or even serialized data structures. Table rows are sorted by row key, aka the table\u2019s primary key. The sort is byte-ordered. All table accesses are via the primary key. Row columns are grouped into column families . All column family members have a common prefix, so, for example, the columns info:format and info:geo are both members of the info column family, whereas contents:image belongs to the contents family. In synopsis, HBase tables are like those in an RDBMS, only cells are versioned, rows are sorted, and columns can be added on the fly by the client as long as the column family they belong to preexists. Regions Tables are automatically partitioned horizontally by HBase into regions . Each region comprises a subset of a table\u2019s rows. A region is denoted by the table it belongs to, its first row (inclusive), and its last row (exclusive). Initially, a table comprises a single region, but as the region grows it eventually crosses a configurable size threshold, at which point it splits at a row boundary into two new regions of approximately equal size. Until this first split happens, all loading will be against the single server hosting the original region. As the table grows, the number of its regions grows. Regions are the units that get distributed over an HBase cluster. Locking Row updates are atomic, no matter how many row columns constitute the row-level transaction. This keeps the locking model simple. Implementation HBase made up of an HBase master node orchestrating a cluster of one or more regionserver workers. The HBase master is responsible for bootstrapping a virgin install, for assigning regions to registered regionservers, and for recovering regionserver failures. The regionservers carry zero or more regions and field client read/write requests. Clients Java","title":"Chapter 20: HBase"},{"location":"hadoop/ch20/#hadoop-the-definitive-guide-20-hbase","text":"Use Apache HBase\u2122 when you need random , realtime read/write access to your Big Data. It's goal is the hosting of very large tables -- billions of rows X millions of columns -- atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google's Bigtable. [ Apache HBase ]","title":"Hadoop: The Definitive Guide 20 - HBase"},{"location":"hadoop/ch20/#0-getting-started","text":"From Apache HBase \u2122 Reference Guide The bin/start-hbase.sh script is provided as a convenient way to start HBase. Before start, make sure to start Hadoop hdfs, yarn, and zookeeper. start-all.sh #hdfs, yarn zkServer.sh start # zookeeper","title":"0 Getting Started"},{"location":"hadoop/ch20/#1-concepts","text":"","title":"1 Concepts"},{"location":"hadoop/ch20/#whirlwind-tour-of-the-data-model","text":"Applications store data in labeled tables. Tables are made of rows and columns. Table cells \u2014 the intersection of row and column coordinates \u2014 are versioned. By default, their version is a timestamp auto-assigned by HBase at the time of cell insertion. A cell\u2019s content is an uninterpreted array of bytes. Table row keys are also byte arrays, so theoretically anything can serve as a row key, from strings to binary representations of long or even serialized data structures. Table rows are sorted by row key, aka the table\u2019s primary key. The sort is byte-ordered. All table accesses are via the primary key. Row columns are grouped into column families . All column family members have a common prefix, so, for example, the columns info:format and info:geo are both members of the info column family, whereas contents:image belongs to the contents family. In synopsis, HBase tables are like those in an RDBMS, only cells are versioned, rows are sorted, and columns can be added on the fly by the client as long as the column family they belong to preexists. Regions Tables are automatically partitioned horizontally by HBase into regions . Each region comprises a subset of a table\u2019s rows. A region is denoted by the table it belongs to, its first row (inclusive), and its last row (exclusive). Initially, a table comprises a single region, but as the region grows it eventually crosses a configurable size threshold, at which point it splits at a row boundary into two new regions of approximately equal size. Until this first split happens, all loading will be against the single server hosting the original region. As the table grows, the number of its regions grows. Regions are the units that get distributed over an HBase cluster. Locking Row updates are atomic, no matter how many row columns constitute the row-level transaction. This keeps the locking model simple.","title":"Whirlwind Tour of the Data Model"},{"location":"hadoop/ch20/#implementation","text":"HBase made up of an HBase master node orchestrating a cluster of one or more regionserver workers. The HBase master is responsible for bootstrapping a virgin install, for assigning regions to registered regionservers, and for recovering regionserver failures. The regionservers carry zero or more regions and field client read/write requests.","title":"Implementation"},{"location":"hadoop/ch20/#clients","text":"","title":"Clients"},{"location":"hadoop/ch20/#java","text":"","title":"Java"},{"location":"hadoop/ch21/","text":"Hadoop: The Definitive Guide 21 - ZooKeeper Resources Apache ZooKeeper ZooKeeper Wiki ZooKeeper Getting Started Guide","title":"Chapter 21: ZooKeeper"},{"location":"hadoop/ch21/#hadoop-the-definitive-guide-21-zookeeper","text":"","title":"Hadoop: The Definitive Guide 21 - ZooKeeper"},{"location":"hadoop/ch21/#resources","text":"Apache ZooKeeper ZooKeeper Wiki ZooKeeper Getting Started Guide","title":"Resources"},{"location":"hadoop/ch22/","text":"Hadoop: The Definitive Guide 22 - Composable Data at Center","title":"Chapter 22: Composable Data at Center"},{"location":"hadoop/ch22/#hadoop-the-definitive-guide-22-composable-data-at-center","text":"","title":"Hadoop: The Definitive Guide 22 - Composable Data at Center"},{"location":"hadoop/ch23/","text":"Hadoop: The Definitive Guide 23 - Biological Data Science: Saving Lives with Software","title":"Chapter 23: Biological Data Science: Saving Lives with Software"},{"location":"hadoop/ch23/#hadoop-the-definitive-guide-23-biological-data-science-saving-lives-with-software","text":"","title":"Hadoop: The Definitive Guide 23 - Biological Data Science: Saving Lives with Software"},{"location":"hadoop/ch24/","text":"Hadoop: The Definitive Guide 24 - Cascading","title":"Chapter 24: Cascading"},{"location":"hadoop/ch24/#hadoop-the-definitive-guide-24-cascading","text":"","title":"Hadoop: The Definitive Guide 24 - Cascading"},{"location":"hadoop/ch3/","text":"Hadoop: The Definitive Guide 3 - The Hadoop Distributed FileSystem Filesystems that manage the storage across a network of machines are called distributed filesystems . Hadoop comes with a distributed filesystem called HDFS, which stands for Hadoop Distributed Filesystem . 1 The Design of HDFS HDFS is a filesystem designed for storing very large files with streaming data access patterns, running on clusters of commodity hardware. Very large files: files that are hundreds of megabytes, gigabytes, or terabytes in size. Streaming data access: HDFS is built around the idea that the most efficient data processing pattern is a write-once, read-many-times pattern. Commodity hardware: It\u2019s designed to run on clusters of commodity hardware. These are areas where HDFS is not a good fit today: Low-latency data access Lots of small files Multiple writers, arbitrary file modifications 2 HDFS Concepts Blocks A disk has a block size, which is the minimum amount of data that it can read or write. Filesystems for a single disk build on this by dealing with data in blocks, which are an integral multiple of the disk block size. HDFS, too, has the concept of a block , but it is a much larger unit \u2014 128 MB by default (typically a few kilobytes for ordinary file system). Unlike a filesystem for a single disk, a file in HDFS that is smaller than a single block does not occupy a full block\u2019s worth of underlying storage. (For example, a 1 MB file stored with a block size of 128 MB uses 1 MB of disk space, not 128 MB.) Question WHY IS A BLOCK IN HDFS SO LARGE? To minimize the cost of seeks. Having a block abstraction for a distributed filesystem brings several benefits. A file can be larger than any single disk in the network. Making the unit of abstraction a block rather than a file simplifies the storage subsystem. storage management: because blocks are a fixed size, it is easy to calculate how many can be stored on a given disk. metadata concerns: because blocks are just chunks of data to be stored, file metadata such as permissions information does not need to be stored with the blocks. Blocks fit well with replication for providing fault tolerance and availability. To insure against corrupted blocks and disk and machine failure, each block is replicated to a small number of physically separate machines (typically three). Namenodes and Datanodes An HDFS cluster has two types of nodes: a namenode (the master) and a number of datanodes (workers). The namenode manages the filesystem namespace. It maintains the filesystem tree and the metadata for all the files and directories in the tree. This information is stored persistently on the local disk in the form of two files: the namespace image and the edit log. The namenode also knows the datanodes on which all the blocks for a given file are located; Datanodes are the workhorses of the filesystem. They store and retrieve blocks when they are told to (by clients or the namenode), and they report back to the namenode periodically with lists of blocks that they are storing. If the machine running the namenode were obliterated, all the files on the filesystem would be lost since there would be no way of knowing how to reconstruct the files from the blocks on the datanodes. Possible solution: to back up the files that make up the persistent state of the filesystem metadata. to run a secondary namenode, which keeps a copy of the merged namespace image. Block Caching For frequently accessed files, the blocks may be explicitly cached in the datanode\u2019s memory, in an off-heap block cache. Users or applications instruct the namenode which files to cache (and for how long) by adding a cache directive to a cache pool . HDFS Federation Problem: On very large clusters with many files, memory becomes the limiting factor for scaling, since namenode keeps a reference to every file and block in the filesystem in memory. For example, a 200-node cluster with 24 TB of disk space per node, a block size of 128 MB, and a replication factor of 3 has room for about 2 million blocks (or more): 200\\times 24TB\u2044(128MB\u00d73) 200\\times 24TB\u2044(128MB\u00d73) , So in this case, setting the namenode memory to 12,000 MB would be a good starting point. Solution: HDFS federation, allows a cluster to scale by adding namenodes, each of which manages a portion of the filesystem namespace. HDFS High Availability To remedy a failed namenode, a pair of namenodes in an active-standby configuration is introduced in Hadoop 2. In the event of the failure of the active namenode, the standby takes over its duties to continue servicing client requests without a significant interruption. 3 The Command-Line Interface Basic Filesystem Operations Hadoop\u2019s filesystem shell command is fs , which supports a number of subcommands (type hadoop fs -help to get detailed help). Copying a file from the local filesystem to HDFS: #The local file is copied tothe HDFS instance running on localhost. $ hadoop fs -copyFromLocal test.copy /test.copy # works as the same $ hadoop fs -copyFromLocal test.copy hdfs://localhost:9000/test2.copy Copying the file from the HDFS to the local filesystem: $ hadoop fs -copyToLocal /test.copy test.copy.txt 4 Hadoop Filesystems Hadoop has an abstract notion of filesystems, of which HDFS is just one implementation. The Java abstract class org.apache.hadoop.fs.FileSystem represents the client interface to a filesystem in Hadoop, and there are several concrete implementations. Filesystem URI scheme Java implementation Description Local file fs.LocalFileSystem A filesystem for a locally connected disk with client-side checksums HDFS hfs hdfs.DistributedFileSystem Hadoop\u2019s distributed filesystem WebHDFS webhdfs hdfs.web.WebHdfsFileSystem Providing authenticated read/write access to HDFS over HTTP. Secure WebHDFS swebhdfs hdfs.web.SWebHdfsFileSystem The HTTPS version of WebHDFS. When you are processing large volumes of data you should choose a distributed filesystem that has the data locality optimization, notably HDFS. HTTP The HTTP REST API exposed by the WebHDFS protocol makes it easier for other languages to interact with HDFS. Note that the HTTP interface is slower than the native Java client, so should be avoided for very large data transfers if possible. There are two ways of accessing HDFS over HTTP: Directly, where the HDFS daemons serve HTTP requests to clients; Via a proxy (or proxies), which accesses HDFS on the client\u2019s behalf using the usual DistributedFileSystem API. HDFS proxy allows for stricter firewall and bandwidth-limiting policies to be put in place. It\u2019s common to use a proxy for transfers between Hadoop clusters located in different data centers, or when accessing a Hadoop cluster running in the cloud from an external network. 5 The Java Interface Hadoop FileSystem class is the API for interacting with one of Hadoop\u2019s filesystems. In general you should strive to write your code against the FileSystem abstract class , to retain portability across filesystems. This is very useful when testing your program, for example, because you can rapidly run tests using data stored on the local filesystem. Reading Data from a Hadoop URL NOT recommended, because setURLStreamHandlerFactory() method can be called only once per JVM, which means that if some other part of your program sets it, you won't be able to use. Reading Data Using the FileSystem API A file in a Hadoop filesystem is represented by a Hadoop Path object( org.apache.hadoop.fs.Path , not java.io.File ). You can think of a Path as a Hadoop filesystem URI, such as hdfs://localhost/user/tom/test.copy Since FileSystem is a general filesystem API, so the first step is to retrieve an instance for the filesystem we want. There are several static factory methods for getting a FileSystem instance: // Returns the default filesystem public static FileSystem get ( Configuration conf ) throws IOException // Uses the given URI\u2019s scheme and authority to determine the filesystem to use public static FileSystem get ( URI uri , Configuration conf ) throws IOException // Retrieves the filesystem as the given user public static FileSystem get ( URI uri , Configuration conf , String user ) throws IOException // Retrieves a local filesystem instance public static LocalFileSystem getLocal ( Configuration conf ) throws IOException A Configuration object encapsulates a client or server's configuration, which is set using configuration files read from the classpath, such as etc/hadoop/core-site.xml . With a FileSystem instance in hand, we invoke an open() method to get the input stream for a file: // Uses a default buffer size of 4 KB public FSDataInputStream open ( Path f ) throws IOException // Uses a buffer size of bufferSize public abstract FSDataInputStream open ( Path f , int bufferSize ) throws IOException Displaying files from a Hadoop filesystem on standard output by using the FileSystem directly: // $ hdfs://localhost:9000/test2.copy import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.fs.FileSystem ; import org.apache.hadoop.fs.Path ; import org.apache.hadoop.io.IOUtils ; import java.io.InputStream ; import java.net.URI ; public class FileSystemCat { public static void main ( String [] args ) throws Exception { String uri = args [ 0 ]; Configuration conf = new Configuration (); FileSystem fs = FileSystem . get ( URI . create ( uri ), conf ); InputStream in = null ; try { in = fs . open ( new Path ( uri )); IOUtils . copyBytes ( in , System . out , 4096 , false ); } finally { IOUtils . closeStream ( in ); } } } FSDataInputStream The open() method on FileSystem actually returns an FSDataInputStream rather than a standard java.io class. This class is a specialization of java.io.DataInputStream with support for random access, so you can read from any part of the stream: The Seekable interface permits seeking to a position in the file and provides a query method for the current offset from the start of the file ( getPos() ): public interface Seekable { void seek ( long pos ) throws IOException ; long getPos () throws IOException ; } Displaying files from a Hadoop filesystem on standard output twice, by using seek() : // hdfs://localhost:9000/test2.copy import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.fs.FSDataInputStream ; import org.apache.hadoop.fs.FileSystem ; import org.apache.hadoop.fs.Path ; import org.apache.hadoop.io.IOUtils ; import java.net.URI ; public class FileSystemDoubleCat { public static void main ( String [] args ) throws Exception { String uri = args [ 0 ]; Configuration conf = new Configuration (); FileSystem fs = FileSystem . get ( URI . create ( uri ), conf ); FSDataInputStream in = null ; try { in = fs . open ( new Path ( uri )); IOUtils . copyBytes ( in , System . out , 4096 , false ); in . seek ( 0 ); // go back to the start of the file IOUtils . copyBytes ( in , System . out , 4096 , false ); } catch ( Exception ex ) { ex . printStackTrace (); } finally { IOUtils . closeStream ( in ); } } } Writing Data The FileSystem class has a number of methods for creating a file. // takes a Path object for the file to be created and returns an output stream to write to public FSDataOutputStream create ( Path f ) throws IOException // appends to an existing file public FSDataOutputStream append ( Path f ) throws IOException Warning The create() methods create any parent directories of the file to be written that don\u2019t already exist. There\u2019s an overloaded method of for passing a callback interface, Progressable /C, so your application can be notified of the progress of the data being written to the datanodes: public interface Progressable { public void progress(); } Here, we illustrate progress by printing a period every time the progress() method is called by Hadoop, which is after each 64 KB packet of data is written to the datanode pipeline. // args: /Users/larry/test.copy hdfs://localhost:9000/test4.copy import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.fs.FileSystem ; import org.apache.hadoop.fs.Path ; import org.apache.hadoop.io.IOUtils ; import org.apache.hadoop.util.Progressable ; import java.io.* ; import java.net.URI ; // Copying a local file to a Hadoop filesystem public class FileCopyWithProgress { public static void main ( String [] args ) throws Exception { String localsrc = args [ 0 ]; String dstsrc = args [ 1 ]; BufferedInputStream in = new BufferedInputStream ( new FileInputStream ( localsrc )); Configuration conf = new Configuration (); FileSystem fs = FileSystem . get ( URI . create ( dstsrc ), conf ); try { OutputStream out = fs . create ( new Path ( dstsrc ), new Progressable () { @Override public void progress () { System . out . println ( . ); } }); IOUtils . copyBytes ( in , out , 4096 , true ); } finally { IOUtils . closeStream ( in ); } //end try } // end main } FSDataOutputStream The create() method on FileSystem returns an FSDataOutputStream , which, like FSDataInputStream , has a method for querying the current position in the file: public class FSDataOutputStream extends DataOutputStream implements Syncable { public long getPos () throws IOException { // implementation elided } // implementation elided } However, because HDFS allows only sequential writes to an open file or appends to an already written file, FSDataOutputStream does not permit seeking. Directories FileSystem provides a method to create a directory: public boolean mkdirs ( Path f ) throws IOException This method creates all of the necessary parent directories if they don\u2019t already exist. Querying the Filesystem File metadata: FileStatus Listing files File patterns Deleting Data Use the delete() method on FileSystem to permanently remove files or directories: public boolean delete ( Path f , boolean recursive ) throws IOException If f is a file or an empty directory, the value of recursive is ignored. 6 Data Flow Anatomy of a File Read The figure below shows the main sequence of events when reading a file. step 1: The client opens the file it wishes to read by calling open() on the FileSystem object, which for HDFS is an instance of DistributedFileSystem . step 2: DistributedFileSystem calls the namenode, using remote procedure calls (RPCs), to determine the locations of the first few blocks in the file. step 3: For each block, the namenode returns the addresses of the datanodes that have a copy of that block. Furthermore, the datanodes are sorted according to their proximity to the client. If the client is itself a datanode, the client will read from the local datanode if that datanode hosts a copy of the block. The DistributedFileSystem returns an FSDataInputStream to the client for it to read data from. FSDataInputStream in turn wraps a DFSInputStream , which manages the datanode and namenode I/O. The client then calls read() on the stream. step 4: DFSInputStream , which has stored the datanode addresses for the first few blocks in the file, then connects to the first (closest) datanode for the first block in the file. Data is streamed from the datanode back to the client, which calls read() repeatedly on the stream. step 5: When the end of the block is reached, DFSInputStream will close the connection to the datanode, then find the best datanode for the next block. step 6: This happens transparently to the client, which from its point of view is just reading a continuous stream. Blocks are read in order, with the DFSInputStream opening new connections to datanodes as the client reads through the stream. It will also call the namenode to retrieve the datanode locations for the next batch of blocks as needed. When the client has finished reading, it calls close() on the FSDataInputStream . Anatomy of a File Write The figure below illustrates the case of creating a new file, writing data to it, then closing the file. step 1: The client creates the file by calling create() on DistributedFileSystem . step 2: DistributedFileSystem makes an RPC call to the namenode to create a new file in the filesystem\u2019s namespace, with no blocks associated with it. The namenode performs various checks to make sure the file doesn\u2019t already exist and that the client has the right permissions to create the file. If these checks pass, the namenode makes a record of the new file; otherwise, file creation fails and the client is thrown an IOException . The DistributedFileSystem returns an FSDataOutputStream for the client to start writing data to. Just as in the read case, FSDataOutputStream wraps a DFSOutputStream , which handles communication with the datanodes and namenode. step 3: As the client writes data, the DFSOutputStream splits it into packets, which it writes to an internal queue called the data queue . The data queue is consumed by the DataStreamer , which is responsible for asking the namenode to allocate new blocks by picking a list of suitable datanodes to store the replicas. step 4: The list of datanodes forms a pipeline, and here we\u2019ll assume the replication level is three, so there are three nodes in the pipeline. The DataStreamer streams the packets to the first datanode in the pipeline, which stores each packet and forwards it to the second datanode in the pipeline. Similarly, the second datanode stores the packet and forwards it to the third (and last) datanode in the pipeline . step 5: The DFSOutputStream also maintains an internal queue of packets that are waiting to be acknowledged by datanodes, called the ack queue . A packet is removed from the ack queue only when it has been acknowledged by all the datanodes in the pipeline. step 6: When the client has finished writing data, it calls close() on the stream. This action flushes all the remaining packets to the datanode pipeline. step 7: It waits for acknowledgments before contacting the namenode to signal that the file is complete. Coherency Model A coherency model for a filesystem describes the data visibility of reads and writes for a file. Any content written to the file is not guaranteed to be visible, even if the stream is flushed. Once more than a block\u2019s worth of data has been written, the first block will be visible to new readers. The FSDataOutputStream.hflush() method force all buffers to be flushed to the datanodes. The hflush() guarantees that the data written up to that point in the file has reached all the datanodes in the write pipeline and is visible to all new readers. But it does not guarantee that the datanodes have written the data to disk, only that it\u2019s in the datanodes\u2019 memory. Closing a file in HDFS performs an implicit hflush() . The hsync() method syncs to disk for a file descriptor. FileOutputStream out = new FileOutputStream ( localFile ); out . write ( content . getBytes ( UTF-8 )); out . flush (); // flush to operating system out . getFD (). sync (); // sync to disk assertThat ( localFile . length (), is ((( long ) content . length ()))); Consequences for application design You should call hflush() at suitable points, such as after writing a certain number of records or number of bytes. 7 Parallel Copying with distcp The program distcp copys data to and from Hadoop filesystems in parallel. $ hadoop distcp file1 file2 distcp is implemented as a MapReduce job where the work of copying is done by the maps that run in parallel across the cluster, with no reducers.","title":"Chapter 3: The Hadoop Distributed FileSystem"},{"location":"hadoop/ch3/#hadoop-the-definitive-guide-3-the-hadoop-distributed-filesystem","text":"Filesystems that manage the storage across a network of machines are called distributed filesystems . Hadoop comes with a distributed filesystem called HDFS, which stands for Hadoop Distributed Filesystem .","title":"Hadoop: The Definitive Guide 3 - The Hadoop Distributed FileSystem"},{"location":"hadoop/ch3/#1-the-design-of-hdfs","text":"HDFS is a filesystem designed for storing very large files with streaming data access patterns, running on clusters of commodity hardware. Very large files: files that are hundreds of megabytes, gigabytes, or terabytes in size. Streaming data access: HDFS is built around the idea that the most efficient data processing pattern is a write-once, read-many-times pattern. Commodity hardware: It\u2019s designed to run on clusters of commodity hardware. These are areas where HDFS is not a good fit today: Low-latency data access Lots of small files Multiple writers, arbitrary file modifications","title":"1 The Design of HDFS"},{"location":"hadoop/ch3/#2-hdfs-concepts","text":"","title":"2 HDFS Concepts"},{"location":"hadoop/ch3/#blocks","text":"A disk has a block size, which is the minimum amount of data that it can read or write. Filesystems for a single disk build on this by dealing with data in blocks, which are an integral multiple of the disk block size. HDFS, too, has the concept of a block , but it is a much larger unit \u2014 128 MB by default (typically a few kilobytes for ordinary file system). Unlike a filesystem for a single disk, a file in HDFS that is smaller than a single block does not occupy a full block\u2019s worth of underlying storage. (For example, a 1 MB file stored with a block size of 128 MB uses 1 MB of disk space, not 128 MB.) Question WHY IS A BLOCK IN HDFS SO LARGE? To minimize the cost of seeks. Having a block abstraction for a distributed filesystem brings several benefits. A file can be larger than any single disk in the network. Making the unit of abstraction a block rather than a file simplifies the storage subsystem. storage management: because blocks are a fixed size, it is easy to calculate how many can be stored on a given disk. metadata concerns: because blocks are just chunks of data to be stored, file metadata such as permissions information does not need to be stored with the blocks. Blocks fit well with replication for providing fault tolerance and availability. To insure against corrupted blocks and disk and machine failure, each block is replicated to a small number of physically separate machines (typically three).","title":"Blocks"},{"location":"hadoop/ch3/#namenodes-and-datanodes","text":"An HDFS cluster has two types of nodes: a namenode (the master) and a number of datanodes (workers). The namenode manages the filesystem namespace. It maintains the filesystem tree and the metadata for all the files and directories in the tree. This information is stored persistently on the local disk in the form of two files: the namespace image and the edit log. The namenode also knows the datanodes on which all the blocks for a given file are located; Datanodes are the workhorses of the filesystem. They store and retrieve blocks when they are told to (by clients or the namenode), and they report back to the namenode periodically with lists of blocks that they are storing. If the machine running the namenode were obliterated, all the files on the filesystem would be lost since there would be no way of knowing how to reconstruct the files from the blocks on the datanodes. Possible solution: to back up the files that make up the persistent state of the filesystem metadata. to run a secondary namenode, which keeps a copy of the merged namespace image.","title":"Namenodes and Datanodes"},{"location":"hadoop/ch3/#block-caching","text":"For frequently accessed files, the blocks may be explicitly cached in the datanode\u2019s memory, in an off-heap block cache. Users or applications instruct the namenode which files to cache (and for how long) by adding a cache directive to a cache pool .","title":"Block Caching"},{"location":"hadoop/ch3/#hdfs-federation","text":"Problem: On very large clusters with many files, memory becomes the limiting factor for scaling, since namenode keeps a reference to every file and block in the filesystem in memory. For example, a 200-node cluster with 24 TB of disk space per node, a block size of 128 MB, and a replication factor of 3 has room for about 2 million blocks (or more): 200\\times 24TB\u2044(128MB\u00d73) 200\\times 24TB\u2044(128MB\u00d73) , So in this case, setting the namenode memory to 12,000 MB would be a good starting point. Solution: HDFS federation, allows a cluster to scale by adding namenodes, each of which manages a portion of the filesystem namespace.","title":"HDFS Federation"},{"location":"hadoop/ch3/#hdfs-high-availability","text":"To remedy a failed namenode, a pair of namenodes in an active-standby configuration is introduced in Hadoop 2. In the event of the failure of the active namenode, the standby takes over its duties to continue servicing client requests without a significant interruption.","title":"HDFS High Availability"},{"location":"hadoop/ch3/#3-the-command-line-interface","text":"","title":"3 The Command-Line Interface"},{"location":"hadoop/ch3/#basic-filesystem-operations","text":"Hadoop\u2019s filesystem shell command is fs , which supports a number of subcommands (type hadoop fs -help to get detailed help). Copying a file from the local filesystem to HDFS: #The local file is copied tothe HDFS instance running on localhost. $ hadoop fs -copyFromLocal test.copy /test.copy # works as the same $ hadoop fs -copyFromLocal test.copy hdfs://localhost:9000/test2.copy Copying the file from the HDFS to the local filesystem: $ hadoop fs -copyToLocal /test.copy test.copy.txt","title":"Basic Filesystem Operations"},{"location":"hadoop/ch3/#4-hadoop-filesystems","text":"Hadoop has an abstract notion of filesystems, of which HDFS is just one implementation. The Java abstract class org.apache.hadoop.fs.FileSystem represents the client interface to a filesystem in Hadoop, and there are several concrete implementations. Filesystem URI scheme Java implementation Description Local file fs.LocalFileSystem A filesystem for a locally connected disk with client-side checksums HDFS hfs hdfs.DistributedFileSystem Hadoop\u2019s distributed filesystem WebHDFS webhdfs hdfs.web.WebHdfsFileSystem Providing authenticated read/write access to HDFS over HTTP. Secure WebHDFS swebhdfs hdfs.web.SWebHdfsFileSystem The HTTPS version of WebHDFS. When you are processing large volumes of data you should choose a distributed filesystem that has the data locality optimization, notably HDFS.","title":"4 Hadoop Filesystems"},{"location":"hadoop/ch3/#http","text":"The HTTP REST API exposed by the WebHDFS protocol makes it easier for other languages to interact with HDFS. Note that the HTTP interface is slower than the native Java client, so should be avoided for very large data transfers if possible. There are two ways of accessing HDFS over HTTP: Directly, where the HDFS daemons serve HTTP requests to clients; Via a proxy (or proxies), which accesses HDFS on the client\u2019s behalf using the usual DistributedFileSystem API. HDFS proxy allows for stricter firewall and bandwidth-limiting policies to be put in place. It\u2019s common to use a proxy for transfers between Hadoop clusters located in different data centers, or when accessing a Hadoop cluster running in the cloud from an external network.","title":"HTTP"},{"location":"hadoop/ch3/#5-the-java-interface","text":"Hadoop FileSystem class is the API for interacting with one of Hadoop\u2019s filesystems. In general you should strive to write your code against the FileSystem abstract class , to retain portability across filesystems. This is very useful when testing your program, for example, because you can rapidly run tests using data stored on the local filesystem.","title":"5 The Java Interface"},{"location":"hadoop/ch3/#reading-data-from-a-hadoop-url","text":"NOT recommended, because setURLStreamHandlerFactory() method can be called only once per JVM, which means that if some other part of your program sets it, you won't be able to use.","title":"Reading Data from a Hadoop URL"},{"location":"hadoop/ch3/#reading-data-using-the-filesystem-api","text":"A file in a Hadoop filesystem is represented by a Hadoop Path object( org.apache.hadoop.fs.Path , not java.io.File ). You can think of a Path as a Hadoop filesystem URI, such as hdfs://localhost/user/tom/test.copy Since FileSystem is a general filesystem API, so the first step is to retrieve an instance for the filesystem we want. There are several static factory methods for getting a FileSystem instance: // Returns the default filesystem public static FileSystem get ( Configuration conf ) throws IOException // Uses the given URI\u2019s scheme and authority to determine the filesystem to use public static FileSystem get ( URI uri , Configuration conf ) throws IOException // Retrieves the filesystem as the given user public static FileSystem get ( URI uri , Configuration conf , String user ) throws IOException // Retrieves a local filesystem instance public static LocalFileSystem getLocal ( Configuration conf ) throws IOException A Configuration object encapsulates a client or server's configuration, which is set using configuration files read from the classpath, such as etc/hadoop/core-site.xml . With a FileSystem instance in hand, we invoke an open() method to get the input stream for a file: // Uses a default buffer size of 4 KB public FSDataInputStream open ( Path f ) throws IOException // Uses a buffer size of bufferSize public abstract FSDataInputStream open ( Path f , int bufferSize ) throws IOException Displaying files from a Hadoop filesystem on standard output by using the FileSystem directly: // $ hdfs://localhost:9000/test2.copy import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.fs.FileSystem ; import org.apache.hadoop.fs.Path ; import org.apache.hadoop.io.IOUtils ; import java.io.InputStream ; import java.net.URI ; public class FileSystemCat { public static void main ( String [] args ) throws Exception { String uri = args [ 0 ]; Configuration conf = new Configuration (); FileSystem fs = FileSystem . get ( URI . create ( uri ), conf ); InputStream in = null ; try { in = fs . open ( new Path ( uri )); IOUtils . copyBytes ( in , System . out , 4096 , false ); } finally { IOUtils . closeStream ( in ); } } }","title":"Reading Data Using the FileSystem API"},{"location":"hadoop/ch3/#fsdatainputstream","text":"The open() method on FileSystem actually returns an FSDataInputStream rather than a standard java.io class. This class is a specialization of java.io.DataInputStream with support for random access, so you can read from any part of the stream: The Seekable interface permits seeking to a position in the file and provides a query method for the current offset from the start of the file ( getPos() ): public interface Seekable { void seek ( long pos ) throws IOException ; long getPos () throws IOException ; } Displaying files from a Hadoop filesystem on standard output twice, by using seek() : // hdfs://localhost:9000/test2.copy import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.fs.FSDataInputStream ; import org.apache.hadoop.fs.FileSystem ; import org.apache.hadoop.fs.Path ; import org.apache.hadoop.io.IOUtils ; import java.net.URI ; public class FileSystemDoubleCat { public static void main ( String [] args ) throws Exception { String uri = args [ 0 ]; Configuration conf = new Configuration (); FileSystem fs = FileSystem . get ( URI . create ( uri ), conf ); FSDataInputStream in = null ; try { in = fs . open ( new Path ( uri )); IOUtils . copyBytes ( in , System . out , 4096 , false ); in . seek ( 0 ); // go back to the start of the file IOUtils . copyBytes ( in , System . out , 4096 , false ); } catch ( Exception ex ) { ex . printStackTrace (); } finally { IOUtils . closeStream ( in ); } } }","title":"FSDataInputStream"},{"location":"hadoop/ch3/#writing-data","text":"The FileSystem class has a number of methods for creating a file. // takes a Path object for the file to be created and returns an output stream to write to public FSDataOutputStream create ( Path f ) throws IOException // appends to an existing file public FSDataOutputStream append ( Path f ) throws IOException Warning The create() methods create any parent directories of the file to be written that don\u2019t already exist. There\u2019s an overloaded method of for passing a callback interface, Progressable /C, so your application can be notified of the progress of the data being written to the datanodes: public interface Progressable { public void progress(); } Here, we illustrate progress by printing a period every time the progress() method is called by Hadoop, which is after each 64 KB packet of data is written to the datanode pipeline. // args: /Users/larry/test.copy hdfs://localhost:9000/test4.copy import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.fs.FileSystem ; import org.apache.hadoop.fs.Path ; import org.apache.hadoop.io.IOUtils ; import org.apache.hadoop.util.Progressable ; import java.io.* ; import java.net.URI ; // Copying a local file to a Hadoop filesystem public class FileCopyWithProgress { public static void main ( String [] args ) throws Exception { String localsrc = args [ 0 ]; String dstsrc = args [ 1 ]; BufferedInputStream in = new BufferedInputStream ( new FileInputStream ( localsrc )); Configuration conf = new Configuration (); FileSystem fs = FileSystem . get ( URI . create ( dstsrc ), conf ); try { OutputStream out = fs . create ( new Path ( dstsrc ), new Progressable () { @Override public void progress () { System . out . println ( . ); } }); IOUtils . copyBytes ( in , out , 4096 , true ); } finally { IOUtils . closeStream ( in ); } //end try } // end main }","title":"Writing Data"},{"location":"hadoop/ch3/#fsdataoutputstream","text":"The create() method on FileSystem returns an FSDataOutputStream , which, like FSDataInputStream , has a method for querying the current position in the file: public class FSDataOutputStream extends DataOutputStream implements Syncable { public long getPos () throws IOException { // implementation elided } // implementation elided } However, because HDFS allows only sequential writes to an open file or appends to an already written file, FSDataOutputStream does not permit seeking.","title":"FSDataOutputStream"},{"location":"hadoop/ch3/#directories","text":"FileSystem provides a method to create a directory: public boolean mkdirs ( Path f ) throws IOException This method creates all of the necessary parent directories if they don\u2019t already exist.","title":"Directories"},{"location":"hadoop/ch3/#querying-the-filesystem","text":"File metadata: FileStatus Listing files File patterns","title":"Querying the Filesystem"},{"location":"hadoop/ch3/#deleting-data","text":"Use the delete() method on FileSystem to permanently remove files or directories: public boolean delete ( Path f , boolean recursive ) throws IOException If f is a file or an empty directory, the value of recursive is ignored.","title":"Deleting Data"},{"location":"hadoop/ch3/#6-data-flow","text":"","title":"6 Data Flow"},{"location":"hadoop/ch3/#anatomy-of-a-file-read","text":"The figure below shows the main sequence of events when reading a file. step 1: The client opens the file it wishes to read by calling open() on the FileSystem object, which for HDFS is an instance of DistributedFileSystem . step 2: DistributedFileSystem calls the namenode, using remote procedure calls (RPCs), to determine the locations of the first few blocks in the file. step 3: For each block, the namenode returns the addresses of the datanodes that have a copy of that block. Furthermore, the datanodes are sorted according to their proximity to the client. If the client is itself a datanode, the client will read from the local datanode if that datanode hosts a copy of the block. The DistributedFileSystem returns an FSDataInputStream to the client for it to read data from. FSDataInputStream in turn wraps a DFSInputStream , which manages the datanode and namenode I/O. The client then calls read() on the stream. step 4: DFSInputStream , which has stored the datanode addresses for the first few blocks in the file, then connects to the first (closest) datanode for the first block in the file. Data is streamed from the datanode back to the client, which calls read() repeatedly on the stream. step 5: When the end of the block is reached, DFSInputStream will close the connection to the datanode, then find the best datanode for the next block. step 6: This happens transparently to the client, which from its point of view is just reading a continuous stream. Blocks are read in order, with the DFSInputStream opening new connections to datanodes as the client reads through the stream. It will also call the namenode to retrieve the datanode locations for the next batch of blocks as needed. When the client has finished reading, it calls close() on the FSDataInputStream .","title":"Anatomy of a File Read"},{"location":"hadoop/ch3/#anatomy-of-a-file-write","text":"The figure below illustrates the case of creating a new file, writing data to it, then closing the file. step 1: The client creates the file by calling create() on DistributedFileSystem . step 2: DistributedFileSystem makes an RPC call to the namenode to create a new file in the filesystem\u2019s namespace, with no blocks associated with it. The namenode performs various checks to make sure the file doesn\u2019t already exist and that the client has the right permissions to create the file. If these checks pass, the namenode makes a record of the new file; otherwise, file creation fails and the client is thrown an IOException . The DistributedFileSystem returns an FSDataOutputStream for the client to start writing data to. Just as in the read case, FSDataOutputStream wraps a DFSOutputStream , which handles communication with the datanodes and namenode. step 3: As the client writes data, the DFSOutputStream splits it into packets, which it writes to an internal queue called the data queue . The data queue is consumed by the DataStreamer , which is responsible for asking the namenode to allocate new blocks by picking a list of suitable datanodes to store the replicas. step 4: The list of datanodes forms a pipeline, and here we\u2019ll assume the replication level is three, so there are three nodes in the pipeline. The DataStreamer streams the packets to the first datanode in the pipeline, which stores each packet and forwards it to the second datanode in the pipeline. Similarly, the second datanode stores the packet and forwards it to the third (and last) datanode in the pipeline . step 5: The DFSOutputStream also maintains an internal queue of packets that are waiting to be acknowledged by datanodes, called the ack queue . A packet is removed from the ack queue only when it has been acknowledged by all the datanodes in the pipeline. step 6: When the client has finished writing data, it calls close() on the stream. This action flushes all the remaining packets to the datanode pipeline. step 7: It waits for acknowledgments before contacting the namenode to signal that the file is complete.","title":"Anatomy of a File Write"},{"location":"hadoop/ch3/#coherency-model","text":"A coherency model for a filesystem describes the data visibility of reads and writes for a file. Any content written to the file is not guaranteed to be visible, even if the stream is flushed. Once more than a block\u2019s worth of data has been written, the first block will be visible to new readers. The FSDataOutputStream.hflush() method force all buffers to be flushed to the datanodes. The hflush() guarantees that the data written up to that point in the file has reached all the datanodes in the write pipeline and is visible to all new readers. But it does not guarantee that the datanodes have written the data to disk, only that it\u2019s in the datanodes\u2019 memory. Closing a file in HDFS performs an implicit hflush() . The hsync() method syncs to disk for a file descriptor. FileOutputStream out = new FileOutputStream ( localFile ); out . write ( content . getBytes ( UTF-8 )); out . flush (); // flush to operating system out . getFD (). sync (); // sync to disk assertThat ( localFile . length (), is ((( long ) content . length ()))); Consequences for application design You should call hflush() at suitable points, such as after writing a certain number of records or number of bytes.","title":"Coherency Model"},{"location":"hadoop/ch3/#7-parallel-copying-with-distcp","text":"The program distcp copys data to and from Hadoop filesystems in parallel. $ hadoop distcp file1 file2 distcp is implemented as a MapReduce job where the work of copying is done by the maps that run in parallel across the cluster, with no reducers.","title":"7 Parallel Copying with distcp"},{"location":"hadoop/ch4/","text":"Hadoop: The Definitive Guide 4 - YARN Apache YARN(Yet Another Resource Negotiator) is Hadoop s cluster resource management system. YARN provides APIs for requesting and working with cluster resources, but these APIs are not typically used directly by user code. Distributed computing frameworks (MapReduce, Spark, and so on) running as YARN applications on the cluster compute layer (YARN) and the cluster storage layer (HDFS and HBase). 1 Anatomy of a YARN Application Run YARN provides its core services via two types of long-running daemon: a resource manager (one per cluster) to manage the use of resources across the cluster, node managers running on all the nodes in the cluster to launch and monitor containers . step1 : To run an application on YARN, a client contacts the resource manager and asks it to run an application master process. steps 2a and 2b: The resource manager then finds a node manager that can launch the application master in a container. It could simply run a computation in the container it is running in and return the result to the client. step 3: Or it could request more containers from the resource managers steps 4a and 4b: use them to run a distributed computation. The ApplicationMaster is an instance of a framework-specific library that negotiates resources from the ResourceManager and works with the NodeManager to execute and monitor the granted resources (bundled as containers) for a given application. The ApplicationMaster runs in a container like any other application. Resource Requests A YARN application can make resource requests at any time while it is running. Spark starts a fixed number of executors on the cluster (i.e. make all of requests up front). MapReduce, has two phases: the map task containers are requested up front, but the reduce task containers are not started until later. (i.e. take a more dynamic approach whereby it requests more resources dynamically to meet the changing needs of the application). Application Lifespan The lifespan of a YARN application can vary dramatically. Rather than look at how long the application runs for, it s useful to categorize applications in terms of how they map to the jobs that users run. The simplest case is one application per user job, which is the approach that MapReduce takes. The second model is to run one application per workflow or user session of (possibly unrelated) jobs, which is the approach that Spark takes. This approach can be more efficient than the first, since containers can be reused between jobs, and there is also the potential to cache intermediate data between jobs. The third model is a long-running application that is shared by different users, which is the approach that Apache Slider takes. Building YARN Applications Writing a YARN application from scratch is fairly involved, but in many cases is not necessary, as it is often possible to use an existing application that fits the bill. 2 YARN Compared to MapReduce 1 The distributed implementation of MapReduce in the original version of Hadoop is sometimes referred to as MapReduce 1 to distinguish it from MapReduce 2, the implementation that uses YARN. A comparison of MapReduce 1 and YARN components: MapReduce1 YARN Jobtracker Resource manager, application master, timeline server TaskTracker Node manager Slot Container The Timeline Server addresses the problem of the storage and retrieval of application s current and historic information in a generic fashion. 3 Scheduling in YARN The job of the YARN scheduler to allocate resources to applications according to some defined policy. Scheduling in general is a difficult problem and there is no one \"best\" policy , which is why YARN provides a choice of schedulers and configurable policies. Scheduler Options Three schedulers are available in YARN: the FIFO, Capacity, and Fair Schedulers. The FIFO: places applications in a queue and runs them in the order of submission (first in, first out) Not suitable for shared clusters, because large applications will use all the resources in a cluster, so each application has to wait its turn. Capacity Scheduler: a separate dedicated queue allows the small job to start as soon as it is submitted, since the queue capacity is reserved for jobs in that queue. Fair Scheduler: dynamically balance resources between all running jobs, each job is using its fair share of resources. There is a lag between the time the second job starts and when it receives its fair share, since it has to wait for resources to free up as containers used by the first job complete. After the small job completes and no longer requires resources, the large job goes back to using the full cluster capacity again.","title":"Chapter 4: YARN"},{"location":"hadoop/ch4/#hadoop-the-definitive-guide-4-yarn","text":"Apache YARN(Yet Another Resource Negotiator) is Hadoop s cluster resource management system. YARN provides APIs for requesting and working with cluster resources, but these APIs are not typically used directly by user code. Distributed computing frameworks (MapReduce, Spark, and so on) running as YARN applications on the cluster compute layer (YARN) and the cluster storage layer (HDFS and HBase).","title":"Hadoop: The Definitive Guide 4 - YARN"},{"location":"hadoop/ch4/#1-anatomy-of-a-yarn-application-run","text":"YARN provides its core services via two types of long-running daemon: a resource manager (one per cluster) to manage the use of resources across the cluster, node managers running on all the nodes in the cluster to launch and monitor containers . step1 : To run an application on YARN, a client contacts the resource manager and asks it to run an application master process. steps 2a and 2b: The resource manager then finds a node manager that can launch the application master in a container. It could simply run a computation in the container it is running in and return the result to the client. step 3: Or it could request more containers from the resource managers steps 4a and 4b: use them to run a distributed computation. The ApplicationMaster is an instance of a framework-specific library that negotiates resources from the ResourceManager and works with the NodeManager to execute and monitor the granted resources (bundled as containers) for a given application. The ApplicationMaster runs in a container like any other application.","title":"1 Anatomy of a YARN Application Run"},{"location":"hadoop/ch4/#resource-requests","text":"A YARN application can make resource requests at any time while it is running. Spark starts a fixed number of executors on the cluster (i.e. make all of requests up front). MapReduce, has two phases: the map task containers are requested up front, but the reduce task containers are not started until later. (i.e. take a more dynamic approach whereby it requests more resources dynamically to meet the changing needs of the application).","title":"Resource Requests"},{"location":"hadoop/ch4/#application-lifespan","text":"The lifespan of a YARN application can vary dramatically. Rather than look at how long the application runs for, it s useful to categorize applications in terms of how they map to the jobs that users run. The simplest case is one application per user job, which is the approach that MapReduce takes. The second model is to run one application per workflow or user session of (possibly unrelated) jobs, which is the approach that Spark takes. This approach can be more efficient than the first, since containers can be reused between jobs, and there is also the potential to cache intermediate data between jobs. The third model is a long-running application that is shared by different users, which is the approach that Apache Slider takes.","title":"Application Lifespan"},{"location":"hadoop/ch4/#building-yarn-applications","text":"Writing a YARN application from scratch is fairly involved, but in many cases is not necessary, as it is often possible to use an existing application that fits the bill.","title":"Building YARN Applications"},{"location":"hadoop/ch4/#2-yarn-compared-to-mapreduce-1","text":"The distributed implementation of MapReduce in the original version of Hadoop is sometimes referred to as MapReduce 1 to distinguish it from MapReduce 2, the implementation that uses YARN. A comparison of MapReduce 1 and YARN components: MapReduce1 YARN Jobtracker Resource manager, application master, timeline server TaskTracker Node manager Slot Container The Timeline Server addresses the problem of the storage and retrieval of application s current and historic information in a generic fashion.","title":"2 YARN Compared to MapReduce 1"},{"location":"hadoop/ch4/#3-scheduling-in-yarn","text":"The job of the YARN scheduler to allocate resources to applications according to some defined policy. Scheduling in general is a difficult problem and there is no one \"best\" policy , which is why YARN provides a choice of schedulers and configurable policies.","title":"3 Scheduling in YARN"},{"location":"hadoop/ch4/#scheduler-options","text":"Three schedulers are available in YARN: the FIFO, Capacity, and Fair Schedulers. The FIFO: places applications in a queue and runs them in the order of submission (first in, first out) Not suitable for shared clusters, because large applications will use all the resources in a cluster, so each application has to wait its turn. Capacity Scheduler: a separate dedicated queue allows the small job to start as soon as it is submitted, since the queue capacity is reserved for jobs in that queue. Fair Scheduler: dynamically balance resources between all running jobs, each job is using its fair share of resources. There is a lag between the time the second job starts and when it receives its fair share, since it has to wait for resources to free up as containers used by the first job complete. After the small job completes and no longer requires resources, the large job goes back to using the full cluster capacity again.","title":"Scheduler Options"},{"location":"hadoop/ch5/","text":"Hadoop: The Definitive Guide 5 - Hadoop I/O 1 Data Integrity The usual way of detecting corrupted data is by computing a checksum (\u6821\u9a8c\u548c) for the data when it first enters the system, and again whenever it is transmitted across a channel that is unreliable and hence capable of corrupting the data. A commonly used error-detecting code is CRC-32 (32-bit cyclic redundancy check, \u5faa\u73af\u5197\u4f59\u6821\u9a8c), which computes a 32-bit integer checksum for input of any size. CRC32 is used for checksumming in Hadoop's checksumFileSystem , while HDFS uses a more efficient variant called CRC-32C. Data Integrity in HDFS HDFS transparently checksums all data written to it and by default verifies checksums when reading data. A separate checksum is created for every ChecksumFileSystem.bytesPerChecksum (default 512) bytes of data. Datanodes are responsible for verifying the data they receive before storing the data and its checksum. When clients read data from datanodes, they verify checksums as well. In addition to block verification on client reads, each datanode runs a DataBlockScanner in a background thread that periodically verifies all the blocks stored on the datanode. You can find a file\u2019s checksum with hadoop fs -checksum . LocalFileSystem The Hadoop LocalFileSystem performs client-side checksumming. It is possible to disable checksums, by using RawLocalFileSystem in place of LocalFileSystem . ChecksumFileSystem LocalFileSystem extends ChecksumFileSystem , and ChecksumFileSystem is also a wrapper around FileSystem (uses decorator pattern here). The general idiom is as follows: FileSystem rawFs = ... FileSystem checksummedFs = new ChecksumFileSystem ( rawFs ); 2 Compression File compression brings two major benefits: it reduces the space needed to store files, and it speeds up data transfer across the network or to or from disk. When dealing with large volumes of data, both of these savings can be significant. A summary of compression formats: Compression format Tools Algorithm File Extension CompressionCodec Splittable? DEFLATE N/A DEFLATE .deflate DefaultCodec No gzip gzip DEFLATE .gz GzipCodec No bzip2 bzip2 bzip2 .bz2 BZip2Codec Yes LZO lzop LZO .lzo LzoCodec No Snappy N/A Snappy .snappy SnappyCodec No All compression algorithm exhibit a space/time trade-off. Splittable compression formats are especially suitable for MapReduce. Codecs A codec is the implementation of a compression-decompression algorithm. In Hadoop, a codec is represented by an implementation of the CompressionCodec interface. So, for example, GzipCodec encapsulates the compression and decompression algorithm for gzip. Compressing and decompressing streams with CompressionCodec Interface CompressionCodec has two methods that allow you to easily compress or decompress data. To compress data being written to an output stream, use the createOutputStream(OutputStream out) method to create a CompressionOutputStream Conversely, to decompress data being read from an input stream, call createInputStream(InputStream in) to obtain a CompressionInputStream . The code below illustrates how to use the API to compress data read from standard input and write it to standard output. import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.io.IOUtils ; import org.apache.hadoop.io.compress.CompressionCodec ; import org.apache.hadoop.io.compress.CompressionOutputStream ; import org.apache.hadoop.util.ReflectionUtils ; // vv StreamCompressor public class StreamCompressor { public static void main ( String [] args ) throws Exception { String codecClassname = args [ 0 ]; Class ? codecClass = Class . forName ( codecClassname ); Configuration conf = new Configuration (); CompressionCodec codec = ( CompressionCodec ) ReflectionUtils . newInstance ( codecClass , conf ); CompressionOutputStream out = codec . createOutputStream ( System . out ); IOUtils . copyBytes ( System . in , out , 4096 , false ); out . finish (); } } We can try it out with the following command line, which compresses the string \u201cText\u201d using the StreamCompressor program with the GzipCodec , then decompresses it from standard input using gunzip : export HADOOP_CLASSPATH = /Users/larry/JavaProject/out/artifacts/StreamCompressor/StreamCompressor.jar echo Text | hadoop com.definitivehadoop.compression.StreamCompressor org.apache.hadoop.io.compress.GzipCodec | gunzip Inferring CompressionCodecs using CompressionCodecFactory CompressionCodecFactory provides a way of mapping a filename extension to a CompressionCodec using its getCodec() method, CodecPool . If you are using a native library and you are doing a lot of compression or decompression in your application, consider using CodecPool , which allows you to reuse compressors and decompressors, thereby amortizing the cost of creating these objects. Compression and Input Splits If a compressed file using a format that does not support splitting, say gzip format, MapReduce will not try to split the gzipped file, at the expense of locality: a single map will process all blocks containing the file, most of which will not be local to the map. For an LZO file, in spite of not supporting splitting, it is possible to preprocess LZO files using an indexer tool that comes with the Hadoop LZO libraries. Using Compression in MapReduce In order to compress the output of a MapReduce, job you can use the static convenience methods on FileOutputFormat to set properties. Application to run the maximum temperature job producing compressed output: Maxtemperaturewithcompression public class MaxTemperatureWithCompression { public static void main ( String [] args ) throws Exception { if ( args . length != 2 ) { System . err . println ( Usage: MaxTemperatureWithCompression input path + output path ); System . exit (- 1 ); } Job job = Job . getInstance (); job . setJarByClass ( com . definitivehadoop . weatherdata . MaxTemperature . class ); FileInputFormat . addInputPath ( job , new Path ( args [ 0 ])); FileOutputFormat . setOutputPath ( job , new Path ( args [ 1 ])); job . setOutputKeyClass ( Text . class ); job . setOutputValueClass ( IntWritable . class ); /*[*/ FileOutputFormat . setCompressOutput ( job , true ); FileOutputFormat . setOutputCompressorClass ( job , GzipCodec . class ); /*]*/ job . setMapperClass ( com . definitivehadoop . weatherdata . MaxTemperatureMapper . class ); job . setCombinerClass ( com . definitivehadoop . weatherdata . MaxTemperatureReducer . class ); job . setReducerClass ( com . definitivehadoop . weatherdata . MaxTemperatureReducer . class ); System . exit ( job . waitForCompletion ( true ) ? 0 : 1 ); } } //^^ MaxTemperatureWithCompression Usage $ export HADOOP_CLASSPATH = /Users/larry/JavaProject/out/artifacts/MaxTemperatureWithCompression/MaxTemperatureWithCompression.jar $ hadoop com.definitivehadoop.compression.MaxTemperatureWithCompression /Users/larry/JavaProject/resources/HadoopBook/ncdc/sample.txt output 3 Serialization See concepts of serialization and deserialization in Head First Java Chapter 14 . Serialization is the process of turning structured objects into a byte stream for transmission over a network or for writing to persistent storage. Deserialization is the reverse process of turning a byte stream back into a series of structured objects. Serialization is used in two quite distinct areas of distributed data processing: for interprocess communication and for persistent storage . In Hadoop, interprocess communication between nodes in the system is implemented using remote procedure calls (RPCs). The RPC protocol uses serialization to render the message into a binary stream to be sent to the remote node, which then deserializes the binary stream into the original message. In general, four desirable properties are crucial for an RPC serialization and persistent storage: Properties PRC Serialization Persistent Storage Compact makes the best use of network bandwidth make efficient use of storage space Fast little performance overhead little overhead in reading or writing Extensible meet new requirements transparently read data of older formats Interoperable support clients written in different languages read/write using different languages Hadoop uses its own serialization format, Writables , which is certainly compact and fast, but not so easy to extend or use from languages other than Java. Avro, a serialization system that was designed to overcome some of the limitations of Writables , is covered in Chapter 12 . The Writable Interface The Writable interface defines two methods \u2014 one for writing its state to a DataOutput binary stream and one for reading its state from a DataInput binary stream (note: DataOutput and DataInput are also inferfaces): package org.apache.hadoop.io ; import java.io.DataOutput ; import java.io.DataInput ; import java.io.IOException ; public interface Writable { void write ( DataOutput out ) throws IOException ; void readFields ( DataInput in ) throws IOException ; } Writable Classes Writable wrappers for Java primitives There are Writable wrappers for all the Java primitive types except char (which can be stored in an IntWritable ). All have a get() and set() method for retrieving and storing the wrapped value. When it comes to encoding integers, there is a choice between the fixed-length formats ( IntWritable and LongWritable ) and the variable-length formats ( VIntWritable and VLongWritable ). Fixed-length encodings are good when the distribution of values is fairly uniform across the whole value space, such as when using a (well-designed) hash function. Most numeric variables tend to have nonuniform distributions, though, and on average, the variable-length encoding will save space. Text Text is a Writable for UTF-8 sequences. It can be thought of as the Writable equivalent of java.lang.String . Indexing for the Text class is in terms of position in the encoded byte sequence, not the Unicode character in the string or the Java char code unit (as it is for String ). For ASCII strings, these three concepts of index position coincide. Another difference from String is that Text is mutable. You can reuse a Text instance by calling one of the set() methods on it. Text t = new Text ( hadoop ); t . set ( pig ); Text doesn\u2019t have as rich an API for manipulating java.lang.String , so in many cases, you need to convert the Text object to a String : Text ( hadoop ). toString () Implementing a Custom Writable Tip If you are considering writing a custom Writable , it may be worth trying another serialization framework, like Avro, that allows you to define custom types declaratively. Serialization Frameworks Any type can be used to serialize, because Hadoop has an API for pluggable serialization frameworks, which is represented by an implementation of Serialization . For instance, WritableSerialization , is the implementation of Serialization for Writable types; AvroSerialization , is the implementation of Serialization for Avro types. public class WritableSerialization extends Configured implements Serialization Writable public abstract class AvroSerialization T extends Configured implements Serialization T A Serialization defines a mapping from types to Serializer instances (for turning an object into a byte stream) and Deserializer instances (for turning a byte stream into an object). public interface Serialization T { // Allows clients to test whether this Serialization // supports the given class. boolean accept ( Class ? c ); // @return a {@link Serializer} for the given class. Serializer T getSerializer ( Class T c ); //return a {@link Deserializer} for the given class. Deserializer T getDeserializer ( Class T c ); } Note Although it makes it convenient to be able to use standard Java types such as Integer or String in MapReduce programs, Java Object Serialization is not as efficient as Writables , so it\u2019s not worth making this trade-off. Serialization IDL Apache Thrift and Google Protocol Buffers are both popular serialization frameworks, and both are commonly used as a format for persistent binary data. Avro is an IDL-based serialization framework designed to work well with large-scale data processing in Hadoop. 4 File-Based Data Structures For some applications, you need a specialized data structure to hold your data. For doing MapReduce-based processing, putting each blob of binary data into its own file doesn\u2019t scale, so Hadoop developed a number of higher-level containers for these situations. SequenceFile Hadoop\u2019s SequenceFile provides a persistent data structure for binary key-value pairs. It is suitable for a log file, where each log record is a new line of text. To use it as a logfile format, you would choose a key, such as timestamp represented by a LongWritable , and the value would be a Writable that represents the quantity being logged. Writing a SequenceFile To create a SequenceFile , use one of its createWriter() static methods, which return a SequenceFile.Writer instance. Then write key-value pairs using the append() method. When you\u2019ve finished, you call the close() method. Displaying a SequenceFile with the command-line interface The hadoop fs command has a -text option to display sequence files in textual form. % hadoop fs - text numbers . seq | head The SequenceFile format A sequence file(\u987a\u5e8f\u6587\u4ef6) consists of a header followed by one or more records. The sync marker(\u540c\u6b65\u6807\u8bc6) is used to allow a reader synchronize to a record boundary from any position in the file, which incurs less than a 1% storage overhead. The internal format of the records depends on whether compression is enabled, and if it is, whether it is record compression(\u8bb0\u5f55\u538b\u7f29) or block compression(\u5757\u538b\u7f29). The format for record compression is almost identical to that for no compression, except the value bytes are compressed using the codec defined in the header. Note that keys are not compressed. Block compression compresses multiple records at once; it is therefore more compact than and should generally be preferred over record compression because it has the opportunity to take advantage of similarities between records. A sync marker is written before the start of every block. The format of a block is a field indicating the number of records in the block, followed by four compressed fields: the key lengths, the keys, the value lengths, and the values. MapFile A MapFile is a sorted SequenceFile with an index to permit lookups by key. The index is itself a SequenceFile that contains a fraction of the keys in the map.","title":"Chapter 5: Hadoop I/O"},{"location":"hadoop/ch5/#hadoop-the-definitive-guide-5-hadoop-io","text":"","title":"Hadoop: The Definitive Guide 5 - Hadoop I/O"},{"location":"hadoop/ch5/#1-data-integrity","text":"The usual way of detecting corrupted data is by computing a checksum (\u6821\u9a8c\u548c) for the data when it first enters the system, and again whenever it is transmitted across a channel that is unreliable and hence capable of corrupting the data. A commonly used error-detecting code is CRC-32 (32-bit cyclic redundancy check, \u5faa\u73af\u5197\u4f59\u6821\u9a8c), which computes a 32-bit integer checksum for input of any size. CRC32 is used for checksumming in Hadoop's checksumFileSystem , while HDFS uses a more efficient variant called CRC-32C.","title":"1 Data Integrity"},{"location":"hadoop/ch5/#data-integrity-in-hdfs","text":"HDFS transparently checksums all data written to it and by default verifies checksums when reading data. A separate checksum is created for every ChecksumFileSystem.bytesPerChecksum (default 512) bytes of data. Datanodes are responsible for verifying the data they receive before storing the data and its checksum. When clients read data from datanodes, they verify checksums as well. In addition to block verification on client reads, each datanode runs a DataBlockScanner in a background thread that periodically verifies all the blocks stored on the datanode. You can find a file\u2019s checksum with hadoop fs -checksum .","title":"Data Integrity in HDFS"},{"location":"hadoop/ch5/#localfilesystem","text":"The Hadoop LocalFileSystem performs client-side checksumming. It is possible to disable checksums, by using RawLocalFileSystem in place of LocalFileSystem .","title":"LocalFileSystem"},{"location":"hadoop/ch5/#checksumfilesystem","text":"LocalFileSystem extends ChecksumFileSystem , and ChecksumFileSystem is also a wrapper around FileSystem (uses decorator pattern here). The general idiom is as follows: FileSystem rawFs = ... FileSystem checksummedFs = new ChecksumFileSystem ( rawFs );","title":"ChecksumFileSystem"},{"location":"hadoop/ch5/#2-compression","text":"File compression brings two major benefits: it reduces the space needed to store files, and it speeds up data transfer across the network or to or from disk. When dealing with large volumes of data, both of these savings can be significant. A summary of compression formats: Compression format Tools Algorithm File Extension CompressionCodec Splittable? DEFLATE N/A DEFLATE .deflate DefaultCodec No gzip gzip DEFLATE .gz GzipCodec No bzip2 bzip2 bzip2 .bz2 BZip2Codec Yes LZO lzop LZO .lzo LzoCodec No Snappy N/A Snappy .snappy SnappyCodec No All compression algorithm exhibit a space/time trade-off. Splittable compression formats are especially suitable for MapReduce.","title":"2 Compression"},{"location":"hadoop/ch5/#codecs","text":"A codec is the implementation of a compression-decompression algorithm. In Hadoop, a codec is represented by an implementation of the CompressionCodec interface. So, for example, GzipCodec encapsulates the compression and decompression algorithm for gzip. Compressing and decompressing streams with CompressionCodec Interface CompressionCodec has two methods that allow you to easily compress or decompress data. To compress data being written to an output stream, use the createOutputStream(OutputStream out) method to create a CompressionOutputStream Conversely, to decompress data being read from an input stream, call createInputStream(InputStream in) to obtain a CompressionInputStream . The code below illustrates how to use the API to compress data read from standard input and write it to standard output. import org.apache.hadoop.conf.Configuration ; import org.apache.hadoop.io.IOUtils ; import org.apache.hadoop.io.compress.CompressionCodec ; import org.apache.hadoop.io.compress.CompressionOutputStream ; import org.apache.hadoop.util.ReflectionUtils ; // vv StreamCompressor public class StreamCompressor { public static void main ( String [] args ) throws Exception { String codecClassname = args [ 0 ]; Class ? codecClass = Class . forName ( codecClassname ); Configuration conf = new Configuration (); CompressionCodec codec = ( CompressionCodec ) ReflectionUtils . newInstance ( codecClass , conf ); CompressionOutputStream out = codec . createOutputStream ( System . out ); IOUtils . copyBytes ( System . in , out , 4096 , false ); out . finish (); } } We can try it out with the following command line, which compresses the string \u201cText\u201d using the StreamCompressor program with the GzipCodec , then decompresses it from standard input using gunzip : export HADOOP_CLASSPATH = /Users/larry/JavaProject/out/artifacts/StreamCompressor/StreamCompressor.jar echo Text | hadoop com.definitivehadoop.compression.StreamCompressor org.apache.hadoop.io.compress.GzipCodec | gunzip Inferring CompressionCodecs using CompressionCodecFactory CompressionCodecFactory provides a way of mapping a filename extension to a CompressionCodec using its getCodec() method, CodecPool . If you are using a native library and you are doing a lot of compression or decompression in your application, consider using CodecPool , which allows you to reuse compressors and decompressors, thereby amortizing the cost of creating these objects.","title":"Codecs"},{"location":"hadoop/ch5/#compression-and-input-splits","text":"If a compressed file using a format that does not support splitting, say gzip format, MapReduce will not try to split the gzipped file, at the expense of locality: a single map will process all blocks containing the file, most of which will not be local to the map. For an LZO file, in spite of not supporting splitting, it is possible to preprocess LZO files using an indexer tool that comes with the Hadoop LZO libraries.","title":"Compression and Input Splits"},{"location":"hadoop/ch5/#using-compression-in-mapreduce","text":"In order to compress the output of a MapReduce, job you can use the static convenience methods on FileOutputFormat to set properties. Application to run the maximum temperature job producing compressed output: Maxtemperaturewithcompression public class MaxTemperatureWithCompression { public static void main ( String [] args ) throws Exception { if ( args . length != 2 ) { System . err . println ( Usage: MaxTemperatureWithCompression input path + output path ); System . exit (- 1 ); } Job job = Job . getInstance (); job . setJarByClass ( com . definitivehadoop . weatherdata . MaxTemperature . class ); FileInputFormat . addInputPath ( job , new Path ( args [ 0 ])); FileOutputFormat . setOutputPath ( job , new Path ( args [ 1 ])); job . setOutputKeyClass ( Text . class ); job . setOutputValueClass ( IntWritable . class ); /*[*/ FileOutputFormat . setCompressOutput ( job , true ); FileOutputFormat . setOutputCompressorClass ( job , GzipCodec . class ); /*]*/ job . setMapperClass ( com . definitivehadoop . weatherdata . MaxTemperatureMapper . class ); job . setCombinerClass ( com . definitivehadoop . weatherdata . MaxTemperatureReducer . class ); job . setReducerClass ( com . definitivehadoop . weatherdata . MaxTemperatureReducer . class ); System . exit ( job . waitForCompletion ( true ) ? 0 : 1 ); } } //^^ MaxTemperatureWithCompression Usage $ export HADOOP_CLASSPATH = /Users/larry/JavaProject/out/artifacts/MaxTemperatureWithCompression/MaxTemperatureWithCompression.jar $ hadoop com.definitivehadoop.compression.MaxTemperatureWithCompression /Users/larry/JavaProject/resources/HadoopBook/ncdc/sample.txt output","title":"Using Compression in MapReduce"},{"location":"hadoop/ch5/#3-serialization","text":"See concepts of serialization and deserialization in Head First Java Chapter 14 . Serialization is the process of turning structured objects into a byte stream for transmission over a network or for writing to persistent storage. Deserialization is the reverse process of turning a byte stream back into a series of structured objects. Serialization is used in two quite distinct areas of distributed data processing: for interprocess communication and for persistent storage . In Hadoop, interprocess communication between nodes in the system is implemented using remote procedure calls (RPCs). The RPC protocol uses serialization to render the message into a binary stream to be sent to the remote node, which then deserializes the binary stream into the original message. In general, four desirable properties are crucial for an RPC serialization and persistent storage: Properties PRC Serialization Persistent Storage Compact makes the best use of network bandwidth make efficient use of storage space Fast little performance overhead little overhead in reading or writing Extensible meet new requirements transparently read data of older formats Interoperable support clients written in different languages read/write using different languages Hadoop uses its own serialization format, Writables , which is certainly compact and fast, but not so easy to extend or use from languages other than Java. Avro, a serialization system that was designed to overcome some of the limitations of Writables , is covered in Chapter 12 .","title":"3 Serialization"},{"location":"hadoop/ch5/#the-writable-interface","text":"The Writable interface defines two methods \u2014 one for writing its state to a DataOutput binary stream and one for reading its state from a DataInput binary stream (note: DataOutput and DataInput are also inferfaces): package org.apache.hadoop.io ; import java.io.DataOutput ; import java.io.DataInput ; import java.io.IOException ; public interface Writable { void write ( DataOutput out ) throws IOException ; void readFields ( DataInput in ) throws IOException ; }","title":"The Writable Interface"},{"location":"hadoop/ch5/#writable-classes","text":"Writable wrappers for Java primitives There are Writable wrappers for all the Java primitive types except char (which can be stored in an IntWritable ). All have a get() and set() method for retrieving and storing the wrapped value. When it comes to encoding integers, there is a choice between the fixed-length formats ( IntWritable and LongWritable ) and the variable-length formats ( VIntWritable and VLongWritable ). Fixed-length encodings are good when the distribution of values is fairly uniform across the whole value space, such as when using a (well-designed) hash function. Most numeric variables tend to have nonuniform distributions, though, and on average, the variable-length encoding will save space. Text Text is a Writable for UTF-8 sequences. It can be thought of as the Writable equivalent of java.lang.String . Indexing for the Text class is in terms of position in the encoded byte sequence, not the Unicode character in the string or the Java char code unit (as it is for String ). For ASCII strings, these three concepts of index position coincide. Another difference from String is that Text is mutable. You can reuse a Text instance by calling one of the set() methods on it. Text t = new Text ( hadoop ); t . set ( pig ); Text doesn\u2019t have as rich an API for manipulating java.lang.String , so in many cases, you need to convert the Text object to a String : Text ( hadoop ). toString ()","title":"Writable Classes"},{"location":"hadoop/ch5/#implementing-a-custom-writable","text":"Tip If you are considering writing a custom Writable , it may be worth trying another serialization framework, like Avro, that allows you to define custom types declaratively.","title":"Implementing a Custom Writable"},{"location":"hadoop/ch5/#serialization-frameworks","text":"Any type can be used to serialize, because Hadoop has an API for pluggable serialization frameworks, which is represented by an implementation of Serialization . For instance, WritableSerialization , is the implementation of Serialization for Writable types; AvroSerialization , is the implementation of Serialization for Avro types. public class WritableSerialization extends Configured implements Serialization Writable public abstract class AvroSerialization T extends Configured implements Serialization T A Serialization defines a mapping from types to Serializer instances (for turning an object into a byte stream) and Deserializer instances (for turning a byte stream into an object). public interface Serialization T { // Allows clients to test whether this Serialization // supports the given class. boolean accept ( Class ? c ); // @return a {@link Serializer} for the given class. Serializer T getSerializer ( Class T c ); //return a {@link Deserializer} for the given class. Deserializer T getDeserializer ( Class T c ); } Note Although it makes it convenient to be able to use standard Java types such as Integer or String in MapReduce programs, Java Object Serialization is not as efficient as Writables , so it\u2019s not worth making this trade-off. Serialization IDL Apache Thrift and Google Protocol Buffers are both popular serialization frameworks, and both are commonly used as a format for persistent binary data. Avro is an IDL-based serialization framework designed to work well with large-scale data processing in Hadoop.","title":"Serialization Frameworks"},{"location":"hadoop/ch5/#4-file-based-data-structures","text":"For some applications, you need a specialized data structure to hold your data. For doing MapReduce-based processing, putting each blob of binary data into its own file doesn\u2019t scale, so Hadoop developed a number of higher-level containers for these situations.","title":"4 File-Based Data Structures"},{"location":"hadoop/ch5/#sequencefile","text":"Hadoop\u2019s SequenceFile provides a persistent data structure for binary key-value pairs. It is suitable for a log file, where each log record is a new line of text. To use it as a logfile format, you would choose a key, such as timestamp represented by a LongWritable , and the value would be a Writable that represents the quantity being logged. Writing a SequenceFile To create a SequenceFile , use one of its createWriter() static methods, which return a SequenceFile.Writer instance. Then write key-value pairs using the append() method. When you\u2019ve finished, you call the close() method. Displaying a SequenceFile with the command-line interface The hadoop fs command has a -text option to display sequence files in textual form. % hadoop fs - text numbers . seq | head The SequenceFile format A sequence file(\u987a\u5e8f\u6587\u4ef6) consists of a header followed by one or more records. The sync marker(\u540c\u6b65\u6807\u8bc6) is used to allow a reader synchronize to a record boundary from any position in the file, which incurs less than a 1% storage overhead. The internal format of the records depends on whether compression is enabled, and if it is, whether it is record compression(\u8bb0\u5f55\u538b\u7f29) or block compression(\u5757\u538b\u7f29). The format for record compression is almost identical to that for no compression, except the value bytes are compressed using the codec defined in the header. Note that keys are not compressed. Block compression compresses multiple records at once; it is therefore more compact than and should generally be preferred over record compression because it has the opportunity to take advantage of similarities between records. A sync marker is written before the start of every block. The format of a block is a field indicating the number of records in the block, followed by four compressed fields: the key lengths, the keys, the value lengths, and the values.","title":"SequenceFile"},{"location":"hadoop/ch5/#mapfile","text":"A MapFile is a sorted SequenceFile with an index to permit lookups by key. The index is itself a SequenceFile that contains a fraction of the keys in the map.","title":"MapFile"},{"location":"hadoop/ch6/","text":"Hadoop: The Definitive Guide 6 - Developing a MapReduce Application 1 The Configuration API Components in Hadoop are configured using Hadoop\u2019s own configuration API. An instance of the org.apache.hadoop.conf.Configuration represents a collection of configuration properties and their values. Configuration s read their properties from XML files, which have a simple structure for defining name-value pairs. Note XML(E x tensible M arkup L anguage, \u53ef\u6269\u5c55\u6807\u8bb0\u8bed\u8a00), is a markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. Combining Resources When more than one resource is used to define a Configuration , properties added later override the earlier definitions. However, properties that are marked as final cannot be overridden in later definitions. Java Configuration conf = new Configuration (); conf . addResource ( configuration-1.xml ); conf . addResource ( configuration-2.xml ); assertThat ( conf . getInt ( size , 0 ), is ( 12 )); assertThat ( conf . get ( weight ), is ( heavy )); Configuration-1.xml ?xml version= 1.0 ? configuration property name color /name value yellow /value description Color /description /property property name size /name value 10 /value description Size /description /property property name weight /name value heavy /value final true /final description Weight /description /property property name size-weight /name value ${size},${weight} /value description Size and weight /description /property /configuration Configuration-2.xml ?xml version= 1.0 ? configuration property name size /name value 12 /value /property property name weight /name value light /value /property /configuration Variable Expansion Configuration properties can be defined in terms of other properties, or system properties. For example, the property size-weight in configuration-1.xml file is defined as $text {size}$, ${ weight } $ . 2 Setting Up the Development Environment The first step is to create a project so you can build MapReduce programs and run them in local (standalone) mode from the command line or within your IDE. Using the Maven POM to manage your project is an easy way to start. Specifically, for building MapReduce jobs, you only need to have the hadoop-client dependency, which contains all the Hadoop client-side classes needed to interact with HDFS and MapReduce. For running unit tests, use junit , and for writing MapReduce tests, use mrunit . The hadoop-minicluster library contains the \u201cmini-\u201d clusters that are useful for testing with Hadoop clusters running in a single JVM. Managing Configuration When developing Hadoop applications, it is common to switch between running the application locally and running it on a cluster. One way to accommodate these variations is to have different versions of Hadoop configuration files and use them with the -conf command-line switch. For example, the following command shows a directory listing on the HDFS server running in pseudodistributed mode on localhost: $ hadoop fs -conf conf/hadoop-localhost.xml -ls Another way of managing configuration settings is to copy the etc/hadoop directory from your Hadoop installation to another location, place the *-site.xml configuration files there (with appropriate settings), and set the HADOOP_CONF_DIR environment variable to the alternative location. The main advantage of this approach is that you don\u2019t need to specify -conf for every command. GenericOptionsParser, Tool, and ToolRunner It\u2019s more convenient to implement the Tool interface and run your application with the ToolRunner . ToolRunner uses GenericOptionsParser internally, which interprets common Hadoop command-line options and sets them on a Configuration object for your application to use as desired. // A tool interface that supports handling of generic command-line options. public interface Tool extends Configurable { int run ( String [] args ) throws Exception ; } Detailed examples, are \"Application to find the maximum temperature\"[ code ], \"MapReduce program to find the maximum temperature, creating Avro output\"[ code ]. 3 Writing a Unit Test with MRUnit MRUnit is a testing library that makes it easy to pass known inputs to a mapper or a reducer and check that the outputs are as expected. However, MRUnit is DEPRECATED !!! 4 Running Locally on Test Data Running a Job in a Local Job Runner Using the Tool interface, it\u2019s easy to write a driver to run our MapReduce job for finding the maximum temperature by year. public class MaxTemperatureDriver extends Configured implements Tool { @Override public int run ( String [] args ) throws Exception { if ( args . length != 2 ) { System . err . printf ( Usage: %s [generic options] input output \\n , getClass (). getSimpleName ()); ToolRunner . printGenericCommandUsage ( System . err ); return - 1 ; } Job job = new Job ( getConf (), Max temperature ); job . setJarByClass ( getClass ()); FileInputFormat . addInputPath ( job , new Path ( args [ 0 ])); FileOutputFormat . setOutputPath ( job , new Path ( args [ 1 ])); job . setMapperClass ( MaxTemperatureMapper . class ); job . setCombinerClass ( MaxTemperatureReducer . class ); job . setReducerClass ( MaxTemperatureReducer . class ); job . setOutputKeyClass ( Text . class ); job . setOutputValueClass ( IntWritable . class ); return job . waitForCompletion ( true ) ? 0 : 1 ; } public static void main ( String [] args ) throws Exception { int exitCode = ToolRunner . run ( new MaxTemperatureDriver (), args ); System . exit ( exitCode ); } } From the command line, we can run the driver by typing: Testing the Driver 5 Running on a Cluster 6 Tuning a Job 7 MapReduce Workflows","title":"Chapter 6: Developing a MapReduce Application"},{"location":"hadoop/ch6/#hadoop-the-definitive-guide-6-developing-a-mapreduce-application","text":"","title":"Hadoop: The Definitive Guide 6 - Developing a MapReduce Application"},{"location":"hadoop/ch6/#1-the-configuration-api","text":"Components in Hadoop are configured using Hadoop\u2019s own configuration API. An instance of the org.apache.hadoop.conf.Configuration represents a collection of configuration properties and their values. Configuration s read their properties from XML files, which have a simple structure for defining name-value pairs. Note XML(E x tensible M arkup L anguage, \u53ef\u6269\u5c55\u6807\u8bb0\u8bed\u8a00), is a markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable.","title":"1 The Configuration API"},{"location":"hadoop/ch6/#combining-resources","text":"When more than one resource is used to define a Configuration , properties added later override the earlier definitions. However, properties that are marked as final cannot be overridden in later definitions. Java Configuration conf = new Configuration (); conf . addResource ( configuration-1.xml ); conf . addResource ( configuration-2.xml ); assertThat ( conf . getInt ( size , 0 ), is ( 12 )); assertThat ( conf . get ( weight ), is ( heavy )); Configuration-1.xml ?xml version= 1.0 ? configuration property name color /name value yellow /value description Color /description /property property name size /name value 10 /value description Size /description /property property name weight /name value heavy /value final true /final description Weight /description /property property name size-weight /name value ${size},${weight} /value description Size and weight /description /property /configuration Configuration-2.xml ?xml version= 1.0 ? configuration property name size /name value 12 /value /property property name weight /name value light /value /property /configuration","title":"Combining Resources"},{"location":"hadoop/ch6/#variable-expansion","text":"Configuration properties can be defined in terms of other properties, or system properties. For example, the property size-weight in configuration-1.xml file is defined as $text {size}$, ${ weight } $ .","title":"Variable Expansion"},{"location":"hadoop/ch6/#2-setting-up-the-development-environment","text":"The first step is to create a project so you can build MapReduce programs and run them in local (standalone) mode from the command line or within your IDE. Using the Maven POM to manage your project is an easy way to start. Specifically, for building MapReduce jobs, you only need to have the hadoop-client dependency, which contains all the Hadoop client-side classes needed to interact with HDFS and MapReduce. For running unit tests, use junit , and for writing MapReduce tests, use mrunit . The hadoop-minicluster library contains the \u201cmini-\u201d clusters that are useful for testing with Hadoop clusters running in a single JVM.","title":"2 Setting Up the Development Environment"},{"location":"hadoop/ch6/#managing-configuration","text":"When developing Hadoop applications, it is common to switch between running the application locally and running it on a cluster. One way to accommodate these variations is to have different versions of Hadoop configuration files and use them with the -conf command-line switch. For example, the following command shows a directory listing on the HDFS server running in pseudodistributed mode on localhost: $ hadoop fs -conf conf/hadoop-localhost.xml -ls Another way of managing configuration settings is to copy the etc/hadoop directory from your Hadoop installation to another location, place the *-site.xml configuration files there (with appropriate settings), and set the HADOOP_CONF_DIR environment variable to the alternative location. The main advantage of this approach is that you don\u2019t need to specify -conf for every command.","title":"Managing Configuration"},{"location":"hadoop/ch6/#genericoptionsparser-tool-and-toolrunner","text":"It\u2019s more convenient to implement the Tool interface and run your application with the ToolRunner . ToolRunner uses GenericOptionsParser internally, which interprets common Hadoop command-line options and sets them on a Configuration object for your application to use as desired. // A tool interface that supports handling of generic command-line options. public interface Tool extends Configurable { int run ( String [] args ) throws Exception ; } Detailed examples, are \"Application to find the maximum temperature\"[ code ], \"MapReduce program to find the maximum temperature, creating Avro output\"[ code ].","title":"GenericOptionsParser, Tool, and ToolRunner"},{"location":"hadoop/ch6/#3-writing-a-unit-test-with-mrunit","text":"MRUnit is a testing library that makes it easy to pass known inputs to a mapper or a reducer and check that the outputs are as expected. However, MRUnit is DEPRECATED !!!","title":"3 Writing a Unit Test with MRUnit"},{"location":"hadoop/ch6/#4-running-locally-on-test-data","text":"","title":"4 Running Locally on Test Data"},{"location":"hadoop/ch6/#running-a-job-in-a-local-job-runner","text":"Using the Tool interface, it\u2019s easy to write a driver to run our MapReduce job for finding the maximum temperature by year. public class MaxTemperatureDriver extends Configured implements Tool { @Override public int run ( String [] args ) throws Exception { if ( args . length != 2 ) { System . err . printf ( Usage: %s [generic options] input output \\n , getClass (). getSimpleName ()); ToolRunner . printGenericCommandUsage ( System . err ); return - 1 ; } Job job = new Job ( getConf (), Max temperature ); job . setJarByClass ( getClass ()); FileInputFormat . addInputPath ( job , new Path ( args [ 0 ])); FileOutputFormat . setOutputPath ( job , new Path ( args [ 1 ])); job . setMapperClass ( MaxTemperatureMapper . class ); job . setCombinerClass ( MaxTemperatureReducer . class ); job . setReducerClass ( MaxTemperatureReducer . class ); job . setOutputKeyClass ( Text . class ); job . setOutputValueClass ( IntWritable . class ); return job . waitForCompletion ( true ) ? 0 : 1 ; } public static void main ( String [] args ) throws Exception { int exitCode = ToolRunner . run ( new MaxTemperatureDriver (), args ); System . exit ( exitCode ); } } From the command line, we can run the driver by typing:","title":"Running a Job in a Local Job Runner"},{"location":"hadoop/ch6/#testing-the-driver","text":"","title":"Testing the Driver"},{"location":"hadoop/ch6/#5-running-on-a-cluster","text":"","title":"5 Running on a Cluster"},{"location":"hadoop/ch6/#6-tuning-a-job","text":"","title":"6 Tuning a Job"},{"location":"hadoop/ch6/#7-mapreduce-workflows","text":"","title":"7 MapReduce Workflows"},{"location":"hadoop/ch7/","text":"Hadoop: The Definitive Guide 7 - How MapReduce Works 1 Anatomy of a MapReduce Job Run Job Submission Job Initialization Task Assignment Task Execution Progress and Status Updates Job Completion 2 Failures 3 Shuffle and Sort MapReduce makes the guarantee that the input to every reducer is sorted by key . The process by which the system performs the sort \u2014 and transfers the map outputs to the reducers as inputs \u2014 is known as the shuffle . In many ways, the shuffle is the heart of MapReduce and is where the \u201cmagic\u201d happens. The Map Side Each map task has a circular memory buffer that it writes the output to. When the contents of the buffer reach a certain threshold size, a background thread will start to spill the contents to disk. Before it writes to disk, the thread first divides the data into partitions corresponding to the reducers that they will ultimately be sent to. Within each partition, the background thread performs an in-memory sort by key, and if there is a combiner function, it is run on the output of the sort. Before the task is finished, the spill files are merged into a single partitioned and sorted output file. It is often a good idea to compress the map output as it is written to disk, because doing so makes it faster to write to disk, saves disk space, and reduces the amount of data to transfer to the reducer. The Reduce Side the copy phase of the reduce task: The map tasks may finish at different times, so the reduce task starts copying their outputs as soon as each completes. Map outputs are copied to the reduce task JVM\u2019s memory if they are small enough; otherwise, they are copied to disk. As the copies accumulate on disk, a background thread merges them into larger, sorted files. This saves some time merging later on. When all the map outputs have been copied, the reduce task moves into the sort phase (which should properly be called the merge phase , as the sorting was carried out on the map side), which merges the map outputs, maintaining their sort ordering. This is done in rounds.(Figure below) For the last merge, directly feeding the reduce function in what is the last phase: the reduce phase. The output of the last phase is written directly to the output filesystem, typically HDFS. Configuration Tuning 4 Task Execution","title":"Chapter 7: How MapReduce Works"},{"location":"hadoop/ch7/#hadoop-the-definitive-guide-7-how-mapreduce-works","text":"","title":"Hadoop: The Definitive Guide 7 - How MapReduce Works"},{"location":"hadoop/ch7/#1-anatomy-of-a-mapreduce-job-run","text":"","title":"1 Anatomy of a MapReduce Job Run"},{"location":"hadoop/ch7/#job-submission","text":"","title":"Job Submission"},{"location":"hadoop/ch7/#job-initialization","text":"","title":"Job Initialization"},{"location":"hadoop/ch7/#task-assignment","text":"","title":"Task Assignment"},{"location":"hadoop/ch7/#task-execution","text":"","title":"Task Execution"},{"location":"hadoop/ch7/#progress-and-status-updates","text":"","title":"Progress and Status Updates"},{"location":"hadoop/ch7/#job-completion","text":"","title":"Job Completion"},{"location":"hadoop/ch7/#2-failures","text":"","title":"2 Failures"},{"location":"hadoop/ch7/#3-shuffle-and-sort","text":"MapReduce makes the guarantee that the input to every reducer is sorted by key . The process by which the system performs the sort \u2014 and transfers the map outputs to the reducers as inputs \u2014 is known as the shuffle . In many ways, the shuffle is the heart of MapReduce and is where the \u201cmagic\u201d happens.","title":"3 Shuffle and Sort"},{"location":"hadoop/ch7/#the-map-side","text":"Each map task has a circular memory buffer that it writes the output to. When the contents of the buffer reach a certain threshold size, a background thread will start to spill the contents to disk. Before it writes to disk, the thread first divides the data into partitions corresponding to the reducers that they will ultimately be sent to. Within each partition, the background thread performs an in-memory sort by key, and if there is a combiner function, it is run on the output of the sort. Before the task is finished, the spill files are merged into a single partitioned and sorted output file. It is often a good idea to compress the map output as it is written to disk, because doing so makes it faster to write to disk, saves disk space, and reduces the amount of data to transfer to the reducer.","title":"The Map Side"},{"location":"hadoop/ch7/#the-reduce-side","text":"the copy phase of the reduce task: The map tasks may finish at different times, so the reduce task starts copying their outputs as soon as each completes. Map outputs are copied to the reduce task JVM\u2019s memory if they are small enough; otherwise, they are copied to disk. As the copies accumulate on disk, a background thread merges them into larger, sorted files. This saves some time merging later on. When all the map outputs have been copied, the reduce task moves into the sort phase (which should properly be called the merge phase , as the sorting was carried out on the map side), which merges the map outputs, maintaining their sort ordering. This is done in rounds.(Figure below) For the last merge, directly feeding the reduce function in what is the last phase: the reduce phase. The output of the last phase is written directly to the output filesystem, typically HDFS.","title":"The Reduce Side"},{"location":"hadoop/ch7/#configuration-tuning","text":"","title":"Configuration Tuning"},{"location":"hadoop/ch7/#4-task-execution","text":"","title":"4 Task Execution"},{"location":"hadoop/ch8/","text":"Hadoop: The Definitive Guide 8 - MapReduce Types and Formats","title":"Chapter 8: MapReduce Types and Formats"},{"location":"hadoop/ch8/#hadoop-the-definitive-guide-8-mapreduce-types-and-formats","text":"","title":"Hadoop: The Definitive Guide 8 - MapReduce Types and Formats"},{"location":"hadoop/ch9/","text":"Hadoop: The Definitive Guide 9 - MapReduce Features","title":"Chapter 9: MapReduce Features"},{"location":"hadoop/ch9/#hadoop-the-definitive-guide-9-mapreduce-features","text":"","title":"Hadoop: The Definitive Guide 9 - MapReduce Features"},{"location":"prob/","text":"PROB Chapter 1: \u6837\u672c\u7a7a\u95f4\u548c\u6982\u7387","title":"Contents"},{"location":"prob/#prob","text":"Chapter 1: \u6837\u672c\u7a7a\u95f4\u548c\u6982\u7387","title":"PROB"},{"location":"prob/ch1/","text":"\u6982\u7387\u5bfc\u8bba 1 - \u6837\u672c\u7a7a\u95f4\u548c\u6982\u7387 1 \u96c6\u5408 \u5c06\u4e00\u4e9b\u7814\u7a76\u5bf9\u8c61\u653e\u5728\u4e00\u8d77\uff0c\u5f62\u6210 \u96c6\u5408 \uff0c\u800c\u8fd9\u4e9b\u5bf9\u8c61\u5c31\u79f0\u4e3a\u96c6\u5408\u7684 \u5143\u7d20 \u3002\u8bbe S S \u662f\u4e00\u4e2a\u96c6\u5408\uff0c x x \u662f S S \u7684\u5143\u7d20\uff0c\u6211\u4eec\u5c06\u5143\u7d20\u548c\u96c6\u5408\u7684\u8fd9\u79cd\u5173\u7cfb\u5199\u6210 x\\in S x\\in S \u3002\u82e5 x x \u4e0d\u662f S S \u7684\u5143\u7d20\uff0c\u5c31\u5199\u6210 x \\notin S x \\notin S . \u4e00\u4e2a\u96c6\u5408\u53ef\u4ee5\u6ca1\u6709\u5143\u7d20\uff0c\u8fd9\u4e2a\u7279\u6b8a\u7684\u96c6\u5408\u5c31\u79f0\u4e3a \u7a7a\u96c6 \u3002 \u5c06\u6211\u4eec\u611f\u5174\u8da3\u7684\u6240\u6709\u5143\u7d20\u653e\u5728\u4e00\u8d77\uff0c\u5f62\u6210\u4e00\u4e2a\u96c6\u5408\uff0c\u8fd9\u4e2a\u96c6\u5408\u79f0\u4e3a \u7a7a\u95f4 \uff0c\u8bb0\u505a \\Omega \\Omega \u3002\u5f53 \\Omega \\Omega \u786e\u5b9a\u4ee5\u540e\uff0c\u6211\u4eec\u6240\u8ba8\u8bba\u7684\u96c6\u5408 S S \u90fd\u662f \\Omega \\Omega \u7684\u5b50\u96c6\u3002 2 \u6982\u7387\u6a21\u578b \u6982\u7387\u6a21\u578b\u662f\u5bf9\u4e0d\u786e\u5b9a\u73b0\u8c61\u7684\u6570\u5b66\u63cf\u8ff0\u3002\u6982\u7387\u6a21\u578b\u7684\u57fa\u672c\u6784\u6210\uff1a \u6837\u672c\u7a7a\u95f4 \\Omega \\Omega \uff0c\u8fd9\u662f\u4e00\u4e2a\u8bd5\u9a8c\u7684\u6240\u6709\u53ef\u80fd\u7ed3\u679c\u7684\u96c6\u5408\u3002 \u6982\u7387\u5f8b \uff0c\u6982\u7387\u5f8b\u4e3a\u8bd5\u9a8c\u7ed3\u679c\u7684\u96c6\u5408A\uff08\u79f0\u4e4b\u4e3a \u4e8b\u4ef6 \uff09\u786e\u5b9a\u4e00\u4e2a\u975e\u8d1f\u6570 P(A) P(A) \uff08\u79f0\u4e3a\u4e8b\u4ef6A\u7684 \u6982\u7387 )\u3002 \u6982\u7387\u5f8b \u6982\u7387\u5f8b\u786e\u5b9a\u4e86\u4efb\u4f55\u7ed3\u679c\u6216\u8005\u4efb\u4f55\u7ed3\u679c\u7684\u96c6\u5408(\u79f0\u4e4b\u4e3a\u4e8b\u4ef6)\u7684\u4f3c\u7136\u7a0b\u5ea6\u3002\u66f4\u7cbe\u786e\u4e00\u70b9\u8bf4\uff0c\u5b83\u7ed9\u6bcf\u4e00\u4e2a\u4e8b\u4ef6 A A \uff0c\u786e\u5b9a\u4e00\u4e2a\u6570 P(A) P(A) \uff0c\u79f0\u4e4b\u4e3a\u4e8b\u4ef6 A A \u7684\u6982\u7387\u3002 \u6982\u7387\u516c\u7406(Probability axioms): \u975e\u8d1f\u6027: \u5bf9\u4e00\u5207\u4e8b\u4ef6 A A \uff0c\u6ee1\u8db3 P(A)\\ge 0 P(A)\\ge 0 \u53ef\u52a0\u6027\uff1a \u82e5 A A \u548c B B \u4e3a\u4e92\u4e0d\u76f8\u5bb9\u7684\u4e8b\u4ef6\uff0c\u5219\u5b83\u4eec\u7684\u5e76\u6ee1\u8db3 P(A \\cup B) = P(A)+P(B) P(A \\cup B) = P(A)+P(B) \u5f52\u4e00\u5316\uff1a\u6574\u4e2a\u6837\u672c\u7a7a\u95f4 \\Omega \\Omega \u7684\u6982\u7387\u4e3a1\uff0c\u5373 P(\\Omega)=1 P(\\Omega)=1 \u79bb\u6563\u6a21\u578b \u79bb\u6563\u6982\u7387\u5f8b \uff1a\u8bbe\u6837\u672c\u7a7a\u95f4\u7531\u6709\u9650\u4e2a\u53ef\u80fd\u7684\u7ed3\u679c\u7ec4\u6210\uff0c\u5219\u4e8b\u4ef6\u7684\u6982\u7387\u53ef\u7531\u7ec4\u6210\u8fd9\u4e2a\u4e8b\u4ef6\u7684\u8bd5\u9a8c\u7ed3\u679c\u7684\u6982\u7387\u6240\u51b3\u5b9a\u3002\u4e8b\u4ef6 \\{s_1, s_2,...,s_n\\} \\{s_1, s_2,...,s_n\\} \u7684\u6982\u7387\u662f P(s_i) P(s_i) \u4e4b\u548c\uff0c\u5373 P(\\{s_1,s_2,...,s_n\\}) = P(s_1)+P(s_1)P(s_2)+P(s_n) P(\\{s_1,s_2,...,s_n\\}) = P(s_1)+P(s_1)P(s_2)+P(s_n) \u79bb\u6563\u5747\u5300\u6982\u7387\u5f8b(\u53e4\u5178\u6982\u578b) \uff1a\u8bbe\u6837\u672c\u7a7a\u95f4\u7531 n n \u4e2a\u7b49\u53ef\u80fd\u6027\u7684\u8bd5\u9a8c\u7ed3\u679c\u7ec4\u6210\uff0c\u56e0\u6b64\u6bcf\u4e2a\u8bd5\u9a8c\u7ed3\u679c\u7ec4\u6210\u7684\u4e8b\u4ef6(\u79f0\u4e3a\u57fa\u672c\u4e8b\u4ef6)\u7684\u6982\u7387\u662f\u76f8\u7b49\u7684\u3002\u7531\u6b64\u5f97\u5230 P(A) = \\frac{\\text{\u542b\u4e8e\u4e8b\u4ef6A\u7684\u8bd5\u9a8c\u7ed3\u679c\u6570}}{n} P(A) = \\frac{\\text{\u542b\u4e8e\u4e8b\u4ef6A\u7684\u8bd5\u9a8c\u7ed3\u679c\u6570}}{n} 3 \u6761\u4ef6\u6982\u7387 \u7ed9\u5b9a B B \u53d1\u751f\u4e4b\u4e0b\u4e8b\u4ef6 A A \u7684\u6761\u4ef6\u6982\u7387\uff0c\u8bb0\u505a P(A|B) P(A|B) , P(A|B) = \\frac{P(A\\cap B)}{P(B)} P(A|B) = \\frac{P(A\\cap B)}{P(B)} \u6761\u4ef6\u6982\u7387\u6ee1\u8db3\u6982\u7387\u76843\u6761\u516c\u7406 \u975e\u8d1f\u6027\u548c\u5f52\u4e00\u5316\u662f\u660e\u663e\u7684 \u53ef\u52a0\u6027\uff1a P(A_1\\cup A_2|B) = P(A_1|B) + P(A_2|B) P(A_1\\cup A_2|B) = P(A_1|B) + P(A_2|B) 4 \u5168\u6982\u7387\u5b9a\u7406\u548c\u8d1d\u53f6\u65af\u51c6\u5219 \u5168\u6982\u7387\u5b9a\u7406 \uff1a\u8bbe A_1, A_2,..., A_n A_1, A_2,..., A_n \u662f\u4e00\u7ec4\u4e92\u4e0d\u76f8\u5bb9\u7684\u4e8b\u4ef6\uff0c\u5b83\u5f62\u6210\u6837\u672c\u7a7a\u95f4\u7684\u4e00\u4e2a\u5206\u5272(\u6bcf\u4e00\u4e2a\u8bd5\u9a8c\u7ed3\u679c\u5fc5\u5b9a\u4f7f\u5f97\u5176\u4e2d\u4e00\u4e2a\u4e8b\u4ef6\u53d1\u751f!). \u53c8\u5047\u5b9a\u5bf9\u6bcf\u4e00\u4e2a i, P(A_i) 0 i, P(A_i)>0 . \u5219\u5bf9\u4e8e\u4efb\u4f55\u4e8b\u4ef6 B B \uff0c\u4e0b\u5217\u516c\u5f0f\u6210\u7acb\uff1a $$P(B) = P(A_1\\cap B) + ...+ P(A_n\\cap B) = P(A_1)P(B|A_1) + ...+ P(A_n)P(B|A_n)=\\sum P(B|A_i)P(A_i) $$ \u8d1d\u53f6\u65af\u51c6\u5219 \uff1a\u8bbe A_1, A_2,..., A_n A_1, A_2,..., A_n \u662f\u4e00\u7ec4\u4e92\u4e0d\u76f8\u5bb9\u7684\u4e8b\u4ef6\uff0c\u5b83\u5f62\u6210\u6837\u672c\u7a7a\u95f4\u7684\u4e00\u4e2a\u5206\u5272(\u6bcf\u4e00\u4e2a\u8bd5\u9a8c\u7ed3\u679c\u5fc5\u5b9a\u4f7f\u5f97\u5176\u4e2d\u4e00\u4e2a\u4e8b\u4ef6\u53d1\u751f!). \u53c8\u5047\u5b9a\u5bf9\u6bcf\u4e00\u4e2a i, P(A_i) 0 i, P(A_i)>0 . \u5219\u5bf9\u4e8e\u4efb\u4f55\u4e8b\u4ef6 B B \uff0c\u53ea\u8981\u5b83\u6ee1\u8db3 P(B) 0 P(B)>0 \uff0c\u4e0b\u5217\u516c\u5f0f\u6210\u7acb\uff1a P(A_i|B) = \\frac{P(A_i)P(B|A_i)}{P(B)} =\\frac{P(A_i)P(B|A_i)}{\\sum P(A_i)P(B|A_i)} P(A_i|B) = \\frac{P(A_i)P(B|A_i)}{P(B)} =\\frac{P(A_i)P(B|A_i)}{\\sum P(A_i)P(B|A_i)} \u4e3a\u8bc1\u660e\u8d1d\u53f6\u65af\u51c6\u5219\uff0c\u53ea\u9700\u6ce8\u610f\u5230 P(A_i)P(B|A_i) P(A_i)P(B|A_i) \u4e0e P(A_i|B)P(B) P(A_i|B)P(B) \u662f\u76f8\u7b49\u7684\uff0c\u5b83\u4eec\u90fd\u7b49\u4e8e P(A_i\\cap B) P(A_i\\cap B) \u3002 P(A_i|B) P(A_i|B) \u4e3a\u7531\u4e8e\u4ee3\u8868\u65b0\u8fd1\u5f97\u5230\u7684\u4fe1\u606f B B \u4e4b\u540e A_i A_i \u51fa\u73b0\u7684\u6982\u7387\uff0c\u79f0\u4e4b\u4e3a \u540e\u9a8c\u6982\u7387 \uff0c\u800c\u539f\u6765\u7684 P(A_i) P(A_i) \u5c31\u79f0\u4e3a \u5148\u9a8c\u6982\u7387 \u3002 5 \u72ec\u7acb\u6027 \u5982\u679c P(A|B)=P(A) P(A|B)=P(A) \uff0c\u5219\u79f0\u4e8b\u4ef6 A A \u72ec\u7acb \u4e8e\u4e8b\u4ef6 B B \u3002\u5373\u4e8b\u4ef6 B B \u7684\u53d1\u751f\u5e76\u6ca1\u6709\u7ed9\u4e8b\u4ef6 A A \u5e26\u6765\u65b0\u7684\u4fe1\u606f\uff0c\u6ca1\u6709\u6539\u53d8\u4e8b\u4ef6 A A \u53d1\u751f\u7684\u6982\u7387\u3002 \u6761\u4ef6\u72ec\u7acb\uff1a\u7279\u522b\u5730\uff0c\u5728\u7ed9\u5b9a C C \u4e4b\u4e0b\uff0c\u82e5\u4e8b\u4ef6 A A \u548c\u4e8b\u4ef6 B B \u6ee1\u8db3 P(A\\cap B|C) = P(A|C)P(B|C) P(A\\cap B|C) = P(A|C)P(B|C) \uff0c\u5219\u79f0 A A \u548c B B \u5728\u7ed9\u5b9a C C \u4e4b\u4e0b \u6761\u4ef6\u72ec\u7acb \u3002","title":"Chapter 1: \u6837\u672c\u7a7a\u95f4\u548c\u6982\u7387"},{"location":"prob/ch1/#1-","text":"","title":"\u6982\u7387\u5bfc\u8bba 1 - \u6837\u672c\u7a7a\u95f4\u548c\u6982\u7387"},{"location":"prob/ch1/#1","text":"\u5c06\u4e00\u4e9b\u7814\u7a76\u5bf9\u8c61\u653e\u5728\u4e00\u8d77\uff0c\u5f62\u6210 \u96c6\u5408 \uff0c\u800c\u8fd9\u4e9b\u5bf9\u8c61\u5c31\u79f0\u4e3a\u96c6\u5408\u7684 \u5143\u7d20 \u3002\u8bbe S S \u662f\u4e00\u4e2a\u96c6\u5408\uff0c x x \u662f S S \u7684\u5143\u7d20\uff0c\u6211\u4eec\u5c06\u5143\u7d20\u548c\u96c6\u5408\u7684\u8fd9\u79cd\u5173\u7cfb\u5199\u6210 x\\in S x\\in S \u3002\u82e5 x x \u4e0d\u662f S S \u7684\u5143\u7d20\uff0c\u5c31\u5199\u6210 x \\notin S x \\notin S . \u4e00\u4e2a\u96c6\u5408\u53ef\u4ee5\u6ca1\u6709\u5143\u7d20\uff0c\u8fd9\u4e2a\u7279\u6b8a\u7684\u96c6\u5408\u5c31\u79f0\u4e3a \u7a7a\u96c6 \u3002 \u5c06\u6211\u4eec\u611f\u5174\u8da3\u7684\u6240\u6709\u5143\u7d20\u653e\u5728\u4e00\u8d77\uff0c\u5f62\u6210\u4e00\u4e2a\u96c6\u5408\uff0c\u8fd9\u4e2a\u96c6\u5408\u79f0\u4e3a \u7a7a\u95f4 \uff0c\u8bb0\u505a \\Omega \\Omega \u3002\u5f53 \\Omega \\Omega \u786e\u5b9a\u4ee5\u540e\uff0c\u6211\u4eec\u6240\u8ba8\u8bba\u7684\u96c6\u5408 S S \u90fd\u662f \\Omega \\Omega \u7684\u5b50\u96c6\u3002","title":"1 \u96c6\u5408"},{"location":"prob/ch1/#2","text":"\u6982\u7387\u6a21\u578b\u662f\u5bf9\u4e0d\u786e\u5b9a\u73b0\u8c61\u7684\u6570\u5b66\u63cf\u8ff0\u3002\u6982\u7387\u6a21\u578b\u7684\u57fa\u672c\u6784\u6210\uff1a \u6837\u672c\u7a7a\u95f4 \\Omega \\Omega \uff0c\u8fd9\u662f\u4e00\u4e2a\u8bd5\u9a8c\u7684\u6240\u6709\u53ef\u80fd\u7ed3\u679c\u7684\u96c6\u5408\u3002 \u6982\u7387\u5f8b \uff0c\u6982\u7387\u5f8b\u4e3a\u8bd5\u9a8c\u7ed3\u679c\u7684\u96c6\u5408A\uff08\u79f0\u4e4b\u4e3a \u4e8b\u4ef6 \uff09\u786e\u5b9a\u4e00\u4e2a\u975e\u8d1f\u6570 P(A) P(A) \uff08\u79f0\u4e3a\u4e8b\u4ef6A\u7684 \u6982\u7387 )\u3002","title":"2 \u6982\u7387\u6a21\u578b"},{"location":"prob/ch1/#_1","text":"\u6982\u7387\u5f8b\u786e\u5b9a\u4e86\u4efb\u4f55\u7ed3\u679c\u6216\u8005\u4efb\u4f55\u7ed3\u679c\u7684\u96c6\u5408(\u79f0\u4e4b\u4e3a\u4e8b\u4ef6)\u7684\u4f3c\u7136\u7a0b\u5ea6\u3002\u66f4\u7cbe\u786e\u4e00\u70b9\u8bf4\uff0c\u5b83\u7ed9\u6bcf\u4e00\u4e2a\u4e8b\u4ef6 A A \uff0c\u786e\u5b9a\u4e00\u4e2a\u6570 P(A) P(A) \uff0c\u79f0\u4e4b\u4e3a\u4e8b\u4ef6 A A \u7684\u6982\u7387\u3002 \u6982\u7387\u516c\u7406(Probability axioms): \u975e\u8d1f\u6027: \u5bf9\u4e00\u5207\u4e8b\u4ef6 A A \uff0c\u6ee1\u8db3 P(A)\\ge 0 P(A)\\ge 0 \u53ef\u52a0\u6027\uff1a \u82e5 A A \u548c B B \u4e3a\u4e92\u4e0d\u76f8\u5bb9\u7684\u4e8b\u4ef6\uff0c\u5219\u5b83\u4eec\u7684\u5e76\u6ee1\u8db3 P(A \\cup B) = P(A)+P(B) P(A \\cup B) = P(A)+P(B) \u5f52\u4e00\u5316\uff1a\u6574\u4e2a\u6837\u672c\u7a7a\u95f4 \\Omega \\Omega \u7684\u6982\u7387\u4e3a1\uff0c\u5373 P(\\Omega)=1 P(\\Omega)=1","title":"\u6982\u7387\u5f8b"},{"location":"prob/ch1/#_2","text":"\u79bb\u6563\u6982\u7387\u5f8b \uff1a\u8bbe\u6837\u672c\u7a7a\u95f4\u7531\u6709\u9650\u4e2a\u53ef\u80fd\u7684\u7ed3\u679c\u7ec4\u6210\uff0c\u5219\u4e8b\u4ef6\u7684\u6982\u7387\u53ef\u7531\u7ec4\u6210\u8fd9\u4e2a\u4e8b\u4ef6\u7684\u8bd5\u9a8c\u7ed3\u679c\u7684\u6982\u7387\u6240\u51b3\u5b9a\u3002\u4e8b\u4ef6 \\{s_1, s_2,...,s_n\\} \\{s_1, s_2,...,s_n\\} \u7684\u6982\u7387\u662f P(s_i) P(s_i) \u4e4b\u548c\uff0c\u5373 P(\\{s_1,s_2,...,s_n\\}) = P(s_1)+P(s_1)P(s_2)+P(s_n) P(\\{s_1,s_2,...,s_n\\}) = P(s_1)+P(s_1)P(s_2)+P(s_n) \u79bb\u6563\u5747\u5300\u6982\u7387\u5f8b(\u53e4\u5178\u6982\u578b) \uff1a\u8bbe\u6837\u672c\u7a7a\u95f4\u7531 n n \u4e2a\u7b49\u53ef\u80fd\u6027\u7684\u8bd5\u9a8c\u7ed3\u679c\u7ec4\u6210\uff0c\u56e0\u6b64\u6bcf\u4e2a\u8bd5\u9a8c\u7ed3\u679c\u7ec4\u6210\u7684\u4e8b\u4ef6(\u79f0\u4e3a\u57fa\u672c\u4e8b\u4ef6)\u7684\u6982\u7387\u662f\u76f8\u7b49\u7684\u3002\u7531\u6b64\u5f97\u5230 P(A) = \\frac{\\text{\u542b\u4e8e\u4e8b\u4ef6A\u7684\u8bd5\u9a8c\u7ed3\u679c\u6570}}{n} P(A) = \\frac{\\text{\u542b\u4e8e\u4e8b\u4ef6A\u7684\u8bd5\u9a8c\u7ed3\u679c\u6570}}{n}","title":"\u79bb\u6563\u6a21\u578b"},{"location":"prob/ch1/#3","text":"\u7ed9\u5b9a B B \u53d1\u751f\u4e4b\u4e0b\u4e8b\u4ef6 A A \u7684\u6761\u4ef6\u6982\u7387\uff0c\u8bb0\u505a P(A|B) P(A|B) , P(A|B) = \\frac{P(A\\cap B)}{P(B)} P(A|B) = \\frac{P(A\\cap B)}{P(B)} \u6761\u4ef6\u6982\u7387\u6ee1\u8db3\u6982\u7387\u76843\u6761\u516c\u7406 \u975e\u8d1f\u6027\u548c\u5f52\u4e00\u5316\u662f\u660e\u663e\u7684 \u53ef\u52a0\u6027\uff1a P(A_1\\cup A_2|B) = P(A_1|B) + P(A_2|B) P(A_1\\cup A_2|B) = P(A_1|B) + P(A_2|B)","title":"3  \u6761\u4ef6\u6982\u7387"},{"location":"prob/ch1/#4","text":"\u5168\u6982\u7387\u5b9a\u7406 \uff1a\u8bbe A_1, A_2,..., A_n A_1, A_2,..., A_n \u662f\u4e00\u7ec4\u4e92\u4e0d\u76f8\u5bb9\u7684\u4e8b\u4ef6\uff0c\u5b83\u5f62\u6210\u6837\u672c\u7a7a\u95f4\u7684\u4e00\u4e2a\u5206\u5272(\u6bcf\u4e00\u4e2a\u8bd5\u9a8c\u7ed3\u679c\u5fc5\u5b9a\u4f7f\u5f97\u5176\u4e2d\u4e00\u4e2a\u4e8b\u4ef6\u53d1\u751f!). \u53c8\u5047\u5b9a\u5bf9\u6bcf\u4e00\u4e2a i, P(A_i) 0 i, P(A_i)>0 . \u5219\u5bf9\u4e8e\u4efb\u4f55\u4e8b\u4ef6 B B \uff0c\u4e0b\u5217\u516c\u5f0f\u6210\u7acb\uff1a $$P(B) = P(A_1\\cap B) + ...+ P(A_n\\cap B) = P(A_1)P(B|A_1) + ...+ P(A_n)P(B|A_n)=\\sum P(B|A_i)P(A_i) $$ \u8d1d\u53f6\u65af\u51c6\u5219 \uff1a\u8bbe A_1, A_2,..., A_n A_1, A_2,..., A_n \u662f\u4e00\u7ec4\u4e92\u4e0d\u76f8\u5bb9\u7684\u4e8b\u4ef6\uff0c\u5b83\u5f62\u6210\u6837\u672c\u7a7a\u95f4\u7684\u4e00\u4e2a\u5206\u5272(\u6bcf\u4e00\u4e2a\u8bd5\u9a8c\u7ed3\u679c\u5fc5\u5b9a\u4f7f\u5f97\u5176\u4e2d\u4e00\u4e2a\u4e8b\u4ef6\u53d1\u751f!). \u53c8\u5047\u5b9a\u5bf9\u6bcf\u4e00\u4e2a i, P(A_i) 0 i, P(A_i)>0 . \u5219\u5bf9\u4e8e\u4efb\u4f55\u4e8b\u4ef6 B B \uff0c\u53ea\u8981\u5b83\u6ee1\u8db3 P(B) 0 P(B)>0 \uff0c\u4e0b\u5217\u516c\u5f0f\u6210\u7acb\uff1a P(A_i|B) = \\frac{P(A_i)P(B|A_i)}{P(B)} =\\frac{P(A_i)P(B|A_i)}{\\sum P(A_i)P(B|A_i)} P(A_i|B) = \\frac{P(A_i)P(B|A_i)}{P(B)} =\\frac{P(A_i)P(B|A_i)}{\\sum P(A_i)P(B|A_i)} \u4e3a\u8bc1\u660e\u8d1d\u53f6\u65af\u51c6\u5219\uff0c\u53ea\u9700\u6ce8\u610f\u5230 P(A_i)P(B|A_i) P(A_i)P(B|A_i) \u4e0e P(A_i|B)P(B) P(A_i|B)P(B) \u662f\u76f8\u7b49\u7684\uff0c\u5b83\u4eec\u90fd\u7b49\u4e8e P(A_i\\cap B) P(A_i\\cap B) \u3002 P(A_i|B) P(A_i|B) \u4e3a\u7531\u4e8e\u4ee3\u8868\u65b0\u8fd1\u5f97\u5230\u7684\u4fe1\u606f B B \u4e4b\u540e A_i A_i \u51fa\u73b0\u7684\u6982\u7387\uff0c\u79f0\u4e4b\u4e3a \u540e\u9a8c\u6982\u7387 \uff0c\u800c\u539f\u6765\u7684 P(A_i) P(A_i) \u5c31\u79f0\u4e3a \u5148\u9a8c\u6982\u7387 \u3002","title":"4 \u5168\u6982\u7387\u5b9a\u7406\u548c\u8d1d\u53f6\u65af\u51c6\u5219"},{"location":"prob/ch1/#5","text":"\u5982\u679c P(A|B)=P(A) P(A|B)=P(A) \uff0c\u5219\u79f0\u4e8b\u4ef6 A A \u72ec\u7acb \u4e8e\u4e8b\u4ef6 B B \u3002\u5373\u4e8b\u4ef6 B B \u7684\u53d1\u751f\u5e76\u6ca1\u6709\u7ed9\u4e8b\u4ef6 A A \u5e26\u6765\u65b0\u7684\u4fe1\u606f\uff0c\u6ca1\u6709\u6539\u53d8\u4e8b\u4ef6 A A \u53d1\u751f\u7684\u6982\u7387\u3002 \u6761\u4ef6\u72ec\u7acb\uff1a\u7279\u522b\u5730\uff0c\u5728\u7ed9\u5b9a C C \u4e4b\u4e0b\uff0c\u82e5\u4e8b\u4ef6 A A \u548c\u4e8b\u4ef6 B B \u6ee1\u8db3 P(A\\cap B|C) = P(A|C)P(B|C) P(A\\cap B|C) = P(A|C)P(B|C) \uff0c\u5219\u79f0 A A \u548c B B \u5728\u7ed9\u5b9a C C \u4e4b\u4e0b \u6761\u4ef6\u72ec\u7acb \u3002","title":"5 \u72ec\u7acb\u6027"},{"location":"projects/","text":"Projects SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406","title":"Contents"},{"location":"projects/#projects","text":"SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406","title":"Projects"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/","text":"Spark Streaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee \u8be5\u9879\u76ee\u4ece\u5b9e\u65f6\u6570\u636e\u4ea7\u751f\u548c\u6d41\u5411\u7684\u4e0d\u540c\u73af\u8282\u51fa\u53d1\uff0c\u901a\u8fc7\u96c6\u6210\u4e3b\u6d41\u7684\u5206\u5e03\u5f0f\u65e5\u5fd7\u6536\u96c6\u6846\u67b6Flume\u3001\u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217Kafka\u3001\u5206\u5e03\u5f0f\u5217\u5f0f\u6570\u636e\u5e93HBase\u3001\u4ee5\u53caSpark Streaming\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5904\u7406\u3002 1 \u521d\u8bc6\u5b9e\u65f6\u6d41\u5904\u7406 \u4e1a\u52a1\u73b0\u72b6\u5206\u6790 \u9700\u6c42\uff1a\u7edf\u8ba1\u4e3b\u7ad9\u6bcf\u4e2a\uff08\u6307\u5b9a\uff09\u8bfe\u7a0b\u8bbf\u95ee\u7684\u5ba2\u6237\u7aef\u3001\u5730\u57df\u4fe1\u606f\u5206\u5e03 == \u5982\u4e0a\u4e24\u4e2a\u64cd\u4f5c\uff1a\u91c7\u7528\u79bb\u7ebf\uff08spark/mapreduce\uff09\u7684\u65b9\u5f0f\u8fdb\u884c\u7edf\u8ba1 \u5b9e\u73b0\u6b65\u9aa4\uff1a \u8bfe\u7a0b\u7f16\u53f7\uff0cip\u4fe1\u606f\uff0cuser-agent \u8fdb\u884c\u76f8\u5e94\u7684\u7edf\u8ba1\u5206\u6790\u64cd\u4f5c\uff1aMapReduce/Spark \u9879\u76ee\u67b6\u6784\uff1a \u65e5\u5fd7\u6536\u96c6\uff1aFlume \u79bb\u7ebf\u5206\u6790\uff1aMapReduce/Spark \u7edf\u8ba1\u7ed3\u679c\u56fe\u5f62\u5316\u5c55\u793a \u95ee\u9898\uff1a \u5c0f\u65f6\u7ea7\u522b 10\u5206\u949f \u79d2\u7ea7\u522b \u5b9e\u65f6\u6d41\u5904\u7406\u4ea7\u751f\u80cc\u666f \u65f6\u6548\u6027\u9ad8 \u6570\u636e\u91cf\u5927 \u5b9e\u65f6\u6d41\u5904\u7406\u6982\u8ff0 https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101 \u5b9e\u65f6\u8ba1\u7b97 apache storm \u6d41\u5f0f\u8ba1\u7b97 \u5b9e\u65f6\u6d41\u5f0f\u8ba1\u7b97 \u79bb\u7ebf\u8ba1\u7b97\u4e0e\u5b9e\u65f6\u8ba1\u7b97\u5bf9\u6bd4 \u6570\u636e\u6765\u6e90 \u79bb\u7ebf\uff1a\u6765\u81eaHDFS\u4e0a\u7684\u5386\u53f2\u6570\u636e\uff0c\u6570\u636e\u91cf\u6bd4\u8f83\u5927 \u5b9e\u65f6\uff1a\u6765\u81ea\u6d88\u606f\u961f\u5217(Kafka)\uff0c\u662f\u5b9e\u65f6\u65b0\u589e/\u4fee\u6539\u8bb0\u5f55\u8fc7\u6765\u7684\u67d0\u4e00\u7b14\u6570\u636e \u5904\u7406\u8fc7\u7a0b \u79bb\u7ebf\uff1aMapReduce, map + reduce \u5b9e\u65f6: Spark(DStream/SS) \u5904\u7406\u901f\u5ea6 \u79bb\u7ebf\uff1a\u5e54 \u5b9e\u65f6\uff1a\u5feb\u901f \u8fdb\u7a0b \u79bb\u7ebf\uff1a\u8fdb\u7a0b\u6709\u542f\u52a8+\u9500\u6bc1\u7684\u8fc7\u7a0b \u5b9e\u65f6\uff1a 7*24\u5c0f\u65f6\u8fd0\u884c \u5b9e\u65f6\u6d41\u5904\u7406\u6846\u67b6\u5bf9\u6bd4 Apache Storm Apache Storm is a free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use! Apache Spark Streaming \u5b9e\u9645\u4e0a\u662f\u5fae\u6279\u5904\u7406\uff08\u6279\u5904\u7406\u95f4\u9694\u975e\u5e38\u5c0f) Apache kafka Apache Flink Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. \u5b9e\u65f6\u6d41\u5904\u7406\u67b6\u6784\u548c\u6280\u672f\u9009\u578b \u52a0\u4e00\u5c42flume\u6d88\u606f\u961f\u5217\uff0c\u4e3b\u8981\u4e3a\u4e86\u51cf\u8f7b\u538b\u529b\uff0c\u8d77\u5230\u7f13\u51b2\u4f5c\u7528 \u5b9e\u65f6\u6d41\u5904\u7406\u5728\u4f01\u4e1a\u4e2d\u7684\u5e94\u7528 \u7535\u4fe1\u884c\u4e1a\uff1a \u4f60\u7684\u624b\u673a\u5957\u9910\u6d41\u91cf\u7528\u5b8c\uff0c\u6536\u5230\u77ed\u4fe1\u63d0\u793a \u7535\u5546\u884c\u4e1a\uff1a\u641c\u7d22\u5546\u54c1\u65f6\uff0c\u8fdb\u884c\u63a8\u8350 2 \u5206\u5e03\u5f0f\u65e5\u5fd7\u6536\u96c6\u6846\u67b6Flume see detail in Hadoop: definitive Guide, Chapter 14 \u4e1a\u52a1\u73b0\u72b6\u5206\u6790 You have a lot of servers and systems network devices operating system web servers applications And they generate large amount of logs and other data. Problem: Since you have a business idea, how to implement the idea? OPTION: You may move logs and data generated to hadoop hdfs directly. \u4f46\u662f\u5b58\u5728\u95ee\u9898\uff1a \u5982\u4f55\u505a\u76d1\u63a7 \u5982\u4f55\u4fdd\u8bc1\u65f6\u6548\u6027 \u76f4\u63a5\u4f20\u9001\u6587\u672c\u6570\u636e\uff0c\u5f00\u9500\u592a\u5927 \u5bb9\u9519 \u8d1f\u8f7d\u5747\u8861 SOLUTION: \u4f7f\u7528Flume\uff0c\u57fa\u672c\u4e0a\u5199\u914d\u7f6e\u6587\u4ef6\u5c31OK\u4e86\uff0cFlume\u81ea\u52a8\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002 Flume\u6982\u8ff0 Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data . It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application. [ Apache Flume ] Flume\u67b6\u6784\u53ca\u6838\u5fc3\u7ec4\u4ef6 see detail in Hadoop: definitive Guide, Chapter 14 Flume\u5b9e\u6218 \u9700\u6c42\uff1a \u4ece\u6307\u5b9a\u7f51\u7edc\u7aef\u53e3\u91c7\u96c6\u6570\u636e \u4f7f\u7528Flume\u7684\u5173\u952e\u5c31\u662f\u5199\u914d\u7f6e\u6587\u4ef6 \u914d\u7f6eSource, Channel, Sink \u628a\u4ee5\u4e0a\u4e09\u4e2a\u7ec4\u4ef6\u4e32\u8d77\u6765 http://flume.apache.org/FlumeUserGuide.html#example-2 # example.conf: A single-node Flume configuration # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 # Describe the sink a1.sinks.k1.type = logger # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 netcat source : A netcat-like source that listens on a given port and turns each line of text into an event. It opens a specified port and listens for data. The expectation is that the supplied data is newline separated text. Each line of text is turned into a Flume event and sent via the connected channel. [ NetCat TCP Source ] logger sink : Logs event at INFO level. Typically useful for testing/debugging purpose. [ Logger Sink ] memory channel : The events are stored in an in-memory queue with configurable max size. It\u2019s ideal for flows that need higher throughput and are prepared to lose the staged data in the event of an agent failures. [ memory channel ] ## \u542f\u52a8flume $ flume-ng agent \\ --name a1 \\ # agent name --conf $F LUME_HOME/conf \\ # use configs in conf directory --conf-file example.conf \\ # specify a config file -Dflume.root.logger = INFO,console # sets a Java system property value ## \u5728\u53e6\u5916\u4e00\u4e2aterminal\u7528telnet\u6a21\u62df\u6570\u636e\u6e90 $ telnet localhost 44444 Trying 127.0.0.1... Connected to localhost. Escape character is ^] . hello OK hellomy OK \u9700\u6c42\uff1a \u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\u5b9e\u65f6\u91c7\u96c6\u65b0\u589e\u7684\u6570\u636e\u8f93\u51fa\u5230\u63a7\u5236\u53f0 Agent\u9009\u578b\uff1a exec source + memory channel + logger sink # filename: exec-memeory-logger.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /tmp/data.log # Describe the sink a1.sinks.k1.type = logger # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 exec source runs a given Unix command on start-up and expects that process to continuously produce data on standard out (stderr is simply discarded, unless property logStdErr is set to true). If the process exits for any reason, the source also exits and will produce no further data. This means configurations such as cat [named pipe] or tail -F [file] are going to produce the desired results where as date will probably not - the former two commands produce streams of data where as the latter produces a single event and exits. [ exec source ] \u5c06\u5185\u5bb9\u8f93\u5165\u5230 /tmp/data.log \u6587\u4ef6\u4e2d\uff1a $ echo hello data.log $ echo hello data.log \u9700\u6c42\uff1a \u5c06A\u670d\u52a1\u5668\u4e0a\u7684\u65e5\u5fd7\u5b9e\u65f6\u91c7\u96c6\u5230B\u670d\u52a1\u5668 \u65e5\u5fd7\u6536\u96c6\u8fc7\u7a0b\uff1a \u673a\u56681\u4e0a\u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\uff0c\u5f53\u6211\u4eec\u8bbf\u95ee\u4e3b\u7ad9\u65f6\u4f1a\u6709\u7528\u6237\u884c\u4e3a\u65e5\u5fd7\u8bb0\u5f55\u5230 access.log \u4e2d\u3002 avro sink\u628a\u65b0\u4ea7\u751f\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u5bf9\u5e94\u7684avro source\u6307\u5b9a\u7684hostname\u548cport\u4e0a\u3002 \u901a\u8fc7avro\u5bf9\u5e94\u7684agent\u5c06\u6211\u4eec\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u63a7\u5236\u53f0\u3002 avro sink : forms one half of Flume\u2019s tiered collection support. Flume events sent to this sink are turned into Avro events and sent to the configured hostname / port pair. [ Avro sink ] Exec-Memeory-Avro.conf # filename: exec-memeory-avro.conf # Name the components on this agent a1.sources = exec-source a1.sinks = avro-sink a1.channels = memory-channel # Describe/configure the source a1.sources.exec-source.type = exec a1.sources.exec-source.command = tail -F /tmp/data.log # Describe the sink a1.sinks.avro-sink.type = avro a1.sinks.avro-sink.hostname = localhost a1.sinks.avro-sink.port = 44444 # Use a channel which buffers events in memory a1.channels.memory-channel.type = memory a1.channels.memory-channel.capacity = 1000 a1.channels.memory-channel.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.exec-source.channels = memory-channel a1.sinks.avro-sink.channel = memory-channel Avro-Memeory-Logger.conf # filename: avro-memeory-logger.conf # Name the components on this agent a2.sources = avro-source a2.sinks = logger-sink a2.channels = memory-channel # Describe/configure the source a2.sources.avro-source.type = avro a2.sources.avro-source.bind = localhost a2.sources.avro-source.port = 44444 # Describe the sink a2.sinks.logger-sink.type = logger # Use a channel which buffers events in memory a2.channels.memory-channel.type = memory a2.channels.memory-channel.capacity = 1000 a2.channels.memory-channel.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.avro-source.channels = memory-channel a2.sinks.logger-sink.channel = memory-channel \u542f\u52a8flume\uff0c \u6ce8\u610f\u4e24\u4e2aagent\u7684\u542f\u52a8\u987a\u5e8f $ flume-ng agent \\ --name a2 \\ --conf $F LUME-HOME/conf \\ --conf-file avro-memory-logger.conf \\ -Dflume.root.logger = INFO,console $ flume-ng agent \\ --name a1 \\ --conf $F LUME-HOME/conf \\ --conf-file exec-memory-avro.conf \\ -Dflume.root.logger = INFO,console \u5c06\u5185\u5bb9\u8f93\u5165\u5230 /tmp/data.log \u6587\u4ef6\u4e2d\uff1a $ echo welcome data.log $ echo welcome data.log 3 \u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217Kafka First a few concepts: Kafka is run as a cluster on one or more servers that can span multiple datacenters. The Kafka cluster stores streams of records in categories called topic s. Each record consists of a key, a value, and a timestamp. Broker s are the Kafka processes that manage topics and partitions and serve producer and consumer request. Kafka\u90e8\u7f72\u53ca\u4f7f\u7528 \u5355\u8282\u70b9\u5355Broker\u90e8\u7f72\u53ca\u4f7f\u7528 # \u542f\u52a8Zookeeper $ zkServer.sh start # \u542f\u52a8kafka $ kafka-server-start.sh $KAFKA_HOME/config/server.properties # \u521b\u5efa\u540d\u4e3atest\u7684topic(single partition and only one replica) $ kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test # \u67e5\u770btopic $ kafka-topics.sh --list --zookeeper localhost:2181 ### \u542f\u52a8\u751f\u4ea7\u8005, 9092\u662fserver\u76d1\u542c\u7aef\u53e3 $ kafka-console-producer.sh --broker-list localhost:9092 --topic test This is a message This is another message ### \u542f\u52a8\u6d88\u8d39\u8005 --from-beginning\u4ece\u5934\u5f00\u59cb\u63a5\u6536\u6d88\u606f $ kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning This is a message This is another message ### \u67e5\u770b\u6240\u6709topics\u7684\u8be6\u7ec6\u4fe1\u606f $ kafka-topics.sh --describe --zookeeper localhost:2181 ### \u67e5\u770b\u6307\u5b9atopic\u7684\u8be6\u7ec6\u4fe1\u606f $ kafka-topics.sh --describe --zookeeper localhost:2181 --topic test \u5355\u8282\u70b9\u591aBroker\u90e8\u7f72\u53ca\u4f7f\u7528 cp $KAFKA_HOME/config/server.properties $KAFKA_HOME/config/server-1.properties cp $KAFKA_HOME/config/server.properties $KAFKA_HOME/config/server-2.properties \u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b config/server-1.properties: broker.id=1 listeners=PLAINTEXT://:9093 log.dirs=/tmp/kafka-logs-1 config/server-2.properties: broker.id=2 listeners=PLAINTEXT://:9094 log.dirs=/tmp/kafka-logs-2 \u542f\u52a8kafka # \u542f\u52a8ZooKeeper $ zkServer.sh start # \u542f\u52a8kafka server $ kafka-server-start.sh $KAFKA_HOME/config/server.properties $ kafka-server-start.sh $KAFKA_HOME/config/server-1.properties $ kafka-server-start.sh $KAFKA_HOME/config/server-2.properties # \u521b\u5efatopic, 1\u4e2a\u5206\u533a\uff0c\u4e09\u4e2a\u526f\u672c $ kafka-topics.sh --create --zookeeper localhost:2181 \\ --replication-factor 3 --partitions 1 --topic my-replicated-topic # \u67e5\u770btopic\u4fe1\u606f $ kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic Topic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 2 Replicas: 2,0,1 Isr: 2,0,1 # \u542f\u52a8\u751f\u4ea7\u8005 $ kafka-console-producer.sh --broker-list localhost:9092, localhost:9093, localhost:9094 --topic my-replicated-topic # \u542f\u52a8\u6d88\u8d39\u8005 $ kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic Kafka Java \u7f16\u7a0b \u4f7f\u7528\u547d\u4ee4\u884c\u603b\u662f\u4e0d\u65b9\u4fbf\u7684\uff0c\u4e0b\u9762\u6211\u4eec\u5c1d\u8bd5\u7740\u4f7f\u7528Kafka Java API\u7f16\u7a0b\uff0c\u5b9e\u9645\u64cd\u4f5c\u5185\u5bb9\u548c\u4e0a\u4e00\u8282\u662f\u4e00\u6478\u4e00\u6837\u7684\uff0c\u6240\u4ee5\u76f4\u63a5\u9644\u4e0a\u4ee3\u7801\u4e86\u3002\u6ce8\u610f\u8fd9\u91cc\u4f7f\u7528\u7684API\u662f0.8.2\u7248\u672c\u4ee5\u540e\u7684\uff0c\u4e4b\u524d\u7248\u672c\u4e0e\u4e4b\u540e\u7248\u672c\u7684API\u76f8\u5dee\u975e\u5e38\u5927\u3002 Producer import org.apache.kafka.clients.producer.KafkaProducer ; import org.apache.kafka.clients.producer.ProducerRecord ; import java.util.Properties ; /** * Kafka\u751f\u4ea7\u8005 * \u89c1\u5b98\u65b9\u6587\u6863 * http://kafka.apache.org/20/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html */ public class MyKafkaProducer implements Runnable { private String topic ; private KafkaProducer String , String producer ; public MyKafkaProducer ( String topic ) { this . topic = topic ; Properties props = new Properties (); props . put ( bootstrap.servers , localhost:9092 ); props . put ( acks , all ); props . put ( key.serializer , org.apache.kafka.common.serialization.StringSerializer ); props . put ( value.serializer , org.apache.kafka.common.serialization.StringSerializer ); producer = new KafkaProducer String , String ( props ); } public void run () { int messageNumber = 1 ; while ( true ) { String message = message + messageNumber ; producer . send ( new ProducerRecord String , String ( topic , message )); messageNumber ++; try { Thread . sleep ( 5000 ); } catch ( Exception ex ) { ex . printStackTrace (); } } } } Consumer import org.apache.kafka.clients.consumer.ConsumerRecord ; import org.apache.kafka.clients.consumer.ConsumerRecords ; import org.apache.kafka.clients.consumer.KafkaConsumer ; import java.time.Duration ; import java.util.Arrays ; import java.util.List ; import java.util.Properties ; import java.util.concurrent.atomic.AtomicBoolean ; /** * Kafka\u6d88\u8d39\u8005 * \u5b98\u65b9\u6587\u6863 * http://kafka.apache.org/20/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html */ public class MyKafkaConsumer implements Runnable { private final AtomicBoolean closed = new AtomicBoolean ( false ); private String topic ; private KafkaConsumer String , String consumer ; private ConsumerRecords String , String records ; public MyKafkaConsumer ( String topic ) { this . topic = topic ; Properties props = new Properties (); // connect to cluster props . put ( bootstrap.servers , localhost:9092 ); // subscribing to the topics- test props . put ( group.id , test ); // offsets are committed automatically props . put ( enable.auto.commit , true ); // specify how to turn bytes into objects props . put ( key.deserializer , org.apache.kafka.common.serialization.StringDeserializer ); props . put ( value.deserializer , org.apache.kafka.common.serialization.StringDeserializer ); consumer = new KafkaConsumer ( props ); } public void run () { try { // subsribes to topic consumer . subscribe ( Arrays . asList ( topic )); while (! closed . get ()) { records = consumer . poll ( Duration . ofMillis ( 10000 )); for ( ConsumerRecord String , String record : records ) System . out . printf ( offset = %d, key = %s, value = %s%n , record . offset (), record . key (), record . value ()); } } catch ( Exception e ) { // Ignore exception if closing if (! closed . get ()) throw e ; } finally { consumer . close (); } } // Shutdown hook which can be called from a separate thread public void shutdown () { closed . set ( true ); consumer . wakeup (); } } Clientapp public class ClientApp { public static void main ( String [] args ) { Thread job = new Thread ( new MyKafkaProducer ( test )); job . start (); Thread job2 = new Thread ( new MyKafkaConsumer ( test )); job2 . start (); } } \u6574\u5408Flume\u548cKafka\u5b8c\u6210\u5b9e\u65f6\u6570\u636e\u91c7\u96c6 \u4e3a\u4e86\u5c06Flume\u7684\u8f93\u51fa\u5230Kafka\uff0c\u53ef\u4ee5\u5c06agent2\u7684logger sink\u66ff\u6362\u6210Kafka Sink\u3002\u7136\u540e\u542f\u52a8\u4e00\u4e2aKafka consumer\u4eceKafka sink\u8ba2\u9605\u6d88\u606f\u3002 kafka sink can publish data to a Kafka topic. One of the objective is to integrate Flume with Kafka so that pull based processing systems can process the data coming through various Flume sources. [ Kafka Sink ] \u4e0b\u9762\u662fagent2\u5bf9\u5e94\u7684Kafka\u914d\u7f6e\u6587\u4ef6\uff0c\u5728\u8fd9\u91ccagent2\u6539\u540d\u4e3a avro-memory-kafka \u3002 # filename: avro-memeory-kafka.conf # Name the components on this agent avro-memory-kafka.sources = avro-source avro-memory-kafka.sinks = kafka-sink avro-memory-kafka.channels = memory-channel # Describe/configure the source avro-memory-kafka.sources.avro-source.type = avro avro-memory-kafka.sources.avro-source.bind = localhost avro-memory-kafka.sources.avro-source.port = 44444 # Describe the sink avro-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink avro-memory-kafka.sinks.kafka-sink.kafka.bootstrap.servers = localhost:9092 avro-memory-kafka.sinks.kafka-sink.kafka.topic = test # Use a channel which buffers events in memory avro-memory-kafka.channels.memory-channel.type = memory avro-memory-kafka.channels.memory-channel.capacity = 1000 avro-memory-kafka.channels.memory-channel.transactionCapacity = 100 # Bind the source and sink to the channel avro-memory-kafka.sources.avro-source.channels = memory-channel avro-memory-kafka.sinks.kafka-sink.channel = memory-channel \u4e0b\u9762\u662f\u5177\u4f53\u7684\u64cd\u4f5c\u6d41\u7a0b\uff0c\u540c\u6837\u9700\u8981\u6ce8\u610f\u4e24\u4e2aagent\u7684\u542f\u52a8\u987a\u5e8f\uff1a ## \u542f\u52a8zookeeper, kafka\uff0c\u7701\u7565 ## \u542f\u52a8agent $ flume-ng agent \\ --name avro-memory-kafka \\ --conf $F LUME-HOME/conf \\ --conf-file avro-memory-kafka.conf \\ -Dflume.root.logger = INFO,console $ flume-ng agent \\ --name a1 \\ --conf $F LUME-HOME/conf \\ --conf-file exec-memory-avro.conf \\ -Dflume.root.logger = INFO,console ## \u542f\u52a8\u6d88\u8d39\u8005 $ kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test 4 Spark Streaming \u5165\u95e8 Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map , reduce , join and window . Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark\u2019s machine learning and graph processing algorithms on data streams. [ ref ] Spark Streaming receives live input data streams and divides the data into batches , which are then processed by the Spark engine to generate the final stream of results in batches. \u5e94\u7528\u573a\u666f React to anomalies in sensors in real-time Spark Streaming\u96c6\u6210Spark\u751f\u6001\u7cfb\u7edf\u7684\u4f7f\u7528 Join data streams with static data sets // create data set from hadoop file val dataset = sparkContext . hadoopFile ( file ) // join each batch in stream with the dataset kafakaStream . transform { batchRDD = batchRDD . join ( dataset ). filter (...) } Learn models offline, apply them online //Learn model offline val model = KMeans . train ( dataset ,...) //apply model online on stream kafkaStream . map { event =? model . predict ( event . feature ) } Interactively query streaming data with SQL // Register each batch in stream as table kafkaStream . map { batchRDD =? batchRDD . registerTempTable ( lastestEvents ) } //INteractively query table sqlContext . sql ( select * from latestEvents ) \u53d1\u5c55\u53f2 Example: \u8bcd\u9891\u7edf\u8ba1 spark-submit\u6267\u884c \u4f7f\u7528spark-submit\u6765\u63d0\u4ea4\u5e94\u7528\u7a0b\u5e8f $ spark-submit --master local \\ --class org.apache.spark.examples.streaming.JavaNetworkWordCount \\ --name NetworkWordCount \\ spark-examples_2.11-2.3.1.jar localhost 9999 spark-shell\u6267\u884c \u4f7f\u7528spark-submit\u6765\u6d4b\u8bd5\u5e94\u7528\u7a0b\u5e8f","title":"SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#spark-streaming","text":"\u8be5\u9879\u76ee\u4ece\u5b9e\u65f6\u6570\u636e\u4ea7\u751f\u548c\u6d41\u5411\u7684\u4e0d\u540c\u73af\u8282\u51fa\u53d1\uff0c\u901a\u8fc7\u96c6\u6210\u4e3b\u6d41\u7684\u5206\u5e03\u5f0f\u65e5\u5fd7\u6536\u96c6\u6846\u67b6Flume\u3001\u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217Kafka\u3001\u5206\u5e03\u5f0f\u5217\u5f0f\u6570\u636e\u5e93HBase\u3001\u4ee5\u53caSpark Streaming\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5904\u7406\u3002","title":"Spark Streaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#1","text":"","title":"1 \u521d\u8bc6\u5b9e\u65f6\u6d41\u5904\u7406"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_1","text":"\u9700\u6c42\uff1a\u7edf\u8ba1\u4e3b\u7ad9\u6bcf\u4e2a\uff08\u6307\u5b9a\uff09\u8bfe\u7a0b\u8bbf\u95ee\u7684\u5ba2\u6237\u7aef\u3001\u5730\u57df\u4fe1\u606f\u5206\u5e03 == \u5982\u4e0a\u4e24\u4e2a\u64cd\u4f5c\uff1a\u91c7\u7528\u79bb\u7ebf\uff08spark/mapreduce\uff09\u7684\u65b9\u5f0f\u8fdb\u884c\u7edf\u8ba1 \u5b9e\u73b0\u6b65\u9aa4\uff1a \u8bfe\u7a0b\u7f16\u53f7\uff0cip\u4fe1\u606f\uff0cuser-agent \u8fdb\u884c\u76f8\u5e94\u7684\u7edf\u8ba1\u5206\u6790\u64cd\u4f5c\uff1aMapReduce/Spark \u9879\u76ee\u67b6\u6784\uff1a \u65e5\u5fd7\u6536\u96c6\uff1aFlume \u79bb\u7ebf\u5206\u6790\uff1aMapReduce/Spark \u7edf\u8ba1\u7ed3\u679c\u56fe\u5f62\u5316\u5c55\u793a \u95ee\u9898\uff1a \u5c0f\u65f6\u7ea7\u522b 10\u5206\u949f \u79d2\u7ea7\u522b","title":"\u4e1a\u52a1\u73b0\u72b6\u5206\u6790"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_2","text":"\u65f6\u6548\u6027\u9ad8 \u6570\u636e\u91cf\u5927","title":"\u5b9e\u65f6\u6d41\u5904\u7406\u4ea7\u751f\u80cc\u666f"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_3","text":"https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101 \u5b9e\u65f6\u8ba1\u7b97 apache storm \u6d41\u5f0f\u8ba1\u7b97 \u5b9e\u65f6\u6d41\u5f0f\u8ba1\u7b97","title":"\u5b9e\u65f6\u6d41\u5904\u7406\u6982\u8ff0"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_4","text":"\u6570\u636e\u6765\u6e90 \u79bb\u7ebf\uff1a\u6765\u81eaHDFS\u4e0a\u7684\u5386\u53f2\u6570\u636e\uff0c\u6570\u636e\u91cf\u6bd4\u8f83\u5927 \u5b9e\u65f6\uff1a\u6765\u81ea\u6d88\u606f\u961f\u5217(Kafka)\uff0c\u662f\u5b9e\u65f6\u65b0\u589e/\u4fee\u6539\u8bb0\u5f55\u8fc7\u6765\u7684\u67d0\u4e00\u7b14\u6570\u636e \u5904\u7406\u8fc7\u7a0b \u79bb\u7ebf\uff1aMapReduce, map + reduce \u5b9e\u65f6: Spark(DStream/SS) \u5904\u7406\u901f\u5ea6 \u79bb\u7ebf\uff1a\u5e54 \u5b9e\u65f6\uff1a\u5feb\u901f \u8fdb\u7a0b \u79bb\u7ebf\uff1a\u8fdb\u7a0b\u6709\u542f\u52a8+\u9500\u6bc1\u7684\u8fc7\u7a0b \u5b9e\u65f6\uff1a 7*24\u5c0f\u65f6\u8fd0\u884c","title":"\u79bb\u7ebf\u8ba1\u7b97\u4e0e\u5b9e\u65f6\u8ba1\u7b97\u5bf9\u6bd4"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_5","text":"Apache Storm Apache Storm is a free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use! Apache Spark Streaming \u5b9e\u9645\u4e0a\u662f\u5fae\u6279\u5904\u7406\uff08\u6279\u5904\u7406\u95f4\u9694\u975e\u5e38\u5c0f) Apache kafka Apache Flink Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.","title":"\u5b9e\u65f6\u6d41\u5904\u7406\u6846\u67b6\u5bf9\u6bd4"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_6","text":"\u52a0\u4e00\u5c42flume\u6d88\u606f\u961f\u5217\uff0c\u4e3b\u8981\u4e3a\u4e86\u51cf\u8f7b\u538b\u529b\uff0c\u8d77\u5230\u7f13\u51b2\u4f5c\u7528","title":"\u5b9e\u65f6\u6d41\u5904\u7406\u67b6\u6784\u548c\u6280\u672f\u9009\u578b"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_7","text":"\u7535\u4fe1\u884c\u4e1a\uff1a \u4f60\u7684\u624b\u673a\u5957\u9910\u6d41\u91cf\u7528\u5b8c\uff0c\u6536\u5230\u77ed\u4fe1\u63d0\u793a \u7535\u5546\u884c\u4e1a\uff1a\u641c\u7d22\u5546\u54c1\u65f6\uff0c\u8fdb\u884c\u63a8\u8350","title":"\u5b9e\u65f6\u6d41\u5904\u7406\u5728\u4f01\u4e1a\u4e2d\u7684\u5e94\u7528"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#2-flume","text":"see detail in Hadoop: definitive Guide, Chapter 14","title":"2 \u5206\u5e03\u5f0f\u65e5\u5fd7\u6536\u96c6\u6846\u67b6Flume"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_8","text":"You have a lot of servers and systems network devices operating system web servers applications And they generate large amount of logs and other data. Problem: Since you have a business idea, how to implement the idea? OPTION: You may move logs and data generated to hadoop hdfs directly. \u4f46\u662f\u5b58\u5728\u95ee\u9898\uff1a \u5982\u4f55\u505a\u76d1\u63a7 \u5982\u4f55\u4fdd\u8bc1\u65f6\u6548\u6027 \u76f4\u63a5\u4f20\u9001\u6587\u672c\u6570\u636e\uff0c\u5f00\u9500\u592a\u5927 \u5bb9\u9519 \u8d1f\u8f7d\u5747\u8861 SOLUTION: \u4f7f\u7528Flume\uff0c\u57fa\u672c\u4e0a\u5199\u914d\u7f6e\u6587\u4ef6\u5c31OK\u4e86\uff0cFlume\u81ea\u52a8\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002","title":"\u4e1a\u52a1\u73b0\u72b6\u5206\u6790"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#flume","text":"Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data . It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application. [ Apache Flume ]","title":"Flume\u6982\u8ff0"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#flume_1","text":"see detail in Hadoop: definitive Guide, Chapter 14","title":"Flume\u67b6\u6784\u53ca\u6838\u5fc3\u7ec4\u4ef6"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#flume_2","text":"\u9700\u6c42\uff1a \u4ece\u6307\u5b9a\u7f51\u7edc\u7aef\u53e3\u91c7\u96c6\u6570\u636e \u4f7f\u7528Flume\u7684\u5173\u952e\u5c31\u662f\u5199\u914d\u7f6e\u6587\u4ef6 \u914d\u7f6eSource, Channel, Sink \u628a\u4ee5\u4e0a\u4e09\u4e2a\u7ec4\u4ef6\u4e32\u8d77\u6765 http://flume.apache.org/FlumeUserGuide.html#example-2 # example.conf: A single-node Flume configuration # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 # Describe the sink a1.sinks.k1.type = logger # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 netcat source : A netcat-like source that listens on a given port and turns each line of text into an event. It opens a specified port and listens for data. The expectation is that the supplied data is newline separated text. Each line of text is turned into a Flume event and sent via the connected channel. [ NetCat TCP Source ] logger sink : Logs event at INFO level. Typically useful for testing/debugging purpose. [ Logger Sink ] memory channel : The events are stored in an in-memory queue with configurable max size. It\u2019s ideal for flows that need higher throughput and are prepared to lose the staged data in the event of an agent failures. [ memory channel ] ## \u542f\u52a8flume $ flume-ng agent \\ --name a1 \\ # agent name --conf $F LUME_HOME/conf \\ # use configs in conf directory --conf-file example.conf \\ # specify a config file -Dflume.root.logger = INFO,console # sets a Java system property value ## \u5728\u53e6\u5916\u4e00\u4e2aterminal\u7528telnet\u6a21\u62df\u6570\u636e\u6e90 $ telnet localhost 44444 Trying 127.0.0.1... Connected to localhost. Escape character is ^] . hello OK hellomy OK \u9700\u6c42\uff1a \u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\u5b9e\u65f6\u91c7\u96c6\u65b0\u589e\u7684\u6570\u636e\u8f93\u51fa\u5230\u63a7\u5236\u53f0 Agent\u9009\u578b\uff1a exec source + memory channel + logger sink # filename: exec-memeory-logger.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /tmp/data.log # Describe the sink a1.sinks.k1.type = logger # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 exec source runs a given Unix command on start-up and expects that process to continuously produce data on standard out (stderr is simply discarded, unless property logStdErr is set to true). If the process exits for any reason, the source also exits and will produce no further data. This means configurations such as cat [named pipe] or tail -F [file] are going to produce the desired results where as date will probably not - the former two commands produce streams of data where as the latter produces a single event and exits. [ exec source ] \u5c06\u5185\u5bb9\u8f93\u5165\u5230 /tmp/data.log \u6587\u4ef6\u4e2d\uff1a $ echo hello data.log $ echo hello data.log \u9700\u6c42\uff1a \u5c06A\u670d\u52a1\u5668\u4e0a\u7684\u65e5\u5fd7\u5b9e\u65f6\u91c7\u96c6\u5230B\u670d\u52a1\u5668 \u65e5\u5fd7\u6536\u96c6\u8fc7\u7a0b\uff1a \u673a\u56681\u4e0a\u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\uff0c\u5f53\u6211\u4eec\u8bbf\u95ee\u4e3b\u7ad9\u65f6\u4f1a\u6709\u7528\u6237\u884c\u4e3a\u65e5\u5fd7\u8bb0\u5f55\u5230 access.log \u4e2d\u3002 avro sink\u628a\u65b0\u4ea7\u751f\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u5bf9\u5e94\u7684avro source\u6307\u5b9a\u7684hostname\u548cport\u4e0a\u3002 \u901a\u8fc7avro\u5bf9\u5e94\u7684agent\u5c06\u6211\u4eec\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u63a7\u5236\u53f0\u3002 avro sink : forms one half of Flume\u2019s tiered collection support. Flume events sent to this sink are turned into Avro events and sent to the configured hostname / port pair. [ Avro sink ] Exec-Memeory-Avro.conf # filename: exec-memeory-avro.conf # Name the components on this agent a1.sources = exec-source a1.sinks = avro-sink a1.channels = memory-channel # Describe/configure the source a1.sources.exec-source.type = exec a1.sources.exec-source.command = tail -F /tmp/data.log # Describe the sink a1.sinks.avro-sink.type = avro a1.sinks.avro-sink.hostname = localhost a1.sinks.avro-sink.port = 44444 # Use a channel which buffers events in memory a1.channels.memory-channel.type = memory a1.channels.memory-channel.capacity = 1000 a1.channels.memory-channel.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.exec-source.channels = memory-channel a1.sinks.avro-sink.channel = memory-channel Avro-Memeory-Logger.conf # filename: avro-memeory-logger.conf # Name the components on this agent a2.sources = avro-source a2.sinks = logger-sink a2.channels = memory-channel # Describe/configure the source a2.sources.avro-source.type = avro a2.sources.avro-source.bind = localhost a2.sources.avro-source.port = 44444 # Describe the sink a2.sinks.logger-sink.type = logger # Use a channel which buffers events in memory a2.channels.memory-channel.type = memory a2.channels.memory-channel.capacity = 1000 a2.channels.memory-channel.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.avro-source.channels = memory-channel a2.sinks.logger-sink.channel = memory-channel \u542f\u52a8flume\uff0c \u6ce8\u610f\u4e24\u4e2aagent\u7684\u542f\u52a8\u987a\u5e8f $ flume-ng agent \\ --name a2 \\ --conf $F LUME-HOME/conf \\ --conf-file avro-memory-logger.conf \\ -Dflume.root.logger = INFO,console $ flume-ng agent \\ --name a1 \\ --conf $F LUME-HOME/conf \\ --conf-file exec-memory-avro.conf \\ -Dflume.root.logger = INFO,console \u5c06\u5185\u5bb9\u8f93\u5165\u5230 /tmp/data.log \u6587\u4ef6\u4e2d\uff1a $ echo welcome data.log $ echo welcome data.log","title":"Flume\u5b9e\u6218"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#3-kafka","text":"First a few concepts: Kafka is run as a cluster on one or more servers that can span multiple datacenters. The Kafka cluster stores streams of records in categories called topic s. Each record consists of a key, a value, and a timestamp. Broker s are the Kafka processes that manage topics and partitions and serve producer and consumer request.","title":"3 \u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217Kafka"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#kafka","text":"\u5355\u8282\u70b9\u5355Broker\u90e8\u7f72\u53ca\u4f7f\u7528 # \u542f\u52a8Zookeeper $ zkServer.sh start # \u542f\u52a8kafka $ kafka-server-start.sh $KAFKA_HOME/config/server.properties # \u521b\u5efa\u540d\u4e3atest\u7684topic(single partition and only one replica) $ kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test # \u67e5\u770btopic $ kafka-topics.sh --list --zookeeper localhost:2181 ### \u542f\u52a8\u751f\u4ea7\u8005, 9092\u662fserver\u76d1\u542c\u7aef\u53e3 $ kafka-console-producer.sh --broker-list localhost:9092 --topic test This is a message This is another message ### \u542f\u52a8\u6d88\u8d39\u8005 --from-beginning\u4ece\u5934\u5f00\u59cb\u63a5\u6536\u6d88\u606f $ kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning This is a message This is another message ### \u67e5\u770b\u6240\u6709topics\u7684\u8be6\u7ec6\u4fe1\u606f $ kafka-topics.sh --describe --zookeeper localhost:2181 ### \u67e5\u770b\u6307\u5b9atopic\u7684\u8be6\u7ec6\u4fe1\u606f $ kafka-topics.sh --describe --zookeeper localhost:2181 --topic test \u5355\u8282\u70b9\u591aBroker\u90e8\u7f72\u53ca\u4f7f\u7528 cp $KAFKA_HOME/config/server.properties $KAFKA_HOME/config/server-1.properties cp $KAFKA_HOME/config/server.properties $KAFKA_HOME/config/server-2.properties \u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b config/server-1.properties: broker.id=1 listeners=PLAINTEXT://:9093 log.dirs=/tmp/kafka-logs-1 config/server-2.properties: broker.id=2 listeners=PLAINTEXT://:9094 log.dirs=/tmp/kafka-logs-2 \u542f\u52a8kafka # \u542f\u52a8ZooKeeper $ zkServer.sh start # \u542f\u52a8kafka server $ kafka-server-start.sh $KAFKA_HOME/config/server.properties $ kafka-server-start.sh $KAFKA_HOME/config/server-1.properties $ kafka-server-start.sh $KAFKA_HOME/config/server-2.properties # \u521b\u5efatopic, 1\u4e2a\u5206\u533a\uff0c\u4e09\u4e2a\u526f\u672c $ kafka-topics.sh --create --zookeeper localhost:2181 \\ --replication-factor 3 --partitions 1 --topic my-replicated-topic # \u67e5\u770btopic\u4fe1\u606f $ kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic Topic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 2 Replicas: 2,0,1 Isr: 2,0,1 # \u542f\u52a8\u751f\u4ea7\u8005 $ kafka-console-producer.sh --broker-list localhost:9092, localhost:9093, localhost:9094 --topic my-replicated-topic # \u542f\u52a8\u6d88\u8d39\u8005 $ kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic","title":"Kafka\u90e8\u7f72\u53ca\u4f7f\u7528"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#kafka-java","text":"\u4f7f\u7528\u547d\u4ee4\u884c\u603b\u662f\u4e0d\u65b9\u4fbf\u7684\uff0c\u4e0b\u9762\u6211\u4eec\u5c1d\u8bd5\u7740\u4f7f\u7528Kafka Java API\u7f16\u7a0b\uff0c\u5b9e\u9645\u64cd\u4f5c\u5185\u5bb9\u548c\u4e0a\u4e00\u8282\u662f\u4e00\u6478\u4e00\u6837\u7684\uff0c\u6240\u4ee5\u76f4\u63a5\u9644\u4e0a\u4ee3\u7801\u4e86\u3002\u6ce8\u610f\u8fd9\u91cc\u4f7f\u7528\u7684API\u662f0.8.2\u7248\u672c\u4ee5\u540e\u7684\uff0c\u4e4b\u524d\u7248\u672c\u4e0e\u4e4b\u540e\u7248\u672c\u7684API\u76f8\u5dee\u975e\u5e38\u5927\u3002 Producer import org.apache.kafka.clients.producer.KafkaProducer ; import org.apache.kafka.clients.producer.ProducerRecord ; import java.util.Properties ; /** * Kafka\u751f\u4ea7\u8005 * \u89c1\u5b98\u65b9\u6587\u6863 * http://kafka.apache.org/20/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html */ public class MyKafkaProducer implements Runnable { private String topic ; private KafkaProducer String , String producer ; public MyKafkaProducer ( String topic ) { this . topic = topic ; Properties props = new Properties (); props . put ( bootstrap.servers , localhost:9092 ); props . put ( acks , all ); props . put ( key.serializer , org.apache.kafka.common.serialization.StringSerializer ); props . put ( value.serializer , org.apache.kafka.common.serialization.StringSerializer ); producer = new KafkaProducer String , String ( props ); } public void run () { int messageNumber = 1 ; while ( true ) { String message = message + messageNumber ; producer . send ( new ProducerRecord String , String ( topic , message )); messageNumber ++; try { Thread . sleep ( 5000 ); } catch ( Exception ex ) { ex . printStackTrace (); } } } } Consumer import org.apache.kafka.clients.consumer.ConsumerRecord ; import org.apache.kafka.clients.consumer.ConsumerRecords ; import org.apache.kafka.clients.consumer.KafkaConsumer ; import java.time.Duration ; import java.util.Arrays ; import java.util.List ; import java.util.Properties ; import java.util.concurrent.atomic.AtomicBoolean ; /** * Kafka\u6d88\u8d39\u8005 * \u5b98\u65b9\u6587\u6863 * http://kafka.apache.org/20/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html */ public class MyKafkaConsumer implements Runnable { private final AtomicBoolean closed = new AtomicBoolean ( false ); private String topic ; private KafkaConsumer String , String consumer ; private ConsumerRecords String , String records ; public MyKafkaConsumer ( String topic ) { this . topic = topic ; Properties props = new Properties (); // connect to cluster props . put ( bootstrap.servers , localhost:9092 ); // subscribing to the topics- test props . put ( group.id , test ); // offsets are committed automatically props . put ( enable.auto.commit , true ); // specify how to turn bytes into objects props . put ( key.deserializer , org.apache.kafka.common.serialization.StringDeserializer ); props . put ( value.deserializer , org.apache.kafka.common.serialization.StringDeserializer ); consumer = new KafkaConsumer ( props ); } public void run () { try { // subsribes to topic consumer . subscribe ( Arrays . asList ( topic )); while (! closed . get ()) { records = consumer . poll ( Duration . ofMillis ( 10000 )); for ( ConsumerRecord String , String record : records ) System . out . printf ( offset = %d, key = %s, value = %s%n , record . offset (), record . key (), record . value ()); } } catch ( Exception e ) { // Ignore exception if closing if (! closed . get ()) throw e ; } finally { consumer . close (); } } // Shutdown hook which can be called from a separate thread public void shutdown () { closed . set ( true ); consumer . wakeup (); } } Clientapp public class ClientApp { public static void main ( String [] args ) { Thread job = new Thread ( new MyKafkaProducer ( test )); job . start (); Thread job2 = new Thread ( new MyKafkaConsumer ( test )); job2 . start (); } }","title":"Kafka Java \u7f16\u7a0b"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#flumekafka","text":"\u4e3a\u4e86\u5c06Flume\u7684\u8f93\u51fa\u5230Kafka\uff0c\u53ef\u4ee5\u5c06agent2\u7684logger sink\u66ff\u6362\u6210Kafka Sink\u3002\u7136\u540e\u542f\u52a8\u4e00\u4e2aKafka consumer\u4eceKafka sink\u8ba2\u9605\u6d88\u606f\u3002 kafka sink can publish data to a Kafka topic. One of the objective is to integrate Flume with Kafka so that pull based processing systems can process the data coming through various Flume sources. [ Kafka Sink ] \u4e0b\u9762\u662fagent2\u5bf9\u5e94\u7684Kafka\u914d\u7f6e\u6587\u4ef6\uff0c\u5728\u8fd9\u91ccagent2\u6539\u540d\u4e3a avro-memory-kafka \u3002 # filename: avro-memeory-kafka.conf # Name the components on this agent avro-memory-kafka.sources = avro-source avro-memory-kafka.sinks = kafka-sink avro-memory-kafka.channels = memory-channel # Describe/configure the source avro-memory-kafka.sources.avro-source.type = avro avro-memory-kafka.sources.avro-source.bind = localhost avro-memory-kafka.sources.avro-source.port = 44444 # Describe the sink avro-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink avro-memory-kafka.sinks.kafka-sink.kafka.bootstrap.servers = localhost:9092 avro-memory-kafka.sinks.kafka-sink.kafka.topic = test # Use a channel which buffers events in memory avro-memory-kafka.channels.memory-channel.type = memory avro-memory-kafka.channels.memory-channel.capacity = 1000 avro-memory-kafka.channels.memory-channel.transactionCapacity = 100 # Bind the source and sink to the channel avro-memory-kafka.sources.avro-source.channels = memory-channel avro-memory-kafka.sinks.kafka-sink.channel = memory-channel \u4e0b\u9762\u662f\u5177\u4f53\u7684\u64cd\u4f5c\u6d41\u7a0b\uff0c\u540c\u6837\u9700\u8981\u6ce8\u610f\u4e24\u4e2aagent\u7684\u542f\u52a8\u987a\u5e8f\uff1a ## \u542f\u52a8zookeeper, kafka\uff0c\u7701\u7565 ## \u542f\u52a8agent $ flume-ng agent \\ --name avro-memory-kafka \\ --conf $F LUME-HOME/conf \\ --conf-file avro-memory-kafka.conf \\ -Dflume.root.logger = INFO,console $ flume-ng agent \\ --name a1 \\ --conf $F LUME-HOME/conf \\ --conf-file exec-memory-avro.conf \\ -Dflume.root.logger = INFO,console ## \u542f\u52a8\u6d88\u8d39\u8005 $ kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test","title":"\u6574\u5408Flume\u548cKafka\u5b8c\u6210\u5b9e\u65f6\u6570\u636e\u91c7\u96c6"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#4-spark-streaming","text":"Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map , reduce , join and window . Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark\u2019s machine learning and graph processing algorithms on data streams. [ ref ] Spark Streaming receives live input data streams and divides the data into batches , which are then processed by the Spark engine to generate the final stream of results in batches.","title":"4 Spark Streaming \u5165\u95e8"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_9","text":"React to anomalies in sensors in real-time","title":"\u5e94\u7528\u573a\u666f"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#spark-streamingspark","text":"Join data streams with static data sets // create data set from hadoop file val dataset = sparkContext . hadoopFile ( file ) // join each batch in stream with the dataset kafakaStream . transform { batchRDD = batchRDD . join ( dataset ). filter (...) } Learn models offline, apply them online //Learn model offline val model = KMeans . train ( dataset ,...) //apply model online on stream kafkaStream . map { event =? model . predict ( event . feature ) } Interactively query streaming data with SQL // Register each batch in stream as table kafkaStream . map { batchRDD =? batchRDD . registerTempTable ( lastestEvents ) } //INteractively query table sqlContext . sql ( select * from latestEvents )","title":"Spark Streaming\u96c6\u6210Spark\u751f\u6001\u7cfb\u7edf\u7684\u4f7f\u7528"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_10","text":"","title":"\u53d1\u5c55\u53f2"},{"location":"projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#example","text":"spark-submit\u6267\u884c \u4f7f\u7528spark-submit\u6765\u63d0\u4ea4\u5e94\u7528\u7a0b\u5e8f $ spark-submit --master local \\ --class org.apache.spark.examples.streaming.JavaNetworkWordCount \\ --name NetworkWordCount \\ spark-examples_2.11-2.3.1.jar localhost 9999 spark-shell\u6267\u884c \u4f7f\u7528spark-submit\u6765\u6d4b\u8bd5\u5e94\u7528\u7a0b\u5e8f","title":"Example: \u8bcd\u9891\u7edf\u8ba1"},{"location":"spark/","text":"Spark Chapter 1: Introduction to Data Analysis with Spark Chapter 2: Downloading Spark and Getting Started Chapter 3: Programming with RDDs Chapter 4: Working with Key/Value Pairs Chapter 5: Loading and Saving Your Data Chapter 6: Advanced Spark Programming Chapter 7: Running on a Cluster Chapter 8: Tuning and Debugging Spark Chapter 9: Spark SQL Chapter 10: Spark Streaming Chapter 11: Machine Learning with MLlib","title":"Contents"},{"location":"spark/#spark","text":"Chapter 1: Introduction to Data Analysis with Spark Chapter 2: Downloading Spark and Getting Started Chapter 3: Programming with RDDs Chapter 4: Working with Key/Value Pairs Chapter 5: Loading and Saving Your Data Chapter 6: Advanced Spark Programming Chapter 7: Running on a Cluster Chapter 8: Tuning and Debugging Spark Chapter 9: Spark SQL Chapter 10: Spark Streaming Chapter 11: Machine Learning with MLlib","title":"Spark"},{"location":"spark/ch1/","text":"Learning Spark 1 - Introduction to Data Analysis with Spark","title":"Chapter 1: Introduction to Data Analysis with Spark"},{"location":"spark/ch1/#learning-spark-1-introduction-to-data-analysis-with-spark","text":"","title":"Learning Spark  1 - Introduction to Data Analysis with Spark"},{"location":"spark/ch10/","text":"Learning Spark 10 - Spark Streaming","title":"Chapter 10: Spark Streaming"},{"location":"spark/ch10/#learning-spark-10-spark-streaming","text":"","title":"Learning Spark 10 - Spark Streaming"},{"location":"spark/ch11/","text":"Learning Spark 11 - Machine Learning with MLlib","title":"Chapter 11: Machine Learning with MLlib"},{"location":"spark/ch11/#learning-spark-11-machine-learning-with-mllib","text":"","title":"Learning Spark 11 - Machine Learning with MLlib"},{"location":"spark/ch2/","text":"Learning Spark 2 - Downloading Spark and Getting Started","title":"Chapter 2: Downloading Spark and Getting Started"},{"location":"spark/ch2/#learning-spark-2-downloading-spark-and-getting-started","text":"","title":"Learning Spark 2 - Downloading Spark and Getting Started"},{"location":"spark/ch3/","text":"Learning Spark 3 - Programming with RDDs","title":"Chapter 3: Programming with RDDs"},{"location":"spark/ch3/#learning-spark-3-programming-with-rdds","text":"","title":"Learning Spark 3 - Programming with RDDs"},{"location":"spark/ch4/","text":"Learning Spark 4 - Working with Key/Value Pairs","title":"Chapter 4: Working with Key/Value Pairs"},{"location":"spark/ch4/#learning-spark-4-working-with-keyvalue-pairs","text":"","title":"Learning Spark 4 - Working with Key/Value Pairs"},{"location":"spark/ch5/","text":"Learning Spark 5 - Loading and Saving Your Data","title":"Chapter 5: Loading and Saving Your Data"},{"location":"spark/ch5/#learning-spark-5-loading-and-saving-your-data","text":"","title":"Learning Spark 5 - Loading and Saving Your Data"},{"location":"spark/ch6/","text":"Learning Spark 6 - Advanced Spark Programming","title":"Chapter 6: Advanced Spark Programming"},{"location":"spark/ch6/#learning-spark-6-advanced-spark-programming","text":"","title":"Learning Spark 6 - Advanced Spark Programming"},{"location":"spark/ch7/","text":"Learning Spark 7 - Running on a Cluster","title":"Chapter 7: Running on a Cluster"},{"location":"spark/ch7/#learning-spark-7-running-on-a-cluster","text":"","title":"Learning Spark  7 - Running on a Cluster"},{"location":"spark/ch8/","text":"Learning Spark 8 - Tuning and Debugging Spark","title":"Chapter 8: Tuning and Debugging Spark"},{"location":"spark/ch8/#learning-spark-8-tuning-and-debugging-spark","text":"","title":"Learning Spark 8 - Tuning and Debugging Spark"},{"location":"spark/ch9/","text":"Learning Spark 9 - Spark SQL","title":"Chapter 9: Spark SQL"},{"location":"spark/ch9/#learning-spark-9-spark-sql","text":"","title":"Learning Spark 9 - Spark SQL"}]}