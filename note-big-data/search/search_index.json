{
    "docs": [
        {
            "location": "/hadoop/ch4/", 
            "text": "Hadoop: The Definitive Guide 4 - YARN\n\n\nApache YARN(Yet Another Resource Negotiator) is Hadoop\ns cluster resource management system. YARN provides APIs for requesting and working with cluster resources, but these APIs are not typically used directly by user code. Distributed computing frameworks (MapReduce, Spark, and so on) running as YARN applications on the cluster compute layer (YARN) and the cluster storage layer (HDFS and HBase).\n\n\n\n\n1 Anatomy of a YARN Application Run\n\n\nYARN provides its core services via two types of long-running daemon:\n\n\n\n\na \nresource manager\n (one per cluster) to manage the use of resources across the cluster,\n\n\nnode managers\n running on all the nodes in the cluster to launch and monitor \ncontainers\n.\n\n\n\n\n\n\n\n\nstep1 : To run an application on YARN, a client contacts the resource manager and asks it to run an \napplication master\n process.\n\n\nsteps 2a and 2b: The resource manager then finds a node manager that can launch the application master in a container. It could simply run a computation in the container it is running in and return the result to the client.\n\n\nstep 3: Or it could request more containers from the resource managers\n\n\nsteps 4a and 4b: use them to run a distributed computation.\n\n\n\n\n\n\nThe ApplicationMaster is an instance of a framework-specific library that negotiates resources from the ResourceManager and works with the NodeManager to execute and monitor the granted resources (bundled as containers) for a given application. The ApplicationMaster runs in a container like any other application.\n\n\n\n\nResource Requests\n\n\nA YARN application can make resource requests at any time while it is running. \n\n\n\n\nSpark starts a fixed number of executors on the cluster (i.e. make all of requests up front). \n\n\nMapReduce, has two phases: the map task containers are requested up front, but the reduce task containers are not started until later. (i.e. take a more dynamic approach whereby it requests more resources dynamically to meet the changing needs of the application).\n\n\n\n\nApplication Lifespan\n\n\nThe lifespan of a YARN application can vary dramatically. Rather than look at how long the application runs for, it\ns useful to categorize applications in terms of how they map to the jobs that users run. \n\n\n\n\nThe simplest case is one application per user job, which is the approach that MapReduce takes.\n\n\nThe second model is to run one application per workflow or user session of (possibly unrelated) jobs, which is the approach that Spark takes. This approach can be more efficient than the first, since containers can be reused between jobs, and there is also the potential to cache intermediate data between jobs.\n\n\nThe third model is a long-running application that is shared by different users, which is the approach that Apache Slider takes.\n\n\n\n\nBuilding YARN Applications\n\n\nWriting a YARN application from scratch is fairly involved, but in many cases is not necessary, as it is often possible to use an existing application that fits the bill.\n\n\n2 YARN Compared to MapReduce 1\n\n\nThe distributed implementation of MapReduce in the original version of Hadoop is sometimes referred to as \nMapReduce 1\n to distinguish it from MapReduce 2, the implementation that uses YARN.\n\n\nA comparison of MapReduce 1 and YARN components:\n\n\n\n\n\n\n\n\nMapReduce1\n\n\nYARN\n\n\n\n\n\n\n\n\n\n\nJobtracker\n\n\nResource manager, application master, timeline server\n\n\n\n\n\n\nTaskTracker\n\n\nNode manager\n\n\n\n\n\n\nSlot\n\n\nContainer\n\n\n\n\n\n\n\n\nThe Timeline Server addresses the problem of the storage and retrieval of application\ns current and historic information in a generic fashion.\n\n\n3 Scheduling in YARN\n\n\nThe job of the YARN scheduler to allocate resources to applications according to some defined policy. Scheduling in general is a difficult problem and there is \nno one \"best\" policy\n, which is why YARN provides a choice of schedulers and configurable policies.\n\n\nScheduler Options\n\n\nThree schedulers are available in YARN: the FIFO, Capacity, and Fair Schedulers.\n\n\n\n\nThe FIFO: places applications in a queue and runs them in the order of submission (first in, first out)\n\n\nNot suitable for shared clusters, because large applications will use all the resources in a cluster, so each application has to wait its turn. \n\n\n\n\n\n\nCapacity Scheduler: a separate dedicated queue allows the small job to start as soon as it is submitted, since the queue capacity is reserved for jobs in that queue.\n\n\nFair Scheduler: dynamically balance resources between all running jobs, each job is using its fair share of resources.\n\n\nThere is a lag between the time the second job starts and when it receives its fair share, since it has to wait for resources to free up as containers used by the first job complete. After the small job completes and no longer requires resources, the large job goes back to using the full cluster capacity again.", 
            "title": "Chapter 4: YARN"
        }, 
        {
            "location": "/hadoop/ch4/#hadoop-the-definitive-guide-4-yarn", 
            "text": "Apache YARN(Yet Another Resource Negotiator) is Hadoop s cluster resource management system. YARN provides APIs for requesting and working with cluster resources, but these APIs are not typically used directly by user code. Distributed computing frameworks (MapReduce, Spark, and so on) running as YARN applications on the cluster compute layer (YARN) and the cluster storage layer (HDFS and HBase).", 
            "title": "Hadoop: The Definitive Guide 4 - YARN"
        }, 
        {
            "location": "/hadoop/ch4/#1-anatomy-of-a-yarn-application-run", 
            "text": "YARN provides its core services via two types of long-running daemon:   a  resource manager  (one per cluster) to manage the use of resources across the cluster,  node managers  running on all the nodes in the cluster to launch and monitor  containers .     step1 : To run an application on YARN, a client contacts the resource manager and asks it to run an  application master  process.  steps 2a and 2b: The resource manager then finds a node manager that can launch the application master in a container. It could simply run a computation in the container it is running in and return the result to the client.  step 3: Or it could request more containers from the resource managers  steps 4a and 4b: use them to run a distributed computation.    The ApplicationMaster is an instance of a framework-specific library that negotiates resources from the ResourceManager and works with the NodeManager to execute and monitor the granted resources (bundled as containers) for a given application. The ApplicationMaster runs in a container like any other application.", 
            "title": "1 Anatomy of a YARN Application Run"
        }, 
        {
            "location": "/hadoop/ch4/#resource-requests", 
            "text": "A YARN application can make resource requests at any time while it is running.    Spark starts a fixed number of executors on the cluster (i.e. make all of requests up front).   MapReduce, has two phases: the map task containers are requested up front, but the reduce task containers are not started until later. (i.e. take a more dynamic approach whereby it requests more resources dynamically to meet the changing needs of the application).", 
            "title": "Resource Requests"
        }, 
        {
            "location": "/hadoop/ch4/#application-lifespan", 
            "text": "The lifespan of a YARN application can vary dramatically. Rather than look at how long the application runs for, it s useful to categorize applications in terms of how they map to the jobs that users run.    The simplest case is one application per user job, which is the approach that MapReduce takes.  The second model is to run one application per workflow or user session of (possibly unrelated) jobs, which is the approach that Spark takes. This approach can be more efficient than the first, since containers can be reused between jobs, and there is also the potential to cache intermediate data between jobs.  The third model is a long-running application that is shared by different users, which is the approach that Apache Slider takes.", 
            "title": "Application Lifespan"
        }, 
        {
            "location": "/hadoop/ch4/#building-yarn-applications", 
            "text": "Writing a YARN application from scratch is fairly involved, but in many cases is not necessary, as it is often possible to use an existing application that fits the bill.", 
            "title": "Building YARN Applications"
        }, 
        {
            "location": "/hadoop/ch4/#2-yarn-compared-to-mapreduce-1", 
            "text": "The distributed implementation of MapReduce in the original version of Hadoop is sometimes referred to as  MapReduce 1  to distinguish it from MapReduce 2, the implementation that uses YARN.  A comparison of MapReduce 1 and YARN components:     MapReduce1  YARN      Jobtracker  Resource manager, application master, timeline server    TaskTracker  Node manager    Slot  Container     The Timeline Server addresses the problem of the storage and retrieval of application s current and historic information in a generic fashion.", 
            "title": "2 YARN Compared to MapReduce 1"
        }, 
        {
            "location": "/hadoop/ch4/#3-scheduling-in-yarn", 
            "text": "The job of the YARN scheduler to allocate resources to applications according to some defined policy. Scheduling in general is a difficult problem and there is  no one \"best\" policy , which is why YARN provides a choice of schedulers and configurable policies.", 
            "title": "3 Scheduling in YARN"
        }, 
        {
            "location": "/hadoop/ch4/#scheduler-options", 
            "text": "Three schedulers are available in YARN: the FIFO, Capacity, and Fair Schedulers.   The FIFO: places applications in a queue and runs them in the order of submission (first in, first out)  Not suitable for shared clusters, because large applications will use all the resources in a cluster, so each application has to wait its turn.     Capacity Scheduler: a separate dedicated queue allows the small job to start as soon as it is submitted, since the queue capacity is reserved for jobs in that queue.  Fair Scheduler: dynamically balance resources between all running jobs, each job is using its fair share of resources.  There is a lag between the time the second job starts and when it receives its fair share, since it has to wait for resources to free up as containers used by the first job complete. After the small job completes and no longer requires resources, the large job goes back to using the full cluster capacity again.", 
            "title": "Scheduler Options"
        }, 
        {
            "location": "/hadoop/ch7/", 
            "text": "Hadoop: The Definitive Guide 7 - How MapReduce Works\n\n\n1 Anatomy of a MapReduce Job Run\n\n\nJob Submission\n\n\nJob Initialization\n\n\nTask Assignment\n\n\nTask Execution\n\n\nProgress and Status Updates\n\n\nJob Completion\n\n\n\n\n\n\n2 Failures\n\n\n3 Shuffle and Sort\n\n\nMapReduce makes the guarantee that the input to every reducer is \nsorted by key\n. The process by which the system performs the sort \u2014 and transfers the map outputs to the reducers as inputs \u2014 is known as the \nshuffle\n. In many ways, the shuffle is the heart of MapReduce and is where the \u201cmagic\u201d happens.\n\n\nThe Map Side\n\n\n\n\nEach map task has a circular memory buffer that it writes the output to. \n\n\nWhen the contents of the buffer reach a certain threshold size, a background thread will start to spill the contents to disk.\n\n\nBefore it writes to disk, the thread first divides the data into partitions corresponding to the reducers that they will ultimately be sent to. \n\n\nWithin each partition, the background thread performs an in-memory sort by key, and if there is a combiner function, it is run on the output of the sort.\n\n\nBefore the task is finished, the spill files are merged into a single partitioned and sorted output file.\n\n\nIt is often a good idea to compress the map output as it is written to disk, because doing so makes it faster to write to disk, saves disk space, and reduces the amount of data to transfer to the reducer.\n\n\n\n\n\n\nThe Reduce Side\n\n\n\n\nthe \ncopy phase\n of the reduce task: The map tasks may finish at different times, so the reduce task starts copying their outputs as soon as each completes.\n\n\nMap outputs are copied to the reduce task JVM\u2019s memory if they are small enough; otherwise, they are copied to disk.\n\n\n\n\n\n\nAs the copies accumulate on disk, a background thread merges them into larger, sorted files. This saves some time merging later on.\n\n\nWhen all the map outputs have been copied, the reduce task moves into the \nsort phase\n (which should properly be called the \nmerge phase\n, as the sorting was carried out on the map side), which merges the map outputs, maintaining their sort ordering.\n\n\nThis is done in rounds.(Figure below) \n\n\n\n\n\n\nFor the last merge, directly feeding the reduce function in what is the last phase: the reduce phase.\n\n\nThe output of the last phase is written directly to the output filesystem, typically HDFS.\n\n\n\n\n\n\nConfiguration Tuning\n\n\n4 Task Execution", 
            "title": "Chapter 7: How MapReduce Works"
        }, 
        {
            "location": "/hadoop/ch7/#hadoop-the-definitive-guide-7-how-mapreduce-works", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 7 - How MapReduce Works"
        }, 
        {
            "location": "/hadoop/ch7/#1-anatomy-of-a-mapreduce-job-run", 
            "text": "", 
            "title": "1 Anatomy of a MapReduce Job Run"
        }, 
        {
            "location": "/hadoop/ch7/#job-submission", 
            "text": "", 
            "title": "Job Submission"
        }, 
        {
            "location": "/hadoop/ch7/#job-initialization", 
            "text": "", 
            "title": "Job Initialization"
        }, 
        {
            "location": "/hadoop/ch7/#task-assignment", 
            "text": "", 
            "title": "Task Assignment"
        }, 
        {
            "location": "/hadoop/ch7/#task-execution", 
            "text": "", 
            "title": "Task Execution"
        }, 
        {
            "location": "/hadoop/ch7/#progress-and-status-updates", 
            "text": "", 
            "title": "Progress and Status Updates"
        }, 
        {
            "location": "/hadoop/ch7/#job-completion", 
            "text": "", 
            "title": "Job Completion"
        }, 
        {
            "location": "/hadoop/ch7/#2-failures", 
            "text": "", 
            "title": "2 Failures"
        }, 
        {
            "location": "/hadoop/ch7/#3-shuffle-and-sort", 
            "text": "MapReduce makes the guarantee that the input to every reducer is  sorted by key . The process by which the system performs the sort \u2014 and transfers the map outputs to the reducers as inputs \u2014 is known as the  shuffle . In many ways, the shuffle is the heart of MapReduce and is where the \u201cmagic\u201d happens.", 
            "title": "3 Shuffle and Sort"
        }, 
        {
            "location": "/hadoop/ch7/#the-map-side", 
            "text": "Each map task has a circular memory buffer that it writes the output to.   When the contents of the buffer reach a certain threshold size, a background thread will start to spill the contents to disk.  Before it writes to disk, the thread first divides the data into partitions corresponding to the reducers that they will ultimately be sent to.   Within each partition, the background thread performs an in-memory sort by key, and if there is a combiner function, it is run on the output of the sort.  Before the task is finished, the spill files are merged into a single partitioned and sorted output file.  It is often a good idea to compress the map output as it is written to disk, because doing so makes it faster to write to disk, saves disk space, and reduces the amount of data to transfer to the reducer.", 
            "title": "The Map Side"
        }, 
        {
            "location": "/hadoop/ch7/#the-reduce-side", 
            "text": "the  copy phase  of the reduce task: The map tasks may finish at different times, so the reduce task starts copying their outputs as soon as each completes.  Map outputs are copied to the reduce task JVM\u2019s memory if they are small enough; otherwise, they are copied to disk.    As the copies accumulate on disk, a background thread merges them into larger, sorted files. This saves some time merging later on.  When all the map outputs have been copied, the reduce task moves into the  sort phase  (which should properly be called the  merge phase , as the sorting was carried out on the map side), which merges the map outputs, maintaining their sort ordering.  This is done in rounds.(Figure below)     For the last merge, directly feeding the reduce function in what is the last phase: the reduce phase.  The output of the last phase is written directly to the output filesystem, typically HDFS.", 
            "title": "The Reduce Side"
        }, 
        {
            "location": "/hadoop/ch7/#configuration-tuning", 
            "text": "", 
            "title": "Configuration Tuning"
        }, 
        {
            "location": "/hadoop/ch7/#4-task-execution", 
            "text": "", 
            "title": "4 Task Execution"
        }
    ]
}