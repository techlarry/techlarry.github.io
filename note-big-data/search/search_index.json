{
    "docs": [
        {
            "location": "/gdm/ch5/", 
            "text": "\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 5 \u6734\u7d20\u8d1d\u53f6\u65af\n\n\n\u6734\u7d20\u8d1d\u53f6\u65af\n\n\n\u4f7f\u7528\u8fd1\u90bb\u7b97\u6cd5\u65f6\uff0c\u6211\u4eec\u5f88\u96be\u5bf9\u5206\u7c7b\u7ed3\u679c\u7684\u7f6e\u4fe1\u5ea6\u8fdb\u884c\u91cf\u5316\u3002\u4f46\u5982\u679c\u4f7f\u7528\u7684\u662f\u57fa\u4e8e\u6982\u7387\u7684\u5206\u7c7b\u7b97\u6cd5\u2014\u2014\u8d1d\u53f6\u65af\u7b97\u6cd5\u2014\u2014\u90a3\u5c31\u53ef\u4ee5\u7ed9\u51fa\u5206\u7c7b\u7ed3\u679c\u7684\u53ef\u80fd\u6027\u4e86\uff1a\u8fd9\u540d\u8fd0\u52a8\u5458\u670980%\u7684\u51e0\u7387\u662f\u7bee\u7403\u8fd0\u52a8\u5458\u3002\n\n\n\u8fd1\u90bb\u7b97\u6cd5\u53c8\u79f0\u4e3a\n\u88ab\u52a8\u5b66\u4e60\u7b97\u6cd5\n\u3002\u8fd9\u79cd\u7b97\u6cd5\u53ea\u662f\u5c06\u8bad\u7ec3\u96c6\u7684\u6570\u636e\u4fdd\u5b58\u8d77\u6765\uff0c\u5728\u6536\u5230\u6d4b\u8bd5\u6570\u636e\u65f6\u624d\u4f1a\u8fdb\u884c\u8ba1\u7b97\u3002\u5982\u679c\u6211\u4eec\u670910\u4e07\u9996\u97f3\u4e50\uff0c\u90a3\u6bcf\u8fdb\u884c\u4e00\u6b21\u5206\u7c7b\uff0c\u90fd\u9700\u8981\u904d\u5386\u8fd910\u4e07\u6761\u8bb0\u5f55\u624d\u884c\u3002\n\n\n\u8d1d\u53f6\u65af\u7b97\u6cd5\u5219\u662f\u4e00\u79cd\n\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\n\u3002\u5b83\u4f1a\u6839\u636e\u8bad\u7ec3\u96c6\u6784\u5efa\u8d77\u4e00\u4e2a\u6a21\u578b\uff0c\u5e76\u7528\u8fd9\u4e2a\u6a21\u578b\u6765\u5bf9\u65b0\u7684\u8bb0\u5f55\u8fdb\u884c\u5206\u7c7b\uff0c\u56e0\u6b64\u901f\u5ea6\u4f1a\u5feb\u5f88\u591a\u3002\n\n\n\u8d1d\u53f6\u65af\u7b97\u6cd5\u7684\u4e24\u4e2a\u4f18\u70b9\u5373\uff1a\n\n\n\n\n\u80fd\u591f\u7ed9\u51fa\u5206\u7c7b\u7ed3\u679c\u7684\u7f6e\u4fe1\u5ea6\uff1b\n\n\n\u5b83\u662f\u4e00\u79cd\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\u3002\n\n\n\n\n\u6982\u7387\n\n\n\u6211\u4eec\u7528\nP(h|D)\nP(h|D)\n\u6765\u8868\u793a\nD\nD\n\u6761\u4ef6\u4e0b\u4e8b\u4ef6\nh\nh\n\u53d1\u751f\u7684\u6982\u7387\u3002\nP(h)\nP(h)\n\u8868\u793a\u4e8b\u4ef6\nh\nh\n\u53d1\u751f\u7684\u6982\u7387\uff0c\u79f0\u4e3ah\u7684\u5148\u9a8c\u6982\u7387\u3002\nP(h|d)\nP(h|d)\n\u79f0\u4e3a\u540e\u9a8c\u6982\u7387\uff0c\u8868\u793a\u5728\u89c2\u5bdf\u4e86\u6570\u636e\u96c6\nd\nd\n\u4e4b\u540e\uff0c\nh\nh\n\u4e8b\u4ef6\u53d1\u751f\u7684\u6982\u7387\u662f\u591a\u5c11\u3002\u540e\u9a8c\u6982\u7387\u53c8\u79f0\u4e3a\u6761\u4ef6\u6982\u7387\u3002\n\n\n\u8d1d\u53f6\u65af\u6cd5\u5219\n\n\n\u8d1d\u53f6\u65af\u6cd5\u5219\u63cf\u8ff0\u4e86\nP(h)\nP(h)\n\u3001\nP(h|D)\nP(h|D)\n\u3001\nP(D)\nP(D)\n\u3001\u4ee5\u53ca\nP(D|h)\nP(D|h)\n\u8fd9\u56db\u4e2a\u6982\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\uff1a\n\n\n\n\nP(h|D) = \\frac{P(D|h)P(h)}{P(D)}\n\n\nP(h|D) = \\frac{P(D|h)P(h)}{P(D)}\n\n\n\n\n\u5982\u679c\u6211\u4eec\u6709\nh_1, h_2,...h_n\nh_1, h_2,...h_n\n\u7b49\u4e8b\u4ef6\u3002\u8ba1\u7b97\u4e0d\u540c\u4e8b\u4ef6\u53d1\u751f\u7684\u6982\u7387\uff0c\n\n\n\n\nP(h_i|D) = \\frac{P(D|h_i)P(h_i)}{P(D)}\n\n\nP(h_i|D) = \\frac{P(D|h_i)P(h_i)}{P(D)}\n\n\n\n\n\u9009\u53d6\u6700\u5927\u7684\u6982\u7387\uff0c\u5c31\u80fd\u7528\u4f5c\u5206\u7c7b\u4e86\u3002\u8fd9\u79cd\u65b9\u6cd5\u53eb\u6700\u5927\u540e\u9a8c\u4f30\u8ba1\uff0c\u8bb0\u4e3a\nh_{MAP}\nh_{MAP}\n\uff1a\n\n\n\n\nh_{MAP} = \\arg \\max_{h\\in H} P(h|D) =  \\arg \\max_{h\\in H} \\frac{P(D|h)P(h)}{P(D)}\n\n\nh_{MAP} = \\arg \\max_{h\\in H} P(h|D) =  \\arg \\max_{h\\in H} \\frac{P(D|h)P(h)}{P(D)}\n\n\n\n\nH\u8868\u793a\u6240\u6709\u7684\u4e8b\u4ef6\uff0c\u6240\u4ee5h\u2208H\u8868\u793a\u201c\u5bf9\u4e8e\u96c6\u5408\u4e2d\u7684\u6bcf\u4e00\u4e2a\u4e8b\u4ef6\u201d\u3002\u6574\u4e2a\u516c\u5f0f\u7684\u542b\u4e49\u5c31\u662f\uff1a\u5bf9\u4e8e\u96c6\u5408\u4e2d\u7684\u6bcf\u4e00\u4e2a\u4e8b\u4ef6\uff0c\u8ba1\u7b97\u51fa\nP(h|D)\nP(h|D)\n\u7684\u503c\uff0c\u5e76\u53d6\u6700\u5927\u7684\u7ed3\u679c\u3002\n\n\n\u53ef\u4ee5\u53d1\u73b0\u5bf9\u4e8e\u6240\u6709\u7684\u4e8b\u4ef6\uff0c\u516c\u5f0f\u4e2d\u7684\u5206\u6bcd\u90fd\u662f\nP(D)\nP(D)\n\uff0c\u56e0\u6b64\u5373\u4fbf\u53ea\u8ba1\u7b97\nP(D|h)P(h)\nP(D|h)P(h)\n\uff0c\u4e5f\u53ef\u4ee5\u5224\u65ad\u51fa\u6700\u5927\u7684\u7ed3\u679c\u3002\u90a3\u4e48\u8fd9\u4e2a\u516c\u5f0f\u5c31\u53ef\u4ee5\u7b80\u5316\u4e3a\uff1a\n\n\n\n\nh_{MAP} = \\arg \\max_{h\\in H} P(D|h)P(h)\n\n\nh_{MAP} = \\arg \\max_{h\\in H} P(D|h)P(h)\n\n\n\n\n\u4f7f\u7528Python\u7f16\u5199\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\n\n\n\u5171\u548c\u515a\u8fd8\u662f\u6c11\u4e3b\u515a\n\n\n\u6570\u503c\u578b\u6570\u636e\n\n\n\u4f7f\u7528Python\u5b9e\u73b0", 
            "title": "Chapter 5: \u6982\u7387\u548c\u6734\u7d20\u8d1d\u53f6\u65af"
        }, 
        {
            "location": "/gdm/ch5/#-5", 
            "text": "", 
            "title": "\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 - 5 \u6734\u7d20\u8d1d\u53f6\u65af"
        }, 
        {
            "location": "/gdm/ch5/#_1", 
            "text": "\u4f7f\u7528\u8fd1\u90bb\u7b97\u6cd5\u65f6\uff0c\u6211\u4eec\u5f88\u96be\u5bf9\u5206\u7c7b\u7ed3\u679c\u7684\u7f6e\u4fe1\u5ea6\u8fdb\u884c\u91cf\u5316\u3002\u4f46\u5982\u679c\u4f7f\u7528\u7684\u662f\u57fa\u4e8e\u6982\u7387\u7684\u5206\u7c7b\u7b97\u6cd5\u2014\u2014\u8d1d\u53f6\u65af\u7b97\u6cd5\u2014\u2014\u90a3\u5c31\u53ef\u4ee5\u7ed9\u51fa\u5206\u7c7b\u7ed3\u679c\u7684\u53ef\u80fd\u6027\u4e86\uff1a\u8fd9\u540d\u8fd0\u52a8\u5458\u670980%\u7684\u51e0\u7387\u662f\u7bee\u7403\u8fd0\u52a8\u5458\u3002  \u8fd1\u90bb\u7b97\u6cd5\u53c8\u79f0\u4e3a \u88ab\u52a8\u5b66\u4e60\u7b97\u6cd5 \u3002\u8fd9\u79cd\u7b97\u6cd5\u53ea\u662f\u5c06\u8bad\u7ec3\u96c6\u7684\u6570\u636e\u4fdd\u5b58\u8d77\u6765\uff0c\u5728\u6536\u5230\u6d4b\u8bd5\u6570\u636e\u65f6\u624d\u4f1a\u8fdb\u884c\u8ba1\u7b97\u3002\u5982\u679c\u6211\u4eec\u670910\u4e07\u9996\u97f3\u4e50\uff0c\u90a3\u6bcf\u8fdb\u884c\u4e00\u6b21\u5206\u7c7b\uff0c\u90fd\u9700\u8981\u904d\u5386\u8fd910\u4e07\u6761\u8bb0\u5f55\u624d\u884c\u3002  \u8d1d\u53f6\u65af\u7b97\u6cd5\u5219\u662f\u4e00\u79cd \u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5 \u3002\u5b83\u4f1a\u6839\u636e\u8bad\u7ec3\u96c6\u6784\u5efa\u8d77\u4e00\u4e2a\u6a21\u578b\uff0c\u5e76\u7528\u8fd9\u4e2a\u6a21\u578b\u6765\u5bf9\u65b0\u7684\u8bb0\u5f55\u8fdb\u884c\u5206\u7c7b\uff0c\u56e0\u6b64\u901f\u5ea6\u4f1a\u5feb\u5f88\u591a\u3002  \u8d1d\u53f6\u65af\u7b97\u6cd5\u7684\u4e24\u4e2a\u4f18\u70b9\u5373\uff1a   \u80fd\u591f\u7ed9\u51fa\u5206\u7c7b\u7ed3\u679c\u7684\u7f6e\u4fe1\u5ea6\uff1b  \u5b83\u662f\u4e00\u79cd\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\u3002", 
            "title": "\u6734\u7d20\u8d1d\u53f6\u65af"
        }, 
        {
            "location": "/gdm/ch5/#_2", 
            "text": "\u6211\u4eec\u7528 P(h|D) P(h|D) \u6765\u8868\u793a D D \u6761\u4ef6\u4e0b\u4e8b\u4ef6 h h \u53d1\u751f\u7684\u6982\u7387\u3002 P(h) P(h) \u8868\u793a\u4e8b\u4ef6 h h \u53d1\u751f\u7684\u6982\u7387\uff0c\u79f0\u4e3ah\u7684\u5148\u9a8c\u6982\u7387\u3002 P(h|d) P(h|d) \u79f0\u4e3a\u540e\u9a8c\u6982\u7387\uff0c\u8868\u793a\u5728\u89c2\u5bdf\u4e86\u6570\u636e\u96c6 d d \u4e4b\u540e\uff0c h h \u4e8b\u4ef6\u53d1\u751f\u7684\u6982\u7387\u662f\u591a\u5c11\u3002\u540e\u9a8c\u6982\u7387\u53c8\u79f0\u4e3a\u6761\u4ef6\u6982\u7387\u3002", 
            "title": "\u6982\u7387"
        }, 
        {
            "location": "/gdm/ch5/#_3", 
            "text": "\u8d1d\u53f6\u65af\u6cd5\u5219\u63cf\u8ff0\u4e86 P(h) P(h) \u3001 P(h|D) P(h|D) \u3001 P(D) P(D) \u3001\u4ee5\u53ca P(D|h) P(D|h) \u8fd9\u56db\u4e2a\u6982\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\uff1a   P(h|D) = \\frac{P(D|h)P(h)}{P(D)}  P(h|D) = \\frac{P(D|h)P(h)}{P(D)}   \u5982\u679c\u6211\u4eec\u6709 h_1, h_2,...h_n h_1, h_2,...h_n \u7b49\u4e8b\u4ef6\u3002\u8ba1\u7b97\u4e0d\u540c\u4e8b\u4ef6\u53d1\u751f\u7684\u6982\u7387\uff0c   P(h_i|D) = \\frac{P(D|h_i)P(h_i)}{P(D)}  P(h_i|D) = \\frac{P(D|h_i)P(h_i)}{P(D)}   \u9009\u53d6\u6700\u5927\u7684\u6982\u7387\uff0c\u5c31\u80fd\u7528\u4f5c\u5206\u7c7b\u4e86\u3002\u8fd9\u79cd\u65b9\u6cd5\u53eb\u6700\u5927\u540e\u9a8c\u4f30\u8ba1\uff0c\u8bb0\u4e3a h_{MAP} h_{MAP} \uff1a   h_{MAP} = \\arg \\max_{h\\in H} P(h|D) =  \\arg \\max_{h\\in H} \\frac{P(D|h)P(h)}{P(D)}  h_{MAP} = \\arg \\max_{h\\in H} P(h|D) =  \\arg \\max_{h\\in H} \\frac{P(D|h)P(h)}{P(D)}   H\u8868\u793a\u6240\u6709\u7684\u4e8b\u4ef6\uff0c\u6240\u4ee5h\u2208H\u8868\u793a\u201c\u5bf9\u4e8e\u96c6\u5408\u4e2d\u7684\u6bcf\u4e00\u4e2a\u4e8b\u4ef6\u201d\u3002\u6574\u4e2a\u516c\u5f0f\u7684\u542b\u4e49\u5c31\u662f\uff1a\u5bf9\u4e8e\u96c6\u5408\u4e2d\u7684\u6bcf\u4e00\u4e2a\u4e8b\u4ef6\uff0c\u8ba1\u7b97\u51fa P(h|D) P(h|D) \u7684\u503c\uff0c\u5e76\u53d6\u6700\u5927\u7684\u7ed3\u679c\u3002  \u53ef\u4ee5\u53d1\u73b0\u5bf9\u4e8e\u6240\u6709\u7684\u4e8b\u4ef6\uff0c\u516c\u5f0f\u4e2d\u7684\u5206\u6bcd\u90fd\u662f P(D) P(D) \uff0c\u56e0\u6b64\u5373\u4fbf\u53ea\u8ba1\u7b97 P(D|h)P(h) P(D|h)P(h) \uff0c\u4e5f\u53ef\u4ee5\u5224\u65ad\u51fa\u6700\u5927\u7684\u7ed3\u679c\u3002\u90a3\u4e48\u8fd9\u4e2a\u516c\u5f0f\u5c31\u53ef\u4ee5\u7b80\u5316\u4e3a\uff1a   h_{MAP} = \\arg \\max_{h\\in H} P(D|h)P(h)  h_{MAP} = \\arg \\max_{h\\in H} P(D|h)P(h)", 
            "title": "\u8d1d\u53f6\u65af\u6cd5\u5219"
        }, 
        {
            "location": "/gdm/ch5/#python", 
            "text": "", 
            "title": "\u4f7f\u7528Python\u7f16\u5199\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668"
        }, 
        {
            "location": "/gdm/ch5/#_4", 
            "text": "", 
            "title": "\u5171\u548c\u515a\u8fd8\u662f\u6c11\u4e3b\u515a"
        }, 
        {
            "location": "/gdm/ch5/#_5", 
            "text": "", 
            "title": "\u6570\u503c\u578b\u6570\u636e"
        }, 
        {
            "location": "/gdm/ch5/#python_1", 
            "text": "", 
            "title": "\u4f7f\u7528Python\u5b9e\u73b0"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/", 
            "text": "Spark Streaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee\n\n\n\u8be5\u9879\u76ee\u4ece\u5b9e\u65f6\u6570\u636e\u4ea7\u751f\u548c\u6d41\u5411\u7684\u4e0d\u540c\u73af\u8282\u51fa\u53d1\uff0c\u901a\u8fc7\u96c6\u6210\u4e3b\u6d41\u7684\u5206\u5e03\u5f0f\u65e5\u5fd7\u6536\u96c6\u6846\u67b6Flume\u3001\u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217Kafka\u3001\u5206\u5e03\u5f0f\u5217\u5f0f\u6570\u636e\u5e93HBase\u3001\u4ee5\u53caSpark Streaming\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5904\u7406\u3002\n\n\n1 \u521d\u8bc6\u5b9e\u65f6\u6d41\u5904\u7406\n\n\n\u4e1a\u52a1\u73b0\u72b6\u5206\u6790\n\n\n\u9700\u6c42\uff1a\u7edf\u8ba1\u4e3b\u7ad9\u6bcf\u4e2a\uff08\u6307\u5b9a\uff09\u8bfe\u7a0b\u8bbf\u95ee\u7684\u5ba2\u6237\u7aef\u3001\u5730\u57df\u4fe1\u606f\u5206\u5e03\n\n\n==\n \u5982\u4e0a\u4e24\u4e2a\u64cd\u4f5c\uff1a\u91c7\u7528\u79bb\u7ebf\uff08spark/mapreduce\uff09\u7684\u65b9\u5f0f\u8fdb\u884c\u7edf\u8ba1\n\n\n\u5b9e\u73b0\u6b65\u9aa4\uff1a\n\n\n\n\n\u8bfe\u7a0b\u7f16\u53f7\uff0cip\u4fe1\u606f\uff0cuser-agent\n\n\n\u8fdb\u884c\u76f8\u5e94\u7684\u7edf\u8ba1\u5206\u6790\u64cd\u4f5c\uff1aMapReduce/Spark\n\n\n\n\n\u9879\u76ee\u67b6\u6784\uff1a\n\n\n\n\n\u65e5\u5fd7\u6536\u96c6\uff1aFlume\n\n\n\u79bb\u7ebf\u5206\u6790\uff1aMapReduce/Spark\n\n\n\u7edf\u8ba1\u7ed3\u679c\u56fe\u5f62\u5316\u5c55\u793a\n\n\n\n\n\u95ee\u9898\uff1a\n\n\n\n\n\u5c0f\u65f6\u7ea7\u522b\n\n\n10\u5206\u949f\n\n\n\u79d2\u7ea7\u522b\n\n\n\n\n\u5b9e\u65f6\u6d41\u5904\u7406\u4ea7\u751f\u80cc\u666f\n\n\n\n\n\u65f6\u6548\u6027\u9ad8\n\n\n\u6570\u636e\u91cf\u5927\n\n\n\n\n\u5b9e\u65f6\u6d41\u5904\u7406\u6982\u8ff0\n\n\nhttps://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101\n\n\n\n\n\u5b9e\u65f6\u8ba1\u7b97 apache storm\n\n\n\u6d41\u5f0f\u8ba1\u7b97\n\n\n\u5b9e\u65f6\u6d41\u5f0f\u8ba1\u7b97\n\n\n\n\n\u79bb\u7ebf\u8ba1\u7b97\u4e0e\u5b9e\u65f6\u8ba1\u7b97\u5bf9\u6bd4\n\n\n\n\n\u6570\u636e\u6765\u6e90\n\n\n\u79bb\u7ebf\uff1a\u6765\u81eaHDFS\u4e0a\u7684\u5386\u53f2\u6570\u636e\uff0c\u6570\u636e\u91cf\u6bd4\u8f83\u5927\n\n\n\u5b9e\u65f6\uff1a\u6765\u81ea\u6d88\u606f\u961f\u5217(Kafka)\uff0c\u662f\u5b9e\u65f6\u65b0\u589e/\u4fee\u6539\u8bb0\u5f55\u8fc7\u6765\u7684\u67d0\u4e00\u7b14\u6570\u636e\n\n\n\n\n\n\n\u5904\u7406\u8fc7\u7a0b\n\n\n\u79bb\u7ebf\uff1aMapReduce, map + reduce\n\n\n\u5b9e\u65f6: Spark(DStream/SS) \n\n\n\n\n\n\n\u5904\u7406\u901f\u5ea6\n\n\n\u79bb\u7ebf\uff1a\u5e54\n\n\n\u5b9e\u65f6\uff1a\u5feb\u901f \n\n\n\n\n\n\n\u8fdb\u7a0b\n\n\n\u79bb\u7ebf\uff1a\u8fdb\u7a0b\u6709\u542f\u52a8+\u9500\u6bc1\u7684\u8fc7\u7a0b\n\n\n\u5b9e\u65f6\uff1a 7*24\u5c0f\u65f6\u8fd0\u884c\n\n\n\n\n\n\n\n\n\u5b9e\u65f6\u6d41\u5904\u7406\u6846\u67b6\u5bf9\u6bd4\n\n\n\n\nApache Storm\n\n\n\n\n\n\nApache Storm is a free and open source distributed \nrealtime\n computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use!\n\n\n\n\n\n\nApache Spark Streaming\n\n\n\n\n\n\n\u5b9e\u9645\u4e0a\u662f\u5fae\u6279\u5904\u7406\uff08\u6279\u5904\u7406\u95f4\u9694\u975e\u5e38\u5c0f)\n\n\n\n\n\n\nApache kafka\n\n\nApache Flink\n\n\n\n\n\n\nApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.\n\n\n\n\n\u5b9e\u65f6\u6d41\u5904\u7406\u67b6\u6784\u548c\u6280\u672f\u9009\u578b\n\n\n\n\n\u52a0\u4e00\u5c42flume\u6d88\u606f\u961f\u5217\uff0c\u4e3b\u8981\u4e3a\u4e86\u51cf\u8f7b\u538b\u529b\uff0c\u8d77\u5230\u7f13\u51b2\u4f5c\u7528\n\n\n\n\n\u5b9e\u65f6\u6d41\u5904\u7406\u5728\u4f01\u4e1a\u4e2d\u7684\u5e94\u7528\n\n\n\n\n\u7535\u4fe1\u884c\u4e1a\uff1a \u4f60\u7684\u624b\u673a\u5957\u9910\u6d41\u91cf\u7528\u5b8c\uff0c\u6536\u5230\u77ed\u4fe1\u63d0\u793a\n\n\n\u7535\u5546\u884c\u4e1a\uff1a\u641c\u7d22\u5546\u54c1\u65f6\uff0c\u8fdb\u884c\u63a8\u8350\n\n\n\n\n2 \u5206\u5e03\u5f0f\u65e5\u5fd7\u6536\u96c6\u6846\u67b6Flume\n\n\nsee detail in Hadoop: definitive Guide, \nChapter 14\n\n\n\u4e1a\u52a1\u73b0\u72b6\u5206\u6790\n\n\nYou have a lot of servers and systems\n\n\n\n\nnetwork devices\n\n\noperating system\n\n\nweb servers\n\n\napplications\n\n\n\n\nAnd they generate large amount of logs and other data.\n\n\nProblem: Since you have a business idea, how to implement the idea?\n\n\nOPTION: You may move logs and data generated to hadoop hdfs directly.\n\n\n\u4f46\u662f\u5b58\u5728\u95ee\u9898\uff1a\n\n\n\n\n\u5982\u4f55\u505a\u76d1\u63a7\n\n\n\u5982\u4f55\u4fdd\u8bc1\u65f6\u6548\u6027\n\n\n\u76f4\u63a5\u4f20\u9001\u6587\u672c\u6570\u636e\uff0c\u5f00\u9500\u592a\u5927\n\n\n\u5bb9\u9519\n\n\n\u8d1f\u8f7d\u5747\u8861\n\n\n\n\nSOLUTION: \u4f7f\u7528Flume\uff0c\u57fa\u672c\u4e0a\u5199\u914d\u7f6e\u6587\u4ef6\u5c31OK\u4e86\uff0cFlume\u81ea\u52a8\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002\n\n\nFlume\u6982\u8ff0\n\n\n\n\nFlume is a distributed, reliable, and available service for efficiently \ncollecting, aggregating, and moving large amounts of log data\n. It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application. [\nApache Flume\n]\n\n\n\n\nFlume\u67b6\u6784\u53ca\u6838\u5fc3\u7ec4\u4ef6\n\n\n\n\nsee detail in Hadoop: definitive Guide, \nChapter 14\n\n\nFlume\u5b9e\u6218\n\n\n\u9700\u6c42\uff1a \u4ece\u6307\u5b9a\u7f51\u7edc\u7aef\u53e3\u91c7\u96c6\u6570\u636e\n\n\n\u4f7f\u7528Flume\u7684\u5173\u952e\u5c31\u662f\u5199\u914d\u7f6e\u6587\u4ef6\n\n\n\n\n\u914d\u7f6eSource, Channel, Sink\n\n\n\u628a\u4ee5\u4e0a\u4e09\u4e2a\u7ec4\u4ef6\u4e32\u8d77\u6765\n\n\n\n\n \nhttp://flume.apache.org/FlumeUserGuide.html#example-2\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 44444\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n\n\n\n\n\nnetcat source\n: A netcat-like source that listens on a given port and turns each line of text into an event. It opens a specified port and listens for data. The expectation is that the supplied data is newline separated text. Each line of text is turned into a Flume event and sent via the connected channel. [\nNetCat TCP Source\n]\n\n\nlogger sink\n: Logs event at INFO level. Typically useful for testing/debugging purpose.  [\nLogger Sink\n]\n\n\nmemory channel\n: The events are stored in an in-memory queue with configurable max size. It\u2019s ideal for flows that need higher throughput and are prepared to lose the staged data in the event of an agent failures. [\nmemory channel\n]\n\n\n\n\n \n## \u542f\u52a8flume\n\n$ flume-ng agent \n\\\n\n--name a1 \n\\ \n \n# agent name\n\n--conf \n$F\nLUME_HOME/conf \n\\ \n# use configs in \nconf\n directory\n\n--conf-file  example.conf \n\\ \n# specify a config file\n\n-Dflume.root.logger\n=\nINFO,console \n# sets a Java system property value\n\n\n\n## \u5728\u53e6\u5916\u4e00\u4e2aterminal\u7528telnet\u6a21\u62df\u6570\u636e\u6e90\n\n$ telnet localhost \n44444\n \nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is \n^]\n.\nhello\nOK\nhellomy\nOK\n\n\n\n\u9700\u6c42\uff1a \u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\u5b9e\u65f6\u91c7\u96c6\u65b0\u589e\u7684\u6570\u636e\u8f93\u51fa\u5230\u63a7\u5236\u53f0\n\n\nAgent\u9009\u578b\uff1a exec source + memory channel + logger sink\n\n\n \n# filename: exec-memeory-logger.conf\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /tmp/data.log\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n\n\n\n\n\nexec source\n runs a given Unix command on start-up and expects that process to continuously produce data on standard out (stderr is simply discarded, unless property logStdErr is set to true). If the process exits for any reason, the source also exits and will produce no further data. This means configurations such as cat [named pipe] or tail -F [file] are going to produce the desired results where as date will probably not - the former two commands produce streams of data where as the latter produces a single event and exits. [\nexec source\n]\n\n\n\n\n\u5c06\u5185\u5bb9\u8f93\u5165\u5230\n/tmp/data.log\n\u6587\u4ef6\u4e2d\uff1a\n\n\n \n$ \necho\n \nhello\n \n data.log\n$ \necho\n \nhello\n \n data.log\n\n\n\n\u9700\u6c42\uff1a \u5c06A\u670d\u52a1\u5668\u4e0a\u7684\u65e5\u5fd7\u5b9e\u65f6\u91c7\u96c6\u5230B\u670d\u52a1\u5668\n\n\n\u65e5\u5fd7\u6536\u96c6\u8fc7\u7a0b\uff1a\n\n\n\n\n\u673a\u56681\u4e0a\u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\uff0c\u5f53\u6211\u4eec\u8bbf\u95ee\u4e3b\u7ad9\u65f6\u4f1a\u6709\u7528\u6237\u884c\u4e3a\u65e5\u5fd7\u8bb0\u5f55\u5230\naccess.log\n\u4e2d\u3002\n\n\navro sink\u628a\u65b0\u4ea7\u751f\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u5bf9\u5e94\u7684avro source\u6307\u5b9a\u7684hostname\u548cport\u4e0a\u3002\n\n\n\u901a\u8fc7avro\u5bf9\u5e94\u7684agent\u5c06\u6211\u4eec\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u63a7\u5236\u53f0\u3002\n\n\n\n\n\n\n\n\navro sink\n: forms one half of Flume\u2019s tiered collection support. Flume events sent to this sink are turned into Avro events and sent to the configured hostname / port pair. [\nAvro sink\n]\n\n\n\n\nExec-Memeory-Avro.conf\n# filename: exec-memeory-avro.conf\n\n# Name the components on this agent\na1.sources = exec-source\na1.sinks = avro-sink\na1.channels = memory-channel\n\n# Describe/configure the source\na1.sources.exec-source.type = exec\na1.sources.exec-source.command = tail -F /tmp/data.log\n\n# Describe the sink\na1.sinks.avro-sink.type = avro\na1.sinks.avro-sink.hostname = localhost\na1.sinks.avro-sink.port = 44444\n\n# Use a channel which buffers events in memory\na1.channels.memory-channel.type = memory\na1.channels.memory-channel.capacity = 1000\na1.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.exec-source.channels = memory-channel\na1.sinks.avro-sink.channel = memory-channel\n\nAvro-Memeory-Logger.conf\n# filename: avro-memeory-logger.conf\n\n# Name the components on this agent\na2.sources = avro-source\na2.sinks = logger-sink\na2.channels = memory-channel\n\n# Describe/configure the source\na2.sources.avro-source.type = avro\na2.sources.avro-source.bind = localhost\na2.sources.avro-source.port = 44444\n\n# Describe the sink\na2.sinks.logger-sink.type = logger\n\n# Use a channel which buffers events in memory\na2.channels.memory-channel.type = memory\na2.channels.memory-channel.capacity = 1000\na2.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na2.sources.avro-source.channels = memory-channel\na2.sinks.logger-sink.channel = memory-channel\n\n\n\n\n\u542f\u52a8flume\uff0c \u6ce8\u610f\u4e24\u4e2aagent\u7684\u542f\u52a8\u987a\u5e8f\n\n\n \n$ flume-ng agent \n\\\n\n--name a2 \n\\\n\n--conf \n$F\nLUME-HOME/conf \n\\\n\n--conf-file avro-memory-logger.conf \n\\\n\n-Dflume.root.logger\n=\nINFO,console\n\n$ flume-ng agent \n\\\n\n--name a1 \n\\\n\n--conf \n$F\nLUME-HOME/conf \n\\\n\n--conf-file exec-memory-avro.conf \n\\\n\n-Dflume.root.logger\n=\nINFO,console\n\n\n\n\u5c06\u5185\u5bb9\u8f93\u5165\u5230\n/tmp/data.log\n\u6587\u4ef6\u4e2d\uff1a\n\n\n \n$ \necho\n \nwelcome\n \n data.log\n$ \necho\n \nwelcome\n \n data.log\n\n\n\n3 \u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217Kafka\n\n\nFirst a few concepts:\n\n\n\n\nKafka is run as a cluster on one or more servers that can span multiple datacenters.\n\n\nThe Kafka cluster stores streams of \nrecords\n in categories called \ntopic\ns.\n\n\nEach record consists of a key, a value, and a timestamp.\n\n\nBroker\ns are the Kafka processes that manage topics and partitions and serve producer and consumer request.\n\n\n\n\n\n\nKafka\u90e8\u7f72\u53ca\u4f7f\u7528\n\n\n\u5355\u8282\u70b9\u5355Broker\u90e8\u7f72\u53ca\u4f7f\u7528\n\n\n \n# \u542f\u52a8Zookeeper\n\n$ zkServer.sh start\n\n# \u542f\u52a8kafka\n\n$ kafka-server-start.sh $KAFKA_HOME/config/server.properties\n\n# \u521b\u5efa\u540d\u4e3atest\u7684topic(single partition and only one replica)\n\n$ kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor \n1\n --partitions \n1\n --topic \ntest\n\n\n# \u67e5\u770btopic\n\n$ kafka-topics.sh --list --zookeeper localhost:2181\n\n### \u542f\u52a8\u751f\u4ea7\u8005, 9092\u662fserver\u76d1\u542c\u7aef\u53e3\n\n$ kafka-console-producer.sh --broker-list localhost:9092 --topic \ntest\n\n\n This is a message\n\n This is another message\n\n### \u542f\u52a8\u6d88\u8d39\u8005 --from-beginning\u4ece\u5934\u5f00\u59cb\u63a5\u6536\u6d88\u606f\n\n$ kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic \ntest\n --from-beginning\nThis is a message\nThis is another message\n\n### \u67e5\u770b\u6240\u6709topics\u7684\u8be6\u7ec6\u4fe1\u606f\n\n$ kafka-topics.sh --describe --zookeeper localhost:2181\n\n### \u67e5\u770b\u6307\u5b9atopic\u7684\u8be6\u7ec6\u4fe1\u606f\n\n$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic \ntest\n\n\n\n\n\u5355\u8282\u70b9\u591aBroker\u90e8\u7f72\u53ca\u4f7f\u7528\n\n\n \ncp $KAFKA_HOME/config/server.properties $KAFKA_HOME/config/server-1.properties\ncp $KAFKA_HOME/config/server.properties $KAFKA_HOME/config/server-2.properties\n\n\n\n\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\n\n\n \nconfig/server-1.properties:\n    broker.id=1\n    listeners=PLAINTEXT://:9093\n    log.dirs=/tmp/kafka-logs-1\n\nconfig/server-2.properties:\n    broker.id=2\n    listeners=PLAINTEXT://:9094\n    log.dirs=/tmp/kafka-logs-2\n\n\n\n\u542f\u52a8kafka\n\n\n \n# \u542f\u52a8ZooKeeper\n\n$ zkServer.sh start\n\n# \u542f\u52a8kafka server\n\n$ kafka-server-start.sh $KAFKA_HOME/config/server.properties \n\n$ kafka-server-start.sh $KAFKA_HOME/config/server-1.properties \n\n$ kafka-server-start.sh $KAFKA_HOME/config/server-2.properties \n\n\n# \u521b\u5efatopic, 1\u4e2a\u5206\u533a\uff0c\u4e09\u4e2a\u526f\u672c\n\n$ kafka-topics.sh --create --zookeeper localhost:2181 \n\\\n\n    --replication-factor \n3\n --partitions \n1\n --topic my-replicated-topic\n\n# \u67e5\u770btopic\u4fe1\u606f\n\n$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic\nTopic:my-replicated-topic   PartitionCount:1    ReplicationFactor:3 Configs:\n    Topic: my-replicated-topic  Partition: \n0\n    Leader: \n2\n   Replicas: 2,0,1 Isr: 2,0,1\n\n# \u542f\u52a8\u751f\u4ea7\u8005\n\n$ kafka-console-producer.sh --broker-list localhost:9092, localhost:9093, localhost:9094 --topic my-replicated-topic\n\n# \u542f\u52a8\u6d88\u8d39\u8005\n\n$ kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic\n\n\n\nKafka Java \u7f16\u7a0b\n\n\n\u4f7f\u7528\u547d\u4ee4\u884c\u603b\u662f\u4e0d\u65b9\u4fbf\u7684\uff0c\u4e0b\u9762\u6211\u4eec\u5c1d\u8bd5\u7740\u4f7f\u7528Kafka Java API\u7f16\u7a0b\uff0c\u5b9e\u9645\u64cd\u4f5c\u5185\u5bb9\u548c\u4e0a\u4e00\u8282\u662f\u4e00\u6478\u4e00\u6837\u7684\uff0c\u6240\u4ee5\u76f4\u63a5\u9644\u4e0a\u4ee3\u7801\u4e86\u3002\u6ce8\u610f\u8fd9\u91cc\u4f7f\u7528\u7684API\u662f0.8.2\u7248\u672c\u4ee5\u540e\u7684\uff0c\u4e4b\u524d\u7248\u672c\u4e0e\u4e4b\u540e\u7248\u672c\u7684API\u76f8\u5dee\u975e\u5e38\u5927\u3002\n\n\nProducer\nimport\n \norg.apache.kafka.clients.producer.KafkaProducer\n;\n\n\nimport\n \norg.apache.kafka.clients.producer.ProducerRecord\n;\n\n\nimport\n \njava.util.Properties\n;\n\n\n/**\n\n\n * Kafka\u751f\u4ea7\u8005\n\n\n * \u89c1\u5b98\u65b9\u6587\u6863\n\n\n * http://kafka.apache.org/20/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html\n\n\n */\n\n\npublic\n \nclass\n \nMyKafkaProducer\n \nimplements\n \nRunnable\n \n{\n\n    \nprivate\n \nString\n \ntopic\n;\n\n    \nprivate\n \nKafkaProducer\nString\n,\n \nString\n \nproducer\n;\n\n\n    \npublic\n \nMyKafkaProducer\n(\nString\n \ntopic\n)\n \n{\n\n        \nthis\n.\ntopic\n \n=\n \ntopic\n;\n\n        \nProperties\n \nprops\n \n=\n \nnew\n \nProperties\n();\n\n        \nprops\n.\nput\n(\nbootstrap.servers\n,\n \nlocalhost:9092\n);\n\n        \nprops\n.\nput\n(\nacks\n,\n \nall\n);\n\n        \nprops\n.\nput\n(\nkey.serializer\n,\n \norg.apache.kafka.common.serialization.StringSerializer\n);\n\n        \nprops\n.\nput\n(\nvalue.serializer\n,\n \norg.apache.kafka.common.serialization.StringSerializer\n);\n\n        \nproducer\n \n=\n \nnew\n \nKafkaProducer\nString\n,\n \nString\n(\nprops\n);\n\n    \n}\n\n\n    \npublic\n \nvoid\n \nrun\n()\n \n{\n\n        \nint\n \nmessageNumber\n \n=\n \n1\n;\n\n        \nwhile\n \n(\ntrue\n)\n \n{\n\n            \nString\n \nmessage\n \n=\n \nmessage\n \n+\n \nmessageNumber\n;\n\n            \nproducer\n.\nsend\n(\nnew\n \nProducerRecord\nString\n,\n \nString\n(\ntopic\n,\n \nmessage\n));\n\n            \nmessageNumber\n++;\n\n            \ntry\n{\n\n                \nThread\n.\nsleep\n(\n5000\n);\n\n            \n}\n \ncatch\n \n(\nException\n \nex\n)\n \n{\n\n                \nex\n.\nprintStackTrace\n();\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n}\n\n\nConsumer\nimport\n \norg.apache.kafka.clients.consumer.ConsumerRecord\n;\n\n\nimport\n \norg.apache.kafka.clients.consumer.ConsumerRecords\n;\n\n\nimport\n \norg.apache.kafka.clients.consumer.KafkaConsumer\n;\n\n\n\nimport\n \njava.time.Duration\n;\n\n\nimport\n \njava.util.Arrays\n;\n\n\nimport\n \njava.util.List\n;\n\n\nimport\n \njava.util.Properties\n;\n\n\nimport\n \njava.util.concurrent.atomic.AtomicBoolean\n;\n\n\n\n/**\n\n\n * Kafka\u6d88\u8d39\u8005\n\n\n * \u5b98\u65b9\u6587\u6863\n\n\n * http://kafka.apache.org/20/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html\n\n\n */\n\n\npublic\n \nclass\n \nMyKafkaConsumer\n \nimplements\n \nRunnable\n \n{\n\n    \nprivate\n \nfinal\n \nAtomicBoolean\n \nclosed\n \n=\n \nnew\n \nAtomicBoolean\n(\nfalse\n);\n\n    \nprivate\n \nString\n \ntopic\n;\n\n    \nprivate\n \nKafkaConsumer\nString\n,\n \nString\n \nconsumer\n;\n\n    \nprivate\n \nConsumerRecords\nString\n,\n \nString\n \nrecords\n;\n\n\n    \npublic\n \nMyKafkaConsumer\n(\nString\n \ntopic\n)\n \n{\n\n        \nthis\n.\ntopic\n \n=\n \ntopic\n;\n\n        \nProperties\n \nprops\n \n=\n \nnew\n \nProperties\n();\n\n        \n// connect to cluster\n\n        \nprops\n.\nput\n(\nbootstrap.servers\n,\n \nlocalhost:9092\n);\n\n        \n//  subscribing to the topics- test\n\n        \nprops\n.\nput\n(\ngroup.id\n,\n \ntest\n);\n\n        \n//  offsets are committed automatically\n\n        \nprops\n.\nput\n(\nenable.auto.commit\n,\n \ntrue\n);\n\n        \n// specify how to turn bytes into objects\n\n        \nprops\n.\nput\n(\nkey.deserializer\n,\n \norg.apache.kafka.common.serialization.StringDeserializer\n);\n\n        \nprops\n.\nput\n(\nvalue.deserializer\n,\n \norg.apache.kafka.common.serialization.StringDeserializer\n);\n\n        \nconsumer\n \n=\n \nnew\n \nKafkaConsumer\n(\nprops\n);\n\n\n    \n}\n\n\n    \npublic\n \nvoid\n \nrun\n()\n \n{\n\n        \ntry\n \n{\n\n            \n// subsribes to topic\n\n            \nconsumer\n.\nsubscribe\n(\nArrays\n.\nasList\n(\ntopic\n));\n\n            \nwhile\n \n(!\nclosed\n.\nget\n())\n \n{\n\n                \nrecords\n \n=\n \nconsumer\n.\npoll\n(\nDuration\n.\nofMillis\n(\n10000\n));\n\n                \nfor\n \n(\nConsumerRecord\nString\n,\n \nString\n \nrecord\n \n:\n \nrecords\n)\n\n                    \nSystem\n.\nout\n.\nprintf\n(\noffset = %d, key = %s, value = %s%n\n,\n \nrecord\n.\noffset\n(),\n \nrecord\n.\nkey\n(),\n \nrecord\n.\nvalue\n());\n\n            \n}\n\n        \n}\n \ncatch\n \n(\nException\n \ne\n)\n \n{\n\n            \n// Ignore exception if closing\n\n            \nif\n \n(!\nclosed\n.\nget\n())\n \nthrow\n \ne\n;\n\n        \n}\n \nfinally\n \n{\n\n            \nconsumer\n.\nclose\n();\n\n        \n}\n\n\n    \n}\n\n\n    \n// Shutdown hook which can be called from a separate thread\n\n    \npublic\n \nvoid\n \nshutdown\n()\n \n{\n\n        \nclosed\n.\nset\n(\ntrue\n);\n\n        \nconsumer\n.\nwakeup\n();\n\n    \n}\n\n\n\n}\n\n\nClientapp\npublic\n \nclass\n \nClientApp\n \n{\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \n{\n\n        \nThread\n \njob\n \n=\n \nnew\n \nThread\n(\nnew\n \nMyKafkaProducer\n(\ntest\n));\n\n        \njob\n.\nstart\n();\n\n        \nThread\n \njob2\n \n=\n \nnew\n \nThread\n(\nnew\n \nMyKafkaConsumer\n(\ntest\n));\n\n        \njob2\n.\nstart\n();\n\n    \n}\n\n\n}\n\n\n\n\n\n\u6574\u5408Flume\u548cKafka\u5b8c\u6210\u5b9e\u65f6\u6570\u636e\u91c7\u96c6\n\n\n\u4e3a\u4e86\u5c06Flume\u7684\u8f93\u51fa\u5230Kafka\uff0c\u53ef\u4ee5\u5c06agent2\u7684logger sink\u66ff\u6362\u6210Kafka Sink\u3002\u7136\u540e\u542f\u52a8\u4e00\u4e2aKafka consumer\u4eceKafka sink\u8ba2\u9605\u6d88\u606f\u3002\n\n\n\n\n\n\nkafka sink\n can publish data to a Kafka topic. One of the objective is to integrate Flume with Kafka so that pull based processing systems can process the data coming through various Flume sources. [\nKafka Sink\n]\n\n\n\n\n\u4e0b\u9762\u662fagent2\u5bf9\u5e94\u7684Kafka\u914d\u7f6e\u6587\u4ef6\uff0c\u5728\u8fd9\u91ccagent2\u6539\u540d\u4e3a\navro-memory-kafka\n\u3002\n\n\n \n# filename: avro-memeory-kafka.conf\n\n# Name the components on this agent\navro-memory-kafka.sources = avro-source\navro-memory-kafka.sinks = kafka-sink\navro-memory-kafka.channels = memory-channel\n\n# Describe/configure the source\navro-memory-kafka.sources.avro-source.type = avro\navro-memory-kafka.sources.avro-source.bind = localhost\navro-memory-kafka.sources.avro-source.port = 44444\n\n# Describe the sink\navro-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink\navro-memory-kafka.sinks.kafka-sink.kafka.bootstrap.servers = localhost:9092\navro-memory-kafka.sinks.kafka-sink.kafka.topic = test\n\n\n# Use a channel which buffers events in memory\navro-memory-kafka.channels.memory-channel.type = memory\navro-memory-kafka.channels.memory-channel.capacity = 1000\navro-memory-kafka.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\navro-memory-kafka.sources.avro-source.channels = memory-channel\navro-memory-kafka.sinks.kafka-sink.channel = memory-channel\n\n\n\n\u4e0b\u9762\u662f\u5177\u4f53\u7684\u64cd\u4f5c\u6d41\u7a0b\uff0c\u540c\u6837\u9700\u8981\u6ce8\u610f\u4e24\u4e2aagent\u7684\u542f\u52a8\u987a\u5e8f\uff1a\n\n\n \n## \u542f\u52a8zookeeper, kafka\uff0c\u7701\u7565\n\n\n## \u542f\u52a8agent\n\n$ flume-ng agent \n\\\n\n--name avro-memory-kafka \n\\\n\n--conf \n$F\nLUME-HOME/conf \n\\\n\n--conf-file avro-memory-kafka.conf \n\\\n\n-Dflume.root.logger\n=\nINFO,console\n\n$ flume-ng agent \n\\\n\n--name a1 \n\\\n\n--conf \n$F\nLUME-HOME/conf \n\\\n\n--conf-file exec-memory-avro.conf \n\\\n\n-Dflume.root.logger\n=\nINFO,console\n\n\n## \u542f\u52a8\u6d88\u8d39\u8005 \n\n$ kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic \ntest\n\n\n\n\n4 Spark Streaming \u5165\u95e8\n\n\n\n\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like \nmap\n, \nreduce\n, \njoin\n and \nwindow\n. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark\u2019s machine learning and graph processing algorithms on data streams. [\nref\n]\n\n\n\n\n\n\n\n\nSpark Streaming receives live input data streams and divides the data into \nbatches\n, which are then processed by the Spark engine to generate the final stream of results in batches.\n\n\n\n\n\n\n\u5e94\u7528\u573a\u666f\n\n\n\n\nReact to anomalies in sensors in real-time\n\n\n\n\nSpark Streaming\u96c6\u6210Spark\u751f\u6001\u7cfb\u7edf\u7684\u4f7f\u7528\n\n\n\n\n\n\nJoin data streams with static data sets\n\n\n\n\n \n// create data set from hadoop file\n\n\nval\n \ndataset\n \n=\n \nsparkContext\n.\nhadoopFile\n(\nfile\n)\n\n\n// join each batch in stream with the dataset\n\n\nkafakaStream\n.\ntransform\n{\nbatchRDD\n=\n\n    \nbatchRDD\n.\njoin\n(\ndataset\n).\nfilter\n(...)\n\n\n}\n\n\n\n\n\n\nLearn models offline, apply them online\n\n\n\n\n \n//Learn model offline\n\n\nval\n \nmodel\n \n=\n \nKMeans\n.\ntrain\n(\ndataset\n,...)\n\n\n\n//apply model online on stream\n\n\nkafkaStream\n.\nmap\n{\nevent\n=?\n \nmodel\n.\npredict\n(\nevent\n.\nfeature\n)\n\n\n}\n\n\n\n\n\n\nInteractively query streaming data with SQL\n\n\n\n\n \n// Register each batch in stream as table\n\n\nkafkaStream\n.\nmap\n{\n \nbatchRDD\n \n=?\n \nbatchRDD\n.\nregisterTempTable\n(\nlastestEvents\n)\n\n\n}\n\n\n//INteractively query table\n\n\nsqlContext\n.\nsql\n(\nselect * from latestEvents\n)\n\n\n\n\n\u53d1\u5c55\u53f2\n\n\n\n\nExample: \u8bcd\u9891\u7edf\u8ba1\n\n\nspark-submit\u6267\u884c\n\n\n\u4f7f\u7528spark-submit\u6765\u63d0\u4ea4\u5e94\u7528\u7a0b\u5e8f\n\n\n \n$ spark-submit --master \nlocal\n \n\\\n\n    --class org.apache.spark.examples.streaming.JavaNetworkWordCount \n\\\n\n    --name NetworkWordCount \n\\\n\n    spark-examples_2.11-2.3.1.jar localhost 9999\n\n\n\nspark-shell\u6267\u884c\n\n\n\u4f7f\u7528spark-submit\u6765\u6d4b\u8bd5\u5e94\u7528\u7a0b\u5e8f", 
            "title": "SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#spark-streaming", 
            "text": "\u8be5\u9879\u76ee\u4ece\u5b9e\u65f6\u6570\u636e\u4ea7\u751f\u548c\u6d41\u5411\u7684\u4e0d\u540c\u73af\u8282\u51fa\u53d1\uff0c\u901a\u8fc7\u96c6\u6210\u4e3b\u6d41\u7684\u5206\u5e03\u5f0f\u65e5\u5fd7\u6536\u96c6\u6846\u67b6Flume\u3001\u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217Kafka\u3001\u5206\u5e03\u5f0f\u5217\u5f0f\u6570\u636e\u5e93HBase\u3001\u4ee5\u53caSpark Streaming\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5904\u7406\u3002", 
            "title": "Spark Streaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#1", 
            "text": "", 
            "title": "1 \u521d\u8bc6\u5b9e\u65f6\u6d41\u5904\u7406"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_1", 
            "text": "\u9700\u6c42\uff1a\u7edf\u8ba1\u4e3b\u7ad9\u6bcf\u4e2a\uff08\u6307\u5b9a\uff09\u8bfe\u7a0b\u8bbf\u95ee\u7684\u5ba2\u6237\u7aef\u3001\u5730\u57df\u4fe1\u606f\u5206\u5e03  ==  \u5982\u4e0a\u4e24\u4e2a\u64cd\u4f5c\uff1a\u91c7\u7528\u79bb\u7ebf\uff08spark/mapreduce\uff09\u7684\u65b9\u5f0f\u8fdb\u884c\u7edf\u8ba1  \u5b9e\u73b0\u6b65\u9aa4\uff1a   \u8bfe\u7a0b\u7f16\u53f7\uff0cip\u4fe1\u606f\uff0cuser-agent  \u8fdb\u884c\u76f8\u5e94\u7684\u7edf\u8ba1\u5206\u6790\u64cd\u4f5c\uff1aMapReduce/Spark   \u9879\u76ee\u67b6\u6784\uff1a   \u65e5\u5fd7\u6536\u96c6\uff1aFlume  \u79bb\u7ebf\u5206\u6790\uff1aMapReduce/Spark  \u7edf\u8ba1\u7ed3\u679c\u56fe\u5f62\u5316\u5c55\u793a   \u95ee\u9898\uff1a   \u5c0f\u65f6\u7ea7\u522b  10\u5206\u949f  \u79d2\u7ea7\u522b", 
            "title": "\u4e1a\u52a1\u73b0\u72b6\u5206\u6790"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_2", 
            "text": "\u65f6\u6548\u6027\u9ad8  \u6570\u636e\u91cf\u5927", 
            "title": "\u5b9e\u65f6\u6d41\u5904\u7406\u4ea7\u751f\u80cc\u666f"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_3", 
            "text": "https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101   \u5b9e\u65f6\u8ba1\u7b97 apache storm  \u6d41\u5f0f\u8ba1\u7b97  \u5b9e\u65f6\u6d41\u5f0f\u8ba1\u7b97", 
            "title": "\u5b9e\u65f6\u6d41\u5904\u7406\u6982\u8ff0"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_4", 
            "text": "\u6570\u636e\u6765\u6e90  \u79bb\u7ebf\uff1a\u6765\u81eaHDFS\u4e0a\u7684\u5386\u53f2\u6570\u636e\uff0c\u6570\u636e\u91cf\u6bd4\u8f83\u5927  \u5b9e\u65f6\uff1a\u6765\u81ea\u6d88\u606f\u961f\u5217(Kafka)\uff0c\u662f\u5b9e\u65f6\u65b0\u589e/\u4fee\u6539\u8bb0\u5f55\u8fc7\u6765\u7684\u67d0\u4e00\u7b14\u6570\u636e    \u5904\u7406\u8fc7\u7a0b  \u79bb\u7ebf\uff1aMapReduce, map + reduce  \u5b9e\u65f6: Spark(DStream/SS)     \u5904\u7406\u901f\u5ea6  \u79bb\u7ebf\uff1a\u5e54  \u5b9e\u65f6\uff1a\u5feb\u901f     \u8fdb\u7a0b  \u79bb\u7ebf\uff1a\u8fdb\u7a0b\u6709\u542f\u52a8+\u9500\u6bc1\u7684\u8fc7\u7a0b  \u5b9e\u65f6\uff1a 7*24\u5c0f\u65f6\u8fd0\u884c", 
            "title": "\u79bb\u7ebf\u8ba1\u7b97\u4e0e\u5b9e\u65f6\u8ba1\u7b97\u5bf9\u6bd4"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_5", 
            "text": "Apache Storm    Apache Storm is a free and open source distributed  realtime  computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use!    Apache Spark Streaming    \u5b9e\u9645\u4e0a\u662f\u5fae\u6279\u5904\u7406\uff08\u6279\u5904\u7406\u95f4\u9694\u975e\u5e38\u5c0f)    Apache kafka  Apache Flink    Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.", 
            "title": "\u5b9e\u65f6\u6d41\u5904\u7406\u6846\u67b6\u5bf9\u6bd4"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_6", 
            "text": "\u52a0\u4e00\u5c42flume\u6d88\u606f\u961f\u5217\uff0c\u4e3b\u8981\u4e3a\u4e86\u51cf\u8f7b\u538b\u529b\uff0c\u8d77\u5230\u7f13\u51b2\u4f5c\u7528", 
            "title": "\u5b9e\u65f6\u6d41\u5904\u7406\u67b6\u6784\u548c\u6280\u672f\u9009\u578b"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_7", 
            "text": "\u7535\u4fe1\u884c\u4e1a\uff1a \u4f60\u7684\u624b\u673a\u5957\u9910\u6d41\u91cf\u7528\u5b8c\uff0c\u6536\u5230\u77ed\u4fe1\u63d0\u793a  \u7535\u5546\u884c\u4e1a\uff1a\u641c\u7d22\u5546\u54c1\u65f6\uff0c\u8fdb\u884c\u63a8\u8350", 
            "title": "\u5b9e\u65f6\u6d41\u5904\u7406\u5728\u4f01\u4e1a\u4e2d\u7684\u5e94\u7528"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#2-flume", 
            "text": "see detail in Hadoop: definitive Guide,  Chapter 14", 
            "title": "2 \u5206\u5e03\u5f0f\u65e5\u5fd7\u6536\u96c6\u6846\u67b6Flume"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_8", 
            "text": "You have a lot of servers and systems   network devices  operating system  web servers  applications   And they generate large amount of logs and other data.  Problem: Since you have a business idea, how to implement the idea?  OPTION: You may move logs and data generated to hadoop hdfs directly.  \u4f46\u662f\u5b58\u5728\u95ee\u9898\uff1a   \u5982\u4f55\u505a\u76d1\u63a7  \u5982\u4f55\u4fdd\u8bc1\u65f6\u6548\u6027  \u76f4\u63a5\u4f20\u9001\u6587\u672c\u6570\u636e\uff0c\u5f00\u9500\u592a\u5927  \u5bb9\u9519  \u8d1f\u8f7d\u5747\u8861   SOLUTION: \u4f7f\u7528Flume\uff0c\u57fa\u672c\u4e0a\u5199\u914d\u7f6e\u6587\u4ef6\u5c31OK\u4e86\uff0cFlume\u81ea\u52a8\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002", 
            "title": "\u4e1a\u52a1\u73b0\u72b6\u5206\u6790"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#flume", 
            "text": "Flume is a distributed, reliable, and available service for efficiently  collecting, aggregating, and moving large amounts of log data . It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application. [ Apache Flume ]", 
            "title": "Flume\u6982\u8ff0"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#flume_1", 
            "text": "see detail in Hadoop: definitive Guide,  Chapter 14", 
            "title": "Flume\u67b6\u6784\u53ca\u6838\u5fc3\u7ec4\u4ef6"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#flume_2", 
            "text": "\u9700\u6c42\uff1a \u4ece\u6307\u5b9a\u7f51\u7edc\u7aef\u53e3\u91c7\u96c6\u6570\u636e  \u4f7f\u7528Flume\u7684\u5173\u952e\u5c31\u662f\u5199\u914d\u7f6e\u6587\u4ef6   \u914d\u7f6eSource, Channel, Sink  \u628a\u4ee5\u4e0a\u4e09\u4e2a\u7ec4\u4ef6\u4e32\u8d77\u6765     http://flume.apache.org/FlumeUserGuide.html#example-2\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 44444\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1   netcat source : A netcat-like source that listens on a given port and turns each line of text into an event. It opens a specified port and listens for data. The expectation is that the supplied data is newline separated text. Each line of text is turned into a Flume event and sent via the connected channel. [ NetCat TCP Source ]  logger sink : Logs event at INFO level. Typically useful for testing/debugging purpose.  [ Logger Sink ]  memory channel : The events are stored in an in-memory queue with configurable max size. It\u2019s ideal for flows that need higher throughput and are prepared to lose the staged data in the event of an agent failures. [ memory channel ]     ## \u542f\u52a8flume \n$ flume-ng agent  \\ \n--name a1  \\    # agent name \n--conf  $F LUME_HOME/conf  \\  # use configs in  conf  directory \n--conf-file  example.conf  \\  # specify a config file \n-Dflume.root.logger = INFO,console  # sets a Java system property value  ## \u5728\u53e6\u5916\u4e00\u4e2aterminal\u7528telnet\u6a21\u62df\u6570\u636e\u6e90 \n$ telnet localhost  44444  \nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is  ^] .\nhello\nOK\nhellomy\nOK  \u9700\u6c42\uff1a \u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\u5b9e\u65f6\u91c7\u96c6\u65b0\u589e\u7684\u6570\u636e\u8f93\u51fa\u5230\u63a7\u5236\u53f0  Agent\u9009\u578b\uff1a exec source + memory channel + logger sink    # filename: exec-memeory-logger.conf\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /tmp/data.log\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1   exec source  runs a given Unix command on start-up and expects that process to continuously produce data on standard out (stderr is simply discarded, unless property logStdErr is set to true). If the process exits for any reason, the source also exits and will produce no further data. This means configurations such as cat [named pipe] or tail -F [file] are going to produce the desired results where as date will probably not - the former two commands produce streams of data where as the latter produces a single event and exits. [ exec source ]   \u5c06\u5185\u5bb9\u8f93\u5165\u5230 /tmp/data.log \u6587\u4ef6\u4e2d\uff1a    $  echo   hello    data.log\n$  echo   hello    data.log  \u9700\u6c42\uff1a \u5c06A\u670d\u52a1\u5668\u4e0a\u7684\u65e5\u5fd7\u5b9e\u65f6\u91c7\u96c6\u5230B\u670d\u52a1\u5668  \u65e5\u5fd7\u6536\u96c6\u8fc7\u7a0b\uff1a   \u673a\u56681\u4e0a\u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\uff0c\u5f53\u6211\u4eec\u8bbf\u95ee\u4e3b\u7ad9\u65f6\u4f1a\u6709\u7528\u6237\u884c\u4e3a\u65e5\u5fd7\u8bb0\u5f55\u5230 access.log \u4e2d\u3002  avro sink\u628a\u65b0\u4ea7\u751f\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u5bf9\u5e94\u7684avro source\u6307\u5b9a\u7684hostname\u548cport\u4e0a\u3002  \u901a\u8fc7avro\u5bf9\u5e94\u7684agent\u5c06\u6211\u4eec\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u63a7\u5236\u53f0\u3002     avro sink : forms one half of Flume\u2019s tiered collection support. Flume events sent to this sink are turned into Avro events and sent to the configured hostname / port pair. [ Avro sink ]   Exec-Memeory-Avro.conf # filename: exec-memeory-avro.conf\n\n# Name the components on this agent\na1.sources = exec-source\na1.sinks = avro-sink\na1.channels = memory-channel\n\n# Describe/configure the source\na1.sources.exec-source.type = exec\na1.sources.exec-source.command = tail -F /tmp/data.log\n\n# Describe the sink\na1.sinks.avro-sink.type = avro\na1.sinks.avro-sink.hostname = localhost\na1.sinks.avro-sink.port = 44444\n\n# Use a channel which buffers events in memory\na1.channels.memory-channel.type = memory\na1.channels.memory-channel.capacity = 1000\na1.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.exec-source.channels = memory-channel\na1.sinks.avro-sink.channel = memory-channel Avro-Memeory-Logger.conf # filename: avro-memeory-logger.conf\n\n# Name the components on this agent\na2.sources = avro-source\na2.sinks = logger-sink\na2.channels = memory-channel\n\n# Describe/configure the source\na2.sources.avro-source.type = avro\na2.sources.avro-source.bind = localhost\na2.sources.avro-source.port = 44444\n\n# Describe the sink\na2.sinks.logger-sink.type = logger\n\n# Use a channel which buffers events in memory\na2.channels.memory-channel.type = memory\na2.channels.memory-channel.capacity = 1000\na2.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na2.sources.avro-source.channels = memory-channel\na2.sinks.logger-sink.channel = memory-channel  \u542f\u52a8flume\uff0c \u6ce8\u610f\u4e24\u4e2aagent\u7684\u542f\u52a8\u987a\u5e8f    $ flume-ng agent  \\ \n--name a2  \\ \n--conf  $F LUME-HOME/conf  \\ \n--conf-file avro-memory-logger.conf  \\ \n-Dflume.root.logger = INFO,console\n\n$ flume-ng agent  \\ \n--name a1  \\ \n--conf  $F LUME-HOME/conf  \\ \n--conf-file exec-memory-avro.conf  \\ \n-Dflume.root.logger = INFO,console  \u5c06\u5185\u5bb9\u8f93\u5165\u5230 /tmp/data.log \u6587\u4ef6\u4e2d\uff1a    $  echo   welcome    data.log\n$  echo   welcome    data.log", 
            "title": "Flume\u5b9e\u6218"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#3-kafka", 
            "text": "First a few concepts:   Kafka is run as a cluster on one or more servers that can span multiple datacenters.  The Kafka cluster stores streams of  records  in categories called  topic s.  Each record consists of a key, a value, and a timestamp.  Broker s are the Kafka processes that manage topics and partitions and serve producer and consumer request.", 
            "title": "3 \u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217Kafka"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#kafka", 
            "text": "\u5355\u8282\u70b9\u5355Broker\u90e8\u7f72\u53ca\u4f7f\u7528    # \u542f\u52a8Zookeeper \n$ zkServer.sh start # \u542f\u52a8kafka \n$ kafka-server-start.sh $KAFKA_HOME/config/server.properties # \u521b\u5efa\u540d\u4e3atest\u7684topic(single partition and only one replica) \n$ kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor  1  --partitions  1  --topic  test  # \u67e5\u770btopic \n$ kafka-topics.sh --list --zookeeper localhost:2181 ### \u542f\u52a8\u751f\u4ea7\u8005, 9092\u662fserver\u76d1\u542c\u7aef\u53e3 \n$ kafka-console-producer.sh --broker-list localhost:9092 --topic  test   This is a message  This is another message ### \u542f\u52a8\u6d88\u8d39\u8005 --from-beginning\u4ece\u5934\u5f00\u59cb\u63a5\u6536\u6d88\u606f \n$ kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic  test  --from-beginning\nThis is a message\nThis is another message ### \u67e5\u770b\u6240\u6709topics\u7684\u8be6\u7ec6\u4fe1\u606f \n$ kafka-topics.sh --describe --zookeeper localhost:2181 ### \u67e5\u770b\u6307\u5b9atopic\u7684\u8be6\u7ec6\u4fe1\u606f \n$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic  test   \u5355\u8282\u70b9\u591aBroker\u90e8\u7f72\u53ca\u4f7f\u7528    cp $KAFKA_HOME/config/server.properties $KAFKA_HOME/config/server-1.properties\ncp $KAFKA_HOME/config/server.properties $KAFKA_HOME/config/server-2.properties  \u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b    config/server-1.properties:\n    broker.id=1\n    listeners=PLAINTEXT://:9093\n    log.dirs=/tmp/kafka-logs-1\n\nconfig/server-2.properties:\n    broker.id=2\n    listeners=PLAINTEXT://:9094\n    log.dirs=/tmp/kafka-logs-2  \u542f\u52a8kafka    # \u542f\u52a8ZooKeeper \n$ zkServer.sh start # \u542f\u52a8kafka server \n$ kafka-server-start.sh $KAFKA_HOME/config/server.properties  \n$ kafka-server-start.sh $KAFKA_HOME/config/server-1.properties  \n$ kafka-server-start.sh $KAFKA_HOME/config/server-2.properties   # \u521b\u5efatopic, 1\u4e2a\u5206\u533a\uff0c\u4e09\u4e2a\u526f\u672c \n$ kafka-topics.sh --create --zookeeper localhost:2181  \\ \n    --replication-factor  3  --partitions  1  --topic my-replicated-topic # \u67e5\u770btopic\u4fe1\u606f \n$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic\nTopic:my-replicated-topic   PartitionCount:1    ReplicationFactor:3 Configs:\n    Topic: my-replicated-topic  Partition:  0     Leader:  2    Replicas: 2,0,1 Isr: 2,0,1 # \u542f\u52a8\u751f\u4ea7\u8005 \n$ kafka-console-producer.sh --broker-list localhost:9092, localhost:9093, localhost:9094 --topic my-replicated-topic # \u542f\u52a8\u6d88\u8d39\u8005 \n$ kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic", 
            "title": "Kafka\u90e8\u7f72\u53ca\u4f7f\u7528"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#kafka-java", 
            "text": "\u4f7f\u7528\u547d\u4ee4\u884c\u603b\u662f\u4e0d\u65b9\u4fbf\u7684\uff0c\u4e0b\u9762\u6211\u4eec\u5c1d\u8bd5\u7740\u4f7f\u7528Kafka Java API\u7f16\u7a0b\uff0c\u5b9e\u9645\u64cd\u4f5c\u5185\u5bb9\u548c\u4e0a\u4e00\u8282\u662f\u4e00\u6478\u4e00\u6837\u7684\uff0c\u6240\u4ee5\u76f4\u63a5\u9644\u4e0a\u4ee3\u7801\u4e86\u3002\u6ce8\u610f\u8fd9\u91cc\u4f7f\u7528\u7684API\u662f0.8.2\u7248\u672c\u4ee5\u540e\u7684\uff0c\u4e4b\u524d\u7248\u672c\u4e0e\u4e4b\u540e\u7248\u672c\u7684API\u76f8\u5dee\u975e\u5e38\u5927\u3002  Producer import   org.apache.kafka.clients.producer.KafkaProducer ;  import   org.apache.kafka.clients.producer.ProducerRecord ;  import   java.util.Properties ;  /**   * Kafka\u751f\u4ea7\u8005   * \u89c1\u5b98\u65b9\u6587\u6863   * http://kafka.apache.org/20/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html   */  public   class   MyKafkaProducer   implements   Runnable   { \n     private   String   topic ; \n     private   KafkaProducer String ,   String   producer ; \n\n     public   MyKafkaProducer ( String   topic )   { \n         this . topic   =   topic ; \n         Properties   props   =   new   Properties (); \n         props . put ( bootstrap.servers ,   localhost:9092 ); \n         props . put ( acks ,   all ); \n         props . put ( key.serializer ,   org.apache.kafka.common.serialization.StringSerializer ); \n         props . put ( value.serializer ,   org.apache.kafka.common.serialization.StringSerializer ); \n         producer   =   new   KafkaProducer String ,   String ( props ); \n     } \n\n     public   void   run ()   { \n         int   messageNumber   =   1 ; \n         while   ( true )   { \n             String   message   =   message   +   messageNumber ; \n             producer . send ( new   ProducerRecord String ,   String ( topic ,   message )); \n             messageNumber ++; \n             try { \n                 Thread . sleep ( 5000 ); \n             }   catch   ( Exception   ex )   { \n                 ex . printStackTrace (); \n             } \n         } \n     }  }  Consumer import   org.apache.kafka.clients.consumer.ConsumerRecord ;  import   org.apache.kafka.clients.consumer.ConsumerRecords ;  import   org.apache.kafka.clients.consumer.KafkaConsumer ;  import   java.time.Duration ;  import   java.util.Arrays ;  import   java.util.List ;  import   java.util.Properties ;  import   java.util.concurrent.atomic.AtomicBoolean ;  /**   * Kafka\u6d88\u8d39\u8005   * \u5b98\u65b9\u6587\u6863   * http://kafka.apache.org/20/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html   */  public   class   MyKafkaConsumer   implements   Runnable   { \n     private   final   AtomicBoolean   closed   =   new   AtomicBoolean ( false ); \n     private   String   topic ; \n     private   KafkaConsumer String ,   String   consumer ; \n     private   ConsumerRecords String ,   String   records ; \n\n     public   MyKafkaConsumer ( String   topic )   { \n         this . topic   =   topic ; \n         Properties   props   =   new   Properties (); \n         // connect to cluster \n         props . put ( bootstrap.servers ,   localhost:9092 ); \n         //  subscribing to the topics- test \n         props . put ( group.id ,   test ); \n         //  offsets are committed automatically \n         props . put ( enable.auto.commit ,   true ); \n         // specify how to turn bytes into objects \n         props . put ( key.deserializer ,   org.apache.kafka.common.serialization.StringDeserializer ); \n         props . put ( value.deserializer ,   org.apache.kafka.common.serialization.StringDeserializer ); \n         consumer   =   new   KafkaConsumer ( props ); \n\n     } \n\n     public   void   run ()   { \n         try   { \n             // subsribes to topic \n             consumer . subscribe ( Arrays . asList ( topic )); \n             while   (! closed . get ())   { \n                 records   =   consumer . poll ( Duration . ofMillis ( 10000 )); \n                 for   ( ConsumerRecord String ,   String   record   :   records ) \n                     System . out . printf ( offset = %d, key = %s, value = %s%n ,   record . offset (),   record . key (),   record . value ()); \n             } \n         }   catch   ( Exception   e )   { \n             // Ignore exception if closing \n             if   (! closed . get ())   throw   e ; \n         }   finally   { \n             consumer . close (); \n         } \n\n     } \n\n     // Shutdown hook which can be called from a separate thread \n     public   void   shutdown ()   { \n         closed . set ( true ); \n         consumer . wakeup (); \n     }  }  Clientapp public   class   ClientApp   { \n     public   static   void   main ( String []   args )   { \n         Thread   job   =   new   Thread ( new   MyKafkaProducer ( test )); \n         job . start (); \n         Thread   job2   =   new   Thread ( new   MyKafkaConsumer ( test )); \n         job2 . start (); \n     }  }", 
            "title": "Kafka Java \u7f16\u7a0b"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#flumekafka", 
            "text": "\u4e3a\u4e86\u5c06Flume\u7684\u8f93\u51fa\u5230Kafka\uff0c\u53ef\u4ee5\u5c06agent2\u7684logger sink\u66ff\u6362\u6210Kafka Sink\u3002\u7136\u540e\u542f\u52a8\u4e00\u4e2aKafka consumer\u4eceKafka sink\u8ba2\u9605\u6d88\u606f\u3002    kafka sink  can publish data to a Kafka topic. One of the objective is to integrate Flume with Kafka so that pull based processing systems can process the data coming through various Flume sources. [ Kafka Sink ]   \u4e0b\u9762\u662fagent2\u5bf9\u5e94\u7684Kafka\u914d\u7f6e\u6587\u4ef6\uff0c\u5728\u8fd9\u91ccagent2\u6539\u540d\u4e3a avro-memory-kafka \u3002    # filename: avro-memeory-kafka.conf\n\n# Name the components on this agent\navro-memory-kafka.sources = avro-source\navro-memory-kafka.sinks = kafka-sink\navro-memory-kafka.channels = memory-channel\n\n# Describe/configure the source\navro-memory-kafka.sources.avro-source.type = avro\navro-memory-kafka.sources.avro-source.bind = localhost\navro-memory-kafka.sources.avro-source.port = 44444\n\n# Describe the sink\navro-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink\navro-memory-kafka.sinks.kafka-sink.kafka.bootstrap.servers = localhost:9092\navro-memory-kafka.sinks.kafka-sink.kafka.topic = test\n\n\n# Use a channel which buffers events in memory\navro-memory-kafka.channels.memory-channel.type = memory\navro-memory-kafka.channels.memory-channel.capacity = 1000\navro-memory-kafka.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\navro-memory-kafka.sources.avro-source.channels = memory-channel\navro-memory-kafka.sinks.kafka-sink.channel = memory-channel  \u4e0b\u9762\u662f\u5177\u4f53\u7684\u64cd\u4f5c\u6d41\u7a0b\uff0c\u540c\u6837\u9700\u8981\u6ce8\u610f\u4e24\u4e2aagent\u7684\u542f\u52a8\u987a\u5e8f\uff1a    ## \u542f\u52a8zookeeper, kafka\uff0c\u7701\u7565  ## \u542f\u52a8agent \n$ flume-ng agent  \\ \n--name avro-memory-kafka  \\ \n--conf  $F LUME-HOME/conf  \\ \n--conf-file avro-memory-kafka.conf  \\ \n-Dflume.root.logger = INFO,console\n\n$ flume-ng agent  \\ \n--name a1  \\ \n--conf  $F LUME-HOME/conf  \\ \n--conf-file exec-memory-avro.conf  \\ \n-Dflume.root.logger = INFO,console ## \u542f\u52a8\u6d88\u8d39\u8005  \n$ kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic  test", 
            "title": "\u6574\u5408Flume\u548cKafka\u5b8c\u6210\u5b9e\u65f6\u6570\u636e\u91c7\u96c6"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#4-spark-streaming", 
            "text": "Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like  map ,  reduce ,  join  and  window . Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark\u2019s machine learning and graph processing algorithms on data streams. [ ref ]     Spark Streaming receives live input data streams and divides the data into  batches , which are then processed by the Spark engine to generate the final stream of results in batches.", 
            "title": "4 Spark Streaming \u5165\u95e8"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_9", 
            "text": "React to anomalies in sensors in real-time", 
            "title": "\u5e94\u7528\u573a\u666f"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#spark-streamingspark", 
            "text": "Join data streams with static data sets     // create data set from hadoop file  val   dataset   =   sparkContext . hadoopFile ( file )  // join each batch in stream with the dataset  kafakaStream . transform { batchRDD = \n     batchRDD . join ( dataset ). filter (...)  }    Learn models offline, apply them online     //Learn model offline  val   model   =   KMeans . train ( dataset ,...)  //apply model online on stream  kafkaStream . map { event =?   model . predict ( event . feature )  }    Interactively query streaming data with SQL     // Register each batch in stream as table  kafkaStream . map {   batchRDD   =?   batchRDD . registerTempTable ( lastestEvents )  }  //INteractively query table  sqlContext . sql ( select * from latestEvents )", 
            "title": "Spark Streaming\u96c6\u6210Spark\u751f\u6001\u7cfb\u7edf\u7684\u4f7f\u7528"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_10", 
            "text": "", 
            "title": "\u53d1\u5c55\u53f2"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#example", 
            "text": "spark-submit\u6267\u884c  \u4f7f\u7528spark-submit\u6765\u63d0\u4ea4\u5e94\u7528\u7a0b\u5e8f    $ spark-submit --master  local   \\ \n    --class org.apache.spark.examples.streaming.JavaNetworkWordCount  \\ \n    --name NetworkWordCount  \\ \n    spark-examples_2.11-2.3.1.jar localhost 9999  spark-shell\u6267\u884c  \u4f7f\u7528spark-submit\u6765\u6d4b\u8bd5\u5e94\u7528\u7a0b\u5e8f", 
            "title": "Example: \u8bcd\u9891\u7edf\u8ba1"
        }
    ]
}