{
    "docs": [
        {
            "location": "/", 
            "text": "Zhenhua's Notes\n\n\nThis site documents reading and learning notes.\n\n\nOther Note Site\n\n\nNote - Java/OS\n\n\nNote - Algorithm\n\n\nSearch\n\n\n\n  \n\n    \n\n      (function() {\n        var cx = '011299089536274713055:ppqfpivtvxy';\n        var gcse = document.createElement('script');\n        gcse.type = 'text/javascript';\n        gcse.async = true;\n        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;\n        var s = document.getElementsByTagName('script')[0];\n        s.parentNode.insertBefore(gcse, s);\n      })();\n    \n\n    \n\n  \n\n\n\n\n\nTOC\n\n\nSee \nTable of Contents\n.\n\n\nBooks and Materials\n\n\nSee \nBooks and Materials\n.\n\n\nRoadmap\n\n\nSee \nRoadmap\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#zhenhuas-notes", 
            "text": "This site documents reading and learning notes.", 
            "title": "Zhenhua's Notes"
        }, 
        {
            "location": "/#other-note-site", 
            "text": "Note - Java/OS  Note - Algorithm", 
            "title": "Other Note Site"
        }, 
        {
            "location": "/#search", 
            "text": "(function() {\n        var cx = '011299089536274713055:ppqfpivtvxy';\n        var gcse = document.createElement('script');\n        gcse.type = 'text/javascript';\n        gcse.async = true;\n        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;\n        var s = document.getElementsByTagName('script')[0];\n        s.parentNode.insertBefore(gcse, s);\n      })();", 
            "title": "Search"
        }, 
        {
            "location": "/#toc", 
            "text": "See  Table of Contents .", 
            "title": "TOC"
        }, 
        {
            "location": "/#books-and-materials", 
            "text": "See  Books and Materials .", 
            "title": "Books and Materials"
        }, 
        {
            "location": "/#roadmap", 
            "text": "See  Roadmap .", 
            "title": "Roadmap"
        }, 
        {
            "location": "/hadoop/", 
            "text": "HADOOP\n\n\n\n\nChapter 1: Meet Hadoop\n\n\nChapter 2: MapReduce\n\n\nChapter 3: The Hadoop Distributed FileSystem\n\n\nChapter 4: YARN\n\n\nChapter 5: Hadoop I/O\n\n\nChapter 6: Developing a MapReduce Application\n\n\nChapter 7: How MapReduce Works\n\n\nChapter 8: MapReduce Types and Formats\n\n\nChapter 9: MapReduce Features\n\n\nChapter 10: Setting Up a Hadoop Cluster\n\n\nChapter 11: Adminstering Hadoop\n\n\nChapter 12: Avro\n\n\nChapter 13: Parquet\n\n\nChapter 14: Flume\n\n\nChapter 15: Sqoop\n\n\nChapter 16: Pig\n\n\nChapter 17: Hive\n\n\nChapter 18: Crunch\n\n\nChapter 19: Spark\n\n\nChapter 20: HBase\n\n\nChapter 21: ZooKeeper\n\n\nChapter 22: Composable Data at Center\n\n\nChapter 23: Biological Data Science: Saving Lives with Software\n\n\nChapter 24: Cascading", 
            "title": "Contents"
        }, 
        {
            "location": "/hadoop/#hadoop", 
            "text": "Chapter 1: Meet Hadoop  Chapter 2: MapReduce  Chapter 3: The Hadoop Distributed FileSystem  Chapter 4: YARN  Chapter 5: Hadoop I/O  Chapter 6: Developing a MapReduce Application  Chapter 7: How MapReduce Works  Chapter 8: MapReduce Types and Formats  Chapter 9: MapReduce Features  Chapter 10: Setting Up a Hadoop Cluster  Chapter 11: Adminstering Hadoop  Chapter 12: Avro  Chapter 13: Parquet  Chapter 14: Flume  Chapter 15: Sqoop  Chapter 16: Pig  Chapter 17: Hive  Chapter 18: Crunch  Chapter 19: Spark  Chapter 20: HBase  Chapter 21: ZooKeeper  Chapter 22: Composable Data at Center  Chapter 23: Biological Data Science: Saving Lives with Software  Chapter 24: Cascading", 
            "title": "HADOOP"
        }, 
        {
            "location": "/hadoop/ch1/", 
            "text": "Hadoop: The Definitive Guide 1 - Meet Hadoop\n\n\n1 \u6570\u636e\uff01\u6570\u636e\uff01\n\n\n\u73b0\u5728\u662f\u6570\u636e\u5927\u7206\u70b8\u65f6\u4ee3\uff0c\u5168\u7403\u6570\u636e\u603b\u91cf\u8fdc\u8fdc\u8d85\u8fc7\u4e86\u5168\u4e16\u754c\u6bcf\u4eba\u4e00\u5757\u786c\u76d8\u4e2d\u6240\u80fd\u4fdd\u5b58\u7684\u6570\u636e\u603b\u91cf\u3002\n\n\n\n\n\u4e2a\u4eba\u4ea7\u751f\u7684\u6570\u636e\u6b63\u5728\u5feb\u901f\u589e\u957f\n\n\n\u4e2a\u4eba\u4fe1\u606f\u6863\u6848\u5c06\u65e5\u76ca\u666e\u53ca\uff08\u7535\u8bdd\u3001\u90ae\u4ef6\u3001\u6587\u4ef6\u3001\u7167\u7247\uff09\n\n\n\n\n\n\n\u7269\u8054\u7f51\u7684\u673a\u5668\u8bbe\u5907\u4ea7\u751f\u7684\u6570\u636e\u53ef\u80fd\u8fdc\u8fdc\u8d85\u8fc7\u4e2a\u4eba\u4ea7\u751f\u7684\u6570\u636e \n\n\n\u673a\u5668\u65e5\u5fd7\u3001\u4f20\u611f\u5668\u7f51\u7edc\u3001\u96f6\u552e\u4ea4\u6613\u6570\u636e\u7b49\n\n\n\n\n\n\n\n\n1.2 \u6570\u636e\u7684\u5b58\u50a8\u4e0e\u5206\u6790\n\n\n\u9047\u5230\u7684\u95ee\u9898\uff1a\u786c\u76d8\u5b58\u50a8\u5bb9\u91cf\u4e0d\u65ad\u63d0\u5347\uff0c\u8bbf\u95ee\u901f\u5ea6\u6ca1\u6709\u4e0e\u65f6\u4ff1\u8fdb\n\n\n\n\n\u8bfb\u5199\u786c\u76d8\u4e2d\u7684\u6570\u636e\u9700\u8981\u66f4\u957f\u65f6\u95f4\n\n\n\n\n\u89e3\u51b3\u65b9\u6cd5\uff1a \u540c\u65f6\u4ece\u591a\u4e2a\u786c\u76d8\u4e0a\u8bfb\u53d6\u6570\u636e\uff0c\u6bcf\u4e2a\u786c\u76d8\u5b58\u50a8\u4e00\u90e8\u5206\u6570\u636e\n\n\n\n\n\u867d\u7136\u6d6a\u8d39\u4e86\u786c\u76d8\u5bb9\u91cf\uff0c\u4f46\u662f\u7531\u4e8e\u7528\u6237\u7684\u5206\u6790\u5de5\u4f5c\u90fd\u662f\u5728\u4e0d\u540c\u65f6\u95f4\u70b9\u8fdb\u884c\u7684\uff0c\u6240\u4ee5\u5f7c\u6b64\u4e4b\u95f4\u7684\u5e72\u6270\u5e76\u4e0d\u592a\u5927\n\n\n\n\n\u65b0\u7684\u95ee\u9898\uff1a\u8981\u5bf9\u591a\u4e2a\u786c\u76d8\u4e2d\u7684\u6570\u636e\u5e76\u884c\u8fdb\u884c\u8bfb\u5199\u6570\u636e\uff0c\u8fd8\u6709\u66f4\u591a\u95ee\u9898\u8981\u89e3\u51b3\n\n\n\n\n\u786c\u4ef6\u6545\u969c\u95ee\u9898\uff1a\u6700\u5e38\u89c1\u7684\u505a\u6cd5\u662f\u7cfb\u7edf\u4fdd\u5b58\u6570\u636e\u7684\u526f\u672c(replica)\n\n\n\u5927\u591a\u6570\u5206\u6790\u4efb\u52a1\u9700\u8981\u4ee5\u67d0\u79cd\u65b9\u5f0f\u7ed3\u5408\u5927\u90e8\u5206\u6570\u636e\u6765\u5171\u540c\u5b8c\u6210\u5206\u6790\uff0c\u4fdd\u8bc1\u5176\u6b63\u786e\u6027\u662f\u4e00\u4e2a\u975e\u5e38\u5927\u7684\u6311\u6218\uff1aMapReduce\n\n\n\n\n1.3 \u67e5\u8be2\u6240\u6709\u6570\u636e\n\n\nMapReduce\u662f\u4e00\u4e2a\n\u6279\u91cf\u67e5\u8be2\u5904\u7406\u5668\n(batch processing system)\uff0c\u80fd\u591f\u5728\u5408\u7406\u7684\u65f6\u95f4\u8303\u56f4\u5185\u5904\u7406\u9488\u5bf9\u6574\u4e2a\u6570\u636e\u96c6\u7684\u52a8\u6001\u67e5\u8be2\u3002\n\n\n1.4 \u4e0d\u4ec5\u4ec5\u662f\u6279\u5904\u7406\n\n\nMapReduce\u57fa\u672c\u4e0a\u662f\u4e00\u4e2a\u6279\u5904\u7406\u7cfb\u7edf\uff0c\u5e76\u4e0d\u9002\u5408\u4ea4\u4e92\u5f0f\u5206\u6790:\n\n\n\n\n\u4f60\u4e0d\u53ef\u80fd\u6267\u884c\u4e00\u6761\u67e5\u8be2\u5e76\u5728\u51e0\u79d2\u5185\u6216\u66f4\u77ed\u65f6\u95f4\u5185\u5f97\u5230\u7ed3\u679c\uff1b\u5178\u578b\u60c5\u51b5\u4e0b\uff0c\u6267\u884c\u67e5\u8be2\u9700\u8981\u51e0\u5206\u949f\u6216\u66f4\u591a\u65f6\u95f4\u3002\n\n\nMapReduce\u66f4\u9002\u5408\u6ca1\u6709\u7528\u6237\u5728\u73b0\u573a\u7b49\u5f85\u67e5\u8be2\u7ed3\u679c\u7684\u79bb\u7ebf\u4f7f\u7528\u573a\u666f\u3002\n\n\n\n\nHadoop\u7684\u53d1\u5c55\u5df2\u7ecf\u8d85\u8d8a\u4e86\u6279\u5904\u7406\u672c\u8eab\u3002\n\n\n\n\nHadoop\u6709\u65f6\u88ab\u7528\u4e8e\u6307\u4ee3\u4e00\u4e2a\u66f4\u5927\u7684\u3001\u591a\u4e2a\u9879\u76ee\u7ec4\u6210\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662fHDFS\u548cMapReduce\n\n\nHBase\uff1a\u4f7f\u7528HDFS\u505a\u5e95\u5c42\u5b58\u50a8\u7684\u952e\u503c\u5b58\u50a8\u6a21\u578b\u3002 \n\n\nYARN: \u96c6\u7fa4\u8d44\u6e90\u7ba1\u7406\u7cfb\u7edf\n\n\n\n\n\n\n\n\n1.6 \u53d1\u5c55\u5386\u53f2\n\n\n\n\nHadoop\u662fDoug Cutting\u521b\u5efa\u7684\uff0c\u8d77\u6e90\u4e8e\u5f00\u6e90\u7f51\u7edc\u641c\u7d22\u5f15\u64ceApache Nutch.\n\n\nApache Nutch\u662f\u8d77\u59cb\u4e8e2002\u5e74\uff0c\u5e76\u501f\u9274\u4e86\u201c\u8c37\u6b4c\u5206\u5e03\u5f0f\u6587\u4ef6\u7cfb\u7edf(GFS)\"\u548cMapReduce\u3002\n\n\n2006\u5e74\uff0cDoug Cutting\u52a0\u5165\u96c5\u864e\uff0c\u96c5\u864e\u4e3a\u6b64\u7ec4\u7ec7\u4e86\u4e13\u95e8\u7684\u56e2\u961f\u548c\u8d44\u6e90\uff0c\u5c06Hadoop\u53d1\u5c55\u6210\u80fd\u591f\u4ee5Web\u89c4\u6a21\u8fd0\u884c\u7684\u7cfb\u7edf\u3002 \n\n\n2008\u5e74\uff0cHadoop\u5df2\u7ecf\u6210\u4e3aApache\u7684\u9876\u7ea7\u9879\u76ee\uff0c\u8bc1\u660e\u4e86\u5b83\u7684\u6210\u529f\u3001\u591a\u6837\u5316\u548c\u751f\u547d\u529b\u3002\n\n\n\u76ee\u524d\uff0cHadoop\u88ab\u4e3b\u6d41\u4f01\u4e1a\u5e7f\u6cdb\u4f7f\u7528\u3002\u5728\u5de5\u4e1a\u754c\uff0cHadoop\u5df2\u7ecf\u662f \n\u516c\u8ba4\u7684\u5927\u6570\u636e\u901a\u7528\u5b58\u50a8\u548c\u5206\u6790\u5e73\u53f0\n \u3002\n\n\n\n\n1.7 Hadoop\u5bb6\u65cf", 
            "title": "Chapter 1: Meet Hadoop"
        }, 
        {
            "location": "/hadoop/ch1/#hadoop-the-definitive-guide-1-meet-hadoop", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 1 - Meet Hadoop"
        }, 
        {
            "location": "/hadoop/ch1/#1", 
            "text": "\u73b0\u5728\u662f\u6570\u636e\u5927\u7206\u70b8\u65f6\u4ee3\uff0c\u5168\u7403\u6570\u636e\u603b\u91cf\u8fdc\u8fdc\u8d85\u8fc7\u4e86\u5168\u4e16\u754c\u6bcf\u4eba\u4e00\u5757\u786c\u76d8\u4e2d\u6240\u80fd\u4fdd\u5b58\u7684\u6570\u636e\u603b\u91cf\u3002   \u4e2a\u4eba\u4ea7\u751f\u7684\u6570\u636e\u6b63\u5728\u5feb\u901f\u589e\u957f  \u4e2a\u4eba\u4fe1\u606f\u6863\u6848\u5c06\u65e5\u76ca\u666e\u53ca\uff08\u7535\u8bdd\u3001\u90ae\u4ef6\u3001\u6587\u4ef6\u3001\u7167\u7247\uff09    \u7269\u8054\u7f51\u7684\u673a\u5668\u8bbe\u5907\u4ea7\u751f\u7684\u6570\u636e\u53ef\u80fd\u8fdc\u8fdc\u8d85\u8fc7\u4e2a\u4eba\u4ea7\u751f\u7684\u6570\u636e   \u673a\u5668\u65e5\u5fd7\u3001\u4f20\u611f\u5668\u7f51\u7edc\u3001\u96f6\u552e\u4ea4\u6613\u6570\u636e\u7b49", 
            "title": "1 \u6570\u636e\uff01\u6570\u636e\uff01"
        }, 
        {
            "location": "/hadoop/ch1/#12", 
            "text": "\u9047\u5230\u7684\u95ee\u9898\uff1a\u786c\u76d8\u5b58\u50a8\u5bb9\u91cf\u4e0d\u65ad\u63d0\u5347\uff0c\u8bbf\u95ee\u901f\u5ea6\u6ca1\u6709\u4e0e\u65f6\u4ff1\u8fdb   \u8bfb\u5199\u786c\u76d8\u4e2d\u7684\u6570\u636e\u9700\u8981\u66f4\u957f\u65f6\u95f4   \u89e3\u51b3\u65b9\u6cd5\uff1a \u540c\u65f6\u4ece\u591a\u4e2a\u786c\u76d8\u4e0a\u8bfb\u53d6\u6570\u636e\uff0c\u6bcf\u4e2a\u786c\u76d8\u5b58\u50a8\u4e00\u90e8\u5206\u6570\u636e   \u867d\u7136\u6d6a\u8d39\u4e86\u786c\u76d8\u5bb9\u91cf\uff0c\u4f46\u662f\u7531\u4e8e\u7528\u6237\u7684\u5206\u6790\u5de5\u4f5c\u90fd\u662f\u5728\u4e0d\u540c\u65f6\u95f4\u70b9\u8fdb\u884c\u7684\uff0c\u6240\u4ee5\u5f7c\u6b64\u4e4b\u95f4\u7684\u5e72\u6270\u5e76\u4e0d\u592a\u5927   \u65b0\u7684\u95ee\u9898\uff1a\u8981\u5bf9\u591a\u4e2a\u786c\u76d8\u4e2d\u7684\u6570\u636e\u5e76\u884c\u8fdb\u884c\u8bfb\u5199\u6570\u636e\uff0c\u8fd8\u6709\u66f4\u591a\u95ee\u9898\u8981\u89e3\u51b3   \u786c\u4ef6\u6545\u969c\u95ee\u9898\uff1a\u6700\u5e38\u89c1\u7684\u505a\u6cd5\u662f\u7cfb\u7edf\u4fdd\u5b58\u6570\u636e\u7684\u526f\u672c(replica)  \u5927\u591a\u6570\u5206\u6790\u4efb\u52a1\u9700\u8981\u4ee5\u67d0\u79cd\u65b9\u5f0f\u7ed3\u5408\u5927\u90e8\u5206\u6570\u636e\u6765\u5171\u540c\u5b8c\u6210\u5206\u6790\uff0c\u4fdd\u8bc1\u5176\u6b63\u786e\u6027\u662f\u4e00\u4e2a\u975e\u5e38\u5927\u7684\u6311\u6218\uff1aMapReduce", 
            "title": "1.2 \u6570\u636e\u7684\u5b58\u50a8\u4e0e\u5206\u6790"
        }, 
        {
            "location": "/hadoop/ch1/#13", 
            "text": "MapReduce\u662f\u4e00\u4e2a \u6279\u91cf\u67e5\u8be2\u5904\u7406\u5668 (batch processing system)\uff0c\u80fd\u591f\u5728\u5408\u7406\u7684\u65f6\u95f4\u8303\u56f4\u5185\u5904\u7406\u9488\u5bf9\u6574\u4e2a\u6570\u636e\u96c6\u7684\u52a8\u6001\u67e5\u8be2\u3002", 
            "title": "1.3 \u67e5\u8be2\u6240\u6709\u6570\u636e"
        }, 
        {
            "location": "/hadoop/ch1/#14", 
            "text": "MapReduce\u57fa\u672c\u4e0a\u662f\u4e00\u4e2a\u6279\u5904\u7406\u7cfb\u7edf\uff0c\u5e76\u4e0d\u9002\u5408\u4ea4\u4e92\u5f0f\u5206\u6790:   \u4f60\u4e0d\u53ef\u80fd\u6267\u884c\u4e00\u6761\u67e5\u8be2\u5e76\u5728\u51e0\u79d2\u5185\u6216\u66f4\u77ed\u65f6\u95f4\u5185\u5f97\u5230\u7ed3\u679c\uff1b\u5178\u578b\u60c5\u51b5\u4e0b\uff0c\u6267\u884c\u67e5\u8be2\u9700\u8981\u51e0\u5206\u949f\u6216\u66f4\u591a\u65f6\u95f4\u3002  MapReduce\u66f4\u9002\u5408\u6ca1\u6709\u7528\u6237\u5728\u73b0\u573a\u7b49\u5f85\u67e5\u8be2\u7ed3\u679c\u7684\u79bb\u7ebf\u4f7f\u7528\u573a\u666f\u3002   Hadoop\u7684\u53d1\u5c55\u5df2\u7ecf\u8d85\u8d8a\u4e86\u6279\u5904\u7406\u672c\u8eab\u3002   Hadoop\u6709\u65f6\u88ab\u7528\u4e8e\u6307\u4ee3\u4e00\u4e2a\u66f4\u5927\u7684\u3001\u591a\u4e2a\u9879\u76ee\u7ec4\u6210\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662fHDFS\u548cMapReduce  HBase\uff1a\u4f7f\u7528HDFS\u505a\u5e95\u5c42\u5b58\u50a8\u7684\u952e\u503c\u5b58\u50a8\u6a21\u578b\u3002   YARN: \u96c6\u7fa4\u8d44\u6e90\u7ba1\u7406\u7cfb\u7edf", 
            "title": "1.4 \u4e0d\u4ec5\u4ec5\u662f\u6279\u5904\u7406"
        }, 
        {
            "location": "/hadoop/ch1/#16", 
            "text": "Hadoop\u662fDoug Cutting\u521b\u5efa\u7684\uff0c\u8d77\u6e90\u4e8e\u5f00\u6e90\u7f51\u7edc\u641c\u7d22\u5f15\u64ceApache Nutch.  Apache Nutch\u662f\u8d77\u59cb\u4e8e2002\u5e74\uff0c\u5e76\u501f\u9274\u4e86\u201c\u8c37\u6b4c\u5206\u5e03\u5f0f\u6587\u4ef6\u7cfb\u7edf(GFS)\"\u548cMapReduce\u3002  2006\u5e74\uff0cDoug Cutting\u52a0\u5165\u96c5\u864e\uff0c\u96c5\u864e\u4e3a\u6b64\u7ec4\u7ec7\u4e86\u4e13\u95e8\u7684\u56e2\u961f\u548c\u8d44\u6e90\uff0c\u5c06Hadoop\u53d1\u5c55\u6210\u80fd\u591f\u4ee5Web\u89c4\u6a21\u8fd0\u884c\u7684\u7cfb\u7edf\u3002   2008\u5e74\uff0cHadoop\u5df2\u7ecf\u6210\u4e3aApache\u7684\u9876\u7ea7\u9879\u76ee\uff0c\u8bc1\u660e\u4e86\u5b83\u7684\u6210\u529f\u3001\u591a\u6837\u5316\u548c\u751f\u547d\u529b\u3002  \u76ee\u524d\uff0cHadoop\u88ab\u4e3b\u6d41\u4f01\u4e1a\u5e7f\u6cdb\u4f7f\u7528\u3002\u5728\u5de5\u4e1a\u754c\uff0cHadoop\u5df2\u7ecf\u662f  \u516c\u8ba4\u7684\u5927\u6570\u636e\u901a\u7528\u5b58\u50a8\u548c\u5206\u6790\u5e73\u53f0  \u3002", 
            "title": "1.6 \u53d1\u5c55\u5386\u53f2"
        }, 
        {
            "location": "/hadoop/ch1/#17-hadoop", 
            "text": "", 
            "title": "1.7 Hadoop\u5bb6\u65cf"
        }, 
        {
            "location": "/hadoop/ch2/", 
            "text": "Hadoop: The Definitive Guide 2 - MapReduce\n\n\nMapReduce is a programming model for data processing. MapReduce programs are inherently parallel, thus putting very large-scale data analysis into the hands of anyone with enough machines at their disposal.\n\n\n1 A Weather Dataset\n\n\nFor our example, we will write a program that mines weather data. The data we will use is from the National Climatic Data Center. It is stored using a line-oriented ASCII format, in which each line is a record.\n\n\nAnalyzing the Data with Hadoop\n\n\nMapReduce works by breaking the processing into two phases: the map phase and the reduce phase. Each phase has key-value pairs as input and output, the types of which may be chosen by the programmer. The programmer also specifies two functions: the map function and the reduce function.\n\n\nMAPINPUT:\n\n\n\n\nKey: the offset of the beginning of the line from the beginning of the file. (no need here, just ignore it)\n\n\nValue: raw NCDC data\n\n\n\n\n \n(0, 0067011990999991950051507004\u20269999999N9+00001+99999999999\u2026) \n(106, 0043011990999991950051512004\u20269999999N9+00221+99999999999\u2026) \n(212, 0043011990999991950051518004\u20269999999N9-00111+99999999999\u2026) \n(318, 0043012650999991949032412004\u20260500001N9+01111+99999999999\u2026) \n(424, 0043012650999991949032418004\u20260500001N9+00781+99999999999\u2026)\n\n\n\nMAPOUTPUT: \n\n\n\n\nThe map function merely extracts the year and the air temperature , and emits them as output.\n\n\nKey: year\n\n\nCalue: air temperature\n\n\n\n\n \n(1950, 0)\n(1950, 22) \n(1950, \u221211) \n(1949, 111) \n(1949, 78)\n\n\n\nThe output from the map function is processed by the MapReduce framework \nbefore\n being sent to the reduce function. This processing \nsorts\n and \ngroups\n the key-value pairs by key.\n\n\n \n(1949, [111, 78]) \n(1950, [0, 22, \u221211])\n\n\n\nAll the reduce function has to do now is iterate through the list and pick up the maximum reading:\n\n\n \n(1949, 111) \n(1950, 22)\n\n\n\n\n\nJava MapReduce\n\n\nWe need three things: a map function, a reduce function, and some code to run the job. \n\n\nMap\n\n\nThe map function is represented by the \nMapper\n class, which declares an abstract \nmap()\n method.\n\n\nThe \nMapper\n class is a generic type, with four formal type parameters that specify the input key, input value, output key, and output value types of the map function.\n\n\n \npublic\n \nclass\n \nMapper\nKEYIN\n,\n \nVALUEIN\n,\n \nKEYOUT\n,\n \nVALUEOUT\n \n{}\n\n\n\n\nRather than using built-in Java types, Hadoop provides its own set of basic types that are optimized for network serialization. These are found in the \norg.apache.hadoop.io\n package.\n\n\nHere we use \nLongWritable\n, which corresponds to a Java \nLong\n, \nText\n (like Java \nString\n), and \nIntWritable\n (like Java \nInteger\n).\n\n\n \nimport\n \norg.apache.hadoop.io.IntWritable\n;\n\n\nimport\n \norg.apache.hadoop.io.LongWritable\n;\n\n\nimport\n \norg.apache.hadoop.io.Text\n;\n\n\nimport\n \norg.apache.hadoop.mapreduce.Mapper\n;\n\n\nimport\n \njava.io.IOException\n;\n\n\n\npublic\n \nclass\n \nMaxTemperatureMapper\n\n        \nextends\n \nMapper\nLongWritable\n,\n \nText\n,\n \nText\n,\n \nIntWritable\n \n{\n\n\n    \nprivate\n \nstatic\n \nfinal\n \nint\n \nMISSING\n \n=\n \n9999\n;\n\n\n    \n@Override\n\n    \nprotected\n \nvoid\n \nmap\n(\nLongWritable\n \nkey\n,\n \nText\n \nvalue\n,\n \nContext\n \ncontext\n)\n\n            \nthrows\n \nIOException\n,\n \nInterruptedException\n \n{\n\n        \nString\n \nline\n \n=\n \nvalue\n.\ntoString\n();\n\n        \nString\n \nyear\n \n=\n \nline\n.\nsubstring\n(\n15\n,\n \n19\n);\n\n        \nint\n \nairTemperature\n;\n\n        \nif\n \n(\nline\n.\ncharAt\n(\n87\n)\n \n==\n \n+\n){\n \n//parseInt doesn\nt like leading plus signs\n\n            \nairTemperature\n \n=\n \nInteger\n.\nparseInt\n(\nline\n.\nsubstring\n(\n88\n,\n \n92\n));\n\n        \n}\n \nelse\n \n{\n\n            \nairTemperature\n \n=\n \nInteger\n.\nparseInt\n(\nline\n.\nsubstring\n(\n87\n,\n \n92\n));\n\n        \n}\n\n\n        \nString\n \nquality\n \n=\n \nline\n.\nsubstring\n(\n92\n,\n \n93\n);\n\n        \nif\n \n(\nairTemperature\n \n!=\n \nMISSING\n \n \nquality\n.\nmatches\n(\n[01459]]\n))\n \n{\n\n            \ncontext\n.\nwrite\n(\nnew\n \nText\n(\nyear\n),\n \nnew\n \nIntWritable\n(\nairTemperature\n));\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\nReduce\n\n\nThe reduce function is similarly defined using a \nReducer\n.\n\n\n \nimport\n \norg.apache.hadoop.io.IntWritable\n;\n\n\nimport\n \norg.apache.hadoop.io.Text\n;\n\n\nimport\n \norg.apache.hadoop.mapreduce.Reducer\n;\n\n\n\nimport\n \njava.io.IOException\n;\n\n\n\npublic\n \nclass\n \nMaxTemperatureReducer\n\n        \nextends\n \nReducer\nText\n,\n \nIntWritable\n,\n \nText\n,\n \nIntWritable\n \n{\n\n\n    \n@Override\n\n    \nprotected\n \nvoid\n \nreduce\n(\nText\n \nkey\n,\n \nIterable\nIntWritable\n \nvalues\n,\n \nContext\n \ncontext\n)\n \nthrows\n \nIOException\n,\n \nInterruptedException\n \n{\n\n        \nint\n \nmaxValue\n \n=\n \nInteger\n.\nMIN_VALUE\n;\n\n        \nfor\n \n(\nIntWritable\n \nvalue\n \n:\n \nvalues\n)\n \n{\n\n            \nmaxValue\n \n=\n \nMath\n.\nmax\n(\nmaxValue\n,\n \nvalue\n.\nget\n());\n\n        \n}\n\n        \ncontext\n.\nwrite\n(\nkey\n,\n \nnew\n \nIntWritable\n(\nmaxValue\n));\n\n\n    \n}\n\n\n}\n\n\n\n\nMapReduce Job\n\n\nThe third piece of code runs the MapReduce job\n\n\n \nimport\n \norg.apache.hadoop.fs.Path\n;\n\n\nimport\n \norg.apache.hadoop.io.IntWritable\n;\n\n\nimport\n \norg.apache.hadoop.io.Text\n;\n\n\nimport\n \norg.apache.hadoop.mapreduce.Job\n;\n\n\nimport\n \norg.apache.hadoop.mapreduce.lib.input.FileInputFormat\n;\n\n\nimport\n \norg.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n;\n\n\n\npublic\n \nclass\n \nMaxTemperature\n \n{\n\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n\n        \nthrows\n \nException\n \n{\n\n        \nif\n \n(\nargs\n.\nlength\n \n!=\n \n2\n)\n \n{\n\n            \nSystem\n.\nerr\n.\nprintln\n(\nUsage: MaxTempeature \nintput path\n \noutput path\n);\n\n            \nSystem\n.\nexit\n(-\n1\n);\n\n        \n}\n\n\n        \nJob\n \njob\n \n=\n \nnew\n \nJob\n();\n\n        \njob\n.\nsetJarByClass\n(\nMaxTemperature\n.\nclass\n);\n\n        \njob\n.\nsetJobName\n(\nMax Temperature\n);\n\n        \nFileInputFormat\n.\naddInputPath\n(\njob\n,\n \nnew\n \nPath\n(\nargs\n[\n0\n]));\n\n        \nFileOutputFormat\n.\nsetOutputPath\n(\njob\n,\n \nnew\n \nPath\n(\nargs\n[\n1\n]));\n\n        \njob\n.\nsetMapperClass\n(\nMaxTemperatureMapper\n.\nclass\n);\n\n        \njob\n.\nsetReducerClass\n(\nMaxTemperatureReducer\n.\nclass\n);\n\n        \njob\n.\nsetOutputKeyClass\n(\nText\n.\nclass\n);\n\n        \njob\n.\nsetOutputValueClass\n(\nIntWritable\n.\nclass\n);\n\n        \nSystem\n.\nexit\n(\njob\n.\nwaitForCompletion\n(\ntrue\n)\n \n?\n \n0\n \n:\n \n1\n);\n\n    \n}\n\n\n}\n\n\n\n\nA \nJob\n object forms the specification of the job and gives you control over how the job is run.\n\n\nRather than explicitly specifying the name of the JAR file, we can pass a class in the Job\u2019s \nsetJarByClass()\n method, which Hadoop will use to locate the relevant JAR file by looking for the JAR file containing this class.\n\n\nA test run\n\n\n \n$ \nexport\n \nHADOOP_CLASSPATH\n=\n/Users/larry/JavaProject/out/artifacts/MaxTemperature/MaxTemperature.jar \n$ hadoop com.definitivehadoop.weatherdata.MaxTemperature resources/HadoopBook/ncdc/sample.txt output\n\n\n\nWhen the \nhadoop\n command is invoked with a classname as the first argument, it launches a Java virtual machine (JVM) to run the class. The \nhadoop\n command adds the Hadoop libraries (and their dependencies) to the classpath and picks up the Hadoop configuration, too. To add the application classes to the classpath, we\u2019ve defined an environment variable called \nHADOOP_CLASSPATH\n, which the \nhadoop\n script picks up.\n\n\n\n\n \n//OUTPUT \n18:20:18,944 INFO mapreduce.Job: Counters: 30\n        File System Counters\n                FILE: Number of bytes read=148485300\n                FILE: Number of bytes written=150614384\n                FILE: Number of read operations=0\n                FILE: Number of large read operations=0\n                FILE: Number of write operations=0\n        Map-Reduce Framework\n                Map input records=5\n                Map output records=0\n                Map output bytes=0\n                Map output materialized bytes=6\n                Input split bytes=131\n                Combine input records=0\n                Combine output records=0\n                Reduce input groups=0\n                Reduce shuffle bytes=6\n                Reduce input records=0\n                Reduce output records=0\n                Spilled Records=0\n                Shuffled Maps =1\n                Failed Shuffles=0\n                Merged Map outputs=1\n                GC time elapsed (ms)=5\n                Total committed heap usage (bytes)=406847488\n        Shuffle Errors\n                BAD_ID=0\n                CONNECTION=0\n                IO_ERROR=0\n                WRONG_LENGTH=0\n                WRONG_MAP=0\n                WRONG_REDUCE=0\n        File Input Format Counters \n                Bytes Read=529\n        File Output Format Counters \n                Bytes Written=8\n\n\n\n\n\nThe last section of the output, titled \u201cCounters,\u201d shows the statistics that Hadoop generates for each job it runs. These are very useful for checking whether the amount of data processed is what you expected.\n\n\n2 Scaling Out\n\n\nTo scale out, we need to store the data in a distributed filesystem (typically HDFS). This allows Hadoop to move the MapReduce computation to each machine hosting a part of the data, using Hadoop\u2019s resource management system, YARN.\n\n\nData Flow\n\n\nA MapReduce \njob\n (\u4f5c\u4e1a) is a unit of work that the client wants to be performed: it consists of the input data, the MapReduce program, and configuration information. Hadoop runs the job by dividing it into \ntasks\n (\u4efb\u52a1), of which there are two types: \nmap tasks\n and \nreduce tasks\n. The tasks are scheduled using YARN and run on nodes in the cluster.\n\n\nHadoop divides the input to a MapReduce job into fixed-size pieces called \ninput splits\n (\u8f93\u5165\u5206\u7247), or just \nsplits\n (\u5206\u7247). Hadoop creates one map task for each split, which runs the user-defined map function for each record in the split.\n\n\nSo if we are processing the splits in parallel, the processing is better load balanced when the splits are small. On the other hand, if splits are too small, the overhead of managing the splits and map task creation begins to dominate the total job execution time.\n\n\nHadoop does its best to run the map task on a node where the input data resides in HDFS, because it doesn\u2019t use valuable cluster bandwidth. This is called the \ndata locality optimization\n (\u6570\u636e\u672c\u5730\u4f18\u5316).\n\n\n\n\nMap tasks write their output to the local disk, not to HDFS. Why is this? \nMap output is intermediate output:\n it\u2019s processed by reduce tasks to produce the final output, and once the job is complete, the map output can be thrown away. So, storing it in HDFS \nwith replication\n would be overkill.\n\n\nThe data flow for the general case of multiple reduce tasks is illustrated in figure below. This diagram makes it clear why the data flow between map and reduce tasks is colloquially known as \u201cthe shuffle,\u201d as each reduce task is fed by many map tasks.\n\n\n\n\nCombiner Functions\n\n\nMany MapReduce jobs are limited by the bandwidth available on the cluster, so it pays to minimize the data transferred between map and reduce tasks. Hadoop allows the user to specify a \ncombiner\n function to be run on the map output, and the combiner function\u2019s output forms the input to the reduce function.\n\n\nFor max temperature problem described above, the combiner function is the same implementation as the reduce function in \nMaxTemperatureReducer\n. The only change we need to make is to set the combiner class on the Job.\n\n\n \njob\n.\nsetMapperClass\n(\nMaxTemperatureMapper\n.\nclass\n);\n\n\njob\n.\nsetCombinerClass\n(\nMaxTemperatureReducer\n.\nclass\n);\n\n\njob\n.\nsetReducerClass\n(\nMaxTemperatureReducer\n.\nclass\n);\n\n\n\n\nA part of output information for running \nMaxTemperatureReducer\n is:\n\n\n \nMap input records=5\n                Map output records=5\n                Map output bytes=45\n                Map output materialized bytes=28\n                Input split bytes=131\n                Combine input records=5\n                Combine output records=2\n                Reduce input groups=2\n                Reduce shuffle bytes=28\n                Reduce input records=2\n                Reduce output records=2\n\n\n\n3 Hadoop Streaming\n\n\nHadoop provides an API to MapReduce that allows you to write your map and reduce functions in languages other than Java. Hadoop Streaming uses Unix standard streams (Unix\u6807\u51c6\u6d41) as the interface between Hadoop and your program, so you can use any language that can read standard input and write to standard output to write your MapReduce program.\n\n\nStreaming is naturally suited for text processing. Map input data is passed over standard input to map function. A map output key-value pair is written as a single tab-delimited line. The reduce function reads lines from standard input, which the framework guarantees are sorted by key, and writes its results to standard output.\n\n\nHere, We take Python as an example.\n\n\nPython\n\n\nMap\n#!/Users/larry/anaconda3/bin/python\n\n\nimport\n \nre\n\n\nimport\n \nsys\n\n\n\nfor\n \nline\n \nin\n \nsys\n.\nstdin\n:\n\n  \nval\n \n=\n \nline\n.\nstrip\n()\n\n  \n(\nyear\n,\n \ntemp\n,\n \nq\n)\n \n=\n \n(\nval\n[\n15\n:\n19\n],\n \nval\n[\n87\n:\n92\n],\n \nval\n[\n92\n:\n93\n])\n\n  \nif\n \n(\ntemp\n \n!=\n \n+9999\n \nand\n \nre\n.\nmatch\n(\n[01459]\n,\n \nq\n)):\n\n    \nprint\n(\n%s\n\\t\n%s\n%\n(\nyear\n,\n \ntemp\n))\n\n\nReduce\n#!/Users/larry/anaconda3/bin/python\n\n\n\nimport\n \nsys\n\n\n\n(\nlast_key\n,\n \nmax_val\n)\n \n=\n \n(\nNone\n,\n \n-\nsys\n.\nmaxsize\n)\n\n\nfor\n \nline\n \nin\n \nsys\n.\nstdin\n:\n\n  \n(\nkey\n,\n \nval\n)\n \n=\n \nline\n.\nstrip\n()\n.\nsplit\n(\n\\t\n)\n\n  \nif\n \nlast_key\n \nand\n \nlast_key\n \n!=\n \nkey\n:\n\n    \nprint\n(\n%s\n\\t\n%s\n%\n(\nlast_key\n,\n \nmax_val\n))\n\n    \n(\nlast_key\n,\n \nmax_val\n)\n \n=\n \n(\nkey\n,\n \nint\n(\nval\n))\n\n  \nelse\n:\n\n    \n(\nlast_key\n,\n \nmax_val\n)\n \n=\n \n(\nkey\n,\n \nmax\n(\nmax_val\n,\n \nint\n(\nval\n)))\n\n\n\nif\n \nlast_key\n:\n\n  \nprint\n(\n%s\n\\t\n%s\n%\n(\nlast_key\n,\n \nmax_val\n))\n\n\n\n\n\nFor example, to run a test:\n\n\n \n$ cat sample.txt \n|\n ./max_temperature_map.py \n|\n sort \n|\n ./max_temperature_reduce.py\n\n1949\n \n111\n \n\n1950\n 22\n\n\n\nThe \nhadoop\n command doesn\u2019t support a Streaming option; instead, you specify the Streaming JAR file along with the \njar\n option. Options to the Streaming program specify the input and output paths and the map and reduce scripts.\n\n\n \n$hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.0.jar \n\\\n\n-input sample.txt \n\\\n\n-output output \n\\\n\n-mapper max_temperature_map.py \n\\\n\n-reducer max_temperature_reduce.py", 
            "title": "Chapter 2: MapReduce"
        }, 
        {
            "location": "/hadoop/ch2/#hadoop-the-definitive-guide-2-mapreduce", 
            "text": "MapReduce is a programming model for data processing. MapReduce programs are inherently parallel, thus putting very large-scale data analysis into the hands of anyone with enough machines at their disposal.", 
            "title": "Hadoop: The Definitive Guide 2 - MapReduce"
        }, 
        {
            "location": "/hadoop/ch2/#1-a-weather-dataset", 
            "text": "For our example, we will write a program that mines weather data. The data we will use is from the National Climatic Data Center. It is stored using a line-oriented ASCII format, in which each line is a record.", 
            "title": "1 A Weather Dataset"
        }, 
        {
            "location": "/hadoop/ch2/#analyzing-the-data-with-hadoop", 
            "text": "MapReduce works by breaking the processing into two phases: the map phase and the reduce phase. Each phase has key-value pairs as input and output, the types of which may be chosen by the programmer. The programmer also specifies two functions: the map function and the reduce function.  MAPINPUT:   Key: the offset of the beginning of the line from the beginning of the file. (no need here, just ignore it)  Value: raw NCDC data     (0, 0067011990999991950051507004\u20269999999N9+00001+99999999999\u2026) \n(106, 0043011990999991950051512004\u20269999999N9+00221+99999999999\u2026) \n(212, 0043011990999991950051518004\u20269999999N9-00111+99999999999\u2026) \n(318, 0043012650999991949032412004\u20260500001N9+01111+99999999999\u2026) \n(424, 0043012650999991949032418004\u20260500001N9+00781+99999999999\u2026)  MAPOUTPUT:    The map function merely extracts the year and the air temperature , and emits them as output.  Key: year  Calue: air temperature     (1950, 0)\n(1950, 22) \n(1950, \u221211) \n(1949, 111) \n(1949, 78)  The output from the map function is processed by the MapReduce framework  before  being sent to the reduce function. This processing  sorts  and  groups  the key-value pairs by key.    (1949, [111, 78]) \n(1950, [0, 22, \u221211])  All the reduce function has to do now is iterate through the list and pick up the maximum reading:    (1949, 111) \n(1950, 22)", 
            "title": "Analyzing the Data with Hadoop"
        }, 
        {
            "location": "/hadoop/ch2/#java-mapreduce", 
            "text": "We need three things: a map function, a reduce function, and some code to run the job.   Map  The map function is represented by the  Mapper  class, which declares an abstract  map()  method.  The  Mapper  class is a generic type, with four formal type parameters that specify the input key, input value, output key, and output value types of the map function.    public   class   Mapper KEYIN ,   VALUEIN ,   KEYOUT ,   VALUEOUT   {}   Rather than using built-in Java types, Hadoop provides its own set of basic types that are optimized for network serialization. These are found in the  org.apache.hadoop.io  package.  Here we use  LongWritable , which corresponds to a Java  Long ,  Text  (like Java  String ), and  IntWritable  (like Java  Integer ).    import   org.apache.hadoop.io.IntWritable ;  import   org.apache.hadoop.io.LongWritable ;  import   org.apache.hadoop.io.Text ;  import   org.apache.hadoop.mapreduce.Mapper ;  import   java.io.IOException ;  public   class   MaxTemperatureMapper \n         extends   Mapper LongWritable ,   Text ,   Text ,   IntWritable   { \n\n     private   static   final   int   MISSING   =   9999 ; \n\n     @Override \n     protected   void   map ( LongWritable   key ,   Text   value ,   Context   context ) \n             throws   IOException ,   InterruptedException   { \n         String   line   =   value . toString (); \n         String   year   =   line . substring ( 15 ,   19 ); \n         int   airTemperature ; \n         if   ( line . charAt ( 87 )   ==   + ){   //parseInt doesn t like leading plus signs \n             airTemperature   =   Integer . parseInt ( line . substring ( 88 ,   92 )); \n         }   else   { \n             airTemperature   =   Integer . parseInt ( line . substring ( 87 ,   92 )); \n         } \n\n         String   quality   =   line . substring ( 92 ,   93 ); \n         if   ( airTemperature   !=   MISSING     quality . matches ( [01459]] ))   { \n             context . write ( new   Text ( year ),   new   IntWritable ( airTemperature )); \n         } \n     }  }   Reduce  The reduce function is similarly defined using a  Reducer .    import   org.apache.hadoop.io.IntWritable ;  import   org.apache.hadoop.io.Text ;  import   org.apache.hadoop.mapreduce.Reducer ;  import   java.io.IOException ;  public   class   MaxTemperatureReducer \n         extends   Reducer Text ,   IntWritable ,   Text ,   IntWritable   { \n\n     @Override \n     protected   void   reduce ( Text   key ,   Iterable IntWritable   values ,   Context   context )   throws   IOException ,   InterruptedException   { \n         int   maxValue   =   Integer . MIN_VALUE ; \n         for   ( IntWritable   value   :   values )   { \n             maxValue   =   Math . max ( maxValue ,   value . get ()); \n         } \n         context . write ( key ,   new   IntWritable ( maxValue )); \n\n     }  }   MapReduce Job  The third piece of code runs the MapReduce job    import   org.apache.hadoop.fs.Path ;  import   org.apache.hadoop.io.IntWritable ;  import   org.apache.hadoop.io.Text ;  import   org.apache.hadoop.mapreduce.Job ;  import   org.apache.hadoop.mapreduce.lib.input.FileInputFormat ;  import   org.apache.hadoop.mapreduce.lib.output.FileOutputFormat ;  public   class   MaxTemperature   { \n\n     public   static   void   main ( String []   args ) \n         throws   Exception   { \n         if   ( args . length   !=   2 )   { \n             System . err . println ( Usage: MaxTempeature  intput path   output path ); \n             System . exit (- 1 ); \n         } \n\n         Job   job   =   new   Job (); \n         job . setJarByClass ( MaxTemperature . class ); \n         job . setJobName ( Max Temperature ); \n         FileInputFormat . addInputPath ( job ,   new   Path ( args [ 0 ])); \n         FileOutputFormat . setOutputPath ( job ,   new   Path ( args [ 1 ])); \n         job . setMapperClass ( MaxTemperatureMapper . class ); \n         job . setReducerClass ( MaxTemperatureReducer . class ); \n         job . setOutputKeyClass ( Text . class ); \n         job . setOutputValueClass ( IntWritable . class ); \n         System . exit ( job . waitForCompletion ( true )   ?   0   :   1 ); \n     }  }   A  Job  object forms the specification of the job and gives you control over how the job is run.  Rather than explicitly specifying the name of the JAR file, we can pass a class in the Job\u2019s  setJarByClass()  method, which Hadoop will use to locate the relevant JAR file by looking for the JAR file containing this class.  A test run    $  export   HADOOP_CLASSPATH = /Users/larry/JavaProject/out/artifacts/MaxTemperature/MaxTemperature.jar \n$ hadoop com.definitivehadoop.weatherdata.MaxTemperature resources/HadoopBook/ncdc/sample.txt output  When the  hadoop  command is invoked with a classname as the first argument, it launches a Java virtual machine (JVM) to run the class. The  hadoop  command adds the Hadoop libraries (and their dependencies) to the classpath and picks up the Hadoop configuration, too. To add the application classes to the classpath, we\u2019ve defined an environment variable called  HADOOP_CLASSPATH , which the  hadoop  script picks up.     //OUTPUT \n18:20:18,944 INFO mapreduce.Job: Counters: 30\n        File System Counters\n                FILE: Number of bytes read=148485300\n                FILE: Number of bytes written=150614384\n                FILE: Number of read operations=0\n                FILE: Number of large read operations=0\n                FILE: Number of write operations=0\n        Map-Reduce Framework\n                Map input records=5\n                Map output records=0\n                Map output bytes=0\n                Map output materialized bytes=6\n                Input split bytes=131\n                Combine input records=0\n                Combine output records=0\n                Reduce input groups=0\n                Reduce shuffle bytes=6\n                Reduce input records=0\n                Reduce output records=0\n                Spilled Records=0\n                Shuffled Maps =1\n                Failed Shuffles=0\n                Merged Map outputs=1\n                GC time elapsed (ms)=5\n                Total committed heap usage (bytes)=406847488\n        Shuffle Errors\n                BAD_ID=0\n                CONNECTION=0\n                IO_ERROR=0\n                WRONG_LENGTH=0\n                WRONG_MAP=0\n                WRONG_REDUCE=0\n        File Input Format Counters \n                Bytes Read=529\n        File Output Format Counters \n                Bytes Written=8   The last section of the output, titled \u201cCounters,\u201d shows the statistics that Hadoop generates for each job it runs. These are very useful for checking whether the amount of data processed is what you expected.", 
            "title": "Java MapReduce"
        }, 
        {
            "location": "/hadoop/ch2/#2-scaling-out", 
            "text": "To scale out, we need to store the data in a distributed filesystem (typically HDFS). This allows Hadoop to move the MapReduce computation to each machine hosting a part of the data, using Hadoop\u2019s resource management system, YARN.", 
            "title": "2 Scaling Out"
        }, 
        {
            "location": "/hadoop/ch2/#data-flow", 
            "text": "A MapReduce  job  (\u4f5c\u4e1a) is a unit of work that the client wants to be performed: it consists of the input data, the MapReduce program, and configuration information. Hadoop runs the job by dividing it into  tasks  (\u4efb\u52a1), of which there are two types:  map tasks  and  reduce tasks . The tasks are scheduled using YARN and run on nodes in the cluster.  Hadoop divides the input to a MapReduce job into fixed-size pieces called  input splits  (\u8f93\u5165\u5206\u7247), or just  splits  (\u5206\u7247). Hadoop creates one map task for each split, which runs the user-defined map function for each record in the split.  So if we are processing the splits in parallel, the processing is better load balanced when the splits are small. On the other hand, if splits are too small, the overhead of managing the splits and map task creation begins to dominate the total job execution time.  Hadoop does its best to run the map task on a node where the input data resides in HDFS, because it doesn\u2019t use valuable cluster bandwidth. This is called the  data locality optimization  (\u6570\u636e\u672c\u5730\u4f18\u5316).   Map tasks write their output to the local disk, not to HDFS. Why is this?  Map output is intermediate output:  it\u2019s processed by reduce tasks to produce the final output, and once the job is complete, the map output can be thrown away. So, storing it in HDFS  with replication  would be overkill.  The data flow for the general case of multiple reduce tasks is illustrated in figure below. This diagram makes it clear why the data flow between map and reduce tasks is colloquially known as \u201cthe shuffle,\u201d as each reduce task is fed by many map tasks.", 
            "title": "Data Flow"
        }, 
        {
            "location": "/hadoop/ch2/#combiner-functions", 
            "text": "Many MapReduce jobs are limited by the bandwidth available on the cluster, so it pays to minimize the data transferred between map and reduce tasks. Hadoop allows the user to specify a  combiner  function to be run on the map output, and the combiner function\u2019s output forms the input to the reduce function.  For max temperature problem described above, the combiner function is the same implementation as the reduce function in  MaxTemperatureReducer . The only change we need to make is to set the combiner class on the Job.    job . setMapperClass ( MaxTemperatureMapper . class );  job . setCombinerClass ( MaxTemperatureReducer . class );  job . setReducerClass ( MaxTemperatureReducer . class );   A part of output information for running  MaxTemperatureReducer  is:    Map input records=5\n                Map output records=5\n                Map output bytes=45\n                Map output materialized bytes=28\n                Input split bytes=131\n                Combine input records=5\n                Combine output records=2\n                Reduce input groups=2\n                Reduce shuffle bytes=28\n                Reduce input records=2\n                Reduce output records=2", 
            "title": "Combiner Functions"
        }, 
        {
            "location": "/hadoop/ch2/#3-hadoop-streaming", 
            "text": "Hadoop provides an API to MapReduce that allows you to write your map and reduce functions in languages other than Java. Hadoop Streaming uses Unix standard streams (Unix\u6807\u51c6\u6d41) as the interface between Hadoop and your program, so you can use any language that can read standard input and write to standard output to write your MapReduce program.  Streaming is naturally suited for text processing. Map input data is passed over standard input to map function. A map output key-value pair is written as a single tab-delimited line. The reduce function reads lines from standard input, which the framework guarantees are sorted by key, and writes its results to standard output.  Here, We take Python as an example.", 
            "title": "3 Hadoop Streaming"
        }, 
        {
            "location": "/hadoop/ch2/#python", 
            "text": "Map #!/Users/larry/anaconda3/bin/python  import   re  import   sys  for   line   in   sys . stdin : \n   val   =   line . strip () \n   ( year ,   temp ,   q )   =   ( val [ 15 : 19 ],   val [ 87 : 92 ],   val [ 92 : 93 ]) \n   if   ( temp   !=   +9999   and   re . match ( [01459] ,   q )): \n     print ( %s \\t %s % ( year ,   temp ))  Reduce #!/Users/larry/anaconda3/bin/python  import   sys  ( last_key ,   max_val )   =   ( None ,   - sys . maxsize )  for   line   in   sys . stdin : \n   ( key ,   val )   =   line . strip () . split ( \\t ) \n   if   last_key   and   last_key   !=   key : \n     print ( %s \\t %s % ( last_key ,   max_val )) \n     ( last_key ,   max_val )   =   ( key ,   int ( val )) \n   else : \n     ( last_key ,   max_val )   =   ( key ,   max ( max_val ,   int ( val )))  if   last_key : \n   print ( %s \\t %s % ( last_key ,   max_val ))   For example, to run a test:    $ cat sample.txt  |  ./max_temperature_map.py  |  sort  |  ./max_temperature_reduce.py 1949   111   1950  22  The  hadoop  command doesn\u2019t support a Streaming option; instead, you specify the Streaming JAR file along with the  jar  option. Options to the Streaming program specify the input and output paths and the map and reduce scripts.    $hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.1.0.jar  \\ \n-input sample.txt  \\ \n-output output  \\ \n-mapper max_temperature_map.py  \\ \n-reducer max_temperature_reduce.py", 
            "title": "Python"
        }, 
        {
            "location": "/hadoop/ch3/", 
            "text": "Hadoop: The Definitive Guide 3 - The Hadoop Distributed FileSystem\n\n\nFilesystems that manage the storage across a network of machines are called \ndistributed filesystems\n . Hadoop comes with a distributed filesystem called HDFS, which stands for \nHadoop Distributed Filesystem\n .\n\n\n1 The Design of HDFS\n\n\nHDFS is a filesystem designed for storing very large files with streaming data access patterns, running on clusters of commodity hardware.\n\n\n\n\nVery large files: files that are hundreds of megabytes, gigabytes, or terabytes in size.\n\n\nStreaming data access: HDFS is built around the idea that the most efficient data processing pattern is a \nwrite-once, read-many-times\n pattern.\n\n\nCommodity hardware: It\u2019s designed to run on clusters of commodity hardware.\n\n\n\n\nThese are areas where HDFS is not a good fit today:\n\n\n\n\nLow-latency data access\n\n\nLots of small files\n\n\nMultiple writers, arbitrary file modifications\n\n\n\n\n2 HDFS Concepts\n\n\nBlocks\n\n\nA disk has a block size, which is the minimum amount of data that it can read or write. Filesystems for a single disk build on this by dealing with data in blocks, which are an integral multiple of the disk block size.\n\n\nHDFS, too, has the concept of a \nblock\n, but it is a much larger unit \u2014 128 MB by default (typically a few kilobytes for ordinary file system). Unlike a filesystem for a single disk, a file in HDFS that is smaller than a single block does not occupy a full block\u2019s worth of underlying storage. (For example, a 1 MB file stored with a block size of 128 MB uses 1 MB of disk space, not 128 MB.)\n\n\n\n\nQuestion\n\n\nWHY IS A BLOCK IN HDFS SO LARGE? To minimize the cost of seeks.\n\n\n\n\nHaving a block abstraction for a distributed filesystem brings several benefits.\n\n\n\n\nA file can be larger than any single disk in the network.\n\n\nMaking the unit of abstraction a block rather than a file simplifies the storage subsystem.\n\n\nstorage management: because blocks are a fixed size, it is easy to calculate how many can be stored on a given disk.\n\n\nmetadata concerns: because blocks are just chunks of data to be stored, file metadata such as permissions information does not need to be stored with the blocks.\n\n\n\n\n\n\nBlocks fit well with replication for providing fault tolerance and availability.\n\n\nTo insure against corrupted blocks and disk and machine failure, each block is replicated to a small number of physically separate machines (typically three).\n\n\n\n\n\n\n\n\nNamenodes and Datanodes\n\n\nAn HDFS cluster has two types of nodes: a \nnamenode\n (the master) and a number of \ndatanodes\n (workers). \n\n\n\n\nThe namenode manages the filesystem namespace. It maintains the filesystem tree and the metadata for all the files and directories in the tree. This information is stored persistently on the local disk in the form of two files: the namespace image and the edit log.\n\n\nThe namenode also knows the datanodes on which all the blocks for a given file are located;\n\n\nDatanodes are the workhorses of the filesystem. They store and retrieve blocks when they are told to (by clients or the namenode), and they report back to the namenode periodically with lists of blocks that they are storing.\n\n\n\n\nIf the machine running the namenode were obliterated, all the files on the filesystem would be lost since there would be no way of knowing how to reconstruct the files from the blocks on the datanodes. Possible solution:\n\n\n\n\nto back up the files that make up the persistent state of the filesystem metadata.\n\n\nto run a secondary namenode, which keeps a copy of the merged namespace image.\n\n\n\n\nBlock Caching\n\n\nFor frequently accessed files, the blocks may be \nexplicitly\n cached in the datanode\u2019s memory, in an off-heap block cache. Users or applications instruct the namenode which files to cache (and for how long) by adding a \ncache directive\n to a \ncache pool\n.\n\n\nHDFS Federation\n\n\nProblem: On very large clusters with many files, memory becomes the limiting factor for scaling, since namenode keeps a reference to every file and block in the filesystem in memory.\n\n\nFor example, a 200-node cluster with 24 TB of disk space per node, a block size of 128 MB, and a replication factor of 3 has room for about 2 million blocks (or more): \n200\\times 24TB\u2044(128MB\u00d73)\n200\\times 24TB\u2044(128MB\u00d73)\n, So in this case, setting the namenode memory to 12,000 MB would be a good starting point.\n\n\nSolution: HDFS federation, allows a cluster to scale by adding namenodes, each of which manages a portion of the filesystem namespace.\n\n\nHDFS High Availability\n\n\nTo remedy a failed namenode, a pair of namenodes in an \nactive-standby\n configuration is introduced in Hadoop 2. In the event of the failure of the active namenode, the standby takes over its duties to continue servicing client requests \nwithout\n a significant interruption.\n\n\n3 The Command-Line Interface\n\n\nBasic Filesystem Operations\n\n\nHadoop\u2019s filesystem shell command is \nfs\n, which supports a number of subcommands (type \nhadoop fs -help\n to get detailed help).\n\n\nCopying a file from the local filesystem to HDFS:\n\n\n \n#The local file is copied tothe HDFS instance running on localhost.\n\n$ hadoop fs -copyFromLocal test.copy /test.copy\n\n# works as the same\n\n$ hadoop fs -copyFromLocal test.copy hdfs://localhost:9000/test2.copy\n\n\n\nCopying the file from the HDFS to the local filesystem:\n\n\n \n$ hadoop fs -copyToLocal /test.copy test.copy.txt\n\n\n\n4 Hadoop Filesystems\n\n\nHadoop has an abstract notion of filesystems, of which HDFS is just one implementation. The Java abstract class \norg.apache.hadoop.fs.FileSystem\n represents the client interface to a filesystem in Hadoop, and there are several concrete implementations.\n\n\n\n\n\n\n\n\nFilesystem\n\n\nURI scheme\n\n\nJava implementation\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLocal\n\n\nfile\n\n\nfs.LocalFileSystem\n\n\nA filesystem for a locally connected disk with client-side checksums\n\n\n\n\n\n\nHDFS\n\n\nhfs\n\n\nhdfs.DistributedFileSystem\n\n\nHadoop\u2019s distributed filesystem\n\n\n\n\n\n\nWebHDFS\n\n\nwebhdfs\n\n\nhdfs.web.WebHdfsFileSystem\n\n\nProviding authenticated read/write access to HDFS over HTTP.\n\n\n\n\n\n\nSecure WebHDFS\n\n\nswebhdfs\n\n\nhdfs.web.SWebHdfsFileSystem\n\n\nThe HTTPS version of WebHDFS.\n\n\n\n\n\n\n\n\nWhen you are processing large volumes of data you should choose a distributed filesystem that has the data locality optimization, notably HDFS.\n\n\nHTTP\n\n\nThe HTTP REST API exposed by the WebHDFS protocol makes it easier for other languages to interact with HDFS. Note that the HTTP interface is slower than the native Java client, so should be avoided for very large data transfers if possible.\n\n\nThere are two ways of accessing HDFS over HTTP:\n\n\n\n\nDirectly, where the HDFS daemons serve HTTP requests to clients;\n\n\nVia a proxy (or proxies), which accesses HDFS on the client\u2019s behalf using the usual DistributedFileSystem API.\n\n\n\n\n\n\nHDFS proxy allows for stricter firewall and bandwidth-limiting policies to be put in place. It\u2019s common to use a proxy for transfers between Hadoop clusters located in different data centers, or when accessing a Hadoop cluster running in the cloud from an external network.\n\n\n5 The Java Interface\n\n\nHadoop \nFileSystem\n class is the API for interacting with one of Hadoop\u2019s filesystems. In general you should strive to \nwrite your code against the \nFileSystem\n abstract class\n , to retain portability across filesystems. This is very useful when testing your program, for example, because you can rapidly run tests using data stored on the local filesystem.\n\n\nReading Data from a Hadoop URL\n\n\nNOT recommended, because \nsetURLStreamHandlerFactory()\n method can be called only once per JVM, which means that if some other part of your program sets it, you won't be able to use.\n\n\nReading Data Using the FileSystem API\n\n\nA file in a Hadoop filesystem is represented by a Hadoop \nPath\n object(\norg.apache.hadoop.fs.Path\n, not \njava.io.File\n). You can think of a \nPath\n  as a Hadoop filesystem URI, such as \nhdfs://localhost/user/tom/test.copy\n\n\nSince \nFileSystem\n is a general filesystem API, so the first step is to retrieve an instance for the filesystem we want. There are several static factory methods for getting a \nFileSystem\n instance:\n\n\n \n// Returns the default filesystem\n\n\npublic\n \nstatic\n \nFileSystem\n \nget\n(\nConfiguration\n \nconf\n)\n \nthrows\n \nIOException\n \n\n// Uses the given URI\u2019s scheme and authority to determine the filesystem to use\n\n\npublic\n \nstatic\n \nFileSystem\n \nget\n(\nURI\n \nuri\n,\n \nConfiguration\n \nconf\n)\n \nthrows\n \nIOException\n \n\n// Retrieves the filesystem as the given user\n\n\npublic\n \nstatic\n \nFileSystem\n \nget\n(\nURI\n \nuri\n,\n \nConfiguration\n \nconf\n,\n \nString\n \nuser\n)\n \nthrows\n \nIOException\n\n\n// Retrieves a local filesystem instance\n\n\npublic\n \nstatic\n \nLocalFileSystem\n \ngetLocal\n(\nConfiguration\n \nconf\n)\n \nthrows\n \nIOException\n\n\n\n\nA \nConfiguration\n object encapsulates a client or server's configuration, which is set using configuration files read from the classpath, such as \netc/hadoop/core-site.xml\n.\n\n\nWith a \nFileSystem\n instance in hand, we invoke an \nopen()\n method to get the input stream for a file:\n\n\n \n// Uses a default buffer size of 4 KB\n\n\npublic\n \nFSDataInputStream\n \nopen\n(\nPath\n \nf\n)\n \nthrows\n \nIOException\n\n\n// Uses a buffer size of bufferSize\n\n\npublic\n \nabstract\n \nFSDataInputStream\n \nopen\n(\nPath\n \nf\n,\n \nint\n \nbufferSize\n)\n \nthrows\n \nIOException\n\n\n\n\nDisplaying files from a Hadoop filesystem on standard output by using the FileSystem directly:\n\n\n \n// $ hdfs://localhost:9000/test2.copy\n\n\nimport\n \norg.apache.hadoop.conf.Configuration\n;\n\n\nimport\n \norg.apache.hadoop.fs.FileSystem\n;\n\n\nimport\n \norg.apache.hadoop.fs.Path\n;\n\n\nimport\n \norg.apache.hadoop.io.IOUtils\n;\n\n\n\nimport\n \njava.io.InputStream\n;\n\n\nimport\n \njava.net.URI\n;\n\n\n\npublic\n \nclass\n \nFileSystemCat\n \n{\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \nthrows\n \nException\n \n{\n\n        \nString\n \nuri\n \n=\n \nargs\n[\n0\n];\n\n        \nConfiguration\n \nconf\n \n=\n \nnew\n \nConfiguration\n();\n\n        \nFileSystem\n \nfs\n \n=\n \nFileSystem\n.\nget\n(\nURI\n.\ncreate\n(\nuri\n),\n \nconf\n);\n\n        \nInputStream\n \nin\n \n=\n \nnull\n;\n\n        \ntry\n \n{\n\n            \nin\n \n=\n \nfs\n.\nopen\n(\nnew\n \nPath\n(\nuri\n));\n\n            \nIOUtils\n.\ncopyBytes\n(\nin\n,\n \nSystem\n.\nout\n,\n \n4096\n,\n \nfalse\n);\n\n        \n}\n \nfinally\n \n{\n\n            \nIOUtils\n.\ncloseStream\n(\nin\n);\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\nFSDataInputStream\n\n\nThe \nopen()\n method on \nFileSystem\n actually returns an \nFSDataInputStream\n rather than a standard java.io class. This class is a specialization of \njava.io.DataInputStream\n with support for random access, so you can read from any part of the stream:\n\n\n\n\nThe \nSeekable\n interface permits seeking to a position in the file and provides a query method for the \ncurrent\n offset from the start of the file (\ngetPos()\n):\n\n\n \npublic\n \ninterface\n \nSeekable\n \n{\n \n    \nvoid\n \nseek\n(\nlong\n \npos\n)\n \nthrows\n \nIOException\n;\n \n    \nlong\n \ngetPos\n()\n \nthrows\n \nIOException\n;\n \n}\n\n\n\n\nDisplaying files from a Hadoop filesystem on standard output twice, by using \nseek()\n:\n\n\n \n// hdfs://localhost:9000/test2.copy\n\n\n\nimport\n \norg.apache.hadoop.conf.Configuration\n;\n\n\nimport\n \norg.apache.hadoop.fs.FSDataInputStream\n;\n\n\nimport\n \norg.apache.hadoop.fs.FileSystem\n;\n\n\nimport\n \norg.apache.hadoop.fs.Path\n;\n\n\nimport\n \norg.apache.hadoop.io.IOUtils\n;\n\n\nimport\n \njava.net.URI\n;\n\n\n\npublic\n \nclass\n \nFileSystemDoubleCat\n \n{\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \nthrows\n \nException\n \n{\n\n        \nString\n \nuri\n \n=\n \nargs\n[\n0\n];\n\n        \nConfiguration\n \nconf\n \n=\n \nnew\n \nConfiguration\n();\n\n        \nFileSystem\n \nfs\n \n=\n \nFileSystem\n.\nget\n(\nURI\n.\ncreate\n(\nuri\n),\n \nconf\n);\n\n        \nFSDataInputStream\n \nin\n \n=\n \nnull\n;\n\n        \ntry\n \n{\n\n            \nin\n \n=\n \nfs\n.\nopen\n(\nnew\n \nPath\n(\nuri\n));\n\n            \nIOUtils\n.\ncopyBytes\n(\nin\n,\n \nSystem\n.\nout\n,\n \n4096\n,\n \nfalse\n);\n\n            \nin\n.\nseek\n(\n0\n);\n \n// go back to the start of the file\n\n            \nIOUtils\n.\ncopyBytes\n(\nin\n,\n \nSystem\n.\nout\n,\n \n4096\n,\n \nfalse\n);\n\n        \n}\n \ncatch\n \n(\nException\n \nex\n)\n \n{\n\n            \nex\n.\nprintStackTrace\n();\n\n        \n}\n \nfinally\n \n{\n\n            \nIOUtils\n.\ncloseStream\n(\nin\n);\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\nWriting Data\n\n\nThe \nFileSystem class\n has a number of methods for creating a file. \n\n\n \n// takes a Path object for the file to be created and returns an output stream to write to\n\n\npublic\n \nFSDataOutputStream\n \ncreate\n(\nPath\n \nf\n)\n \nthrows\n \nIOException\n\n\n// appends to an existing file\n\n\npublic\n \nFSDataOutputStream\n \nappend\n(\nPath\n \nf\n)\n \nthrows\n \nIOException\n\n\n\n\n\n\nWarning\n\n\nThe \ncreate()\n methods create any parent directories of the file to be written that don\u2019t already exist.\n\n\n\n\nThere\u2019s an overloaded method of \n for passing a callback interface, \nProgressable\n/C, so your application can be notified of the progress of the data being written to the datanodes:\n\n\n \npublic interface Progressable { \n    public void progress(); \n}\n\n\n\nHere, we illustrate progress by printing a period every time the \nprogress()\n method is called by Hadoop, which is after each 64 KB packet of data is written to the datanode pipeline.\n\n\n \n// args: /Users/larry/test.copy hdfs://localhost:9000/test4.copy\n\n\nimport\n \norg.apache.hadoop.conf.Configuration\n;\n\n\nimport\n \norg.apache.hadoop.fs.FileSystem\n;\n\n\nimport\n \norg.apache.hadoop.fs.Path\n;\n\n\nimport\n \norg.apache.hadoop.io.IOUtils\n;\n\n\nimport\n \norg.apache.hadoop.util.Progressable\n;\n\n\n\nimport\n \njava.io.*\n;\n\n\nimport\n \njava.net.URI\n;\n\n\n\n// Copying a local file to a Hadoop filesystem\n\n\npublic\n \nclass\n \nFileCopyWithProgress\n \n{\n\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \nthrows\n \nException\n \n{\n\n        \nString\n \nlocalsrc\n \n=\n \nargs\n[\n0\n];\n\n        \nString\n \ndstsrc\n \n=\n \nargs\n[\n1\n];\n\n        \nBufferedInputStream\n \nin\n \n=\n \nnew\n \nBufferedInputStream\n(\nnew\n \nFileInputStream\n(\nlocalsrc\n));\n\n\n        \nConfiguration\n \nconf\n \n=\n \nnew\n \nConfiguration\n();\n\n        \nFileSystem\n \nfs\n \n=\n \nFileSystem\n.\nget\n(\nURI\n.\ncreate\n(\ndstsrc\n),\n \nconf\n);\n\n        \ntry\n \n{\n\n            \nOutputStream\n \nout\n \n=\n \nfs\n.\ncreate\n(\nnew\n \nPath\n(\ndstsrc\n),\n \nnew\n \nProgressable\n()\n \n{\n\n                \n@Override\n\n                \npublic\n \nvoid\n \nprogress\n()\n \n{\n\n                    \nSystem\n.\nout\n.\nprintln\n(\n.\n);\n\n                \n}\n\n            \n});\n\n            \nIOUtils\n.\ncopyBytes\n(\nin\n,\n \nout\n,\n \n4096\n,\n \ntrue\n);\n\n\n        \n}\n \nfinally\n \n{\n\n            \nIOUtils\n.\ncloseStream\n(\nin\n);\n\n        \n}\n \n//end try\n\n    \n}\n// end main\n\n\n}\n\n\n\n\nFSDataOutputStream\n\n\nThe \ncreate()\n method on \nFileSystem\n returns an \nFSDataOutputStream\n, which, like \nFSDataInputStream\n, has a method for querying the current position in the file:\n\n\n \npublic\n \nclass\n \nFSDataOutputStream\n \nextends\n \nDataOutputStream\n \nimplements\n \nSyncable\n \n{\n\n    \npublic\n \nlong\n \ngetPos\n()\n \nthrows\n \nIOException\n \n{\n \n        \n// implementation elided \n\n    \n}\n// implementation elided\n\n\n}\n\n\n\n\nHowever, because HDFS allows only sequential writes to an open file or appends to an already written file, \nFSDataOutputStream\n does not permit seeking.\n\n\nDirectories\n\n\nFileSystem\n provides a method to create a directory:\n\n\n \npublic\n \nboolean\n \nmkdirs\n(\nPath\n \nf\n)\n \nthrows\n \nIOException\n\n\n\n\nThis method creates all of the necessary parent directories if they don\u2019t already exist.\n\n\nQuerying the Filesystem\n\n\nFile metadata: FileStatus\n\n\nListing files\n\n\nFile patterns\n\n\nDeleting Data\n\n\nUse the \ndelete()\n method on \nFileSystem\n to permanently remove files or directories:\n\n\n \npublic\n \nboolean\n \ndelete\n(\nPath\n \nf\n,\n \nboolean\n \nrecursive\n)\n \nthrows\n \nIOException\n\n\n\n\nIf \nf\n is a file or an empty directory, the value of \nrecursive\n is ignored.\n\n\n6 Data Flow\n\n\nAnatomy of a File Read\n\n\nThe figure below shows the main sequence of events when reading a file.\n\n\n\n\n\n\nstep 1: The client opens the file it wishes to read by calling \nopen()\n on the \nFileSystem\n object, which for HDFS is an instance of \nDistributedFileSystem\n. \n\n\nstep 2: \nDistributedFileSystem\n calls the namenode, using remote procedure calls (RPCs), to determine the locations of the first few blocks in the file. \n\n\nstep 3: For each block, the namenode returns the addresses of the datanodes that have a copy of that block. Furthermore, the datanodes are sorted according to their proximity to the client. \n\n\nIf the client is itself a datanode, the client will read from the local datanode if that datanode hosts a copy of the block.\n\n\nThe \nDistributedFileSystem\n returns an \nFSDataInputStream\n to the client for it to read data from. \nFSDataInputStream\n in turn wraps a \nDFSInputStream\n, which manages the datanode and namenode I/O.\n\n\nThe client then calls \nread()\n on the stream. \n\n\n\n\n\n\nstep 4: \nDFSInputStream\n, which has stored the datanode addresses for the first few blocks in the file, then connects to the first (closest) datanode for the first block in the file. Data is streamed from the datanode back to the client, which calls \nread()\n repeatedly on the stream. \n\n\nstep 5: When the end of the block is reached, \nDFSInputStream\n will close the connection to the datanode, then find the best datanode for the next block. \n\n\nstep 6: This happens transparently to the client, which from its point of view is just reading a continuous stream.\n\n\nBlocks are read in order, with the \nDFSInputStream\n opening new connections to datanodes as the client reads through the stream. \n\n\nIt will also call the namenode to retrieve the datanode locations for the next batch of blocks as needed. When the client has finished reading, it calls \nclose()\n on the \nFSDataInputStream\n.\n\n\n\n\n\n\n\n\nAnatomy of a File Write\n\n\nThe figure below illustrates the case of creating a new file, writing data to it, then closing the file.\n\n\n\n\n\n\nstep 1: The client creates the file by calling \ncreate()\n on \nDistributedFileSystem\n. \n\n\nstep 2: \nDistributedFileSystem\n makes an RPC call to the namenode to create a new file in the filesystem\u2019s namespace, with no blocks associated with it. \n\n\nThe namenode performs various checks to make sure the file doesn\u2019t already exist and that the client has the right permissions to create the file. \n\n\nIf these checks pass, the namenode makes a record of the new file; otherwise, file creation fails and the client is thrown an \nIOException\n. \n\n\nThe \nDistributedFileSystem\n returns an \nFSDataOutputStream\n for the client to start writing data to. Just as in the read case, \nFSDataOutputStream\n wraps a \nDFSOutputStream\n, which handles communication with the datanodes and namenode.\n\n\n\n\n\n\nstep 3: As the client writes data, the \nDFSOutputStream\n splits it into packets, which it writes to an internal queue called the \ndata queue\n . The data queue is consumed by the \nDataStreamer\n, which is responsible for asking the namenode to allocate new blocks by picking a list of suitable datanodes to store the replicas. \n\n\nstep 4: The list of datanodes forms a pipeline, and here we\u2019ll assume the replication level is three, so there are three nodes in the pipeline. The \nDataStreamer\n streams the packets to the first datanode in the pipeline, which stores each packet and forwards it to the second datanode in the pipeline. Similarly, the second datanode stores the packet and forwards it to the third (and last) datanode in the pipeline .\n\n\nstep 5: The \nDFSOutputStream\n also maintains an internal queue of packets that are waiting to be acknowledged by datanodes, called the \nack queue\n. A packet is removed from the ack queue only when it has been acknowledged by all the datanodes in the pipeline.\n\n\nstep 6: When the client has finished writing data, it calls \nclose()\n on the stream. This action flushes all the remaining packets to the datanode pipeline.\n\n\nstep 7:  It waits for acknowledgments before contacting the namenode to signal that the file is complete. \n\n\n\n\nCoherency Model\n\n\nA coherency model for a filesystem describes the data visibility of reads and writes for a file.\n\n\n\n\nAny content written to the file is not guaranteed to be visible, even if the stream is flushed.\n\n\nOnce more than a block\u2019s worth of data has been written, the first block will be visible to new readers.\n\n\nThe \nFSDataOutputStream.hflush()\n method force all buffers to be flushed to the datanodes.\n\n\nThe \nhflush()\n guarantees that the data written up to that point in the file has reached all the datanodes in the write pipeline and is visible to all new readers.\n\n\nBut it does not guarantee that the datanodes have written the data to disk, only that it\u2019s in the datanodes\u2019 memory.\n\n\nClosing a file in HDFS performs an implicit \nhflush()\n.\n\n\n\n\n\n\nThe \nhsync()\n method syncs to disk for a file descriptor.\n\n\n\n\n \nFileOutputStream\n \nout\n \n=\n \nnew\n \nFileOutputStream\n(\nlocalFile\n);\n \nout\n.\nwrite\n(\ncontent\n.\ngetBytes\n(\nUTF-8\n));\n \n\nout\n.\nflush\n();\n \n// flush to operating system \n\n\nout\n.\ngetFD\n().\nsync\n();\n \n// sync to disk \n\n\nassertThat\n(\nlocalFile\n.\nlength\n(),\n \nis\n(((\nlong\n)\n \ncontent\n.\nlength\n())));\n\n\n\n\nConsequences for application design\n\n\nYou should call \nhflush()\n at suitable points, such as after writing a certain number of records or number of bytes.\n\n\n7 Parallel Copying with distcp\n\n\nThe program \ndistcp\n copys data to and from Hadoop filesystems in parallel.\n\n\n \n$ hadoop distcp file1 file2\n\n\n\ndistcp\n is implemented as a MapReduce job where the work of copying is done by the maps that run in parallel across the cluster, with no reducers.", 
            "title": "Chapter 3: The Hadoop Distributed FileSystem"
        }, 
        {
            "location": "/hadoop/ch3/#hadoop-the-definitive-guide-3-the-hadoop-distributed-filesystem", 
            "text": "Filesystems that manage the storage across a network of machines are called  distributed filesystems  . Hadoop comes with a distributed filesystem called HDFS, which stands for  Hadoop Distributed Filesystem  .", 
            "title": "Hadoop: The Definitive Guide 3 - The Hadoop Distributed FileSystem"
        }, 
        {
            "location": "/hadoop/ch3/#1-the-design-of-hdfs", 
            "text": "HDFS is a filesystem designed for storing very large files with streaming data access patterns, running on clusters of commodity hardware.   Very large files: files that are hundreds of megabytes, gigabytes, or terabytes in size.  Streaming data access: HDFS is built around the idea that the most efficient data processing pattern is a  write-once, read-many-times  pattern.  Commodity hardware: It\u2019s designed to run on clusters of commodity hardware.   These are areas where HDFS is not a good fit today:   Low-latency data access  Lots of small files  Multiple writers, arbitrary file modifications", 
            "title": "1 The Design of HDFS"
        }, 
        {
            "location": "/hadoop/ch3/#2-hdfs-concepts", 
            "text": "", 
            "title": "2 HDFS Concepts"
        }, 
        {
            "location": "/hadoop/ch3/#blocks", 
            "text": "A disk has a block size, which is the minimum amount of data that it can read or write. Filesystems for a single disk build on this by dealing with data in blocks, which are an integral multiple of the disk block size.  HDFS, too, has the concept of a  block , but it is a much larger unit \u2014 128 MB by default (typically a few kilobytes for ordinary file system). Unlike a filesystem for a single disk, a file in HDFS that is smaller than a single block does not occupy a full block\u2019s worth of underlying storage. (For example, a 1 MB file stored with a block size of 128 MB uses 1 MB of disk space, not 128 MB.)   Question  WHY IS A BLOCK IN HDFS SO LARGE? To minimize the cost of seeks.   Having a block abstraction for a distributed filesystem brings several benefits.   A file can be larger than any single disk in the network.  Making the unit of abstraction a block rather than a file simplifies the storage subsystem.  storage management: because blocks are a fixed size, it is easy to calculate how many can be stored on a given disk.  metadata concerns: because blocks are just chunks of data to be stored, file metadata such as permissions information does not need to be stored with the blocks.    Blocks fit well with replication for providing fault tolerance and availability.  To insure against corrupted blocks and disk and machine failure, each block is replicated to a small number of physically separate machines (typically three).", 
            "title": "Blocks"
        }, 
        {
            "location": "/hadoop/ch3/#namenodes-and-datanodes", 
            "text": "An HDFS cluster has two types of nodes: a  namenode  (the master) and a number of  datanodes  (workers).    The namenode manages the filesystem namespace. It maintains the filesystem tree and the metadata for all the files and directories in the tree. This information is stored persistently on the local disk in the form of two files: the namespace image and the edit log.  The namenode also knows the datanodes on which all the blocks for a given file are located;  Datanodes are the workhorses of the filesystem. They store and retrieve blocks when they are told to (by clients or the namenode), and they report back to the namenode periodically with lists of blocks that they are storing.   If the machine running the namenode were obliterated, all the files on the filesystem would be lost since there would be no way of knowing how to reconstruct the files from the blocks on the datanodes. Possible solution:   to back up the files that make up the persistent state of the filesystem metadata.  to run a secondary namenode, which keeps a copy of the merged namespace image.", 
            "title": "Namenodes and Datanodes"
        }, 
        {
            "location": "/hadoop/ch3/#block-caching", 
            "text": "For frequently accessed files, the blocks may be  explicitly  cached in the datanode\u2019s memory, in an off-heap block cache. Users or applications instruct the namenode which files to cache (and for how long) by adding a  cache directive  to a  cache pool .", 
            "title": "Block Caching"
        }, 
        {
            "location": "/hadoop/ch3/#hdfs-federation", 
            "text": "Problem: On very large clusters with many files, memory becomes the limiting factor for scaling, since namenode keeps a reference to every file and block in the filesystem in memory.  For example, a 200-node cluster with 24 TB of disk space per node, a block size of 128 MB, and a replication factor of 3 has room for about 2 million blocks (or more):  200\\times 24TB\u2044(128MB\u00d73) 200\\times 24TB\u2044(128MB\u00d73) , So in this case, setting the namenode memory to 12,000 MB would be a good starting point.  Solution: HDFS federation, allows a cluster to scale by adding namenodes, each of which manages a portion of the filesystem namespace.", 
            "title": "HDFS Federation"
        }, 
        {
            "location": "/hadoop/ch3/#hdfs-high-availability", 
            "text": "To remedy a failed namenode, a pair of namenodes in an  active-standby  configuration is introduced in Hadoop 2. In the event of the failure of the active namenode, the standby takes over its duties to continue servicing client requests  without  a significant interruption.", 
            "title": "HDFS High Availability"
        }, 
        {
            "location": "/hadoop/ch3/#3-the-command-line-interface", 
            "text": "", 
            "title": "3 The Command-Line Interface"
        }, 
        {
            "location": "/hadoop/ch3/#basic-filesystem-operations", 
            "text": "Hadoop\u2019s filesystem shell command is  fs , which supports a number of subcommands (type  hadoop fs -help  to get detailed help).  Copying a file from the local filesystem to HDFS:    #The local file is copied tothe HDFS instance running on localhost. \n$ hadoop fs -copyFromLocal test.copy /test.copy # works as the same \n$ hadoop fs -copyFromLocal test.copy hdfs://localhost:9000/test2.copy  Copying the file from the HDFS to the local filesystem:    $ hadoop fs -copyToLocal /test.copy test.copy.txt", 
            "title": "Basic Filesystem Operations"
        }, 
        {
            "location": "/hadoop/ch3/#4-hadoop-filesystems", 
            "text": "Hadoop has an abstract notion of filesystems, of which HDFS is just one implementation. The Java abstract class  org.apache.hadoop.fs.FileSystem  represents the client interface to a filesystem in Hadoop, and there are several concrete implementations.     Filesystem  URI scheme  Java implementation  Description      Local  file  fs.LocalFileSystem  A filesystem for a locally connected disk with client-side checksums    HDFS  hfs  hdfs.DistributedFileSystem  Hadoop\u2019s distributed filesystem    WebHDFS  webhdfs  hdfs.web.WebHdfsFileSystem  Providing authenticated read/write access to HDFS over HTTP.    Secure WebHDFS  swebhdfs  hdfs.web.SWebHdfsFileSystem  The HTTPS version of WebHDFS.     When you are processing large volumes of data you should choose a distributed filesystem that has the data locality optimization, notably HDFS.", 
            "title": "4 Hadoop Filesystems"
        }, 
        {
            "location": "/hadoop/ch3/#http", 
            "text": "The HTTP REST API exposed by the WebHDFS protocol makes it easier for other languages to interact with HDFS. Note that the HTTP interface is slower than the native Java client, so should be avoided for very large data transfers if possible.  There are two ways of accessing HDFS over HTTP:   Directly, where the HDFS daemons serve HTTP requests to clients;  Via a proxy (or proxies), which accesses HDFS on the client\u2019s behalf using the usual DistributedFileSystem API.    HDFS proxy allows for stricter firewall and bandwidth-limiting policies to be put in place. It\u2019s common to use a proxy for transfers between Hadoop clusters located in different data centers, or when accessing a Hadoop cluster running in the cloud from an external network.", 
            "title": "HTTP"
        }, 
        {
            "location": "/hadoop/ch3/#5-the-java-interface", 
            "text": "Hadoop  FileSystem  class is the API for interacting with one of Hadoop\u2019s filesystems. In general you should strive to  write your code against the  FileSystem  abstract class  , to retain portability across filesystems. This is very useful when testing your program, for example, because you can rapidly run tests using data stored on the local filesystem.", 
            "title": "5 The Java Interface"
        }, 
        {
            "location": "/hadoop/ch3/#reading-data-from-a-hadoop-url", 
            "text": "NOT recommended, because  setURLStreamHandlerFactory()  method can be called only once per JVM, which means that if some other part of your program sets it, you won't be able to use.", 
            "title": "Reading Data from a Hadoop URL"
        }, 
        {
            "location": "/hadoop/ch3/#reading-data-using-the-filesystem-api", 
            "text": "A file in a Hadoop filesystem is represented by a Hadoop  Path  object( org.apache.hadoop.fs.Path , not  java.io.File ). You can think of a  Path   as a Hadoop filesystem URI, such as  hdfs://localhost/user/tom/test.copy  Since  FileSystem  is a general filesystem API, so the first step is to retrieve an instance for the filesystem we want. There are several static factory methods for getting a  FileSystem  instance:    // Returns the default filesystem  public   static   FileSystem   get ( Configuration   conf )   throws   IOException   // Uses the given URI\u2019s scheme and authority to determine the filesystem to use  public   static   FileSystem   get ( URI   uri ,   Configuration   conf )   throws   IOException   // Retrieves the filesystem as the given user  public   static   FileSystem   get ( URI   uri ,   Configuration   conf ,   String   user )   throws   IOException  // Retrieves a local filesystem instance  public   static   LocalFileSystem   getLocal ( Configuration   conf )   throws   IOException   A  Configuration  object encapsulates a client or server's configuration, which is set using configuration files read from the classpath, such as  etc/hadoop/core-site.xml .  With a  FileSystem  instance in hand, we invoke an  open()  method to get the input stream for a file:    // Uses a default buffer size of 4 KB  public   FSDataInputStream   open ( Path   f )   throws   IOException  // Uses a buffer size of bufferSize  public   abstract   FSDataInputStream   open ( Path   f ,   int   bufferSize )   throws   IOException   Displaying files from a Hadoop filesystem on standard output by using the FileSystem directly:    // $ hdfs://localhost:9000/test2.copy  import   org.apache.hadoop.conf.Configuration ;  import   org.apache.hadoop.fs.FileSystem ;  import   org.apache.hadoop.fs.Path ;  import   org.apache.hadoop.io.IOUtils ;  import   java.io.InputStream ;  import   java.net.URI ;  public   class   FileSystemCat   { \n     public   static   void   main ( String []   args )   throws   Exception   { \n         String   uri   =   args [ 0 ]; \n         Configuration   conf   =   new   Configuration (); \n         FileSystem   fs   =   FileSystem . get ( URI . create ( uri ),   conf ); \n         InputStream   in   =   null ; \n         try   { \n             in   =   fs . open ( new   Path ( uri )); \n             IOUtils . copyBytes ( in ,   System . out ,   4096 ,   false ); \n         }   finally   { \n             IOUtils . closeStream ( in ); \n         } \n     }  }", 
            "title": "Reading Data Using the FileSystem API"
        }, 
        {
            "location": "/hadoop/ch3/#fsdatainputstream", 
            "text": "The  open()  method on  FileSystem  actually returns an  FSDataInputStream  rather than a standard java.io class. This class is a specialization of  java.io.DataInputStream  with support for random access, so you can read from any part of the stream:   The  Seekable  interface permits seeking to a position in the file and provides a query method for the  current  offset from the start of the file ( getPos() ):    public   interface   Seekable   {  \n     void   seek ( long   pos )   throws   IOException ;  \n     long   getPos ()   throws   IOException ;   }   Displaying files from a Hadoop filesystem on standard output twice, by using  seek() :    // hdfs://localhost:9000/test2.copy  import   org.apache.hadoop.conf.Configuration ;  import   org.apache.hadoop.fs.FSDataInputStream ;  import   org.apache.hadoop.fs.FileSystem ;  import   org.apache.hadoop.fs.Path ;  import   org.apache.hadoop.io.IOUtils ;  import   java.net.URI ;  public   class   FileSystemDoubleCat   { \n     public   static   void   main ( String []   args )   throws   Exception   { \n         String   uri   =   args [ 0 ]; \n         Configuration   conf   =   new   Configuration (); \n         FileSystem   fs   =   FileSystem . get ( URI . create ( uri ),   conf ); \n         FSDataInputStream   in   =   null ; \n         try   { \n             in   =   fs . open ( new   Path ( uri )); \n             IOUtils . copyBytes ( in ,   System . out ,   4096 ,   false ); \n             in . seek ( 0 );   // go back to the start of the file \n             IOUtils . copyBytes ( in ,   System . out ,   4096 ,   false ); \n         }   catch   ( Exception   ex )   { \n             ex . printStackTrace (); \n         }   finally   { \n             IOUtils . closeStream ( in ); \n         } \n     }  }", 
            "title": "FSDataInputStream"
        }, 
        {
            "location": "/hadoop/ch3/#writing-data", 
            "text": "The  FileSystem class  has a number of methods for creating a file.     // takes a Path object for the file to be created and returns an output stream to write to  public   FSDataOutputStream   create ( Path   f )   throws   IOException  // appends to an existing file  public   FSDataOutputStream   append ( Path   f )   throws   IOException    Warning  The  create()  methods create any parent directories of the file to be written that don\u2019t already exist.   There\u2019s an overloaded method of   for passing a callback interface,  Progressable /C, so your application can be notified of the progress of the data being written to the datanodes:    public interface Progressable { \n    public void progress(); \n}  Here, we illustrate progress by printing a period every time the  progress()  method is called by Hadoop, which is after each 64 KB packet of data is written to the datanode pipeline.    // args: /Users/larry/test.copy hdfs://localhost:9000/test4.copy  import   org.apache.hadoop.conf.Configuration ;  import   org.apache.hadoop.fs.FileSystem ;  import   org.apache.hadoop.fs.Path ;  import   org.apache.hadoop.io.IOUtils ;  import   org.apache.hadoop.util.Progressable ;  import   java.io.* ;  import   java.net.URI ;  // Copying a local file to a Hadoop filesystem  public   class   FileCopyWithProgress   { \n\n     public   static   void   main ( String []   args )   throws   Exception   { \n         String   localsrc   =   args [ 0 ]; \n         String   dstsrc   =   args [ 1 ]; \n         BufferedInputStream   in   =   new   BufferedInputStream ( new   FileInputStream ( localsrc )); \n\n         Configuration   conf   =   new   Configuration (); \n         FileSystem   fs   =   FileSystem . get ( URI . create ( dstsrc ),   conf ); \n         try   { \n             OutputStream   out   =   fs . create ( new   Path ( dstsrc ),   new   Progressable ()   { \n                 @Override \n                 public   void   progress ()   { \n                     System . out . println ( . ); \n                 } \n             }); \n             IOUtils . copyBytes ( in ,   out ,   4096 ,   true ); \n\n         }   finally   { \n             IOUtils . closeStream ( in ); \n         }   //end try \n     } // end main  }", 
            "title": "Writing Data"
        }, 
        {
            "location": "/hadoop/ch3/#fsdataoutputstream", 
            "text": "The  create()  method on  FileSystem  returns an  FSDataOutputStream , which, like  FSDataInputStream , has a method for querying the current position in the file:    public   class   FSDataOutputStream   extends   DataOutputStream   implements   Syncable   { \n     public   long   getPos ()   throws   IOException   {  \n         // implementation elided  \n     } // implementation elided  }   However, because HDFS allows only sequential writes to an open file or appends to an already written file,  FSDataOutputStream  does not permit seeking.", 
            "title": "FSDataOutputStream"
        }, 
        {
            "location": "/hadoop/ch3/#directories", 
            "text": "FileSystem  provides a method to create a directory:    public   boolean   mkdirs ( Path   f )   throws   IOException   This method creates all of the necessary parent directories if they don\u2019t already exist.", 
            "title": "Directories"
        }, 
        {
            "location": "/hadoop/ch3/#querying-the-filesystem", 
            "text": "File metadata: FileStatus  Listing files  File patterns", 
            "title": "Querying the Filesystem"
        }, 
        {
            "location": "/hadoop/ch3/#deleting-data", 
            "text": "Use the  delete()  method on  FileSystem  to permanently remove files or directories:    public   boolean   delete ( Path   f ,   boolean   recursive )   throws   IOException   If  f  is a file or an empty directory, the value of  recursive  is ignored.", 
            "title": "Deleting Data"
        }, 
        {
            "location": "/hadoop/ch3/#6-data-flow", 
            "text": "", 
            "title": "6 Data Flow"
        }, 
        {
            "location": "/hadoop/ch3/#anatomy-of-a-file-read", 
            "text": "The figure below shows the main sequence of events when reading a file.    step 1: The client opens the file it wishes to read by calling  open()  on the  FileSystem  object, which for HDFS is an instance of  DistributedFileSystem .   step 2:  DistributedFileSystem  calls the namenode, using remote procedure calls (RPCs), to determine the locations of the first few blocks in the file.   step 3: For each block, the namenode returns the addresses of the datanodes that have a copy of that block. Furthermore, the datanodes are sorted according to their proximity to the client.   If the client is itself a datanode, the client will read from the local datanode if that datanode hosts a copy of the block.  The  DistributedFileSystem  returns an  FSDataInputStream  to the client for it to read data from.  FSDataInputStream  in turn wraps a  DFSInputStream , which manages the datanode and namenode I/O.  The client then calls  read()  on the stream.     step 4:  DFSInputStream , which has stored the datanode addresses for the first few blocks in the file, then connects to the first (closest) datanode for the first block in the file. Data is streamed from the datanode back to the client, which calls  read()  repeatedly on the stream.   step 5: When the end of the block is reached,  DFSInputStream  will close the connection to the datanode, then find the best datanode for the next block.   step 6: This happens transparently to the client, which from its point of view is just reading a continuous stream.  Blocks are read in order, with the  DFSInputStream  opening new connections to datanodes as the client reads through the stream.   It will also call the namenode to retrieve the datanode locations for the next batch of blocks as needed. When the client has finished reading, it calls  close()  on the  FSDataInputStream .", 
            "title": "Anatomy of a File Read"
        }, 
        {
            "location": "/hadoop/ch3/#anatomy-of-a-file-write", 
            "text": "The figure below illustrates the case of creating a new file, writing data to it, then closing the file.    step 1: The client creates the file by calling  create()  on  DistributedFileSystem .   step 2:  DistributedFileSystem  makes an RPC call to the namenode to create a new file in the filesystem\u2019s namespace, with no blocks associated with it.   The namenode performs various checks to make sure the file doesn\u2019t already exist and that the client has the right permissions to create the file.   If these checks pass, the namenode makes a record of the new file; otherwise, file creation fails and the client is thrown an  IOException .   The  DistributedFileSystem  returns an  FSDataOutputStream  for the client to start writing data to. Just as in the read case,  FSDataOutputStream  wraps a  DFSOutputStream , which handles communication with the datanodes and namenode.    step 3: As the client writes data, the  DFSOutputStream  splits it into packets, which it writes to an internal queue called the  data queue  . The data queue is consumed by the  DataStreamer , which is responsible for asking the namenode to allocate new blocks by picking a list of suitable datanodes to store the replicas.   step 4: The list of datanodes forms a pipeline, and here we\u2019ll assume the replication level is three, so there are three nodes in the pipeline. The  DataStreamer  streams the packets to the first datanode in the pipeline, which stores each packet and forwards it to the second datanode in the pipeline. Similarly, the second datanode stores the packet and forwards it to the third (and last) datanode in the pipeline .  step 5: The  DFSOutputStream  also maintains an internal queue of packets that are waiting to be acknowledged by datanodes, called the  ack queue . A packet is removed from the ack queue only when it has been acknowledged by all the datanodes in the pipeline.  step 6: When the client has finished writing data, it calls  close()  on the stream. This action flushes all the remaining packets to the datanode pipeline.  step 7:  It waits for acknowledgments before contacting the namenode to signal that the file is complete.", 
            "title": "Anatomy of a File Write"
        }, 
        {
            "location": "/hadoop/ch3/#coherency-model", 
            "text": "A coherency model for a filesystem describes the data visibility of reads and writes for a file.   Any content written to the file is not guaranteed to be visible, even if the stream is flushed.  Once more than a block\u2019s worth of data has been written, the first block will be visible to new readers.  The  FSDataOutputStream.hflush()  method force all buffers to be flushed to the datanodes.  The  hflush()  guarantees that the data written up to that point in the file has reached all the datanodes in the write pipeline and is visible to all new readers.  But it does not guarantee that the datanodes have written the data to disk, only that it\u2019s in the datanodes\u2019 memory.  Closing a file in HDFS performs an implicit  hflush() .    The  hsync()  method syncs to disk for a file descriptor.     FileOutputStream   out   =   new   FileOutputStream ( localFile );   out . write ( content . getBytes ( UTF-8 ));   out . flush ();   // flush to operating system   out . getFD (). sync ();   // sync to disk   assertThat ( localFile . length (),   is ((( long )   content . length ())));   Consequences for application design  You should call  hflush()  at suitable points, such as after writing a certain number of records or number of bytes.", 
            "title": "Coherency Model"
        }, 
        {
            "location": "/hadoop/ch3/#7-parallel-copying-with-distcp", 
            "text": "The program  distcp  copys data to and from Hadoop filesystems in parallel.    $ hadoop distcp file1 file2  distcp  is implemented as a MapReduce job where the work of copying is done by the maps that run in parallel across the cluster, with no reducers.", 
            "title": "7 Parallel Copying with distcp"
        }, 
        {
            "location": "/hadoop/ch4/", 
            "text": "Hadoop: The Definitive Guide 4 - YARN\n\n\nApache YARN(Yet Another Resource Negotiator) is Hadoop\ns cluster resource management system. YARN provides APIs for requesting and working with cluster resources, but these APIs are not typically used directly by user code. Distributed computing frameworks (MapReduce, Spark, and so on) running as YARN applications on the cluster compute layer (YARN) and the cluster storage layer (HDFS and HBase).\n\n\n\n\n1 Anatomy of a YARN Application Run\n\n\nYARN provides its core services via two types of long-running daemon:\n\n\n\n\na \nresource manager\n (one per cluster) to manage the use of resources across the cluster,\n\n\nnode managers\n running on all the nodes in the cluster to launch and monitor \ncontainers\n.\n\n\n\n\n\n\n\n\nstep1 : To run an application on YARN, a client contacts the resource manager and asks it to run an \napplication master\n process.\n\n\nsteps 2a and 2b: The resource manager then finds a node manager that can launch the application master in a container. It could simply run a computation in the container it is running in and return the result to the client.\n\n\nstep 3: Or it could request more containers from the resource managers\n\n\nsteps 4a and 4b: use them to run a distributed computation.\n\n\n\n\nResource Requests\n\n\nA YARN application can make resource requests at any time while it is running. \n\n\n\n\nSpark starts a fixed number of executors on the cluster (i.e. make all of requests up front). \n\n\nMapReduce, has two phases: the map task containers are requested up front, but the reduce task containers are not started until later. (i.e. take a more dynamic approach whereby it requests more resources dynamically to meet the changing needs of the application).\n\n\n\n\nApplication Lifespan\n\n\nThe lifespan of a YARN application can vary dramatically. Rather than look at how long the application runs for, it\ns useful to categorize applications in terms of how they map to the jobs that users run. \n\n\n\n\nThe simplest case is one application per user job, which is the approach that MapReduce takes.\n\n\nThe second model is to run one application per workflow or user session of (possibly unrelated) jobs, which is the approach that Spark takes. This approach can be more efficient than the first, since containers can be reused between jobs, and there is also the potential to cache intermediate data between jobs.\n\n\nThe third model is a long-running application that is shared by different users, which is the approach that Apache Slider takes.\n\n\n\n\nBuilding YARN Applications\n\n\nWriting a YARN application from scratch is fairly involved, but in many cases is not necessary, as it is often possible to use an existing application that fits the bill.\n\n\n2 YARN Compared to MapReduce 1\n\n\nThe distributed implementation of MapReduce in the original version of Hadoop is sometimes referred to as \nMapReduce 1\n to distinguish it from MapReduce 2, the implementation that uses YARN.\n\n\nA comparison of MapReduce 1 and YARN components:\n\n\n\n\n\n\n\n\nMapReduce1\n\n\nYARN\n\n\n\n\n\n\n\n\n\n\nJobtracker\n\n\nResource manager, application master, timeline server\n\n\n\n\n\n\nTaskTracker\n\n\nNode manager\n\n\n\n\n\n\nSlot\n\n\nContainer\n\n\n\n\n\n\n\n\nThe Timeline Server addresses the problem of the storage and retrieval of application\ns current and historic information in a generic fashion.\n\n\n3 Scheduling in YARN\n\n\nThe job of the YARN scheduler to allocate resources to applications according to some defined policy. Scheduling in general is a difficult problem and there is \nno one \"best\" policy\n, which is why YARN provides a choice of schedulers and configurable policies.\n\n\nScheduler Options\n\n\nThree schedulers are available in YARN: the FIFO, Capacity, and Fair Schedulers.\n\n\n\n\nThe FIFO: places applications in a queue and runs them in the order of submission (first in, first out)\n\n\nNot suitable for shared clusters, because large applications will use all the resources in a cluster, so each application has to wait its turn. \n\n\n\n\n\n\nCapacity Scheduler: a separate dedicated queue allows the small job to start as soon as it is submitted, since the queue capacity is reserved for jobs in that queue.\n\n\nFair Scheduler: dynamically balance resources between all running jobs, each job is using its fair share of resources.\n\n\nThere is a lag between the time the second job starts and when it receives its fair share, since it has to wait for resources to free up as containers used by the first job complete. After the small job completes and no longer requires resources, the large job goes back to using the full cluster capacity again.", 
            "title": "Chapter 4: YARN"
        }, 
        {
            "location": "/hadoop/ch4/#hadoop-the-definitive-guide-4-yarn", 
            "text": "Apache YARN(Yet Another Resource Negotiator) is Hadoop s cluster resource management system. YARN provides APIs for requesting and working with cluster resources, but these APIs are not typically used directly by user code. Distributed computing frameworks (MapReduce, Spark, and so on) running as YARN applications on the cluster compute layer (YARN) and the cluster storage layer (HDFS and HBase).", 
            "title": "Hadoop: The Definitive Guide 4 - YARN"
        }, 
        {
            "location": "/hadoop/ch4/#1-anatomy-of-a-yarn-application-run", 
            "text": "YARN provides its core services via two types of long-running daemon:   a  resource manager  (one per cluster) to manage the use of resources across the cluster,  node managers  running on all the nodes in the cluster to launch and monitor  containers .     step1 : To run an application on YARN, a client contacts the resource manager and asks it to run an  application master  process.  steps 2a and 2b: The resource manager then finds a node manager that can launch the application master in a container. It could simply run a computation in the container it is running in and return the result to the client.  step 3: Or it could request more containers from the resource managers  steps 4a and 4b: use them to run a distributed computation.", 
            "title": "1 Anatomy of a YARN Application Run"
        }, 
        {
            "location": "/hadoop/ch4/#resource-requests", 
            "text": "A YARN application can make resource requests at any time while it is running.    Spark starts a fixed number of executors on the cluster (i.e. make all of requests up front).   MapReduce, has two phases: the map task containers are requested up front, but the reduce task containers are not started until later. (i.e. take a more dynamic approach whereby it requests more resources dynamically to meet the changing needs of the application).", 
            "title": "Resource Requests"
        }, 
        {
            "location": "/hadoop/ch4/#application-lifespan", 
            "text": "The lifespan of a YARN application can vary dramatically. Rather than look at how long the application runs for, it s useful to categorize applications in terms of how they map to the jobs that users run.    The simplest case is one application per user job, which is the approach that MapReduce takes.  The second model is to run one application per workflow or user session of (possibly unrelated) jobs, which is the approach that Spark takes. This approach can be more efficient than the first, since containers can be reused between jobs, and there is also the potential to cache intermediate data between jobs.  The third model is a long-running application that is shared by different users, which is the approach that Apache Slider takes.", 
            "title": "Application Lifespan"
        }, 
        {
            "location": "/hadoop/ch4/#building-yarn-applications", 
            "text": "Writing a YARN application from scratch is fairly involved, but in many cases is not necessary, as it is often possible to use an existing application that fits the bill.", 
            "title": "Building YARN Applications"
        }, 
        {
            "location": "/hadoop/ch4/#2-yarn-compared-to-mapreduce-1", 
            "text": "The distributed implementation of MapReduce in the original version of Hadoop is sometimes referred to as  MapReduce 1  to distinguish it from MapReduce 2, the implementation that uses YARN.  A comparison of MapReduce 1 and YARN components:     MapReduce1  YARN      Jobtracker  Resource manager, application master, timeline server    TaskTracker  Node manager    Slot  Container     The Timeline Server addresses the problem of the storage and retrieval of application s current and historic information in a generic fashion.", 
            "title": "2 YARN Compared to MapReduce 1"
        }, 
        {
            "location": "/hadoop/ch4/#3-scheduling-in-yarn", 
            "text": "The job of the YARN scheduler to allocate resources to applications according to some defined policy. Scheduling in general is a difficult problem and there is  no one \"best\" policy , which is why YARN provides a choice of schedulers and configurable policies.", 
            "title": "3 Scheduling in YARN"
        }, 
        {
            "location": "/hadoop/ch4/#scheduler-options", 
            "text": "Three schedulers are available in YARN: the FIFO, Capacity, and Fair Schedulers.   The FIFO: places applications in a queue and runs them in the order of submission (first in, first out)  Not suitable for shared clusters, because large applications will use all the resources in a cluster, so each application has to wait its turn.     Capacity Scheduler: a separate dedicated queue allows the small job to start as soon as it is submitted, since the queue capacity is reserved for jobs in that queue.  Fair Scheduler: dynamically balance resources between all running jobs, each job is using its fair share of resources.  There is a lag between the time the second job starts and when it receives its fair share, since it has to wait for resources to free up as containers used by the first job complete. After the small job completes and no longer requires resources, the large job goes back to using the full cluster capacity again.", 
            "title": "Scheduler Options"
        }, 
        {
            "location": "/hadoop/ch5/", 
            "text": "Hadoop: The Definitive Guide 5 - Hadoop I/O\n\n\n1 Data Integrity\n\n\nThe usual way of detecting corrupted data is by computing a \nchecksum\n(\u6821\u9a8c\u548c) for the data when it first enters the system, and again whenever it is transmitted across a channel that is unreliable and hence capable of corrupting the data.\n\n\nA commonly used error-detecting code is CRC-32 (32-bit cyclic redundancy check, \u5faa\u73af\u5197\u4f59\u6821\u9a8c), which computes a 32-bit integer checksum for input of any size. CRC32 is used for checksumming in Hadoop's \nchecksumFileSystem\n, while HDFS uses a more efficient variant called CRC-32C.\n\n\nData Integrity in HDFS\n\n\nHDFS transparently checksums all data written to it and by default verifies checksums when reading data. A separate checksum is created for every \nChecksumFileSystem.bytesPerChecksum\n(default 512) bytes of data.\n\n\nDatanodes are responsible for verifying the data they receive before storing the data and its checksum. When clients read data from datanodes, they verify checksums as well.\n\n\nIn addition to block verification on client reads, each datanode runs a \nDataBlockScanner\n in a background thread that \nperiodically\n verifies all the blocks stored on the datanode.\n\n\nYou can find a file\u2019s checksum with \nhadoop fs -checksum\n.\n\n\nLocalFileSystem\n\n\nThe Hadoop \nLocalFileSystem\n performs client-side checksumming. It is possible to disable checksums, by using \nRawLocalFileSystem\n in place of \nLocalFileSystem\n.\n\n\nChecksumFileSystem\n\n\nLocalFileSystem\n extends \nChecksumFileSystem\n, and \nChecksumFileSystem\n is also a wrapper around \nFileSystem\n (uses \ndecorator pattern\n here). The general idiom is as follows:\n\n\n \nFileSystem\n \nrawFs\n \n=\n \n...\n\n\nFileSystem\n \nchecksummedFs\n \n=\n \nnew\n \nChecksumFileSystem\n(\nrawFs\n);\n\n\n\n\n\n\n2 Compression\n\n\nFile compression brings two major benefits: it \nreduces the space\n needed to store files, and it \nspeeds up data transfer\n across the network or to or from disk. When dealing with large volumes of data, both of these savings can be significant.\n\n\nA summary of compression formats:\n\n\n\n\n\n\n\n\nCompression format\n\n\nTools\n\n\nAlgorithm\n\n\nFile Extension\n\n\nCompressionCodec\n\n\nSplittable?\n\n\n\n\n\n\n\n\n\n\nDEFLATE\n\n\nN/A\n\n\nDEFLATE\n\n\n.deflate\n\n\nDefaultCodec\n\n\nNo\n\n\n\n\n\n\ngzip\n\n\ngzip\n\n\nDEFLATE\n\n\n.gz\n\n\nGzipCodec\n\n\nNo\n\n\n\n\n\n\nbzip2\n\n\nbzip2\n\n\nbzip2\n\n\n.bz2\n\n\nBZip2Codec\n\n\nYes\n\n\n\n\n\n\nLZO\n\n\nlzop\n\n\nLZO\n\n\n.lzo\n\n\nLzoCodec\n\n\nNo\n\n\n\n\n\n\nSnappy\n\n\nN/A\n\n\nSnappy\n\n\n.snappy\n\n\nSnappyCodec\n\n\nNo\n\n\n\n\n\n\n\n\nAll compression algorithm exhibit a space/time trade-off. Splittable compression formats are especially suitable for MapReduce.\n\n\nCodecs\n\n\nA \ncodec\n is the implementation of a compression-decompression algorithm. In Hadoop, a codec is represented by an implementation of the \nCompressionCodec\n interface. So, for example, \nGzipCodec\n encapsulates the compression and decompression algorithm for gzip.\n\n\nCompressing and decompressing streams with CompressionCodec\n\n\nInterface CompressionCodec \n has two methods that allow you to easily compress or decompress data. \n\n\n\n\nTo compress data being written to an output stream, use the \ncreateOutputStream(OutputStream out)\n method to create a \nCompressionOutputStream\n\n\nConversely, to decompress data being read from an input stream, call \ncreateInputStream(InputStream in)\n to obtain a \nCompressionInputStream\n.\n\n\n\n\nThe code below illustrates how to use the API to compress data read from standard input and write it to standard output.\n\n\n \nimport\n \norg.apache.hadoop.conf.Configuration\n;\n\n\nimport\n \norg.apache.hadoop.io.IOUtils\n;\n\n\nimport\n \norg.apache.hadoop.io.compress.CompressionCodec\n;\n\n\nimport\n \norg.apache.hadoop.io.compress.CompressionOutputStream\n;\n\n\nimport\n \norg.apache.hadoop.util.ReflectionUtils\n;\n\n\n\n// vv StreamCompressor\n\n\npublic\n \nclass\n \nStreamCompressor\n \n{\n\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \nthrows\n \nException\n \n{\n\n        \nString\n \ncodecClassname\n \n=\n \nargs\n[\n0\n];\n\n        \nClass\n?\n \ncodecClass\n \n=\n \nClass\n.\nforName\n(\ncodecClassname\n);\n\n        \nConfiguration\n \nconf\n \n=\n \nnew\n \nConfiguration\n();\n\n        \nCompressionCodec\n \ncodec\n \n=\n \n(\nCompressionCodec\n)\n\n                \nReflectionUtils\n.\nnewInstance\n(\ncodecClass\n,\n \nconf\n);\n\n\n        \nCompressionOutputStream\n \nout\n \n=\n \ncodec\n.\ncreateOutputStream\n(\nSystem\n.\nout\n);\n\n        \nIOUtils\n.\ncopyBytes\n(\nSystem\n.\nin\n,\n \nout\n,\n \n4096\n,\n \nfalse\n);\n\n        \nout\n.\nfinish\n();\n\n    \n}\n\n\n}\n\n\n\n\nWe can try it out with the following command line, which compresses the string \u201cText\u201d using the \nStreamCompressor\n program with the \nGzipCodec\n, then decompresses it from standard input using \ngunzip\n:\n\n\n \nexport\n \nHADOOP_CLASSPATH\n=\n/Users/larry/JavaProject/out/artifacts/StreamCompressor/StreamCompressor.jar\n\necho\n \nText\n \n|\n hadoop com.definitivehadoop.compression.StreamCompressor org.apache.hadoop.io.compress.GzipCodec \n|\n gunzip \n\n\n\nInferring CompressionCodecs using CompressionCodecFactory\n\n\nCompressionCodecFactory\n provides a way of mapping a filename extension to a \nCompressionCodec\n using its \ngetCodec()\n method, \nCodecPool\n.\n\n\nIf you are using a native library and you are doing a lot of compression or decompression in your application, consider using \nCodecPool\n, which allows you to reuse compressors and decompressors, thereby amortizing the cost of creating these objects.\n\n\nCompression and Input Splits\n\n\nIf a compressed file using a format that does not support splitting, say gzip format, MapReduce will not try to split the gzipped file, at the expense of locality: a single map will process all blocks containing the file, most of which will not be local to the map.\n\n\nFor an LZO file, in spite of not supporting splitting, it is possible to preprocess LZO files using an indexer tool that comes with the Hadoop LZO libraries. \n\n\nUsing Compression in MapReduce\n\n\nIn order to compress the output of a MapReduce, job you can use the static convenience methods on \nFileOutputFormat\n to set properties.\n\n\nApplication to run the maximum temperature job producing compressed output:\n\n\nMaxtemperaturewithcompression\npublic\n \nclass\n \nMaxTemperatureWithCompression\n \n{\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \nthrows\n \nException\n \n{\n\n        \nif\n \n(\nargs\n.\nlength\n \n!=\n \n2\n)\n \n{\n\n            \nSystem\n.\nerr\n.\nprintln\n(\nUsage: MaxTemperatureWithCompression \ninput path\n \n \n+\n\n                    \noutput path\n);\n\n            \nSystem\n.\nexit\n(-\n1\n);\n\n        \n}\n\n\n        \nJob\n \njob\n \n=\n  \nJob\n.\ngetInstance\n();\n\n        \njob\n.\nsetJarByClass\n(\ncom\n.\ndefinitivehadoop\n.\nweatherdata\n.\nMaxTemperature\n.\nclass\n);\n\n\n        \nFileInputFormat\n.\naddInputPath\n(\njob\n,\n \nnew\n \nPath\n(\nargs\n[\n0\n]));\n\n        \nFileOutputFormat\n.\nsetOutputPath\n(\njob\n,\n \nnew\n \nPath\n(\nargs\n[\n1\n]));\n\n\n        \njob\n.\nsetOutputKeyClass\n(\nText\n.\nclass\n);\n\n        \njob\n.\nsetOutputValueClass\n(\nIntWritable\n.\nclass\n);\n\n\n        \n/*[*/\n\n        \nFileOutputFormat\n.\nsetCompressOutput\n(\njob\n,\n \ntrue\n);\n\n        \nFileOutputFormat\n.\nsetOutputCompressorClass\n(\njob\n,\n \nGzipCodec\n.\nclass\n);\n/*]*/\n\n\n\njob\n.\nsetMapperClass\n(\ncom\n.\ndefinitivehadoop\n.\nweatherdata\n.\nMaxTemperatureMapper\n.\nclass\n);\n\n\njob\n.\nsetCombinerClass\n(\ncom\n.\ndefinitivehadoop\n.\nweatherdata\n.\nMaxTemperatureReducer\n.\nclass\n);\n\n\njob\n.\nsetReducerClass\n(\ncom\n.\ndefinitivehadoop\n.\nweatherdata\n.\nMaxTemperatureReducer\n.\nclass\n);\n\n\n        \nSystem\n.\nexit\n(\njob\n.\nwaitForCompletion\n(\ntrue\n)\n \n?\n \n0\n \n:\n \n1\n);\n\n    \n}\n\n\n}\n\n\n//^^ MaxTemperatureWithCompression\n\n\nUsage\n$ \nexport\n \nHADOOP_CLASSPATH\n=\n/Users/larry/JavaProject/out/artifacts/MaxTemperatureWithCompression/MaxTemperatureWithCompression.jar\n$ hadoop com.definitivehadoop.compression.MaxTemperatureWithCompression /Users/larry/JavaProject/resources/HadoopBook/ncdc/sample.txt output\n\n\n\n\n3 Serialization\n\n\nSee concepts of serialization and deserialization in Head First Java \nChapter 14\n.\n\n\nSerialization is used in two quite distinct areas of distributed data processing: for interprocess communication and for persistent storage.\n\n\nIn Hadoop, interprocess communication between nodes in the system is implemented using remote procedure calls (RPCs). The RPC protocol uses serialization to render the message into a binary stream to be sent to the remote node, which then deserializes the binary stream into the original message. In general, four desirable properties are crucial for  an RPC serialization and persistent storage:\n\n\n\n\n\n\n\n\nProperties\n\n\nPRC Serialization\n\n\nPersistent Storage\n\n\n\n\n\n\n\n\n\n\nCompact\n\n\nmakes the best use of network bandwidth\n\n\nmake efficient use of storage space\n\n\n\n\n\n\nFast\n\n\nlittle performance overhead\n\n\nlittle overhead in reading or writing\n\n\n\n\n\n\nExtensible\n\n\nmeet new requirements\n\n\ntransparently read data of older formats\n\n\n\n\n\n\nInteroperable\n\n\nsupport clients written in different languages\n\n\nread/write using different languages\n\n\n\n\n\n\n\n\nHadoop uses its own serialization format, \nWritables\n, which is certainly compact and fast, but not so easy to extend or use from languages other than Java. Avro, a serialization system that was designed to overcome some of the limitations of \nWritables\n, is covered in \nChapter 12\n.\n\n\nThe Writable Interface\n\n\nThe \nWritable\n interface defines two methods \u2014 one for writing its state to a \nDataOutput\n binary stream and one for reading its state from a \nDataInput\n binary stream (note: \nDataOutput\n and \nDataInput\n are also inferfaces):\n\n\n \npackage\n \norg.apache.hadoop.io\n;\n\n\n\nimport\n \njava.io.DataOutput\n;\n \n\nimport\n \njava.io.DataInput\n;\n \n\nimport\n \njava.io.IOException\n;\n\n\n\npublic\n \ninterface\n \nWritable\n \n{\n \n    \nvoid\n \nwrite\n(\nDataOutput\n \nout\n)\n \nthrows\n \nIOException\n;\n \n    \nvoid\n \nreadFields\n(\nDataInput\n \nin\n)\n \nthrows\n \nIOException\n;\n \n\n}\n\n\n\n\nWritable Classes\n\n\nWritable wrappers for Java primitives\n\n\nThere are \nWritable\n wrappers for all the Java primitive types  except char (which can be stored in an \nIntWritable\n). All have a \nget()\n and \nset()\n method for retrieving and storing the wrapped value.\n\n\n\n\nWhen it comes to encoding integers, there is a choice between the fixed-length formats (\nIntWritable\n and \nLongWritable\n) and the variable-length formats (\nVIntWritable\n and \nVLongWritable\n).\n\n\nImplementing a Custom Writable\n\n\nSerialization Frameworks\n\n\n4 File-Based Data Structures\n\n\nFor some applications, you need a specialized data structure to hold your data. For doing MapReduce-based processing, putting each blob of binary data into its own file doesn\u2019t scale, so Hadoop developed a number of higher-level containers for these situations.\n\n\nSequenceFile\n\n\nHadoop\u2019s \nSequenceFile\n provides a persistent data structure for binary key-value pairs. It is suitable for a log file, where each log record is a new line of text. To use it as a logfile format, you would choose a key, such as timestamp represented by a \nLongWritable\n, and the value would be a \nWritable\n that represents the quantity being logged.\n\n\nWriting a SequenceFile\n\n\nTo create a \nSequenceFile\n, use one of its \ncreateWriter()\n static methods, which return a \nSequenceFile.Writer\n instance. Then write key-value pairs using the \nappend()\n method. When you\u2019ve finished, you call the \nclose()\n method.\n\n\nDisplaying a SequenceFile with the command-line interface\n\n\nThe hadoop \nfs\n command has a \n-text\n option to display sequence files in textual form.\n\n\n \n%\n \nhadoop\n \nfs\n \n-\ntext\n \nnumbers\n.\nseq\n \n|\n \nhead\n\n\n\n\nThe SequenceFile format\n\n\nA sequence file(\u987a\u5e8f\u6587\u4ef6) consists of a header followed by one or more records. The sync marker(\u540c\u6b65\u6807\u8bc6) is used to allow a reader synchronize to a record boundary from any position in the file, which incurs less than a 1% storage overhead.\n\n\n\n\nThe internal format of the records depends on whether compression is enabled, and if it is, whether it is record compression(\u8bb0\u5f55\u538b\u7f29) or block compression(\u5757\u538b\u7f29).\n\n\nThe format for record compression is almost identical to that for no compression, except the value bytes are compressed using the codec defined in the header. Note that keys are not compressed.\n\n\n\n\nBlock compression compresses multiple records at once; it is therefore more compact than and should generally be preferred over record compression because it has the opportunity to take advantage of similarities between records. A sync marker is written before the start of every block. The format of a block is a field indicating the number of records in the block, followed by four compressed fields: the key lengths, the keys, the value lengths, and the values.\n\n\nMapFile\n\n\nA \nMapFile\n is a sorted \nSequenceFile\n with an index to permit lookups by key. The index is itself a \nSequenceFile\n that contains a fraction of the keys in the map.", 
            "title": "Chapter 5: Hadoop I/O"
        }, 
        {
            "location": "/hadoop/ch5/#hadoop-the-definitive-guide-5-hadoop-io", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 5 - Hadoop I/O"
        }, 
        {
            "location": "/hadoop/ch5/#1-data-integrity", 
            "text": "The usual way of detecting corrupted data is by computing a  checksum (\u6821\u9a8c\u548c) for the data when it first enters the system, and again whenever it is transmitted across a channel that is unreliable and hence capable of corrupting the data.  A commonly used error-detecting code is CRC-32 (32-bit cyclic redundancy check, \u5faa\u73af\u5197\u4f59\u6821\u9a8c), which computes a 32-bit integer checksum for input of any size. CRC32 is used for checksumming in Hadoop's  checksumFileSystem , while HDFS uses a more efficient variant called CRC-32C.", 
            "title": "1 Data Integrity"
        }, 
        {
            "location": "/hadoop/ch5/#data-integrity-in-hdfs", 
            "text": "HDFS transparently checksums all data written to it and by default verifies checksums when reading data. A separate checksum is created for every  ChecksumFileSystem.bytesPerChecksum (default 512) bytes of data.  Datanodes are responsible for verifying the data they receive before storing the data and its checksum. When clients read data from datanodes, they verify checksums as well.  In addition to block verification on client reads, each datanode runs a  DataBlockScanner  in a background thread that  periodically  verifies all the blocks stored on the datanode.  You can find a file\u2019s checksum with  hadoop fs -checksum .", 
            "title": "Data Integrity in HDFS"
        }, 
        {
            "location": "/hadoop/ch5/#localfilesystem", 
            "text": "The Hadoop  LocalFileSystem  performs client-side checksumming. It is possible to disable checksums, by using  RawLocalFileSystem  in place of  LocalFileSystem .", 
            "title": "LocalFileSystem"
        }, 
        {
            "location": "/hadoop/ch5/#checksumfilesystem", 
            "text": "LocalFileSystem  extends  ChecksumFileSystem , and  ChecksumFileSystem  is also a wrapper around  FileSystem  (uses  decorator pattern  here). The general idiom is as follows:    FileSystem   rawFs   =   ...  FileSystem   checksummedFs   =   new   ChecksumFileSystem ( rawFs );", 
            "title": "ChecksumFileSystem"
        }, 
        {
            "location": "/hadoop/ch5/#2-compression", 
            "text": "File compression brings two major benefits: it  reduces the space  needed to store files, and it  speeds up data transfer  across the network or to or from disk. When dealing with large volumes of data, both of these savings can be significant.  A summary of compression formats:     Compression format  Tools  Algorithm  File Extension  CompressionCodec  Splittable?      DEFLATE  N/A  DEFLATE  .deflate  DefaultCodec  No    gzip  gzip  DEFLATE  .gz  GzipCodec  No    bzip2  bzip2  bzip2  .bz2  BZip2Codec  Yes    LZO  lzop  LZO  .lzo  LzoCodec  No    Snappy  N/A  Snappy  .snappy  SnappyCodec  No     All compression algorithm exhibit a space/time trade-off. Splittable compression formats are especially suitable for MapReduce.", 
            "title": "2 Compression"
        }, 
        {
            "location": "/hadoop/ch5/#codecs", 
            "text": "A  codec  is the implementation of a compression-decompression algorithm. In Hadoop, a codec is represented by an implementation of the  CompressionCodec  interface. So, for example,  GzipCodec  encapsulates the compression and decompression algorithm for gzip.  Compressing and decompressing streams with CompressionCodec  Interface CompressionCodec   has two methods that allow you to easily compress or decompress data.    To compress data being written to an output stream, use the  createOutputStream(OutputStream out)  method to create a  CompressionOutputStream  Conversely, to decompress data being read from an input stream, call  createInputStream(InputStream in)  to obtain a  CompressionInputStream .   The code below illustrates how to use the API to compress data read from standard input and write it to standard output.    import   org.apache.hadoop.conf.Configuration ;  import   org.apache.hadoop.io.IOUtils ;  import   org.apache.hadoop.io.compress.CompressionCodec ;  import   org.apache.hadoop.io.compress.CompressionOutputStream ;  import   org.apache.hadoop.util.ReflectionUtils ;  // vv StreamCompressor  public   class   StreamCompressor   { \n\n     public   static   void   main ( String []   args )   throws   Exception   { \n         String   codecClassname   =   args [ 0 ]; \n         Class ?   codecClass   =   Class . forName ( codecClassname ); \n         Configuration   conf   =   new   Configuration (); \n         CompressionCodec   codec   =   ( CompressionCodec ) \n                 ReflectionUtils . newInstance ( codecClass ,   conf ); \n\n         CompressionOutputStream   out   =   codec . createOutputStream ( System . out ); \n         IOUtils . copyBytes ( System . in ,   out ,   4096 ,   false ); \n         out . finish (); \n     }  }   We can try it out with the following command line, which compresses the string \u201cText\u201d using the  StreamCompressor  program with the  GzipCodec , then decompresses it from standard input using  gunzip :    export   HADOOP_CLASSPATH = /Users/larry/JavaProject/out/artifacts/StreamCompressor/StreamCompressor.jar echo   Text   |  hadoop com.definitivehadoop.compression.StreamCompressor org.apache.hadoop.io.compress.GzipCodec  |  gunzip   Inferring CompressionCodecs using CompressionCodecFactory  CompressionCodecFactory  provides a way of mapping a filename extension to a  CompressionCodec  using its  getCodec()  method,  CodecPool .  If you are using a native library and you are doing a lot of compression or decompression in your application, consider using  CodecPool , which allows you to reuse compressors and decompressors, thereby amortizing the cost of creating these objects.", 
            "title": "Codecs"
        }, 
        {
            "location": "/hadoop/ch5/#compression-and-input-splits", 
            "text": "If a compressed file using a format that does not support splitting, say gzip format, MapReduce will not try to split the gzipped file, at the expense of locality: a single map will process all blocks containing the file, most of which will not be local to the map.  For an LZO file, in spite of not supporting splitting, it is possible to preprocess LZO files using an indexer tool that comes with the Hadoop LZO libraries.", 
            "title": "Compression and Input Splits"
        }, 
        {
            "location": "/hadoop/ch5/#using-compression-in-mapreduce", 
            "text": "In order to compress the output of a MapReduce, job you can use the static convenience methods on  FileOutputFormat  to set properties.  Application to run the maximum temperature job producing compressed output:  Maxtemperaturewithcompression public   class   MaxTemperatureWithCompression   { \n     public   static   void   main ( String []   args )   throws   Exception   { \n         if   ( args . length   !=   2 )   { \n             System . err . println ( Usage: MaxTemperatureWithCompression  input path     + \n                     output path ); \n             System . exit (- 1 ); \n         } \n\n         Job   job   =    Job . getInstance (); \n         job . setJarByClass ( com . definitivehadoop . weatherdata . MaxTemperature . class ); \n\n         FileInputFormat . addInputPath ( job ,   new   Path ( args [ 0 ])); \n         FileOutputFormat . setOutputPath ( job ,   new   Path ( args [ 1 ])); \n\n         job . setOutputKeyClass ( Text . class ); \n         job . setOutputValueClass ( IntWritable . class ); \n\n         /*[*/ \n         FileOutputFormat . setCompressOutput ( job ,   true ); \n         FileOutputFormat . setOutputCompressorClass ( job ,   GzipCodec . class ); /*]*/  job . setMapperClass ( com . definitivehadoop . weatherdata . MaxTemperatureMapper . class );  job . setCombinerClass ( com . definitivehadoop . weatherdata . MaxTemperatureReducer . class );  job . setReducerClass ( com . definitivehadoop . weatherdata . MaxTemperatureReducer . class ); \n\n         System . exit ( job . waitForCompletion ( true )   ?   0   :   1 ); \n     }  }  //^^ MaxTemperatureWithCompression  Usage $  export   HADOOP_CLASSPATH = /Users/larry/JavaProject/out/artifacts/MaxTemperatureWithCompression/MaxTemperatureWithCompression.jar\n$ hadoop com.definitivehadoop.compression.MaxTemperatureWithCompression /Users/larry/JavaProject/resources/HadoopBook/ncdc/sample.txt output", 
            "title": "Using Compression in MapReduce"
        }, 
        {
            "location": "/hadoop/ch5/#3-serialization", 
            "text": "See concepts of serialization and deserialization in Head First Java  Chapter 14 .  Serialization is used in two quite distinct areas of distributed data processing: for interprocess communication and for persistent storage.  In Hadoop, interprocess communication between nodes in the system is implemented using remote procedure calls (RPCs). The RPC protocol uses serialization to render the message into a binary stream to be sent to the remote node, which then deserializes the binary stream into the original message. In general, four desirable properties are crucial for  an RPC serialization and persistent storage:     Properties  PRC Serialization  Persistent Storage      Compact  makes the best use of network bandwidth  make efficient use of storage space    Fast  little performance overhead  little overhead in reading or writing    Extensible  meet new requirements  transparently read data of older formats    Interoperable  support clients written in different languages  read/write using different languages     Hadoop uses its own serialization format,  Writables , which is certainly compact and fast, but not so easy to extend or use from languages other than Java. Avro, a serialization system that was designed to overcome some of the limitations of  Writables , is covered in  Chapter 12 .", 
            "title": "3 Serialization"
        }, 
        {
            "location": "/hadoop/ch5/#the-writable-interface", 
            "text": "The  Writable  interface defines two methods \u2014 one for writing its state to a  DataOutput  binary stream and one for reading its state from a  DataInput  binary stream (note:  DataOutput  and  DataInput  are also inferfaces):    package   org.apache.hadoop.io ;  import   java.io.DataOutput ;   import   java.io.DataInput ;   import   java.io.IOException ;  public   interface   Writable   {  \n     void   write ( DataOutput   out )   throws   IOException ;  \n     void   readFields ( DataInput   in )   throws   IOException ;   }", 
            "title": "The Writable Interface"
        }, 
        {
            "location": "/hadoop/ch5/#writable-classes", 
            "text": "Writable wrappers for Java primitives  There are  Writable  wrappers for all the Java primitive types  except char (which can be stored in an  IntWritable ). All have a  get()  and  set()  method for retrieving and storing the wrapped value.   When it comes to encoding integers, there is a choice between the fixed-length formats ( IntWritable  and  LongWritable ) and the variable-length formats ( VIntWritable  and  VLongWritable ).", 
            "title": "Writable Classes"
        }, 
        {
            "location": "/hadoop/ch5/#implementing-a-custom-writable", 
            "text": "", 
            "title": "Implementing a Custom Writable"
        }, 
        {
            "location": "/hadoop/ch5/#serialization-frameworks", 
            "text": "", 
            "title": "Serialization Frameworks"
        }, 
        {
            "location": "/hadoop/ch5/#4-file-based-data-structures", 
            "text": "For some applications, you need a specialized data structure to hold your data. For doing MapReduce-based processing, putting each blob of binary data into its own file doesn\u2019t scale, so Hadoop developed a number of higher-level containers for these situations.", 
            "title": "4 File-Based Data Structures"
        }, 
        {
            "location": "/hadoop/ch5/#sequencefile", 
            "text": "Hadoop\u2019s  SequenceFile  provides a persistent data structure for binary key-value pairs. It is suitable for a log file, where each log record is a new line of text. To use it as a logfile format, you would choose a key, such as timestamp represented by a  LongWritable , and the value would be a  Writable  that represents the quantity being logged.  Writing a SequenceFile  To create a  SequenceFile , use one of its  createWriter()  static methods, which return a  SequenceFile.Writer  instance. Then write key-value pairs using the  append()  method. When you\u2019ve finished, you call the  close()  method.  Displaying a SequenceFile with the command-line interface  The hadoop  fs  command has a  -text  option to display sequence files in textual form.    %   hadoop   fs   - text   numbers . seq   |   head   The SequenceFile format  A sequence file(\u987a\u5e8f\u6587\u4ef6) consists of a header followed by one or more records. The sync marker(\u540c\u6b65\u6807\u8bc6) is used to allow a reader synchronize to a record boundary from any position in the file, which incurs less than a 1% storage overhead.   The internal format of the records depends on whether compression is enabled, and if it is, whether it is record compression(\u8bb0\u5f55\u538b\u7f29) or block compression(\u5757\u538b\u7f29).  The format for record compression is almost identical to that for no compression, except the value bytes are compressed using the codec defined in the header. Note that keys are not compressed.   Block compression compresses multiple records at once; it is therefore more compact than and should generally be preferred over record compression because it has the opportunity to take advantage of similarities between records. A sync marker is written before the start of every block. The format of a block is a field indicating the number of records in the block, followed by four compressed fields: the key lengths, the keys, the value lengths, and the values.", 
            "title": "SequenceFile"
        }, 
        {
            "location": "/hadoop/ch5/#mapfile", 
            "text": "A  MapFile  is a sorted  SequenceFile  with an index to permit lookups by key. The index is itself a  SequenceFile  that contains a fraction of the keys in the map.", 
            "title": "MapFile"
        }, 
        {
            "location": "/hadoop/ch6/", 
            "text": "Hadoop: The Definitive Guide 6 - Developing a MapReduce Application\n\n\n1 The Configuration API\n\n\nComponents in Hadoop are configured using Hadoop\u2019s own configuration API. An instance of the \norg.apache.hadoop.conf.Configuration\n represents a collection of configuration properties and their values.\n\n\nConfiguration\ns read their properties from  XML files, which have a simple structure for defining name-value pairs.\n\n\n\n\nNote\n\n\nXML(E\nx\ntensible \nM\narkup \nL\nanguage, \u53ef\u6269\u5c55\u6807\u8bb0\u8bed\u8a00), is a markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. \n\n\n\n\nCombining Resources\n\n\nWhen more than one resource is used to define a \nConfiguration\n, properties added later override the earlier definitions. However, properties that are marked as \nfinal\n cannot be overridden in later definitions.\n\n\nJava\nConfiguration\n \nconf\n \n=\n \nnew\n \nConfiguration\n();\n\n\nconf\n.\naddResource\n(\nconfiguration-1.xml\n);\n\n\nconf\n.\naddResource\n(\nconfiguration-2.xml\n);\n\n\nassertThat\n(\nconf\n.\ngetInt\n(\nsize\n,\n \n0\n),\n \nis\n(\n12\n));\n\n\nassertThat\n(\nconf\n.\nget\n(\nweight\n),\n \nis\n(\nheavy\n));\n\n\nConfiguration-1.xml\n?xml version=\n1.0\n?\n \n\nconfiguration\n\n    \nproperty\n \n        \nname\ncolor\n/name\n \n        \nvalue\nyellow\n/value\n \n        \ndescription\nColor\n/description\n \n    \n/property\n\n\n    \nproperty\n \n        \nname\nsize\n/name\n \n        \nvalue\n10\n/value\n \n        \ndescription\nSize\n/description\n \n    \n/property\n\n\n    \nproperty\n \n        \nname\nweight\n/name\n \n        \nvalue\nheavy\n/value\n \n        \nfinal\ntrue\n/final\n \n        \ndescription\nWeight\n/description\n \n    \n/property\n\n\n    \nproperty\n \n        \nname\nsize-weight\n/name\n \n        \nvalue\n${size},${weight}\n/value\n \n        \ndescription\nSize and weight\n/description\n \n    \n/property\n \n\n/configuration\n\n\nConfiguration-2.xml\n?xml version=\n1.0\n?\n \n\nconfiguration\n\n    \nproperty\n \n        \nname\nsize\n/name\n \n        \nvalue\n12\n/value\n \n    \n/property\n\n\n    \nproperty\n \n        \nname\nweight\n/name\n \n        \nvalue\nlight\n/value\n \n    \n/property\n \n\n/configuration\n\n\n\n\n\nVariable Expansion\n\n\nConfiguration properties can be defined in terms of other properties, or system properties. For example, the property \nsize-weight\n in \nconfiguration-1.xml\n file is defined as \n$text\n{size}$, \n${\nweight\n}\n$\n.\n\n\n2 Setting Up the Development Environment\n\n\nThe first step is to create a project so you can build MapReduce programs and run them in local (standalone) mode from the command line or within your IDE.\n\n\nManaging Configuration\n\n\n3 Writing a Unit Test with MRUnit\n\n\nMRUnit\n is a testing library that makes it easy to pass known inputs to a mapper or a reducer and check that the outputs are as expected.  However, MRUnit is \nDEPRECATED\n!!!\n\n\n4 Running Locally on Test Data\n\n\n5 Running on a Cluster\n\n\n6 Tuning a Job\n\n\n7 MapReduce Workflows", 
            "title": "Chapter 6: Developing a MapReduce Application"
        }, 
        {
            "location": "/hadoop/ch6/#hadoop-the-definitive-guide-6-developing-a-mapreduce-application", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 6 - Developing a MapReduce Application"
        }, 
        {
            "location": "/hadoop/ch6/#1-the-configuration-api", 
            "text": "Components in Hadoop are configured using Hadoop\u2019s own configuration API. An instance of the  org.apache.hadoop.conf.Configuration  represents a collection of configuration properties and their values.  Configuration s read their properties from  XML files, which have a simple structure for defining name-value pairs.   Note  XML(E x tensible  M arkup  L anguage, \u53ef\u6269\u5c55\u6807\u8bb0\u8bed\u8a00), is a markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable.", 
            "title": "1 The Configuration API"
        }, 
        {
            "location": "/hadoop/ch6/#combining-resources", 
            "text": "When more than one resource is used to define a  Configuration , properties added later override the earlier definitions. However, properties that are marked as  final  cannot be overridden in later definitions.  Java Configuration   conf   =   new   Configuration ();  conf . addResource ( configuration-1.xml );  conf . addResource ( configuration-2.xml );  assertThat ( conf . getInt ( size ,   0 ),   is ( 12 ));  assertThat ( conf . get ( weight ),   is ( heavy ));  Configuration-1.xml ?xml version= 1.0 ?   configuration \n     property  \n         name color /name  \n         value yellow /value  \n         description Color /description  \n     /property \n\n     property  \n         name size /name  \n         value 10 /value  \n         description Size /description  \n     /property \n\n     property  \n         name weight /name  \n         value heavy /value  \n         final true /final  \n         description Weight /description  \n     /property \n\n     property  \n         name size-weight /name  \n         value ${size},${weight} /value  \n         description Size and weight /description  \n     /property   /configuration  Configuration-2.xml ?xml version= 1.0 ?   configuration \n     property  \n         name size /name  \n         value 12 /value  \n     /property \n\n     property  \n         name weight /name  \n         value light /value  \n     /property   /configuration", 
            "title": "Combining Resources"
        }, 
        {
            "location": "/hadoop/ch6/#variable-expansion", 
            "text": "Configuration properties can be defined in terms of other properties, or system properties. For example, the property  size-weight  in  configuration-1.xml  file is defined as  $text {size}$,  ${ weight } $ .", 
            "title": "Variable Expansion"
        }, 
        {
            "location": "/hadoop/ch6/#2-setting-up-the-development-environment", 
            "text": "The first step is to create a project so you can build MapReduce programs and run them in local (standalone) mode from the command line or within your IDE.", 
            "title": "2 Setting Up the Development Environment"
        }, 
        {
            "location": "/hadoop/ch6/#managing-configuration", 
            "text": "", 
            "title": "Managing Configuration"
        }, 
        {
            "location": "/hadoop/ch6/#3-writing-a-unit-test-with-mrunit", 
            "text": "MRUnit  is a testing library that makes it easy to pass known inputs to a mapper or a reducer and check that the outputs are as expected.  However, MRUnit is  DEPRECATED !!!", 
            "title": "3 Writing a Unit Test with MRUnit"
        }, 
        {
            "location": "/hadoop/ch6/#4-running-locally-on-test-data", 
            "text": "", 
            "title": "4 Running Locally on Test Data"
        }, 
        {
            "location": "/hadoop/ch6/#5-running-on-a-cluster", 
            "text": "", 
            "title": "5 Running on a Cluster"
        }, 
        {
            "location": "/hadoop/ch6/#6-tuning-a-job", 
            "text": "", 
            "title": "6 Tuning a Job"
        }, 
        {
            "location": "/hadoop/ch6/#7-mapreduce-workflows", 
            "text": "", 
            "title": "7 MapReduce Workflows"
        }, 
        {
            "location": "/hadoop/ch7/", 
            "text": "Hadoop: The Definitive Guide 7 - How MapReduce Works", 
            "title": "Chapter 7: How MapReduce Works"
        }, 
        {
            "location": "/hadoop/ch7/#hadoop-the-definitive-guide-7-how-mapreduce-works", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 7 - How MapReduce Works"
        }, 
        {
            "location": "/hadoop/ch8/", 
            "text": "Hadoop: The Definitive Guide 8 - MapReduce Types and Formats", 
            "title": "Chapter 8: MapReduce Types and Formats"
        }, 
        {
            "location": "/hadoop/ch8/#hadoop-the-definitive-guide-8-mapreduce-types-and-formats", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 8 - MapReduce Types and Formats"
        }, 
        {
            "location": "/hadoop/ch9/", 
            "text": "Hadoop: The Definitive Guide 9 - MapReduce Features", 
            "title": "Chapter 9: MapReduce Features"
        }, 
        {
            "location": "/hadoop/ch9/#hadoop-the-definitive-guide-9-mapreduce-features", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 9 - MapReduce Features"
        }, 
        {
            "location": "/hadoop/ch10/", 
            "text": "Hadoop: The Definitive Guide 10 - Setting Up a Hadoop Cluster", 
            "title": "Chapter 10: Setting Up a Hadoop Cluster"
        }, 
        {
            "location": "/hadoop/ch10/#hadoop-the-definitive-guide-10-setting-up-a-hadoop-cluster", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 10 - Setting Up a Hadoop Cluster"
        }, 
        {
            "location": "/hadoop/ch11/", 
            "text": "Hadoop: The Definitive Guide 11 - Adminstering Hadoop", 
            "title": "Chapter 11: Adminstering Hadoop"
        }, 
        {
            "location": "/hadoop/ch11/#hadoop-the-definitive-guide-11-adminstering-hadoop", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 11 - Adminstering Hadoop"
        }, 
        {
            "location": "/hadoop/ch12/", 
            "text": "Hadoop: The Definitive Guide 12 - Avro\n\n\nApache Avro is a language-neutral data serialization system. The project was created by Doug Cutting to address the major downside of Hadoop \nWritables\n: \nlack of language portability\n [see \nChapter5\n]. Having a data format that can be processed by many languages makes it easier to share datasets with a wider audience than one tied to a single language.\n\n\nAvro data is described using a language-independent \nschema\n(\u6a21\u5f0f). Schemas are usually written in JSON, and data is usually encoded using a binary format.\n\n\n1 Avro Data Types and Schemas\n\n\nUseful resources\n\n\n\n\nAvro project page\n\n\nAvro issue tracking page\n\n\nblog about Avro use\n\n\nCDH usage page for Avro\n\n\nAvro Specification", 
            "title": "Chapter 12: Avro"
        }, 
        {
            "location": "/hadoop/ch12/#hadoop-the-definitive-guide-12-avro", 
            "text": "Apache Avro is a language-neutral data serialization system. The project was created by Doug Cutting to address the major downside of Hadoop  Writables :  lack of language portability  [see  Chapter5 ]. Having a data format that can be processed by many languages makes it easier to share datasets with a wider audience than one tied to a single language.  Avro data is described using a language-independent  schema (\u6a21\u5f0f). Schemas are usually written in JSON, and data is usually encoded using a binary format.", 
            "title": "Hadoop: The Definitive Guide 12 - Avro"
        }, 
        {
            "location": "/hadoop/ch12/#1-avro-data-types-and-schemas", 
            "text": "", 
            "title": "1 Avro Data Types and Schemas"
        }, 
        {
            "location": "/hadoop/ch12/#useful-resources", 
            "text": "Avro project page  Avro issue tracking page  blog about Avro use  CDH usage page for Avro  Avro Specification", 
            "title": "Useful resources"
        }, 
        {
            "location": "/hadoop/ch13/", 
            "text": "Hadoop: The Definitive Guide 13 - Parquet\n\n\nApache Parquet is a columnar storage format that can efficiently store nested data.", 
            "title": "Chapter 13: Parquet"
        }, 
        {
            "location": "/hadoop/ch13/#hadoop-the-definitive-guide-13-parquet", 
            "text": "Apache Parquet is a columnar storage format that can efficiently store nested data.", 
            "title": "Hadoop: The Definitive Guide 13 - Parquet"
        }, 
        {
            "location": "/hadoop/ch14/", 
            "text": "Hadoop: The Definitive Guide 14 - Flume\n\n\nFlume is designed for high-volume ingestion into Hadoop of event-based data. The canonical example is using Flume to collect logfiles from a bank of web servers, then moving the log events from those files into new aggregated files in HDFS for processing.\n\n\nTo use Flume, we need to run a Flume \nagent\n, which is a long-lived Java process that runs \nsource\ns and \nsink\ns, connected by \nchannel\ns. A source in Flume produces events and delivers them to the channel, which stores the events until they are forwarded to the sink.\n\n\n\n\n1. An Example\n\n\nTo show how Flume works, let\u2019s start with a setup that:\n\n\n\n\nWatches a \nlocal\n directory for new text files\n\n\nSends each line of each file to the console as files are added\n\n\n\n\nFlume configuration using a spooling directory source and a logger sink\n\n\n \nagent1.sources = source1 \nagent1.sinks = sink1 \nagent1.channels = channel1\nagent1.sources.source1.channels = channel1 \nagent1.sinks.sink1.channel = channel1\nagent1.sources.source1.type = spooldir\nagent1.sources.source1.spoolDir = /tmp/spooldir\nagent1.sinks.sink1.type = logger\nagent1.channels.channel1.type = file\n\n\n\nHere, a \nspooldir\n is a spooling directory source that monitors a spooling directory for new files; a \nlogger\n sink is a sink for logging events to the console. Source and sink must be connected to channel(\nagent1.sources.source1.channels = channel1\n).\n\n\n\n\n \n# create the spooling directory on the local filesystem:\n\n$ mkdir /tmp/spooldir\n\n# start the Flume agent using the flume-ng command:\n\n$ flume-ng agent \n\\\n\n\n --conf-file spool-to-logger.properties \n\\\n\n\n --name agent1 \n\\\n\n\n --conf \n$F\nLUME_HOME/conf \n\\\n\n\n -Dflume.root.logger\n=\nINFO,console\n\n\n# on another terminal, create a file in the spooling directory.\n\n$ \necho\n \nHello Flume\n \n /tmp/spooldir/.file1.txt\n$  ~ mv /tmp/spooldir/.file1.txt /tmp/spooldir/file1.txt\n\n\n\n2. Transactions and Reliability\n\n\nFlume uses separate transactions to guarantee delivery from the source to the channel and from the channel to the sink. In the example in the previous section, the spooling directory source creates an event for each line in the file. The source will only mark the file as completed once the transactions encapsulating the delivery of the events to the channel have been successfully committed. \n\n\nSimilarly, a transaction is used for the delivery of the events from the channel to the sink. If for some unlikely reason the events could not be logged, the transaction would be rolled back and the events would remain in the channel for later redelivery.\n\n\n3. The HDFS Sink\n\n\nEvents may delivered to the HDFS sink and written to a file. Files in the process of being written to have a \n.tmp\n in-use suffix (default, set by \nhdfs.inUsePrefix\n, see below) added to their name to indicate that they are not yet complete.\n\n\nFlume configuration using a spooling directory source and an HDFS sink:\n\n\n \nagent1.sources = source1 \nagent1.sinks = sink1 \nagent1.channels = channel1\nagent1.sources.source1.channels = channel1\nagent1.sinks.sink1.channel = channel1\nagent1.sources.source1.type = spooldir\nagent1.sources.source1.spoolDir = /tmp/spooldir\nagent1.sinks.sink1.type = hdfs\nagent1.sinks.sink1.hdfs.path = /tmp/flume\nagent1.sinks.sink1.hdfs.filePrefix = events\nagent1.sinks.sink1.hdfs.fileSuffix = .log \nagent1.sinks.sink1.hdfs.inUsePrefix = _ \nagent1.sinks.sink1.hdfs.fileType = DataStream\nagent1.channels.channel1.type = file\n\n\n\nPartitioning and Interceptors\n\n\n4 Fan Out\n\n\nFan out\n is the term for delivering events from one source to multiple channels, so they reach multiple sinks.\n\n\nFlume configuration using a spooling directory source, fanning out to an HDFS sink and a logger sink:\n\n\n \nagent1.sources = source1 \nagent1.sinks = sink1a sink1b \nagent1.channels = channel1a channel1b\nagent1.sources.source1.channels = channel1a channel1b \nagent1.sinks.sink1a.channel = channel1a \nagent1.sinks.sink1b.channel = channel1b\nagent1.sources.source1.type = spooldir \nagent1.sources.source1.spoolDir = /tmp/spooldir\nagent1.sinks.sink1a.type = hdfs \nagent1.sinks.sink1a.hdfs.path = /tmp/flume \nagent1.sinks.sink1a.hdfs.filePrefix = events \nagent1.sinks.sink1a.hdfs.fileSuffix = .log \nagent1.sinks.sink1a.hdfs.fileType = DataStream\nagent1.sinks.sink1b.type = logger\nagent1.channels.channel1a.type = file agent1.channels.channel1b.type = memory\n\n\n\n\n\nDelivery Guarantees\n\n\nFlume uses a separate transaction to deliver each batch of events from the spooling directory source to each channel. If either of these transactions fails (if a channel is full, for example), then the events will not be removed from the source, and will be retried later.\n\n\n3 Distribution: Agent Tiers\n\n\nAggregating Flume events is achieved by having tiers of Flume agents. \n\n\nThe first tier collects events from the original sources (such as web servers) and sends them to a smaller set of agents in the second tier, which aggregate events from the first tier before writing them to HDFS. Further tiers may be warranted for very large numbers of source nodes.\n\n\n\n\n4 Sink Groups\n\n\nA \nsink group\n allows multiple sinks to be treated as one, for failover(\u6545\u969c\u8f6c\u79fb) or load-balancing purposes. If a second-tier agent is unavailable, then events will be delivered to another second-tier agent and on to HDFS without disruption.\n\n\n\n\n5 Useful resources\n\n\n\n\nFlume main page \n\n\nFlume user guide\n\n\nFlume Getting Started guide", 
            "title": "Chapter 14: Flume"
        }, 
        {
            "location": "/hadoop/ch14/#hadoop-the-definitive-guide-14-flume", 
            "text": "Flume is designed for high-volume ingestion into Hadoop of event-based data. The canonical example is using Flume to collect logfiles from a bank of web servers, then moving the log events from those files into new aggregated files in HDFS for processing.  To use Flume, we need to run a Flume  agent , which is a long-lived Java process that runs  source s and  sink s, connected by  channel s. A source in Flume produces events and delivers them to the channel, which stores the events until they are forwarded to the sink.", 
            "title": "Hadoop: The Definitive Guide 14 - Flume"
        }, 
        {
            "location": "/hadoop/ch14/#1-an-example", 
            "text": "To show how Flume works, let\u2019s start with a setup that:   Watches a  local  directory for new text files  Sends each line of each file to the console as files are added   Flume configuration using a spooling directory source and a logger sink    agent1.sources = source1 \nagent1.sinks = sink1 \nagent1.channels = channel1\nagent1.sources.source1.channels = channel1 \nagent1.sinks.sink1.channel = channel1\nagent1.sources.source1.type = spooldir\nagent1.sources.source1.spoolDir = /tmp/spooldir\nagent1.sinks.sink1.type = logger\nagent1.channels.channel1.type = file  Here, a  spooldir  is a spooling directory source that monitors a spooling directory for new files; a  logger  sink is a sink for logging events to the console. Source and sink must be connected to channel( agent1.sources.source1.channels = channel1 ).     # create the spooling directory on the local filesystem: \n$ mkdir /tmp/spooldir # start the Flume agent using the flume-ng command: \n$ flume-ng agent  \\   --conf-file spool-to-logger.properties  \\   --name agent1  \\   --conf  $F LUME_HOME/conf  \\   -Dflume.root.logger = INFO,console # on another terminal, create a file in the spooling directory. \n$  echo   Hello Flume    /tmp/spooldir/.file1.txt\n$  ~ mv /tmp/spooldir/.file1.txt /tmp/spooldir/file1.txt", 
            "title": "1. An Example"
        }, 
        {
            "location": "/hadoop/ch14/#2-transactions-and-reliability", 
            "text": "Flume uses separate transactions to guarantee delivery from the source to the channel and from the channel to the sink. In the example in the previous section, the spooling directory source creates an event for each line in the file. The source will only mark the file as completed once the transactions encapsulating the delivery of the events to the channel have been successfully committed.   Similarly, a transaction is used for the delivery of the events from the channel to the sink. If for some unlikely reason the events could not be logged, the transaction would be rolled back and the events would remain in the channel for later redelivery.", 
            "title": "2. Transactions and Reliability"
        }, 
        {
            "location": "/hadoop/ch14/#3-the-hdfs-sink", 
            "text": "Events may delivered to the HDFS sink and written to a file. Files in the process of being written to have a  .tmp  in-use suffix (default, set by  hdfs.inUsePrefix , see below) added to their name to indicate that they are not yet complete.  Flume configuration using a spooling directory source and an HDFS sink:    agent1.sources = source1 \nagent1.sinks = sink1 \nagent1.channels = channel1\nagent1.sources.source1.channels = channel1\nagent1.sinks.sink1.channel = channel1\nagent1.sources.source1.type = spooldir\nagent1.sources.source1.spoolDir = /tmp/spooldir\nagent1.sinks.sink1.type = hdfs\nagent1.sinks.sink1.hdfs.path = /tmp/flume\nagent1.sinks.sink1.hdfs.filePrefix = events\nagent1.sinks.sink1.hdfs.fileSuffix = .log \nagent1.sinks.sink1.hdfs.inUsePrefix = _ \nagent1.sinks.sink1.hdfs.fileType = DataStream\nagent1.channels.channel1.type = file", 
            "title": "3. The HDFS Sink"
        }, 
        {
            "location": "/hadoop/ch14/#partitioning-and-interceptors", 
            "text": "", 
            "title": "Partitioning and Interceptors"
        }, 
        {
            "location": "/hadoop/ch14/#4-fan-out", 
            "text": "Fan out  is the term for delivering events from one source to multiple channels, so they reach multiple sinks.  Flume configuration using a spooling directory source, fanning out to an HDFS sink and a logger sink:    agent1.sources = source1 \nagent1.sinks = sink1a sink1b \nagent1.channels = channel1a channel1b\nagent1.sources.source1.channels = channel1a channel1b \nagent1.sinks.sink1a.channel = channel1a \nagent1.sinks.sink1b.channel = channel1b\nagent1.sources.source1.type = spooldir \nagent1.sources.source1.spoolDir = /tmp/spooldir\nagent1.sinks.sink1a.type = hdfs \nagent1.sinks.sink1a.hdfs.path = /tmp/flume \nagent1.sinks.sink1a.hdfs.filePrefix = events \nagent1.sinks.sink1a.hdfs.fileSuffix = .log \nagent1.sinks.sink1a.hdfs.fileType = DataStream\nagent1.sinks.sink1b.type = logger\nagent1.channels.channel1a.type = file agent1.channels.channel1b.type = memory", 
            "title": "4 Fan Out"
        }, 
        {
            "location": "/hadoop/ch14/#delivery-guarantees", 
            "text": "Flume uses a separate transaction to deliver each batch of events from the spooling directory source to each channel. If either of these transactions fails (if a channel is full, for example), then the events will not be removed from the source, and will be retried later.", 
            "title": "Delivery Guarantees"
        }, 
        {
            "location": "/hadoop/ch14/#3-distribution-agent-tiers", 
            "text": "Aggregating Flume events is achieved by having tiers of Flume agents.   The first tier collects events from the original sources (such as web servers) and sends them to a smaller set of agents in the second tier, which aggregate events from the first tier before writing them to HDFS. Further tiers may be warranted for very large numbers of source nodes.", 
            "title": "3 Distribution: Agent Tiers"
        }, 
        {
            "location": "/hadoop/ch14/#4-sink-groups", 
            "text": "A  sink group  allows multiple sinks to be treated as one, for failover(\u6545\u969c\u8f6c\u79fb) or load-balancing purposes. If a second-tier agent is unavailable, then events will be delivered to another second-tier agent and on to HDFS without disruption.", 
            "title": "4 Sink Groups"
        }, 
        {
            "location": "/hadoop/ch14/#5-useful-resources", 
            "text": "Flume main page   Flume user guide  Flume Getting Started guide", 
            "title": "5 Useful resources"
        }, 
        {
            "location": "/hadoop/ch15/", 
            "text": "Hadoop: The Definitive Guide 15 - Sqoop", 
            "title": "Chapter 15: Sqoop"
        }, 
        {
            "location": "/hadoop/ch15/#hadoop-the-definitive-guide-15-sqoop", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 15 - Sqoop"
        }, 
        {
            "location": "/hadoop/ch16/", 
            "text": "Hadoop: The Definitive Guide 16 - Pig", 
            "title": "Chapter 16: Pig"
        }, 
        {
            "location": "/hadoop/ch16/#hadoop-the-definitive-guide-16-pig", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 16 - Pig"
        }, 
        {
            "location": "/hadoop/ch17/", 
            "text": "Hadoop: The Definitive Guide 17 - Hive\n\n\nHive is a data warehousing system to store structured data on Hadoop file system. It provide an easy query these data by execution Hadoop MapReduce plans\n1\n.\n\n\n\n\n2 Running Hive\n\n\nHive Services\n\n\nHive clients\n\n\n\n\n\n\nHive project page\n\n\nGetting Started", 
            "title": "Chapter 17: Hive"
        }, 
        {
            "location": "/hadoop/ch17/#hadoop-the-definitive-guide-17-hive", 
            "text": "Hive is a data warehousing system to store structured data on Hadoop file system. It provide an easy query these data by execution Hadoop MapReduce plans 1 .", 
            "title": "Hadoop: The Definitive Guide 17 - Hive"
        }, 
        {
            "location": "/hadoop/ch17/#2-running-hive", 
            "text": "", 
            "title": "2 Running Hive"
        }, 
        {
            "location": "/hadoop/ch17/#hive-services", 
            "text": "Hive clients    Hive project page  Getting Started", 
            "title": "Hive Services"
        }, 
        {
            "location": "/hadoop/ch18/", 
            "text": "Hadoop: The Definitive Guide 18 - Crunch", 
            "title": "Chapter 18: Crunch"
        }, 
        {
            "location": "/hadoop/ch18/#hadoop-the-definitive-guide-18-crunch", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 18 - Crunch"
        }, 
        {
            "location": "/hadoop/ch19/", 
            "text": "Hadoop: The Definitive Guide 19 - Spark", 
            "title": "Chapter 19: Spark"
        }, 
        {
            "location": "/hadoop/ch19/#hadoop-the-definitive-guide-19-spark", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 19 - Spark"
        }, 
        {
            "location": "/hadoop/ch20/", 
            "text": "Hadoop: The Definitive Guide 20 - HBase\n\n\nHBase is a distributed column-oriented database built on top of HDFS. HBase is the Hadoop application to use when you require \nreal-time\n read/write \nrandom\n access to very large datasets.\n\n\nHBase is built to scale linearly just by adding nodes. It can host very large, sparsely populated tables on clusters made from commodity hardware.", 
            "title": "Chapter 20: HBase"
        }, 
        {
            "location": "/hadoop/ch20/#hadoop-the-definitive-guide-20-hbase", 
            "text": "HBase is a distributed column-oriented database built on top of HDFS. HBase is the Hadoop application to use when you require  real-time  read/write  random  access to very large datasets.  HBase is built to scale linearly just by adding nodes. It can host very large, sparsely populated tables on clusters made from commodity hardware.", 
            "title": "Hadoop: The Definitive Guide 20 - HBase"
        }, 
        {
            "location": "/hadoop/ch21/", 
            "text": "Hadoop: The Definitive Guide 21 - ZooKeeper\n\n\nResources\n\n\n\n\nApache ZooKeeper\n\n\nZooKeeper Wiki\n\n\nZooKeeper Getting Started Guide", 
            "title": "Chapter 21: ZooKeeper"
        }, 
        {
            "location": "/hadoop/ch21/#hadoop-the-definitive-guide-21-zookeeper", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 21 - ZooKeeper"
        }, 
        {
            "location": "/hadoop/ch21/#resources", 
            "text": "Apache ZooKeeper  ZooKeeper Wiki  ZooKeeper Getting Started Guide", 
            "title": "Resources"
        }, 
        {
            "location": "/hadoop/ch22/", 
            "text": "Hadoop: The Definitive Guide 22 - Composable Data at Center", 
            "title": "Chapter 22: Composable Data at Center"
        }, 
        {
            "location": "/hadoop/ch22/#hadoop-the-definitive-guide-22-composable-data-at-center", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 22 - Composable Data at Center"
        }, 
        {
            "location": "/hadoop/ch23/", 
            "text": "Hadoop: The Definitive Guide 23 - Biological Data Science: Saving Lives with Software", 
            "title": "Chapter 23: Biological Data Science: Saving Lives with Software"
        }, 
        {
            "location": "/hadoop/ch23/#hadoop-the-definitive-guide-23-biological-data-science-saving-lives-with-software", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 23 - Biological Data Science: Saving Lives with Software"
        }, 
        {
            "location": "/hadoop/ch24/", 
            "text": "Hadoop: The Definitive Guide 24 - Cascading", 
            "title": "Chapter 24: Cascading"
        }, 
        {
            "location": "/hadoop/ch24/#hadoop-the-definitive-guide-24-cascading", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 24 - Cascading"
        }, 
        {
            "location": "/spark/", 
            "text": "Spark\n\n\n\n\nChapter 1: Introduction to Data Analysis with Spark\n\n\nChapter 2: Downloading Spark and Getting Started\n\n\nChapter 3: Programming with RDDs\n\n\nChapter 4: Working with Key/Value Pairs\n\n\nChapter 5: Loading and Saving Your Data\n\n\nChapter 6: Advanced Spark Programming\n\n\nChapter 7: Running on a Cluster\n\n\nChapter 8: Tuning and Debugging Spark\n\n\nChapter 9: Spark SQL\n\n\nChapter 10: Spark Streaming\n\n\nChapter 11: Machine Learning with MLlib", 
            "title": "Contents"
        }, 
        {
            "location": "/spark/#spark", 
            "text": "Chapter 1: Introduction to Data Analysis with Spark  Chapter 2: Downloading Spark and Getting Started  Chapter 3: Programming with RDDs  Chapter 4: Working with Key/Value Pairs  Chapter 5: Loading and Saving Your Data  Chapter 6: Advanced Spark Programming  Chapter 7: Running on a Cluster  Chapter 8: Tuning and Debugging Spark  Chapter 9: Spark SQL  Chapter 10: Spark Streaming  Chapter 11: Machine Learning with MLlib", 
            "title": "Spark"
        }, 
        {
            "location": "/spark/ch1/", 
            "text": "Learning Spark  1 - Introduction to Data Analysis with Spark", 
            "title": "Chapter 1: Introduction to Data Analysis with Spark"
        }, 
        {
            "location": "/spark/ch1/#learning-spark-1-introduction-to-data-analysis-with-spark", 
            "text": "", 
            "title": "Learning Spark  1 - Introduction to Data Analysis with Spark"
        }, 
        {
            "location": "/spark/ch2/", 
            "text": "Learning Spark 2 - Downloading Spark and Getting Started", 
            "title": "Chapter 2: Downloading Spark and Getting Started"
        }, 
        {
            "location": "/spark/ch2/#learning-spark-2-downloading-spark-and-getting-started", 
            "text": "", 
            "title": "Learning Spark 2 - Downloading Spark and Getting Started"
        }, 
        {
            "location": "/spark/ch3/", 
            "text": "Learning Spark 3 - Programming with RDDs", 
            "title": "Chapter 3: Programming with RDDs"
        }, 
        {
            "location": "/spark/ch3/#learning-spark-3-programming-with-rdds", 
            "text": "", 
            "title": "Learning Spark 3 - Programming with RDDs"
        }, 
        {
            "location": "/spark/ch4/", 
            "text": "Learning Spark 4 - Working with Key/Value Pairs", 
            "title": "Chapter 4: Working with Key/Value Pairs"
        }, 
        {
            "location": "/spark/ch4/#learning-spark-4-working-with-keyvalue-pairs", 
            "text": "", 
            "title": "Learning Spark 4 - Working with Key/Value Pairs"
        }, 
        {
            "location": "/spark/ch5/", 
            "text": "Learning Spark 5 - Loading and Saving Your Data", 
            "title": "Chapter 5: Loading and Saving Your Data"
        }, 
        {
            "location": "/spark/ch5/#learning-spark-5-loading-and-saving-your-data", 
            "text": "", 
            "title": "Learning Spark 5 - Loading and Saving Your Data"
        }, 
        {
            "location": "/spark/ch6/", 
            "text": "Learning Spark 6 - Advanced Spark Programming", 
            "title": "Chapter 6: Advanced Spark Programming"
        }, 
        {
            "location": "/spark/ch6/#learning-spark-6-advanced-spark-programming", 
            "text": "", 
            "title": "Learning Spark 6 - Advanced Spark Programming"
        }, 
        {
            "location": "/spark/ch7/", 
            "text": "Learning Spark  7 - Running on a Cluster", 
            "title": "Chapter 7: Running on a Cluster"
        }, 
        {
            "location": "/spark/ch7/#learning-spark-7-running-on-a-cluster", 
            "text": "", 
            "title": "Learning Spark  7 - Running on a Cluster"
        }, 
        {
            "location": "/spark/ch8/", 
            "text": "Learning Spark 8 - Tuning and Debugging Spark", 
            "title": "Chapter 8: Tuning and Debugging Spark"
        }, 
        {
            "location": "/spark/ch8/#learning-spark-8-tuning-and-debugging-spark", 
            "text": "", 
            "title": "Learning Spark 8 - Tuning and Debugging Spark"
        }, 
        {
            "location": "/spark/ch9/", 
            "text": "Learning Spark 9 - Spark SQL", 
            "title": "Chapter 9: Spark SQL"
        }, 
        {
            "location": "/spark/ch9/#learning-spark-9-spark-sql", 
            "text": "", 
            "title": "Learning Spark 9 - Spark SQL"
        }, 
        {
            "location": "/spark/ch10/", 
            "text": "Learning Spark 10 - Spark Streaming", 
            "title": "Chapter 10: Spark Streaming"
        }, 
        {
            "location": "/spark/ch10/#learning-spark-10-spark-streaming", 
            "text": "", 
            "title": "Learning Spark 10 - Spark Streaming"
        }, 
        {
            "location": "/spark/ch11/", 
            "text": "Learning Spark 11 - Machine Learning with MLlib", 
            "title": "Chapter 11: Machine Learning with MLlib"
        }, 
        {
            "location": "/spark/ch11/#learning-spark-11-machine-learning-with-mllib", 
            "text": "", 
            "title": "Learning Spark 11 - Machine Learning with MLlib"
        }, 
        {
            "location": "/gdm/", 
            "text": "GDM\n\n\n\n\nChapter 1: \u7b80\u4ecb\n\n\nChapter 2: \u63a8\u8350\u7cfb\u7edf\u5165\u95e8\n\n\nChapter 3: \u9690\u5f0f\u8bc4\u4ef7\u548c\u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\u7b97\u6cd5\n\n\nChapter 4: \u5206\u7c7b\n\n\nChapter 5: \u8fdb\u4e00\u6b65\u63a2\u7d22\u5206\u7c7b\n\n\nChapter 6: \u6982\u7387\u548c\u6734\u7d20\u8d1d\u53f6\u65af\n\n\nChapter 7: \u6734\u7d20\u8d1d\u53f6\u65af\u548c\u6587\u672c\u6570\u636e\n\n\nChapter 8: \u805a\u7c7b", 
            "title": "Contents"
        }, 
        {
            "location": "/gdm/#gdm", 
            "text": "Chapter 1: \u7b80\u4ecb  Chapter 2: \u63a8\u8350\u7cfb\u7edf\u5165\u95e8  Chapter 3: \u9690\u5f0f\u8bc4\u4ef7\u548c\u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\u7b97\u6cd5  Chapter 4: \u5206\u7c7b  Chapter 5: \u8fdb\u4e00\u6b65\u63a2\u7d22\u5206\u7c7b  Chapter 6: \u6982\u7387\u548c\u6734\u7d20\u8d1d\u53f6\u65af  Chapter 7: \u6734\u7d20\u8d1d\u53f6\u65af\u548c\u6587\u672c\u6570\u636e  Chapter 8: \u805a\u7c7b", 
            "title": "GDM"
        }, 
        {
            "location": "/gdm/ch1/", 
            "text": "Chapter 1: \u7b80\u4ecb", 
            "title": "Chapter 1: \u7b80\u4ecb"
        }, 
        {
            "location": "/gdm/ch2/", 
            "text": "Chapter 2: \u63a8\u8350\u7cfb\u7edf\u5165\u95e8\n*", 
            "title": "Chapter 2: \u63a8\u8350\u7cfb\u7edf\u5165\u95e8"
        }, 
        {
            "location": "/gdm/ch3/", 
            "text": "Chapter 3: \u9690\u5f0f\u8bc4\u4ef7\u548c\u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\u7b97\u6cd5", 
            "title": "Chapter 3: \u9690\u5f0f\u8bc4\u4ef7\u548c\u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\u7b97\u6cd5"
        }, 
        {
            "location": "/gdm/ch4/", 
            "text": "Chapter 4: \u5206\u7c7b", 
            "title": "Chapter 4: \u5206\u7c7b"
        }, 
        {
            "location": "/gdm/ch5/", 
            "text": "Chapter 5: \u8fdb\u4e00\u6b65\u63a2\u7d22\u5206\u7c7b", 
            "title": "Chapter 5: \u8fdb\u4e00\u6b65\u63a2\u7d22\u5206\u7c7b"
        }, 
        {
            "location": "/gdm/ch6/", 
            "text": "Chapter 6: \u6982\u7387\u548c\u6734\u7d20\u8d1d\u53f6\u65af", 
            "title": "Chapter 6: \u6982\u7387\u548c\u6734\u7d20\u8d1d\u53f6\u65af"
        }, 
        {
            "location": "/gdm/ch7/", 
            "text": "Chapter 7: \u6734\u7d20\u8d1d\u53f6\u65af\u548c\u6587\u672c\u6570\u636e", 
            "title": "Chapter 7: \u6734\u7d20\u8d1d\u53f6\u65af\u548c\u6587\u672c\u6570\u636e"
        }, 
        {
            "location": "/gdm/ch8/", 
            "text": "Chapter 8: \u805a\u7c7b", 
            "title": "Chapter 8: \u805a\u7c7b"
        }, 
        {
            "location": "/datamining/", 
            "text": "DataMining\n\n\n\n\n\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357", 
            "title": "Contents"
        }, 
        {
            "location": "/datamining/#datamining", 
            "text": "\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357", 
            "title": "DataMining"
        }, 
        {
            "location": "/datamining/guideToDataMining/", 
            "text": "\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357\n\n\n1 \u63a8\u8350\u7cfb\u7edf\u5165\u95e8\n\n\n\u672c\u7ae0\u5c06\u4ecb\u7ecd\u534f\u540c\u8fc7\u6ee4\uff0c\u57fa\u672c\u7684\u8ddd\u79bb\u7b97\u6cd5\uff0c\u6700\u540e\u4f7f\u7528Python\u5b9e\u73b0\u4e00\u4e2a\u7b80\u5355\u7684\u63a8\u8350\u7b97\u6cd5\u3002\n\n\n\u534f\u540c\u8fc7\u6ee4\uff0c\u987e\u540d\u601d\u4e49\uff0c\u662f\u5229\u7528\u4ed6\u4eba\u7684\u559c\u597d\u6765\u8fdb\u884c\u63a8\u8350\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u662f\u5927\u5bb6\u4e00\u8d77\u4ea7\u751f\u7684\u63a8\u8350\u3002\u5b83\u7684\u5de5\u4f5c\u539f\u7406\u662f\uff0c\u5728\u7f51\u7ad9\u4e0a\u67e5\u627e\u4e00\u4e2a\u548c\u4f60\u7c7b\u4f3c\u7684\u7528\u6237\uff0c\u7136\u540e\u5c06\u5b83\u559c\u6b22\u7684\u4e66\u7c4d\u63a8\u8350\u7ed9\u4f60\u3002\n\n\n\u5982\u4f55\u627e\u5230\u76f8\u4f3c\u7684\u7528\u6237\uff1f\n\n\n\u66fc\u54c8\u987f\u8ddd\u79bb\n\n\n\u987e\u540d\u601d\u4e49\uff0c\u5728\u66fc\u54c8\u987f\u8857\u533a\u8981\u4ece\u4e00\u4e2a\u5341\u5b57\u8def\u53e3\u5f00\u8f66\u5230\u53e6\u4e00\u4e2a\u5341\u5b57\u8def\u53e3\uff0c\u5b9e\u9645\u9a7e\u9a76\u8ddd\u79bb\u5c31\u662f\u201c\u66fc\u54c8\u987f\u8ddd\u79bb\u201d\u3002\n\n\n\n\n\u6700\u7b80\u5355\u7684\u8ddd\u79bb\u8ba1\u7b97\u65b9\u5f0f\u662f\u66fc\u54c8\u987f\u8ddd\u79bb\u3002\u5728\u4e8c\u7ef4\u6a21\u578b\u4e2d\uff0c\u6bcf\u4e2a\u4eba\u90fd\u53ef\u4ee5\u7528\n(x, y)\n(x, y)\n\u7684\u70b9\u6765\u8868\u793a\uff0c\u8fd9\u91cc\u7528\u4e0b\u6807\u6765\u8868\u793a\u4e0d\u540c\u7684\u4eba\uff0c\n(x_1, y_1)\n(x_1, y_1)\n\u8868\u793a\u827e\u7c73\uff0c\n(x_2, y_2)\n(x_2, y_2)\n\u8868\u793a\u795e\u79d8\u7684X\u5148\u751f\uff0c\u90a3\u4e48\u4ed6\u4eec\u4e4b\u95f4\u7684\u66fc\u54c8\u987f\u8ddd\u79bb\u5c31\u662f\uff1a\n\n\n\n\n|x_1-x_2|+|y_1-y_2|\n\n\n|x_1-x_2|+|y_1-y_2|\n\n\n\n\n\n\n\u66fc\u54c8\u987f\u8ddd\u79bb\u7684\u4f18\u70b9\u4e4b\u4e00\u662f\u8ba1\u7b97\u901f\u5ea6\u5feb\uff0c\u5bf9\u4e8eFacebook\u8fd9\u6837\u9700\u8981\u8ba1\u7b97\u767e\u4e07\u7528\u6237\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\u65f6\u5c31\u975e\u5e38\u6709\u5229\u3002\n\n\n \ndef\n \nmanhattan\n(\nrating1\n,\n \nrating2\n):\n\n    \nComputes the Manhattan distance. Both rating1 and rating2 are dictionaries\n\n\n       of the form {\nThe Strokes\n: 3.0, \nSlightly Stoopid\n: 2.5}\n\n    \ndistance\n \n=\n \n0\n\n    \ncommonRatings\n \n=\n \nFalse\n \n    \nfor\n \nkey\n \nin\n \nrating1\n:\n\n        \nif\n \nkey\n \nin\n \nrating2\n:\n\n            \ndistance\n \n+=\n \nabs\n(\nrating1\n[\nkey\n]\n \n-\n \nrating2\n[\nkey\n])\n\n            \ncommonRatings\n \n=\n \nTrue\n\n    \nif\n \ncommonRatings\n:\n\n        \nreturn\n \ndistance\n\n    \nelse\n:\n\n        \nreturn\n \n-\n1\n \n#Indicates no ratings in common\n\n\n\n\n\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\n\n\n\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5c31\u662f\u4e24\u70b9\u4e4b\u95f4\u7684\u76f4\u7ebf\u8ddd\u79bb\u3002 \u4e0b\u9762\u7684\u659c\u7ebf\u5c31\u662f\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\uff0c\u516c\u5f0f\u662f\uff1a\n\n\n\n\n\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}\n\n\n\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}\n\n\n\n\n\n\n\u66fc\u54c8\u987f\u8ddd\u79bb\u548c\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5728\u6570\u636e\u5b8c\u6574\u7684\u60c5\u51b5\u4e0b\u6548\u679c\u6700\u597d\u3002\n\n\n \ndef\n \neuclidean\n(\nrating1\n,\n \nrating2\n):\n\n    \n\n\n    Computes the Euclidean Distance\n\n\n    :param rating1: rating\n\n\n    :param rating2: rating\n\n\n    :return: distance if common ratings exists, or -1\n\n\n    \n\n    \ndistance\n \n=\n \n0\n\n    \ncommonRatings\n \n=\n \nFalse\n\n    \nfor\n \nkey\n \nin\n \nrating1\n:\n\n        \nif\n \nkey\n \nin\n \nrating2\n:\n\n            \ndistance\n \n+=\n \npow\n(\nrating1\n[\nkey\n]\n \n-\n \nrating2\n[\nkey\n],\n \n2\n)\n\n            \ncommonRatings\n \n=\n \nTrue\n\n\n    \nif\n \ncommonRatings\n:\n\n        \nreturn\n \ndistance\n\n    \nelse\n:\n\n        \nreturn\n \n-\n1\n  \n# Indicates no ratings in common\n\n\n\n\n\u95f5\u53ef\u592b\u65af\u57fa\u8ddd\u79bb\n\n\n\u6211\u4eec\u53ef\u4ee5\u5c06\u66fc\u54c8\u987f\u8ddd\u79bb\u548c\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5f52\u7eb3\u6210\u4e00\u4e2a\u516c\u5f0f\uff0c\u8fd9\u4e2a\u516c\u5f0f\u79f0\u4e3a\u95f5\u53ef\u592b\u65af\u57fa\u8ddd\u79bb(Minkowski Distance)\uff1a\n\n\n\n\nd(x,y) = (\\sum_{k=1}^{n}|x_k-y_k|^r)^{\\frac{1}{r}}\n\n\nd(x,y) = (\\sum_{k=1}^{n}|x_k-y_k|^r)^{\\frac{1}{r}}\n\n\n\n\n\u5176\u4e2d\uff1a\n\n\n\n\nr = 1\nr = 1\n, \u8be5\u516c\u5f0f\u5373\u66fc\u54c8\u987f\u8ddd\u79bb\n\n\nr = 2\nr = 2\n, \u8be5\u516c\u5f0f\u5373\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\n\n\nr = \\infty\nr = \\infty\n, \u5207\u6bd4\u96ea\u592b\u8ddd\u79bb\n\n\n\n\n\n\nNote\n\n\nr\nr\n\u503c\u8d8a\u5927\uff0c\u5355\u4e2a\u7ef4\u5ea6\u7684\u5dee\u503c\u5927\u5c0f\u4f1a\u5bf9\u6574\u4f53\u8ddd\u79bb\u6709\u66f4\u5927\u7684\u5f71\u54cd\u3002\n\n\n\n\n\u5207\u6bd4\u96ea\u592b\u8ddd\u79bb\n\n\n\u5207\u6bd4\u96ea\u592b\u8ddd\u79bb(Chebyshev Distance)\u662f\u5b9a\u4e49\u4e3a\u5176\u5404\u5750\u6807\u6570\u503c\u5dee\u7684\u6700\u5927\u503c\u3002\n\n\n\n\nD_{\\rm Chebyshev}(x,y) = \\max_i(|x_i - y_i|)=\\lim_{k \\to \\infty} \\bigg( \\sum_{i=1}^n \\left| x_i - y_i \\right|^k \\bigg)^{1/k}\n\n\nD_{\\rm Chebyshev}(x,y) = \\max_i(|x_i - y_i|)=\\lim_{k \\to \\infty} \\bigg( \\sum_{i=1}^n \\left| x_i - y_i \\right|^k \\bigg)^{1/k}\n\n\n\n\n \ndef\n \nchebyshev\n(\nrating1\n,\n \nrating2\n):\n\n    \n\n\n    Computes the Chebyshev Distance\n\n\n    :param rating1: rating\n\n\n    :param rating2: rating\n\n\n    :return: distance if common ratings exists, or -1\n\n\n    \n\n    \ndistance\n \n=\n \n0\n\n    \ncommonRatings\n \n=\n \nFalse\n\n    \nfor\n \nkey\n \nin\n \nrating1\n:\n\n        \nif\n \nkey\n \nin\n \nrating2\n:\n\n            \ndistance\n \n=\n \nmax\n(\ndistance\n,\n \nabs\n(\nrating1\n[\nkey\n]\n \n-\n \nrating2\n[\nkey\n]))\n\n            \ncommonRatings\n \n=\n \nTrue\n\n\n    \nif\n \ncommonRatings\n:\n\n        \nreturn\n \ndistance\n\n    \nelse\n:\n\n        \nreturn\n \n-\n1\n  \n# Indicates no ratings in common\n\n\n\n\n\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\n\n\n\u8ba9\u6211\u4eec\u4ed4\u7ec6\u770b\u770b\u7528\u6237\u5bf9\u4e50\u961f\u7684\u8bc4\u5206\uff0c\u53ef\u4ee5\u53d1\u73b0\u6bcf\u4e2a\u7528\u6237\u7684\u6253\u5206\u6807\u51c6\u975e\u5e38\u4e0d\u540c\uff1a\n\n\n\n\nBill\u6ca1\u6709\u6253\u51fa\u6781\u7aef\u7684\u5206\u6570\uff0c\u90fd\u57282\u81f34\u5206\u4e4b\u95f4\uff1b \n\n\nJordyn\u4f3c\u4e4e\u559c\u6b22\u6240\u6709\u7684\u4e50\u961f\uff0c\u6253\u5206\u90fd\u57284\u81f35\u4e4b\u95f4\uff1b \n\n\nHailey\u662f\u4e00\u4e2a\u6709\u8da3\u7684\u4eba\uff0c\u4ed6\u7684\u5206\u6570\u4e0d\u662f1\u5c31\u662f4\u3002\n\n\n\n\n\u90a3\u4e48\uff0c\u5982\u4f55\u6bd4\u8f83\u8fd9\u4e9b\u7528\u6237\u5462\uff1f\u6bd4\u5982Hailey\u76844\u5206\u76f8\u5f53\u4e8eJordan\u76844\u5206\u8fd8\u662f5\u5206\u5462\uff1f\u6211\u89c9\u5f97\u66f4\u63a5\u8fd15\u5206\u3002\u8fd9\u6837\u4e00\u6765\u5c31\u4f1a\u5f71\u54cd\u5230\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u4e86\u3002Clara\u6700\u4f4e\u7ed9\u4e864\u5206\u2014\u2014\u5979\u6240\u6709\u7684\u6253\u5206\u90fd\u57284\u81f35\u5206\u4e4b\u95f4\uff0c\u8fd9\u79cd\u73b0\u8c61\u5728\u6570\u636e\u6316\u6398\u9886\u57df\u79f0\u4e3a\n\u5206\u6570\u81a8\u80c0\n\u3002\n\n\n\n\n\u89e3\u51b3\u65b9\u6cd5\u4e4b\u4e00\u662f\u4f7f\u7528\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570, \u7528\u4e8e\u5ea6\u91cf\u4e24\u4e2a\u53d8\u91cfX\u548cY\u4e4b\u95f4\u7684\u76f8\u5173(\u7ebf\u6027\u76f8\u5173)\uff0c\u5176\u503c\u4ecb\u4e8e-1\u4e0e1\u4e4b\u95f4, 1\u8868\u793a\u5b8c\u5168\u543b\u5408\uff0c-1\u8868\u793a\u5b8c\u5168\u76f8\u6096\u3002\u4e0b\u9762\u662f\u5e38\u89c1\u7684\u51e0\u7ec4\n(x, y)\n(x, y)\n\u70b9\u96c6\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u3002\n\n\n\n\n\u4e24\u4e2a\u53d8\u91cf\u4e4b\u95f4\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u5b9a\u4e49\u4e3a\u4e24\u4e2a\u53d8\u91cf\u4e4b\u95f4\u7684\u534f\u65b9\u5dee(\n\\text{cov}(X,Y)\n\\text{cov}(X,Y)\n)\u548c\u6807\u51c6\u5dee(\n\\sigma_X\n\\sigma_X\n)\u7684\u5546\uff1a\n\n\n\n\n\\rho_{X,Y}={\\mathrm{cov}(X,Y) \\over \\sigma_X \\sigma_Y} ={E[(X-\\mu_X)(Y-\\mu_Y)] \\over \\sigma_X\\sigma_Y}\n\n\n\\rho_{X,Y}={\\mathrm{cov}(X,Y) \\over \\sigma_X \\sigma_Y} ={E[(X-\\mu_X)(Y-\\mu_Y)] \\over \\sigma_X\\sigma_Y}\n\n\n\n\n\u5bf9\u4e8e\u6837\u672c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570:\n\n\n\n\nr_{xy}=\\frac{\\sum x_iy_i-n \\bar{x} \\bar{y}}{(n-1) s_x s_y}=\\frac{n\\sum x_iy_i-\\sum x_i\\sum y_i}{\\sqrt{n\\sum x_i^2-(\\sum x_i)^2}~\\sqrt{n\\sum y_i^2-(\\sum y_i)^2}}.\n\n\nr_{xy}=\\frac{\\sum x_iy_i-n \\bar{x} \\bar{y}}{(n-1) s_x s_y}=\\frac{n\\sum x_iy_i-\\sum x_i\\sum y_i}{\\sqrt{n\\sum x_i^2-(\\sum x_i)^2}~\\sqrt{n\\sum y_i^2-(\\sum y_i)^2}}.\n\n\n\n\n\u4ee5\u4e0a\u65b9\u7a0b\u7ed9\u51fa\u4e86\u8ba1\u7b97\u6837\u672c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u7b80\u5355\u7684\u5355\u6d41\u7a0b\u7b97\u6cd5\uff0c\u4f46\u662f\u5176\u4f9d\u8d56\u4e8e\u6d89\u53ca\u5230\u7684\u6570\u636e\uff0c\u6709\u65f6\u5b83\u53ef\u80fd\u662f\u6570\u503c\u4e0d\u7a33\u5b9a\u7684\u3002\u4f46\u5b83\u6700\u5927\u7684\u4f18\u70b9\u662f\uff0c\u7528\u4ee3\u7801\u5b9e\u73b0\u7684\u65f6\u5019\u53ef\u4ee5\u53ea\u904d\u5386\u4e00\u6b21\u6570\u636e\u3002\n\n\n \n    \ndef\n \npearson\n(\nrating1\n,\n \nrating2\n):\n\n        \n\n\n        Compute pearson coefficient\n\n\n        :param rating1: a dictionary\n\n\n        :param rating2: a dictionary\n\n\n        :return: pearson coefficient\n\n\n        \n\n        \nsum_xy\n \n=\n \n0\n\n        \nsum_x\n \n=\n \n0\n\n        \nsum_y\n \n=\n \n0\n\n        \nsum_x2\n \n=\n \n0\n\n        \nsum_y2\n \n=\n \n0\n\n        \nn\n \n=\n \n0\n\n        \ncommonRatings\n \n=\n \nFalse\n\n        \nfor\n \nkey\n \nin\n \nrating1\n:\n\n            \nif\n \nkey\n \nin\n \nrating2\n:\n\n                \nn\n \n+=\n \n1\n\n                \nx\n \n=\n \nrating1\n[\nkey\n]\n\n                \ny\n \n=\n \nrating2\n[\nkey\n]\n\n                \nsum_xy\n \n+=\n \nx\n \n*\n \ny\n\n                \nsum_x\n \n+=\n \nx\n\n                \nsum_y\n \n+=\n \ny\n\n                \nsum_x2\n \n+=\n \npow\n(\nx\n,\n \n2\n)\n\n                \nsum_y2\n \n+=\n \npow\n(\ny\n,\n \n2\n)\n\n                \ncommonRatings\n \n=\n \nTrue\n\n\n        \nif\n \nnot\n \ncommonRatings\n:\n\n            \nreturn\n \n-\n1\n\n        \n# now compute denominator\n\n        \ndenominator\n \n=\n \nmath\n.\nsqrt\n(\nsum_x2\n \n-\n \npow\n(\nsum_x\n,\n \n2\n)\n \n/\n \nn\n)\n \n*\n \nmath\n.\nsqrt\n(\nsum_y2\n \n-\n \npow\n(\nsum_y\n,\n \n2\n)\n \n/\n \nn\n)\n\n        \nif\n \ndenominator\n \n==\n \n0\n:\n\n            \nreturn\n \n0\n\n        \nelse\n:\n\n            \nreturn\n \n(\nsum_xy\n \n-\n \n(\nsum_x\n \n*\n \nsum_y\n)\n \n/\n \nn\n)\n \n/\n \ndenominator\n\n\n\n\n\u4f59\u5f26\u76f8\u4f3c\u5ea6\n\n\n\u5f53\u6211\u4eec\u75281500\u4e07\u9996\u6b4c\u66f2\u6765\u6bd4\u8f83\u4e24\u4e2a\u7528\u6237\u65f6\uff0c\u5f88\u6709\u53ef\u80fd\u4ed6\u4eec\u4e4b\u95f4\u6ca1\u6709\u4efb\u4f55\u4ea4\u96c6\uff0c\u8fd9\u6837\u4e00\u6765\u5c31\u65e0\u4ece\u8ba1\u7b97\u4ed6\u4eec\u4e4b\u95f4\u7684\u8ddd\u79bb\u4e86\u3002\u7c7b\u4f3c\u7684\u60c5\u51b5\u662f\u5728\u8ba1\u7b97\u4e24\u7bc7\u6587\u7ae0\u7684\u76f8\u4f3c\u5ea6\u65f6\u3002\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u8ba1\u7b97\u4e2d\u4f1a\u7565\u8fc7\u8fd9\u4e9b\u975e\u96f6\u503c\u3002\u5b83\u7684\u8ba1\u7b97\u516c\u5f0f\u662f\uff1a\n\n\n\n\n\\cos(x,y) = \\frac{x\\cdot y}{||x||\\times||y||}\n\n\n\\cos(x,y) = \\frac{x\\cdot y}{||x||\\times||y||}\n\n\n\n\n\u5176\u4e2d\uff0c\n\\cdot\n\\cdot\n \u53f7\u8868\u793a\u6570\u91cf\u79ef\u3002\n||x||\n||x||\n\u8868\u793a\u5411\u91cf\nx\nx\n\u7684\u6a21\u3002\n\n\n\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5728\u6587\u672c\u6316\u6398\u4e2d\u5e94\u7528\u5f97\u8f83\u591a\uff0c\u5728\u534f\u540c\u8fc7\u6ee4\u4e2d\u4e5f\u4f1a\u4f7f\u7528\u5230\u3002\n\n\n\u5e94\u8be5\u4f7f\u7528\u54ea\u79cd\u76f8\u4f3c\u5ea6\uff1f\n\n\n\n\n\u5982\u679c\u6570\u636e\u5b58\u5728\u201c\u5206\u6570\u81a8\u80c0\u201d\u95ee\u9898\uff0c\u5c31\u4f7f\u7528\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u3002 \n\n\n\u5982\u679c\u6570\u636e\u6bd4\u8f83\u201c\u5bc6\u96c6\u201d\uff0c\u53d8\u91cf\u4e4b\u95f4\u57fa\u672c\u90fd\u5b58\u5728\u516c\u6709\u503c\uff0c\u4e14\u8fd9\u4e9b\u8ddd\u79bb\u6570\u636e\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u90a3\u5c31\u4f7f\u7528\u6b27\u51e0\u91cc\u5f97\u6216\u66fc\u54c8\u987f\u8ddd\u79bb\u3002\n\n\n\u5982\u679c\u6570\u636e\u662f\u7a00\u758f\u7684\uff0c\u5219\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3002\n\n\n\n\n\n\nNote\n\n\n\u5728\u6570\u636e\u6807\u51c6\u5316(\n\\mu=0,\\sigma=1\n\\mu=0,\\sigma=1\n\uff09\u540e\uff0cPearson\u76f8\u5173\u6027\u7cfb\u6570\u3001\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9\u53ef\u8ba4\u4e3a\u662f\u7b49\u4ef7\u7684[\n1\n]\u3002\n\n\n\n\nK\u6700\u90bb\u8fd1\u7b97\u6cd5\n\n\n\u4e0a\u9762\u7684\u505a\u6cd5\u4e2d\uff0c\u6211\u4eec\u53ea\u4f9d\u9760\u6700\u76f8\u4f3c\u7684\u4e00\u4e2a\u7528\u6237\u6765\u505a\u63a8\u8350\uff0c\u5982\u679c\u8fd9\u4e2a\u7528\u6237\u6709\u4e9b\u7279\u6b8a\u7684\u504f\u597d\uff0c\u5c31\u4f1a\u76f4\u63a5\u53cd\u6620\u5728\u63a8\u8350\u5185\u5bb9\u91cc\u3002\u89e3\u51b3\u65b9\u6cd5\u4e4b\u4e00\u662f\u627e\u5bfb\u591a\u4e2a\u76f8\u4f3c\u7684\u7528\u6237\uff0c\u8fd9\u91cc\u5c31\u8981\u7528\u5230K\u6700\u90bb\u8fd1\u7b97\u6cd5\u4e86\u3002\n\n\n\u5728\u534f\u540c\u8fc7\u6ee4\u4e2d\u53ef\u4ee5\u4f7f\u7528K\u6700\u90bb\u8fd1\u7b97\u6cd5\u6765\u627e\u51faK\u4e2a\u6700\u76f8\u4f3c\u7684\u7528\u6237\uff0c\u4ee5\u6b64\u4f5c\u4e3a\u63a8\u8350\u7684\u57fa\u7840\u3002\u4e0d\u540c\u7684 \u5e94\u7528\u6709\u4e0d\u540c\u7684K\u503c\uff0c\u9700\u8981\u505a\u4e00\u4e9b\u5b9e\u9a8c\u6765\u5f97\u51fa\u3002\u4ee5\u4e0b\u7ed9\u5230\u8bfb\u8005\u4e00\u4e2a\u57fa\u672c\u7684\u601d\u8def\u3002 \u5047\u8bbe\u6211\u8981\u4e3aAnn\u505a\u63a8\u8350\uff0c\u5e76\u4ee4K=3\u3002\u4f7f\u7528\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u5f97\u5230\u7684\u7ed3\u679c\u662f\uff1a\n\n\n\n\n\n\n\n\nPerson\n\n\nPearson\n\n\n\n\n\n\n\n\n\n\nSally\n\n\n0.8\n\n\n\n\n\n\nEric\n\n\n0.7\n\n\n\n\n\n\nAmanda\n\n\n0.5\n\n\n\n\n\n\n\n\n\u8fd9\u4e09\u4e2a\u4eba\u90fd\u4f1a\u5bf9\u63a8\u8350\u7ed3\u679c\u6709\u6240\u8d21\u732e\uff0c\u95ee\u9898\u5728\u4e8e\u6211\u4eec\u5982\u4f55\u786e\u5b9a\u4ed6\u4eec\u7684\u6bd4\u91cd\u5462\uff1f \u6211\u4eec\u76f4\u63a5\u7528\u76f8\u5173\u7cfb\u6570\u7684\u6bd4\u91cd\u6765\u63cf\u8ff0\uff0cSally\u7684\u6bd4\u91cd\u662f0.8/2=40%\uff0cEric\u662f0.7/2=35%\uff0cAmanda \u5219\u662f25%\uff1a\n\n\n\u5047\u8bbe\u4ed6\u4eec\u4e09\u4eba\u5bf9Grey Wardens\u7684\u8bc4\u5206\u4ee5\u53ca\u52a0\u6743\u540e\u7684\u7ed3\u679c\u5982\u4e0b\uff1a\n\n\n\n\n\n\n\n\nPerson\n\n\nGrey Wardens Rating\n\n\nInfluence\n\n\n\n\n\n\n\n\n\n\nSally\n\n\n4.5\n\n\n25%\n\n\n\n\n\n\nEric\n\n\n5\n\n\n35%\n\n\n\n\n\n\nAmanda\n\n\n3.5\n\n\n40%\n\n\n\n\n\n\n\n\n\u6700\u540e\u8ba1\u7b97\u5f97\u5230\u7684\u5206\u6570\u4e3a\u4e3a\u52a0\u6743\u548c \n4.5\\times 25\\% + 5\\times 35\\% + 3.5 \\times 40\\%\n4.5\\times 25\\% + 5\\times 35\\% + 3.5 \\times 40\\%\n\u3002\n\n\nPython\u63a8\u8350\u6a21\u5757\n\n\nCai-Nicolas Zeigler\u4ece\u56fe\u4e66\u6f02\u6d41\u7ad9\u6536\u96c6\u4e86\u8d85\u8fc7100\u4e07\u6761\u8bc4\u4ef7\u6570\u636e\u2014\u2014278,858\u4f4d\u7528\u6237\u4e3a271,379\u672c\u4e66\u6253\u4e86\u5206\u3002\u6570\u636e\u53ef\u4ee5\u4ece\u8fd9\u4e2a\n\u5730\u5740\n\u83b7\u5f97\u3002\n\n\nCSV\u6587\u4ef6\u5305\u542b\u4e86\u4e09\u5f20\u8868\uff1a\n\n\n\n\n\u7528\u6237\u8868\uff0c\u5305\u62ec\u7528\u6237ID\u3001\u4f4d\u7f6e\u3001\u5e74\u9f84\u7b49\u4fe1\u606f\u3002\u5176\u4e2d\u7528\u6237\u7684\u59d3\u540d\u5df2\u7ecf\u9690\u53bb\uff1b \n\n\n\u4e66\u7c4d\u8868\uff0c\u5305\u62ecISBN\u53f7\u3001\u6807\u9898\u3001\u4f5c\u8005\u3001\u51fa\u7248\u65e5\u671f\u3001\u51fa\u7248\u793e\u7b49\uff1b\n\n\n\u8bc4\u5206\u8868\uff0c\u5305\u62ec\u7528\u6237ID\u3001\u4e66\u7c4dISBN\u53f7\u3001\u4ee5\u53ca\u8bc4\u5206\uff080-10\u5206\uff09\u3002\n\n\n\n\nRecommender\nclass\n \nRecommender\n:\n\n\n    \ndef\n \n__init__\n(\nself\n,\n \nbooks\n,\n \nusers\n,\n \nuser_ratings\n,\n \nbook_ratings\n):\n\n        \n\n\n        initialize basic data\n\n\n        :param books: a dictionary of books, whose key is book id\n\n\n        :param users: a dictionary of users, whose key is user id\n\n\n        :param book_ratings: a dictionary of book ratings, whose key is book id\n\n\n        :param user_ratings: a dictionary of user ratings, whose key is user id\n\n\n        \n\n        \nself\n.\nbooks\n \n=\n \nbooks\n\n        \nself\n.\nusers\n \n=\n \nusers\n\n        \nself\n.\nbook_ratings\n \n=\n \nbook_ratings\n\n        \nself\n.\nuser_ratings\n \n=\n \nuser_ratings\n\n\n    \ndef\n \nrecommend\n(\nself\n,\n \nuser_to_recommend_int\n,\n \nk\n=\n1\n):\n\n        \n\n\n        Recommend user books\n\n\n        :param user_to_recommend_int: int, user id\n\n\n        :param k : int, for nearest k neighbors\n\n\n        :return: a list of books\n\n\n        \n\n        \nuser_to_recommend\n \n=\n \nstr\n(\nuser_to_recommend_int\n)\n\n        \nif\n \nuser_to_recommend\n \nnot\n \nin\n \nself\n.\nusers\n:\n\n            \nraise\n \nException\n(\nuser does not exist!!\n)\n\n\n        \n# find the user having min distances from user_to_recommend\n\n        \ndistances\n \n=\n \n[]\n\n        \nfind_user\n \n=\n \nFalse\n\n        \nfor\n \nuser\n \nin\n \nself\n.\nusers\n:\n\n            \nif\n \nuser_to_recommend\n \n==\n \nuser\n:\n\n                \ncontinue\n\n            \n# extract user ratings based on user ids, \n\n            \n# and compute the distance between them\n\n            \ndistance\n \n=\n \nDistance\n.\npearson\n(\nself\n.\nuser_ratings\n[\nuser_to_recommend\n],\n \n                 \nself\n.\nuser_ratings\n[\nuser\n])\n\n            \nif\n \ndistance\n \n!=\n \n-\n1\n:\n\n                \ndistances\n.\nappend\n([\nuser\n,\n \ndistance\n])\n\n                \nfind_user\n \n=\n \nTrue\n\n\n        \nif\n \nnot\n \nfind_user\n:\n\n            \nreturn\n \n[]\n\n\n        \n# sort user based on their distances\n\n        \n# pearson \u7cfb\u6570\u8d8a\u5927\uff0c\u8ddd\u79bb\u8d8a\u8fd1\uff0c\u6240\u4ee5\u7528reverse\n\n        \ndistances\n.\nsort\n(\nkey\n=\nlambda\n \nx\n:\n \nx\n[\n1\n],\n \nreverse\n=\nTrue\n)\n\n\n        \n# compute weight based on distances\n\n        \ndistances\n \n=\n \ndistances\n[\n0\n:\nk\n]\n\n        \nsum_distance\n \n=\n \nsum\n([\ndistance\n \nfor\n \nuser\n,\n \ndistance\n \nin\n \ndistances\n])\n\n        \nfor\n \ni\n \nin\n \nrange\n(\nlen\n(\ndistances\n)):\n\n            \ndistances\n[\ni\n][\n1\n]\n \n/=\n \nsum_distance\n\n\n        \n# recommend books\n\n        \nbooks_to_recommend\n \n=\n \n{}\n\n        \nfor\n \nuser_id\n,\n \nweight\n \nin\n \ndistances\n:\n\n            \nfor\n \nbook_id\n \nin\n \nself\n.\nuser_ratings\n[\nuser_id\n]:\n\n                    \nif\n \nbook_id\n \nnot\n \nin\n \nself\n.\nuser_ratings\n[\nuser_to_recommend\n]:\n  \n# the user haven\nt seen\n\n                        \nif\n \nbook_id\n \nnot\n \nin\n \nbooks_to_recommend\n:\n  \n# haven\nt recommend\n\n                            \nbooks_to_recommend\n[\nbook_id\n]\n \n=\n \n                                 \nself\n.\nuser_ratings\n[\nuser_id\n][\nbook_id\n]\n*\nweight\n\n                        \nelse\n:\n\n                            \nbooks_to_recommend\n[\nbook_id\n]\n \n=\n \nbooks_to_recommend\n[\nbook_id\n]\n \\\n                                \n+\n \nself\n.\nuser_ratings\n[\nuser_id\n][\nbook_id\n]\n*\nweight\n\n\n        \n# transform to a  list of tuple\n\n        \nbooks_to_recommend\n \n=\n \n[(\nbook_id\n,\n \nproject_rating\n)\n \n              \nfor\n \nbook_id\n,\n \nproject_rating\n \nin\n \nbooks_to_recommend\n.\nitems\n()]\n\n\n        \n# sort based on project_rating\n\n        \nbooks_to_recommend\n.\nsort\n(\nkey\n=\nlambda\n \nx\n:\n \nx\n[\n1\n],\n \nreverse\n=\nTrue\n)\n\n\n        \n# extract book title\n\n        \nbooks_to_recommend\n \n=\n \n[\nself\n.\nbooks\n[\nbook_id\n][\ntitle\n]\n\n               \nfor\n \nbook_id\n,\n \nproject_rating\n \nin\n \nbooks_to_recommend\n]\n\n        \nreturn\n \nbooks_to_recommend\n\n\n\n\nif\n \n__name__\n \n==\n \n__main__\n:\n\n    \nratings\n \n=\n \nBooksImport\n()\n\n    \nbooks\n,\n \nusers\n,\n \nuser_ratings\n,\n \nbook_ratings\n \n=\n \nratings\n.\nrecommender_import\n()\n\n    \ntest\n \n=\n \nRecommender\n(\nbooks\n,\n \nusers\n,\n \nuser_ratings\n,\n \nbook_ratings\n)\n\n    \nprint\n(\ntest\n.\nrecommend\n(\n171118\n))\n\n\nDistance\nimport\n \nmath\n\n\n\n\nclass\n \nDistance\n:\n\n    \n\n\n    Compute distance of two users, having different ratings.\n\n\n\n    Both rating1 and rating2 are\n\n\n    dictionaries of the form {\nThe Strokes\n: 3.0, \nSlightly Stoopid\n: 2.5}\n\n\n    \n\n\n    \ndef\n \n__init__\n(\nself\n):\n\n        \npass\n\n\n    \n@staticmethod\n\n    \ndef\n \nmanhattan\n(\nrating1\n,\n \nrating2\n):\n\n        \n\n\n        Computes the Manhattan distance.\n\n\n        \n\n        \ndistance\n \n=\n \n0\n\n        \ncommon_ratings\n \n=\n \nFalse\n\n        \nfor\n \nkey\n \nin\n \nrating1\n:\n\n            \nif\n \nkey\n \nin\n \nrating2\n:\n\n                \ndistance\n \n+=\n \nabs\n(\nrating1\n[\nkey\n]\n \n-\n \nrating2\n[\nkey\n])\n\n                \ncommon_ratings\n \n=\n \nTrue\n\n        \nif\n \ncommon_ratings\n:\n\n            \nreturn\n \ndistance\n\n        \nelse\n:\n\n            \nreturn\n \n-\n1\n  \n# Indicates no ratings in common\n\n\n    \n@staticmethod\n\n    \ndef\n \neuclidean\n(\nrating1\n,\n \nrating2\n):\n\n        \n\n\n        Computes the Euclidean Distance\n\n\n        :param rating1: rating\n\n\n        :param rating2: rating\n\n\n        :return: distance if common ratings exists, or -1\n\n\n        \n\n        \ndistance\n \n=\n \n0\n\n        \ncommonRatings\n \n=\n \nFalse\n\n        \nfor\n \nkey\n \nin\n \nrating1\n:\n\n            \nif\n \nkey\n \nin\n \nrating2\n:\n\n                \ndistance\n \n+=\n \npow\n(\nrating1\n[\nkey\n]\n \n-\n \nrating2\n[\nkey\n],\n \n2\n)\n\n                \ncommonRatings\n \n=\n \nTrue\n\n\n        \nif\n \ncommonRatings\n:\n\n            \nreturn\n \ndistance\n\n        \nelse\n:\n\n            \nreturn\n \n-\n1\n  \n# Indicates no ratings in common\n\n\n    \n@staticmethod\n\n    \ndef\n \nchebyshev\n(\nrating1\n,\n \nrating2\n):\n\n        \n\n\n        Computes the Chebyshev Distance\n\n\n        :param rating1: rating\n\n\n        :param rating2: rating\n\n\n        :return: distance if common ratings exists, or -1\n\n\n        \n\n        \ndistance\n \n=\n \n0\n\n        \ncommonRatings\n \n=\n \nFalse\n\n        \nfor\n \nkey\n \nin\n \nrating1\n:\n\n            \nif\n \nkey\n \nin\n \nrating2\n:\n\n                \ndistance\n \n=\n \nmax\n(\ndistance\n,\n \nabs\n(\nrating1\n[\nkey\n]\n \n-\n \nrating2\n[\nkey\n]))\n\n                \ncommonRatings\n \n=\n \nTrue\n\n\n        \nif\n \ncommonRatings\n:\n\n            \nreturn\n \ndistance\n\n        \nelse\n:\n\n            \nreturn\n \n-\n1\n  \n# Indicates no ratings in common\n\n\n    \n@staticmethod\n\n    \ndef\n \npearson\n(\nrating1\n,\n \nrating2\n):\n\n        \n\n\n        Compute pearson coefficient\n\n\n        :param rating1: a dictionary\n\n\n        :param rating2: a dictionary\n\n\n        :return: pearson coefficient\n\n\n        \n\n        \nsum_xy\n \n=\n \n0\n\n        \nsum_x\n \n=\n \n0\n\n        \nsum_y\n \n=\n \n0\n\n        \nsum_x2\n \n=\n \n0\n\n        \nsum_y2\n \n=\n \n0\n\n        \nn\n \n=\n \n0\n\n        \ncommonRatings\n \n=\n \nFalse\n\n        \nfor\n \nkey\n \nin\n \nrating1\n:\n\n            \nif\n \nkey\n \nin\n \nrating2\n:\n\n                \nn\n \n+=\n \n1\n\n                \nx\n \n=\n \nrating1\n[\nkey\n]\n\n                \ny\n \n=\n \nrating2\n[\nkey\n]\n\n                \nsum_xy\n \n+=\n \nx\n \n*\n \ny\n\n                \nsum_x\n \n+=\n \nx\n\n                \nsum_y\n \n+=\n \ny\n\n                \nsum_x2\n \n+=\n \npow\n(\nx\n,\n \n2\n)\n\n                \nsum_y2\n \n+=\n \npow\n(\ny\n,\n \n2\n)\n\n                \ncommonRatings\n \n=\n \nTrue\n\n\n        \nif\n \nnot\n \ncommonRatings\n:\n\n            \nreturn\n \n-\n1\n\n        \n# now compute denominator\n\n        \ndenominator\n \n=\n \nmath\n.\nsqrt\n(\nsum_x2\n \n-\n \npow\n(\nsum_x\n,\n \n2\n)\n \n/\n \nn\n)\n\\ \n            \n*\n \nmath\n.\nsqrt\n(\nsum_y2\n \n-\n \npow\n(\nsum_y\n,\n \n2\n)\n \n/\n \nn\n)\n\n        \nif\n \ndenominator\n \n==\n \n0\n:\n\n            \nreturn\n \n0\n\n        \nelse\n:\n\n            \nreturn\n \n(\nsum_xy\n \n-\n \n(\nsum_x\n \n*\n \nsum_y\n)\n \n/\n \nn\n)\n \n/\n \ndenominator\n\n\nBooks_import\nclass\n \nBooksImport\n:\n\n\n    \ndef\n \n__init__\n(\nself\n):\n\n        \nself\n.\nbooks\n \n=\n \n{}\n\n        \nself\n.\nusers\n \n=\n \n{}\n\n        \nself\n.\nbook_ratings\n \n=\n \n{}\n\n        \nself\n.\nuser_ratings\n \n=\n \n{}\n\n        \nself\n.\nbx_books_import\n()\n\n        \nself\n.\nbx_users_import\n()\n\n        \nself\n.\nbx_ratings_import\n()\n\n\n    \ndef\n \nbx_books_import\n(\nself\n):\n\n        \n\n\n        import books meta information\n\n\n        \n\n\n        \ntry\n:\n\n            \nbooksfile\n \n=\n \ncodecs\n.\nopen\n(\nBX-Dump/BX-Books.csv\n,\n \nr\n,\n \nutf-8\n)\n\n\n            \nfor\n \nline\n \nin\n \nbooksfile\n:\n\n                \nprops\n \n=\n \nline\n.\nsplit\n(\n;\n)\n\n                \nisbn\n \n=\n \nprops\n[\n0\n]\n.\nstrip\n(\n)\n\n                \ntitle\n \n=\n \nprops\n[\n1\n]\n.\nstrip\n(\n)\n\n                \nauthor\n \n=\n \nprops\n[\n2\n]\n.\nstrip\n(\n)\n\n                \nyear\n \n=\n \nprops\n[\n3\n]\n.\nstrip\n(\n)\n\n                \nself\n.\nbooks\n[\nisbn\n]\n \n=\n \n{\ntitle\n:\n \ntitle\n,\n \nauthor\n:\n \nauthor\n,\n \nyear\n:\n \nyear\n}\n\n\n            \nbooksfile\n.\nclose\n()\n\n\n        \nexcept\n \nIOError\n \nas\n \ne\n:\n\n            \nerror\n \n=\n \nFailed to load: {0}\n.\nformat\n(\ne\n)\n\n            \nprint\n(\nerror\n)\n\n\n    \ndef\n \nbx_users_import\n(\nself\n):\n\n        \n\n\n        import user meta information\n\n\n        user is a dictionary, whose key is user_id\n\n\n        \n\n        \ntry\n:\n\n            \nusers_file\n \n=\n \ncodecs\n.\nopen\n(\nBX-Dump/BX-Users.csv\n,\n \nr\n,\n \nutf--8\n)\n\n            \nfor\n \nline\n \nin\n \nusers_file\n:\n\n                \nprops\n \n=\n \nline\n.\nsplit\n(\n;\n)\n\n                \nuser_id\n \n=\n \nprops\n[\n0\n]\n.\nstrip\n(\n)\n\n                \nlocation\n \n=\n \nprops\n[\n1\n]\n.\nstrip\n(\n)\n\n                \nself\n.\nusers\n[\nuser_id\n]\n \n=\n \nlocation\n\n                \nself\n.\nuser_ratings\n[\nuser_id\n]\n \n=\n \n{}\n\n            \nusers_file\n.\nclose\n()\n\n\n        \nexcept\n \nIOError\n \nas\n \ne\n:\n\n            \nerror\n \n=\n \nFailed to load: {0}\n.\nformat\n(\ne\n)\n\n            \nprint\n(\nerror\n)\n\n\n    \ndef\n \nbx_ratings_import\n(\nself\n):\n\n        \ntry\n:\n\n            \nratings_file\n \n=\n \ncodecs\n.\nopen\n(\nBX-Dump/BX-Book-Ratings.csv\n,\n \nr\n,\n \nutf--8\n)\n\n            \nfor\n \nline\n \nin\n \nratings_file\n:\n\n                \nprops\n \n=\n \nline\n.\nsplit\n(\n;\n)\n\n                \nuser_id\n \n=\n \nprops\n[\n0\n]\n.\nstrip\n(\n)\n\n                \nbook_id\n \n=\n \nprops\n[\n1\n]\n.\nstrip\n(\n)\n\n                \nrating\n \n=\n \nint\n(\nprops\n[\n2\n]\n.\nstrip\n()\n.\nstrip\n(\n))\n\n\n                \nif\n \nbook_id\n \nin\n \nself\n.\nbook_ratings\n:\n\n                    \nself\n.\nbook_ratings\n[\nbook_id\n]\n.\nappend\n(\nrating\n)\n\n                \nelse\n:\n\n                    \nself\n.\nbook_ratings\n[\nbook_id\n]\n \n=\n \n[\nrating\n]\n\n\n                \nself\n.\nuser_ratings\n[\nuser_id\n][\nbook_id\n]\n \n=\n \nrating\n\n\n            \nratings_file\n.\nclose\n()\n\n\n        \nexcept\n \nIOError\n \nas\n \ne\n:\n\n            \nerror\n \n=\n \nFailed to load: {0}\n.\nformat\n(\ne\n)\n\n            \nprint\n(\nerror\n)\n\n\n    \ndef\n \nget_books\n(\nself\n):\n\n        \nreturn\n \nself\n.\nbooks\n\n\n    \ndef\n \nget_users\n(\nself\n):\n\n        \nreturn\n \nself\n.\nusers\n\n\n    \ndef\n \nget_user_ratings\n(\nself\n):\n\n        \nreturn\n \nself\n.\nuser_ratings\n\n\n    \ndef\n \nget_book_ratings\n(\nself\n):\n\n        \nreturn\n \nself\n.\nbook_ratings\n\n\n    \ndef\n \nrecommender_import\n(\nself\n):\n\n        \nreturn\n \nself\n.\nbooks\n,\n \nself\n.\nusers\n,\n \nself\n.\nuser_ratings\n,\n \nself\n.\nbook_ratings\n\n\n\n\n\n2 \u9690\u5f0f\u8bc4\u4ef7\u548c\u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\u7b97\u6cd5\n\n\n\u9690\u5f0f\u8bc4\u4ef7\n\n\n\u7528\u6237\u7684\u8bc4\u4ef7\u7c7b\u578b\u53ef\u4ee5\u5206\u4e3a\u663e\u5f0f\u8bc4\u4ef7\u548c\u9690\u5f0f\u8bc4\u4ef7\u3002\n\u663e\u5f0f\u8bc4\u4ef7\n\u6307\u7684\u662f\u7528\u6237\u660e\u786e\u5730\u7ed9\u51fa\u5bf9\u7269\u54c1\u7684\u8bc4\u4ef7\u3002\u6700\u5e38\u89c1\u7684\u4f8b\u5b50\u662fYouTube\u4e0a\u7684\u201c\u559c\u6b22\u201d\u548c\u201c\u4e0d\u559c\u6b22\u201d\u6309\u94ae\uff0c\u4ee5\u53ca\u4e9a\u9a6c\u900a\u8bc4\u8bba\u7684\u661f\u7ea7\u7cfb\u7edf\u3002\n\n\n\u9690\u5f0f\u8bc4\u4ef7\n\uff0c\u5c31\u662f\u6211\u4eec\u4e0d\u8ba9\u7528\u6237\u660e\u786e\u7ed9\u51fa\u5bf9\u7269\u54c1\u7684\u8bc4\u4ef7\uff0c\u800c\u662f\u901a\u8fc7\u89c2\u5bdf\u4ed6\u4eec\u7684\u884c\u4e3a\u6765\u83b7\u5f97\u504f\u597d\u4fe1\u606f\u3002\u793a\u4f8b\u4e4b\u4e00\u662f\u8bb0\u5f55\u7528\u6237\u5728\u7ebd\u7ea6\u65f6\u62a5\u7f51\u4e0a\u7684\u70b9\u51fb\u8bb0\u5f55\uff0c\u4e9a\u9a6c\u900a\u4e0a\u7528\u6237\u7684\u5b9e\u9645\u8d2d\u4e70\u8bb0\u5f55\n\n\n\u6211\u4eec\u53ef\u4ee5\u6536\u96c6\u5230\u54ea\u4e9b\u9690\u5f0f\u8bc4\u4ef7\u5462\uff1f \u7f51\u9875\u65b9\u9762\uff1a\u9875\u9762\u70b9\u51fb\u3001\u505c\u7559\u65f6\u95f4\u3001\u91cd\u590d\u8bbf\u95ee\u6b21\u6570\u3001\u5f15 \u7528\u7387\u3001Hulu\u4e0a\u89c2\u770b\u89c6\u9891\u7684\u6b21\u6570\uff1b \u97f3\u4e50\u64ad\u653e\u5668\uff1a\u64ad\u653e\u7684\u66f2\u76ee\u3001\u8df3\u8fc7\u7684\u66f2\u76ee\u3001\u64ad\u653e\u6b21\u6570\uff1b \u8fd9 \u4e9b\u53ea\u662f\u4e00\u5c0f\u90e8\u5206\uff01\n\n\n\u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\u7b97\u6cd5\n\n\n\u76ee\u524d\u4e3a\u6b62\u6211\u4eec\u63cf\u8ff0\u7684\u90fd\u662f\u57fa\u4e8e\u7528\u6237\u7684\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\uff1a\u5c06\u4e00\u4e2a\u7528\u6237\u548c\u5176\u4ed6\n\u6240\u6709\n\u7528\u6237\u8fdb\u884c\u5bf9\u6bd4\uff0c\u627e\u5230\u76f8\u4f3c\u7684\u4eba\u3002\u8fd9\u79cd\u7b97\u6cd5\u6709\u4e24\u4e2a\u5f0a\u7aef\uff1a\n\n\n\n\n\u6269\u5c55\u6027\n\uff1a\u968f\u7740\u7528\u6237\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5176\u8ba1\u7b97\u91cf\u4e5f\u4f1a\u589e\u52a0\u3002\u8fd9\u79cd\u7b97\u6cd5\u5728\u53ea\u6709\u51e0\u5343\u4e2a\u7528\u6237\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u5de5\u4f5c\u5f97\u5f88\u597d\uff0c\u4f46\u8fbe\u5230\u4e00\u767e\u4e07\u4e2a\u7528\u6237\u65f6\u5c31\u4f1a\u51fa\u73b0\u74f6\u9888\u3002\n\n\n\u7a00\u758f\u6027\n\uff1a\u5927\u591a\u6570\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u7269\u54c1\u7684\u6570\u91cf\u8981\u8fdc\u5927\u4e8e\u7528\u6237\u7684\u6570\u91cf\uff0c\u56e0\u6b64\u7528\u6237\u4ec5\u4ec5\u5bf9\u4e00\u5c0f\u90e8\u5206\u7269\u54c1\u8fdb\u884c\u4e86\u8bc4\u4ef7\uff0c\u8fd9\u5c31\u9020\u6210\u4e86\u6570\u636e\u7684\u7a00\u758f\u6027\u3002\u6bd4\u5982\u4e9a\u9a6c\u900a\u6709\u4e0a\u767e\u4e07\u672c\u4e66\uff0c\u4f46\u7528\u6237\u53ea\u8bc4\u8bba \u4e86\u5f88\u5c11\u4e00\u90e8\u5206\uff0c\u4e8e\u662f\u5c31\u5f88\u96be\u627e\u5230\u4e24\u4e2a\u76f8\u4f3c\u7684\u7528\u6237\u4e86\u3002\n\n\n\n\n\u4fee\u6b63\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\n\n\n\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6765\u8ba1\u7b97\u4e24\u4e2a\u7269\u54c1\u7684\u8ddd\u79bb\u3002\u7531\u4e8e\u201c\u5206\u6570\u81a8\u80c0\u201d\u73b0\u8c61\uff0c\u9700\u8981\u4ece\u7528\u6237\u7684\u8bc4\u4ef7\u4e2d\u51cf\u53bb\u4ed6\u6240\u6709\u8bc4\u4ef7\u7684\u5747\u503c\uff0c\u8fd9\u5c31\u662f\u4fee\u6b63\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3002\n\n\n\n\ns(i,j) =\\frac{\\sum_{u\\in U}(R_{u.i}-\\bar R_u)(R_{u,j}-\\bar R_u)}{\\sqrt{\\sum_{u\\in U}(R_{u,i}-\\bar R_u)^2}\\sqrt{\\sum_{u\\in U}(R_{u,j}-\\bar R_u)^2}}\n\n\ns(i,j) =\\frac{\\sum_{u\\in U}(R_{u.i}-\\bar R_u)(R_{u,j}-\\bar R_u)}{\\sqrt{\\sum_{u\\in U}(R_{u,i}-\\bar R_u)^2}\\sqrt{\\sum_{u\\in U}(R_{u,j}-\\bar R_u)^2}}\n\n\n\n\nU\nU\n\u8868\u793a\u540c\u65f6\u8bc4\u4ef7\u8fc7\u7269\u54c1\ni\ni\n\u548c\nj\nj\n\u7684\u7528\u6237\u96c6\u5408\uff0c \n\\bar R_u\n\\bar R_u\n\u8868\u793a\u7528\u6237\nu\nu\n\u5bf9\u6240\u6709\u7269\u54c1\u7684\u8bc4\u4ef7\u5747\u503c\uff0c\ns(i,j)\ns(i,j)\n\u8868\u793a\u7269\u54c1i\u548cj\u7684\u76f8\u4f3c\u5ea6\u3002\n\n\n \ndef\n \ncosinesimilarity\n(\nitem1\n,\n \nitem2\n,\n \nuserRatings\n):\n\n    \naverages\n \n=\n \n{}\n\n    \nfor\n \nitem\n,\n \nratings\n \nin\n \nuserRatings\n.\nitems\n():\n\n        \naverages\n[\nitem\n]\n \n=\n \n(\nfloat\n(\nsum\n(\nratings\n.\nvalues\n()))\n \n/\n \nlen\n(\nratings\n.\nvalues\n()))\n\n\n    \nnum\n \n=\n \n0\n    \n# \u5206\u5b50\n\n    \ndem1\n \n=\n \n0\n   \n# \u5206\u6bcd\u7684\u7b2c\u4e00\u90e8\u5206\n\n    \ndem2\n \n=\n \n0\n\n    \nfor\n \n(\nuser\n,\n \nratings\n)\n \nin\n \nuserRatings\n.\nitems\n():\n\n        \nif\n \nitem1\n \nin\n \nratings\n \nand\n \nitem2\n \nin\n \nratings\n:\n\n            \navg\n \n=\n \naverages\n[\nuser\n]\n\n            \nnum\n \n+=\n \n(\nratings\n[\nitem1\n]\n \n-\n \navg\n)\n \n*\n \n(\nratings\n[\nitem2\n]\n \n-\n \navg\n)\n\n            \ndem1\n \n+=\n \n(\nratings\n[\nitem1\n]\n \n-\n \navg\n)\n \n**\n \n2\n\n            \ndem2\n \n+=\n \n(\nratings\n[\nitem1\n]\n \n-\n \navg\n)\n \n**\n \n2\n\n    \nreturn\n \nnum\n \n/\n \n(\nmath\n.\nsqrt\n(\ndem1\n)\n \n*\n \nmath\n.\nsqrt\n(\ndem2\n))\n\n\n\n\nSlope One\u7b97\u6cd5\n\n\nSlope One\u662f\u4e00\u79cd\u6bd4\u8f83\u6d41\u884c\u7684\u57fa\u4e8e\u7269\u54c1\u7684\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\u3002\u5b83\u6700\u5927\u7684\u4f18\u52bf\u662f\u7b80\u5355\uff0c\u56e0\u6b64\u6613\u4e8e\u5b9e\u73b0\u3002\n\n\n3 \u5206\u7c7b\n\n\n4 \u8fdb\u4e00\u6b65\u63a2\u7d22\u5206\u7c7b\n\n\n5 \u6734\u7d20\u8d1d\u53f6\u65af\n\n\n6 \u6734\u7d20\u8d1d\u53f6\u65af\u7b97\u6cd5\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\n\n\n7 \u805a\u7c7b~~~~", 
            "title": "\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357"
        }, 
        {
            "location": "/datamining/guideToDataMining/#_1", 
            "text": "", 
            "title": "\u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357"
        }, 
        {
            "location": "/datamining/guideToDataMining/#1", 
            "text": "\u672c\u7ae0\u5c06\u4ecb\u7ecd\u534f\u540c\u8fc7\u6ee4\uff0c\u57fa\u672c\u7684\u8ddd\u79bb\u7b97\u6cd5\uff0c\u6700\u540e\u4f7f\u7528Python\u5b9e\u73b0\u4e00\u4e2a\u7b80\u5355\u7684\u63a8\u8350\u7b97\u6cd5\u3002  \u534f\u540c\u8fc7\u6ee4\uff0c\u987e\u540d\u601d\u4e49\uff0c\u662f\u5229\u7528\u4ed6\u4eba\u7684\u559c\u597d\u6765\u8fdb\u884c\u63a8\u8350\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u662f\u5927\u5bb6\u4e00\u8d77\u4ea7\u751f\u7684\u63a8\u8350\u3002\u5b83\u7684\u5de5\u4f5c\u539f\u7406\u662f\uff0c\u5728\u7f51\u7ad9\u4e0a\u67e5\u627e\u4e00\u4e2a\u548c\u4f60\u7c7b\u4f3c\u7684\u7528\u6237\uff0c\u7136\u540e\u5c06\u5b83\u559c\u6b22\u7684\u4e66\u7c4d\u63a8\u8350\u7ed9\u4f60\u3002  \u5982\u4f55\u627e\u5230\u76f8\u4f3c\u7684\u7528\u6237\uff1f", 
            "title": "1 \u63a8\u8350\u7cfb\u7edf\u5165\u95e8"
        }, 
        {
            "location": "/datamining/guideToDataMining/#_2", 
            "text": "\u987e\u540d\u601d\u4e49\uff0c\u5728\u66fc\u54c8\u987f\u8857\u533a\u8981\u4ece\u4e00\u4e2a\u5341\u5b57\u8def\u53e3\u5f00\u8f66\u5230\u53e6\u4e00\u4e2a\u5341\u5b57\u8def\u53e3\uff0c\u5b9e\u9645\u9a7e\u9a76\u8ddd\u79bb\u5c31\u662f\u201c\u66fc\u54c8\u987f\u8ddd\u79bb\u201d\u3002   \u6700\u7b80\u5355\u7684\u8ddd\u79bb\u8ba1\u7b97\u65b9\u5f0f\u662f\u66fc\u54c8\u987f\u8ddd\u79bb\u3002\u5728\u4e8c\u7ef4\u6a21\u578b\u4e2d\uff0c\u6bcf\u4e2a\u4eba\u90fd\u53ef\u4ee5\u7528 (x, y) (x, y) \u7684\u70b9\u6765\u8868\u793a\uff0c\u8fd9\u91cc\u7528\u4e0b\u6807\u6765\u8868\u793a\u4e0d\u540c\u7684\u4eba\uff0c (x_1, y_1) (x_1, y_1) \u8868\u793a\u827e\u7c73\uff0c (x_2, y_2) (x_2, y_2) \u8868\u793a\u795e\u79d8\u7684X\u5148\u751f\uff0c\u90a3\u4e48\u4ed6\u4eec\u4e4b\u95f4\u7684\u66fc\u54c8\u987f\u8ddd\u79bb\u5c31\u662f\uff1a   |x_1-x_2|+|y_1-y_2|  |x_1-x_2|+|y_1-y_2|    \u66fc\u54c8\u987f\u8ddd\u79bb\u7684\u4f18\u70b9\u4e4b\u4e00\u662f\u8ba1\u7b97\u901f\u5ea6\u5feb\uff0c\u5bf9\u4e8eFacebook\u8fd9\u6837\u9700\u8981\u8ba1\u7b97\u767e\u4e07\u7528\u6237\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\u65f6\u5c31\u975e\u5e38\u6709\u5229\u3002    def   manhattan ( rating1 ,   rating2 ): \n     Computes the Manhattan distance. Both rating1 and rating2 are dictionaries         of the form { The Strokes : 3.0,  Slightly Stoopid : 2.5} \n     distance   =   0 \n     commonRatings   =   False  \n     for   key   in   rating1 : \n         if   key   in   rating2 : \n             distance   +=   abs ( rating1 [ key ]   -   rating2 [ key ]) \n             commonRatings   =   True \n     if   commonRatings : \n         return   distance \n     else : \n         return   - 1   #Indicates no ratings in common", 
            "title": "\u66fc\u54c8\u987f\u8ddd\u79bb"
        }, 
        {
            "location": "/datamining/guideToDataMining/#_3", 
            "text": "\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5c31\u662f\u4e24\u70b9\u4e4b\u95f4\u7684\u76f4\u7ebf\u8ddd\u79bb\u3002 \u4e0b\u9762\u7684\u659c\u7ebf\u5c31\u662f\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\uff0c\u516c\u5f0f\u662f\uff1a   \\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}  \\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}    \u66fc\u54c8\u987f\u8ddd\u79bb\u548c\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5728\u6570\u636e\u5b8c\u6574\u7684\u60c5\u51b5\u4e0b\u6548\u679c\u6700\u597d\u3002    def   euclidean ( rating1 ,   rating2 ): \n          Computes the Euclidean Distance      :param rating1: rating      :param rating2: rating      :return: distance if common ratings exists, or -1       \n     distance   =   0 \n     commonRatings   =   False \n     for   key   in   rating1 : \n         if   key   in   rating2 : \n             distance   +=   pow ( rating1 [ key ]   -   rating2 [ key ],   2 ) \n             commonRatings   =   True \n\n     if   commonRatings : \n         return   distance \n     else : \n         return   - 1    # Indicates no ratings in common", 
            "title": "\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb"
        }, 
        {
            "location": "/datamining/guideToDataMining/#_4", 
            "text": "\u6211\u4eec\u53ef\u4ee5\u5c06\u66fc\u54c8\u987f\u8ddd\u79bb\u548c\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5f52\u7eb3\u6210\u4e00\u4e2a\u516c\u5f0f\uff0c\u8fd9\u4e2a\u516c\u5f0f\u79f0\u4e3a\u95f5\u53ef\u592b\u65af\u57fa\u8ddd\u79bb(Minkowski Distance)\uff1a   d(x,y) = (\\sum_{k=1}^{n}|x_k-y_k|^r)^{\\frac{1}{r}}  d(x,y) = (\\sum_{k=1}^{n}|x_k-y_k|^r)^{\\frac{1}{r}}   \u5176\u4e2d\uff1a   r = 1 r = 1 , \u8be5\u516c\u5f0f\u5373\u66fc\u54c8\u987f\u8ddd\u79bb  r = 2 r = 2 , \u8be5\u516c\u5f0f\u5373\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb  r = \\infty r = \\infty , \u5207\u6bd4\u96ea\u592b\u8ddd\u79bb    Note  r r \u503c\u8d8a\u5927\uff0c\u5355\u4e2a\u7ef4\u5ea6\u7684\u5dee\u503c\u5927\u5c0f\u4f1a\u5bf9\u6574\u4f53\u8ddd\u79bb\u6709\u66f4\u5927\u7684\u5f71\u54cd\u3002", 
            "title": "\u95f5\u53ef\u592b\u65af\u57fa\u8ddd\u79bb"
        }, 
        {
            "location": "/datamining/guideToDataMining/#_5", 
            "text": "\u5207\u6bd4\u96ea\u592b\u8ddd\u79bb(Chebyshev Distance)\u662f\u5b9a\u4e49\u4e3a\u5176\u5404\u5750\u6807\u6570\u503c\u5dee\u7684\u6700\u5927\u503c\u3002   D_{\\rm Chebyshev}(x,y) = \\max_i(|x_i - y_i|)=\\lim_{k \\to \\infty} \\bigg( \\sum_{i=1}^n \\left| x_i - y_i \\right|^k \\bigg)^{1/k}  D_{\\rm Chebyshev}(x,y) = \\max_i(|x_i - y_i|)=\\lim_{k \\to \\infty} \\bigg( \\sum_{i=1}^n \\left| x_i - y_i \\right|^k \\bigg)^{1/k}     def   chebyshev ( rating1 ,   rating2 ): \n          Computes the Chebyshev Distance      :param rating1: rating      :param rating2: rating      :return: distance if common ratings exists, or -1       \n     distance   =   0 \n     commonRatings   =   False \n     for   key   in   rating1 : \n         if   key   in   rating2 : \n             distance   =   max ( distance ,   abs ( rating1 [ key ]   -   rating2 [ key ])) \n             commonRatings   =   True \n\n     if   commonRatings : \n         return   distance \n     else : \n         return   - 1    # Indicates no ratings in common", 
            "title": "\u5207\u6bd4\u96ea\u592b\u8ddd\u79bb"
        }, 
        {
            "location": "/datamining/guideToDataMining/#_6", 
            "text": "\u8ba9\u6211\u4eec\u4ed4\u7ec6\u770b\u770b\u7528\u6237\u5bf9\u4e50\u961f\u7684\u8bc4\u5206\uff0c\u53ef\u4ee5\u53d1\u73b0\u6bcf\u4e2a\u7528\u6237\u7684\u6253\u5206\u6807\u51c6\u975e\u5e38\u4e0d\u540c\uff1a   Bill\u6ca1\u6709\u6253\u51fa\u6781\u7aef\u7684\u5206\u6570\uff0c\u90fd\u57282\u81f34\u5206\u4e4b\u95f4\uff1b   Jordyn\u4f3c\u4e4e\u559c\u6b22\u6240\u6709\u7684\u4e50\u961f\uff0c\u6253\u5206\u90fd\u57284\u81f35\u4e4b\u95f4\uff1b   Hailey\u662f\u4e00\u4e2a\u6709\u8da3\u7684\u4eba\uff0c\u4ed6\u7684\u5206\u6570\u4e0d\u662f1\u5c31\u662f4\u3002   \u90a3\u4e48\uff0c\u5982\u4f55\u6bd4\u8f83\u8fd9\u4e9b\u7528\u6237\u5462\uff1f\u6bd4\u5982Hailey\u76844\u5206\u76f8\u5f53\u4e8eJordan\u76844\u5206\u8fd8\u662f5\u5206\u5462\uff1f\u6211\u89c9\u5f97\u66f4\u63a5\u8fd15\u5206\u3002\u8fd9\u6837\u4e00\u6765\u5c31\u4f1a\u5f71\u54cd\u5230\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u4e86\u3002Clara\u6700\u4f4e\u7ed9\u4e864\u5206\u2014\u2014\u5979\u6240\u6709\u7684\u6253\u5206\u90fd\u57284\u81f35\u5206\u4e4b\u95f4\uff0c\u8fd9\u79cd\u73b0\u8c61\u5728\u6570\u636e\u6316\u6398\u9886\u57df\u79f0\u4e3a \u5206\u6570\u81a8\u80c0 \u3002   \u89e3\u51b3\u65b9\u6cd5\u4e4b\u4e00\u662f\u4f7f\u7528\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570, \u7528\u4e8e\u5ea6\u91cf\u4e24\u4e2a\u53d8\u91cfX\u548cY\u4e4b\u95f4\u7684\u76f8\u5173(\u7ebf\u6027\u76f8\u5173)\uff0c\u5176\u503c\u4ecb\u4e8e-1\u4e0e1\u4e4b\u95f4, 1\u8868\u793a\u5b8c\u5168\u543b\u5408\uff0c-1\u8868\u793a\u5b8c\u5168\u76f8\u6096\u3002\u4e0b\u9762\u662f\u5e38\u89c1\u7684\u51e0\u7ec4 (x, y) (x, y) \u70b9\u96c6\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u3002   \u4e24\u4e2a\u53d8\u91cf\u4e4b\u95f4\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u5b9a\u4e49\u4e3a\u4e24\u4e2a\u53d8\u91cf\u4e4b\u95f4\u7684\u534f\u65b9\u5dee( \\text{cov}(X,Y) \\text{cov}(X,Y) )\u548c\u6807\u51c6\u5dee( \\sigma_X \\sigma_X )\u7684\u5546\uff1a   \\rho_{X,Y}={\\mathrm{cov}(X,Y) \\over \\sigma_X \\sigma_Y} ={E[(X-\\mu_X)(Y-\\mu_Y)] \\over \\sigma_X\\sigma_Y}  \\rho_{X,Y}={\\mathrm{cov}(X,Y) \\over \\sigma_X \\sigma_Y} ={E[(X-\\mu_X)(Y-\\mu_Y)] \\over \\sigma_X\\sigma_Y}   \u5bf9\u4e8e\u6837\u672c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570:   r_{xy}=\\frac{\\sum x_iy_i-n \\bar{x} \\bar{y}}{(n-1) s_x s_y}=\\frac{n\\sum x_iy_i-\\sum x_i\\sum y_i}{\\sqrt{n\\sum x_i^2-(\\sum x_i)^2}~\\sqrt{n\\sum y_i^2-(\\sum y_i)^2}}.  r_{xy}=\\frac{\\sum x_iy_i-n \\bar{x} \\bar{y}}{(n-1) s_x s_y}=\\frac{n\\sum x_iy_i-\\sum x_i\\sum y_i}{\\sqrt{n\\sum x_i^2-(\\sum x_i)^2}~\\sqrt{n\\sum y_i^2-(\\sum y_i)^2}}.   \u4ee5\u4e0a\u65b9\u7a0b\u7ed9\u51fa\u4e86\u8ba1\u7b97\u6837\u672c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u7b80\u5355\u7684\u5355\u6d41\u7a0b\u7b97\u6cd5\uff0c\u4f46\u662f\u5176\u4f9d\u8d56\u4e8e\u6d89\u53ca\u5230\u7684\u6570\u636e\uff0c\u6709\u65f6\u5b83\u53ef\u80fd\u662f\u6570\u503c\u4e0d\u7a33\u5b9a\u7684\u3002\u4f46\u5b83\u6700\u5927\u7684\u4f18\u70b9\u662f\uff0c\u7528\u4ee3\u7801\u5b9e\u73b0\u7684\u65f6\u5019\u53ef\u4ee5\u53ea\u904d\u5386\u4e00\u6b21\u6570\u636e\u3002         def   pearson ( rating1 ,   rating2 ): \n                  Compute pearson coefficient          :param rating1: a dictionary          :param rating2: a dictionary          :return: pearson coefficient           \n         sum_xy   =   0 \n         sum_x   =   0 \n         sum_y   =   0 \n         sum_x2   =   0 \n         sum_y2   =   0 \n         n   =   0 \n         commonRatings   =   False \n         for   key   in   rating1 : \n             if   key   in   rating2 : \n                 n   +=   1 \n                 x   =   rating1 [ key ] \n                 y   =   rating2 [ key ] \n                 sum_xy   +=   x   *   y \n                 sum_x   +=   x \n                 sum_y   +=   y \n                 sum_x2   +=   pow ( x ,   2 ) \n                 sum_y2   +=   pow ( y ,   2 ) \n                 commonRatings   =   True \n\n         if   not   commonRatings : \n             return   - 1 \n         # now compute denominator \n         denominator   =   math . sqrt ( sum_x2   -   pow ( sum_x ,   2 )   /   n )   *   math . sqrt ( sum_y2   -   pow ( sum_y ,   2 )   /   n ) \n         if   denominator   ==   0 : \n             return   0 \n         else : \n             return   ( sum_xy   -   ( sum_x   *   sum_y )   /   n )   /   denominator", 
            "title": "\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570"
        }, 
        {
            "location": "/datamining/guideToDataMining/#_7", 
            "text": "\u5f53\u6211\u4eec\u75281500\u4e07\u9996\u6b4c\u66f2\u6765\u6bd4\u8f83\u4e24\u4e2a\u7528\u6237\u65f6\uff0c\u5f88\u6709\u53ef\u80fd\u4ed6\u4eec\u4e4b\u95f4\u6ca1\u6709\u4efb\u4f55\u4ea4\u96c6\uff0c\u8fd9\u6837\u4e00\u6765\u5c31\u65e0\u4ece\u8ba1\u7b97\u4ed6\u4eec\u4e4b\u95f4\u7684\u8ddd\u79bb\u4e86\u3002\u7c7b\u4f3c\u7684\u60c5\u51b5\u662f\u5728\u8ba1\u7b97\u4e24\u7bc7\u6587\u7ae0\u7684\u76f8\u4f3c\u5ea6\u65f6\u3002\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u8ba1\u7b97\u4e2d\u4f1a\u7565\u8fc7\u8fd9\u4e9b\u975e\u96f6\u503c\u3002\u5b83\u7684\u8ba1\u7b97\u516c\u5f0f\u662f\uff1a   \\cos(x,y) = \\frac{x\\cdot y}{||x||\\times||y||}  \\cos(x,y) = \\frac{x\\cdot y}{||x||\\times||y||}   \u5176\u4e2d\uff0c \\cdot \\cdot  \u53f7\u8868\u793a\u6570\u91cf\u79ef\u3002 ||x|| ||x|| \u8868\u793a\u5411\u91cf x x \u7684\u6a21\u3002  \u4f59\u5f26\u76f8\u4f3c\u5ea6\u5728\u6587\u672c\u6316\u6398\u4e2d\u5e94\u7528\u5f97\u8f83\u591a\uff0c\u5728\u534f\u540c\u8fc7\u6ee4\u4e2d\u4e5f\u4f1a\u4f7f\u7528\u5230\u3002", 
            "title": "\u4f59\u5f26\u76f8\u4f3c\u5ea6"
        }, 
        {
            "location": "/datamining/guideToDataMining/#_8", 
            "text": "\u5982\u679c\u6570\u636e\u5b58\u5728\u201c\u5206\u6570\u81a8\u80c0\u201d\u95ee\u9898\uff0c\u5c31\u4f7f\u7528\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u3002   \u5982\u679c\u6570\u636e\u6bd4\u8f83\u201c\u5bc6\u96c6\u201d\uff0c\u53d8\u91cf\u4e4b\u95f4\u57fa\u672c\u90fd\u5b58\u5728\u516c\u6709\u503c\uff0c\u4e14\u8fd9\u4e9b\u8ddd\u79bb\u6570\u636e\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u90a3\u5c31\u4f7f\u7528\u6b27\u51e0\u91cc\u5f97\u6216\u66fc\u54c8\u987f\u8ddd\u79bb\u3002  \u5982\u679c\u6570\u636e\u662f\u7a00\u758f\u7684\uff0c\u5219\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3002    Note  \u5728\u6570\u636e\u6807\u51c6\u5316( \\mu=0,\\sigma=1 \\mu=0,\\sigma=1 \uff09\u540e\uff0cPearson\u76f8\u5173\u6027\u7cfb\u6570\u3001\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9\u53ef\u8ba4\u4e3a\u662f\u7b49\u4ef7\u7684[ 1 ]\u3002", 
            "title": "\u5e94\u8be5\u4f7f\u7528\u54ea\u79cd\u76f8\u4f3c\u5ea6\uff1f"
        }, 
        {
            "location": "/datamining/guideToDataMining/#k", 
            "text": "\u4e0a\u9762\u7684\u505a\u6cd5\u4e2d\uff0c\u6211\u4eec\u53ea\u4f9d\u9760\u6700\u76f8\u4f3c\u7684\u4e00\u4e2a\u7528\u6237\u6765\u505a\u63a8\u8350\uff0c\u5982\u679c\u8fd9\u4e2a\u7528\u6237\u6709\u4e9b\u7279\u6b8a\u7684\u504f\u597d\uff0c\u5c31\u4f1a\u76f4\u63a5\u53cd\u6620\u5728\u63a8\u8350\u5185\u5bb9\u91cc\u3002\u89e3\u51b3\u65b9\u6cd5\u4e4b\u4e00\u662f\u627e\u5bfb\u591a\u4e2a\u76f8\u4f3c\u7684\u7528\u6237\uff0c\u8fd9\u91cc\u5c31\u8981\u7528\u5230K\u6700\u90bb\u8fd1\u7b97\u6cd5\u4e86\u3002  \u5728\u534f\u540c\u8fc7\u6ee4\u4e2d\u53ef\u4ee5\u4f7f\u7528K\u6700\u90bb\u8fd1\u7b97\u6cd5\u6765\u627e\u51faK\u4e2a\u6700\u76f8\u4f3c\u7684\u7528\u6237\uff0c\u4ee5\u6b64\u4f5c\u4e3a\u63a8\u8350\u7684\u57fa\u7840\u3002\u4e0d\u540c\u7684 \u5e94\u7528\u6709\u4e0d\u540c\u7684K\u503c\uff0c\u9700\u8981\u505a\u4e00\u4e9b\u5b9e\u9a8c\u6765\u5f97\u51fa\u3002\u4ee5\u4e0b\u7ed9\u5230\u8bfb\u8005\u4e00\u4e2a\u57fa\u672c\u7684\u601d\u8def\u3002 \u5047\u8bbe\u6211\u8981\u4e3aAnn\u505a\u63a8\u8350\uff0c\u5e76\u4ee4K=3\u3002\u4f7f\u7528\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u5f97\u5230\u7684\u7ed3\u679c\u662f\uff1a     Person  Pearson      Sally  0.8    Eric  0.7    Amanda  0.5     \u8fd9\u4e09\u4e2a\u4eba\u90fd\u4f1a\u5bf9\u63a8\u8350\u7ed3\u679c\u6709\u6240\u8d21\u732e\uff0c\u95ee\u9898\u5728\u4e8e\u6211\u4eec\u5982\u4f55\u786e\u5b9a\u4ed6\u4eec\u7684\u6bd4\u91cd\u5462\uff1f \u6211\u4eec\u76f4\u63a5\u7528\u76f8\u5173\u7cfb\u6570\u7684\u6bd4\u91cd\u6765\u63cf\u8ff0\uff0cSally\u7684\u6bd4\u91cd\u662f0.8/2=40%\uff0cEric\u662f0.7/2=35%\uff0cAmanda \u5219\u662f25%\uff1a  \u5047\u8bbe\u4ed6\u4eec\u4e09\u4eba\u5bf9Grey Wardens\u7684\u8bc4\u5206\u4ee5\u53ca\u52a0\u6743\u540e\u7684\u7ed3\u679c\u5982\u4e0b\uff1a     Person  Grey Wardens Rating  Influence      Sally  4.5  25%    Eric  5  35%    Amanda  3.5  40%     \u6700\u540e\u8ba1\u7b97\u5f97\u5230\u7684\u5206\u6570\u4e3a\u4e3a\u52a0\u6743\u548c  4.5\\times 25\\% + 5\\times 35\\% + 3.5 \\times 40\\% 4.5\\times 25\\% + 5\\times 35\\% + 3.5 \\times 40\\% \u3002", 
            "title": "K\u6700\u90bb\u8fd1\u7b97\u6cd5"
        }, 
        {
            "location": "/datamining/guideToDataMining/#python", 
            "text": "Cai-Nicolas Zeigler\u4ece\u56fe\u4e66\u6f02\u6d41\u7ad9\u6536\u96c6\u4e86\u8d85\u8fc7100\u4e07\u6761\u8bc4\u4ef7\u6570\u636e\u2014\u2014278,858\u4f4d\u7528\u6237\u4e3a271,379\u672c\u4e66\u6253\u4e86\u5206\u3002\u6570\u636e\u53ef\u4ee5\u4ece\u8fd9\u4e2a \u5730\u5740 \u83b7\u5f97\u3002  CSV\u6587\u4ef6\u5305\u542b\u4e86\u4e09\u5f20\u8868\uff1a   \u7528\u6237\u8868\uff0c\u5305\u62ec\u7528\u6237ID\u3001\u4f4d\u7f6e\u3001\u5e74\u9f84\u7b49\u4fe1\u606f\u3002\u5176\u4e2d\u7528\u6237\u7684\u59d3\u540d\u5df2\u7ecf\u9690\u53bb\uff1b   \u4e66\u7c4d\u8868\uff0c\u5305\u62ecISBN\u53f7\u3001\u6807\u9898\u3001\u4f5c\u8005\u3001\u51fa\u7248\u65e5\u671f\u3001\u51fa\u7248\u793e\u7b49\uff1b  \u8bc4\u5206\u8868\uff0c\u5305\u62ec\u7528\u6237ID\u3001\u4e66\u7c4dISBN\u53f7\u3001\u4ee5\u53ca\u8bc4\u5206\uff080-10\u5206\uff09\u3002   Recommender class   Recommender : \n\n     def   __init__ ( self ,   books ,   users ,   user_ratings ,   book_ratings ): \n                  initialize basic data          :param books: a dictionary of books, whose key is book id          :param users: a dictionary of users, whose key is user id          :param book_ratings: a dictionary of book ratings, whose key is book id          :param user_ratings: a dictionary of user ratings, whose key is user id           \n         self . books   =   books \n         self . users   =   users \n         self . book_ratings   =   book_ratings \n         self . user_ratings   =   user_ratings \n\n     def   recommend ( self ,   user_to_recommend_int ,   k = 1 ): \n                  Recommend user books          :param user_to_recommend_int: int, user id          :param k : int, for nearest k neighbors          :return: a list of books           \n         user_to_recommend   =   str ( user_to_recommend_int ) \n         if   user_to_recommend   not   in   self . users : \n             raise   Exception ( user does not exist!! ) \n\n         # find the user having min distances from user_to_recommend \n         distances   =   [] \n         find_user   =   False \n         for   user   in   self . users : \n             if   user_to_recommend   ==   user : \n                 continue \n             # extract user ratings based on user ids,  \n             # and compute the distance between them \n             distance   =   Distance . pearson ( self . user_ratings [ user_to_recommend ],  \n                  self . user_ratings [ user ]) \n             if   distance   !=   - 1 : \n                 distances . append ([ user ,   distance ]) \n                 find_user   =   True \n\n         if   not   find_user : \n             return   [] \n\n         # sort user based on their distances \n         # pearson \u7cfb\u6570\u8d8a\u5927\uff0c\u8ddd\u79bb\u8d8a\u8fd1\uff0c\u6240\u4ee5\u7528reverse \n         distances . sort ( key = lambda   x :   x [ 1 ],   reverse = True ) \n\n         # compute weight based on distances \n         distances   =   distances [ 0 : k ] \n         sum_distance   =   sum ([ distance   for   user ,   distance   in   distances ]) \n         for   i   in   range ( len ( distances )): \n             distances [ i ][ 1 ]   /=   sum_distance \n\n         # recommend books \n         books_to_recommend   =   {} \n         for   user_id ,   weight   in   distances : \n             for   book_id   in   self . user_ratings [ user_id ]: \n                     if   book_id   not   in   self . user_ratings [ user_to_recommend ]:    # the user haven t seen \n                         if   book_id   not   in   books_to_recommend :    # haven t recommend \n                             books_to_recommend [ book_id ]   =  \n                                  self . user_ratings [ user_id ][ book_id ] * weight \n                         else : \n                             books_to_recommend [ book_id ]   =   books_to_recommend [ book_id ]  \\\n                                 +   self . user_ratings [ user_id ][ book_id ] * weight \n\n         # transform to a  list of tuple \n         books_to_recommend   =   [( book_id ,   project_rating )  \n               for   book_id ,   project_rating   in   books_to_recommend . items ()] \n\n         # sort based on project_rating \n         books_to_recommend . sort ( key = lambda   x :   x [ 1 ],   reverse = True ) \n\n         # extract book title \n         books_to_recommend   =   [ self . books [ book_id ][ title ] \n                for   book_id ,   project_rating   in   books_to_recommend ] \n         return   books_to_recommend  if   __name__   ==   __main__ : \n     ratings   =   BooksImport () \n     books ,   users ,   user_ratings ,   book_ratings   =   ratings . recommender_import () \n     test   =   Recommender ( books ,   users ,   user_ratings ,   book_ratings ) \n     print ( test . recommend ( 171118 ))  Distance import   math  class   Distance : \n          Compute distance of two users, having different ratings.      Both rating1 and rating2 are      dictionaries of the form { The Strokes : 3.0,  Slightly Stoopid : 2.5}       \n\n     def   __init__ ( self ): \n         pass \n\n     @staticmethod \n     def   manhattan ( rating1 ,   rating2 ): \n                  Computes the Manhattan distance.           \n         distance   =   0 \n         common_ratings   =   False \n         for   key   in   rating1 : \n             if   key   in   rating2 : \n                 distance   +=   abs ( rating1 [ key ]   -   rating2 [ key ]) \n                 common_ratings   =   True \n         if   common_ratings : \n             return   distance \n         else : \n             return   - 1    # Indicates no ratings in common \n\n     @staticmethod \n     def   euclidean ( rating1 ,   rating2 ): \n                  Computes the Euclidean Distance          :param rating1: rating          :param rating2: rating          :return: distance if common ratings exists, or -1           \n         distance   =   0 \n         commonRatings   =   False \n         for   key   in   rating1 : \n             if   key   in   rating2 : \n                 distance   +=   pow ( rating1 [ key ]   -   rating2 [ key ],   2 ) \n                 commonRatings   =   True \n\n         if   commonRatings : \n             return   distance \n         else : \n             return   - 1    # Indicates no ratings in common \n\n     @staticmethod \n     def   chebyshev ( rating1 ,   rating2 ): \n                  Computes the Chebyshev Distance          :param rating1: rating          :param rating2: rating          :return: distance if common ratings exists, or -1           \n         distance   =   0 \n         commonRatings   =   False \n         for   key   in   rating1 : \n             if   key   in   rating2 : \n                 distance   =   max ( distance ,   abs ( rating1 [ key ]   -   rating2 [ key ])) \n                 commonRatings   =   True \n\n         if   commonRatings : \n             return   distance \n         else : \n             return   - 1    # Indicates no ratings in common \n\n     @staticmethod \n     def   pearson ( rating1 ,   rating2 ): \n                  Compute pearson coefficient          :param rating1: a dictionary          :param rating2: a dictionary          :return: pearson coefficient           \n         sum_xy   =   0 \n         sum_x   =   0 \n         sum_y   =   0 \n         sum_x2   =   0 \n         sum_y2   =   0 \n         n   =   0 \n         commonRatings   =   False \n         for   key   in   rating1 : \n             if   key   in   rating2 : \n                 n   +=   1 \n                 x   =   rating1 [ key ] \n                 y   =   rating2 [ key ] \n                 sum_xy   +=   x   *   y \n                 sum_x   +=   x \n                 sum_y   +=   y \n                 sum_x2   +=   pow ( x ,   2 ) \n                 sum_y2   +=   pow ( y ,   2 ) \n                 commonRatings   =   True \n\n         if   not   commonRatings : \n             return   - 1 \n         # now compute denominator \n         denominator   =   math . sqrt ( sum_x2   -   pow ( sum_x ,   2 )   /   n ) \\ \n             *   math . sqrt ( sum_y2   -   pow ( sum_y ,   2 )   /   n ) \n         if   denominator   ==   0 : \n             return   0 \n         else : \n             return   ( sum_xy   -   ( sum_x   *   sum_y )   /   n )   /   denominator  Books_import class   BooksImport : \n\n     def   __init__ ( self ): \n         self . books   =   {} \n         self . users   =   {} \n         self . book_ratings   =   {} \n         self . user_ratings   =   {} \n         self . bx_books_import () \n         self . bx_users_import () \n         self . bx_ratings_import () \n\n     def   bx_books_import ( self ): \n                  import books meta information           \n\n         try : \n             booksfile   =   codecs . open ( BX-Dump/BX-Books.csv ,   r ,   utf-8 ) \n\n             for   line   in   booksfile : \n                 props   =   line . split ( ; ) \n                 isbn   =   props [ 0 ] . strip ( ) \n                 title   =   props [ 1 ] . strip ( ) \n                 author   =   props [ 2 ] . strip ( ) \n                 year   =   props [ 3 ] . strip ( ) \n                 self . books [ isbn ]   =   { title :   title ,   author :   author ,   year :   year } \n\n             booksfile . close () \n\n         except   IOError   as   e : \n             error   =   Failed to load: {0} . format ( e ) \n             print ( error ) \n\n     def   bx_users_import ( self ): \n                  import user meta information          user is a dictionary, whose key is user_id           \n         try : \n             users_file   =   codecs . open ( BX-Dump/BX-Users.csv ,   r ,   utf--8 ) \n             for   line   in   users_file : \n                 props   =   line . split ( ; ) \n                 user_id   =   props [ 0 ] . strip ( ) \n                 location   =   props [ 1 ] . strip ( ) \n                 self . users [ user_id ]   =   location \n                 self . user_ratings [ user_id ]   =   {} \n             users_file . close () \n\n         except   IOError   as   e : \n             error   =   Failed to load: {0} . format ( e ) \n             print ( error ) \n\n     def   bx_ratings_import ( self ): \n         try : \n             ratings_file   =   codecs . open ( BX-Dump/BX-Book-Ratings.csv ,   r ,   utf--8 ) \n             for   line   in   ratings_file : \n                 props   =   line . split ( ; ) \n                 user_id   =   props [ 0 ] . strip ( ) \n                 book_id   =   props [ 1 ] . strip ( ) \n                 rating   =   int ( props [ 2 ] . strip () . strip ( )) \n\n                 if   book_id   in   self . book_ratings : \n                     self . book_ratings [ book_id ] . append ( rating ) \n                 else : \n                     self . book_ratings [ book_id ]   =   [ rating ] \n\n                 self . user_ratings [ user_id ][ book_id ]   =   rating \n\n             ratings_file . close () \n\n         except   IOError   as   e : \n             error   =   Failed to load: {0} . format ( e ) \n             print ( error ) \n\n     def   get_books ( self ): \n         return   self . books \n\n     def   get_users ( self ): \n         return   self . users \n\n     def   get_user_ratings ( self ): \n         return   self . user_ratings \n\n     def   get_book_ratings ( self ): \n         return   self . book_ratings \n\n     def   recommender_import ( self ): \n         return   self . books ,   self . users ,   self . user_ratings ,   self . book_ratings", 
            "title": "Python\u63a8\u8350\u6a21\u5757"
        }, 
        {
            "location": "/datamining/guideToDataMining/#2", 
            "text": "", 
            "title": "2 \u9690\u5f0f\u8bc4\u4ef7\u548c\u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\u7b97\u6cd5"
        }, 
        {
            "location": "/datamining/guideToDataMining/#_9", 
            "text": "\u7528\u6237\u7684\u8bc4\u4ef7\u7c7b\u578b\u53ef\u4ee5\u5206\u4e3a\u663e\u5f0f\u8bc4\u4ef7\u548c\u9690\u5f0f\u8bc4\u4ef7\u3002 \u663e\u5f0f\u8bc4\u4ef7 \u6307\u7684\u662f\u7528\u6237\u660e\u786e\u5730\u7ed9\u51fa\u5bf9\u7269\u54c1\u7684\u8bc4\u4ef7\u3002\u6700\u5e38\u89c1\u7684\u4f8b\u5b50\u662fYouTube\u4e0a\u7684\u201c\u559c\u6b22\u201d\u548c\u201c\u4e0d\u559c\u6b22\u201d\u6309\u94ae\uff0c\u4ee5\u53ca\u4e9a\u9a6c\u900a\u8bc4\u8bba\u7684\u661f\u7ea7\u7cfb\u7edf\u3002  \u9690\u5f0f\u8bc4\u4ef7 \uff0c\u5c31\u662f\u6211\u4eec\u4e0d\u8ba9\u7528\u6237\u660e\u786e\u7ed9\u51fa\u5bf9\u7269\u54c1\u7684\u8bc4\u4ef7\uff0c\u800c\u662f\u901a\u8fc7\u89c2\u5bdf\u4ed6\u4eec\u7684\u884c\u4e3a\u6765\u83b7\u5f97\u504f\u597d\u4fe1\u606f\u3002\u793a\u4f8b\u4e4b\u4e00\u662f\u8bb0\u5f55\u7528\u6237\u5728\u7ebd\u7ea6\u65f6\u62a5\u7f51\u4e0a\u7684\u70b9\u51fb\u8bb0\u5f55\uff0c\u4e9a\u9a6c\u900a\u4e0a\u7528\u6237\u7684\u5b9e\u9645\u8d2d\u4e70\u8bb0\u5f55  \u6211\u4eec\u53ef\u4ee5\u6536\u96c6\u5230\u54ea\u4e9b\u9690\u5f0f\u8bc4\u4ef7\u5462\uff1f \u7f51\u9875\u65b9\u9762\uff1a\u9875\u9762\u70b9\u51fb\u3001\u505c\u7559\u65f6\u95f4\u3001\u91cd\u590d\u8bbf\u95ee\u6b21\u6570\u3001\u5f15 \u7528\u7387\u3001Hulu\u4e0a\u89c2\u770b\u89c6\u9891\u7684\u6b21\u6570\uff1b \u97f3\u4e50\u64ad\u653e\u5668\uff1a\u64ad\u653e\u7684\u66f2\u76ee\u3001\u8df3\u8fc7\u7684\u66f2\u76ee\u3001\u64ad\u653e\u6b21\u6570\uff1b \u8fd9 \u4e9b\u53ea\u662f\u4e00\u5c0f\u90e8\u5206\uff01", 
            "title": "\u9690\u5f0f\u8bc4\u4ef7"
        }, 
        {
            "location": "/datamining/guideToDataMining/#_10", 
            "text": "\u76ee\u524d\u4e3a\u6b62\u6211\u4eec\u63cf\u8ff0\u7684\u90fd\u662f\u57fa\u4e8e\u7528\u6237\u7684\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\uff1a\u5c06\u4e00\u4e2a\u7528\u6237\u548c\u5176\u4ed6 \u6240\u6709 \u7528\u6237\u8fdb\u884c\u5bf9\u6bd4\uff0c\u627e\u5230\u76f8\u4f3c\u7684\u4eba\u3002\u8fd9\u79cd\u7b97\u6cd5\u6709\u4e24\u4e2a\u5f0a\u7aef\uff1a   \u6269\u5c55\u6027 \uff1a\u968f\u7740\u7528\u6237\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5176\u8ba1\u7b97\u91cf\u4e5f\u4f1a\u589e\u52a0\u3002\u8fd9\u79cd\u7b97\u6cd5\u5728\u53ea\u6709\u51e0\u5343\u4e2a\u7528\u6237\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u5de5\u4f5c\u5f97\u5f88\u597d\uff0c\u4f46\u8fbe\u5230\u4e00\u767e\u4e07\u4e2a\u7528\u6237\u65f6\u5c31\u4f1a\u51fa\u73b0\u74f6\u9888\u3002  \u7a00\u758f\u6027 \uff1a\u5927\u591a\u6570\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u7269\u54c1\u7684\u6570\u91cf\u8981\u8fdc\u5927\u4e8e\u7528\u6237\u7684\u6570\u91cf\uff0c\u56e0\u6b64\u7528\u6237\u4ec5\u4ec5\u5bf9\u4e00\u5c0f\u90e8\u5206\u7269\u54c1\u8fdb\u884c\u4e86\u8bc4\u4ef7\uff0c\u8fd9\u5c31\u9020\u6210\u4e86\u6570\u636e\u7684\u7a00\u758f\u6027\u3002\u6bd4\u5982\u4e9a\u9a6c\u900a\u6709\u4e0a\u767e\u4e07\u672c\u4e66\uff0c\u4f46\u7528\u6237\u53ea\u8bc4\u8bba \u4e86\u5f88\u5c11\u4e00\u90e8\u5206\uff0c\u4e8e\u662f\u5c31\u5f88\u96be\u627e\u5230\u4e24\u4e2a\u76f8\u4f3c\u7684\u7528\u6237\u4e86\u3002", 
            "title": "\u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\u7b97\u6cd5"
        }, 
        {
            "location": "/datamining/guideToDataMining/#_11", 
            "text": "\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6765\u8ba1\u7b97\u4e24\u4e2a\u7269\u54c1\u7684\u8ddd\u79bb\u3002\u7531\u4e8e\u201c\u5206\u6570\u81a8\u80c0\u201d\u73b0\u8c61\uff0c\u9700\u8981\u4ece\u7528\u6237\u7684\u8bc4\u4ef7\u4e2d\u51cf\u53bb\u4ed6\u6240\u6709\u8bc4\u4ef7\u7684\u5747\u503c\uff0c\u8fd9\u5c31\u662f\u4fee\u6b63\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3002   s(i,j) =\\frac{\\sum_{u\\in U}(R_{u.i}-\\bar R_u)(R_{u,j}-\\bar R_u)}{\\sqrt{\\sum_{u\\in U}(R_{u,i}-\\bar R_u)^2}\\sqrt{\\sum_{u\\in U}(R_{u,j}-\\bar R_u)^2}}  s(i,j) =\\frac{\\sum_{u\\in U}(R_{u.i}-\\bar R_u)(R_{u,j}-\\bar R_u)}{\\sqrt{\\sum_{u\\in U}(R_{u,i}-\\bar R_u)^2}\\sqrt{\\sum_{u\\in U}(R_{u,j}-\\bar R_u)^2}}   U U \u8868\u793a\u540c\u65f6\u8bc4\u4ef7\u8fc7\u7269\u54c1 i i \u548c j j \u7684\u7528\u6237\u96c6\u5408\uff0c  \\bar R_u \\bar R_u \u8868\u793a\u7528\u6237 u u \u5bf9\u6240\u6709\u7269\u54c1\u7684\u8bc4\u4ef7\u5747\u503c\uff0c s(i,j) s(i,j) \u8868\u793a\u7269\u54c1i\u548cj\u7684\u76f8\u4f3c\u5ea6\u3002    def   cosinesimilarity ( item1 ,   item2 ,   userRatings ): \n     averages   =   {} \n     for   item ,   ratings   in   userRatings . items (): \n         averages [ item ]   =   ( float ( sum ( ratings . values ()))   /   len ( ratings . values ())) \n\n     num   =   0      # \u5206\u5b50 \n     dem1   =   0     # \u5206\u6bcd\u7684\u7b2c\u4e00\u90e8\u5206 \n     dem2   =   0 \n     for   ( user ,   ratings )   in   userRatings . items (): \n         if   item1   in   ratings   and   item2   in   ratings : \n             avg   =   averages [ user ] \n             num   +=   ( ratings [ item1 ]   -   avg )   *   ( ratings [ item2 ]   -   avg ) \n             dem1   +=   ( ratings [ item1 ]   -   avg )   **   2 \n             dem2   +=   ( ratings [ item1 ]   -   avg )   **   2 \n     return   num   /   ( math . sqrt ( dem1 )   *   math . sqrt ( dem2 ))", 
            "title": "\u4fee\u6b63\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6"
        }, 
        {
            "location": "/datamining/guideToDataMining/#slope-one", 
            "text": "Slope One\u662f\u4e00\u79cd\u6bd4\u8f83\u6d41\u884c\u7684\u57fa\u4e8e\u7269\u54c1\u7684\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\u3002\u5b83\u6700\u5927\u7684\u4f18\u52bf\u662f\u7b80\u5355\uff0c\u56e0\u6b64\u6613\u4e8e\u5b9e\u73b0\u3002", 
            "title": "Slope One\u7b97\u6cd5"
        }, 
        {
            "location": "/datamining/guideToDataMining/#3", 
            "text": "", 
            "title": "3 \u5206\u7c7b"
        }, 
        {
            "location": "/datamining/guideToDataMining/#4", 
            "text": "", 
            "title": "4 \u8fdb\u4e00\u6b65\u63a2\u7d22\u5206\u7c7b"
        }, 
        {
            "location": "/datamining/guideToDataMining/#5", 
            "text": "", 
            "title": "5 \u6734\u7d20\u8d1d\u53f6\u65af"
        }, 
        {
            "location": "/datamining/guideToDataMining/#6", 
            "text": "", 
            "title": "6 \u6734\u7d20\u8d1d\u53f6\u65af\u7b97\u6cd5\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c"
        }, 
        {
            "location": "/datamining/guideToDataMining/#7", 
            "text": "", 
            "title": "7 \u805a\u7c7b~~~~"
        }, 
        {
            "location": "/projects/", 
            "text": "Projects\n\n\n\n\nSparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406", 
            "title": "Contents"
        }, 
        {
            "location": "/projects/#projects", 
            "text": "SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406", 
            "title": "Projects"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/", 
            "text": "Spark Streaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee\n\n\n\u8be5\u9879\u76ee\u4ece\u5b9e\u65f6\u6570\u636e\u4ea7\u751f\u548c\u6d41\u5411\u7684\u4e0d\u540c\u73af\u8282\u51fa\u53d1\uff0c\u901a\u8fc7\u96c6\u6210\u4e3b\u6d41\u7684\u5206\u5e03\u5f0f\u65e5\u5fd7\u6536\u96c6\u6846\u67b6Flume\u3001\u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217Kafka\u3001\u5206\u5e03\u5f0f\u5217\u5f0f\u6570\u636e\u5e93HBase\u3001\u4ee5\u53caSpark Streaming\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5904\u7406\u3002\n\n\n1 \u521d\u8bc6\u5b9e\u65f6\u6d41\u5904\u7406\n\n\n\u4e1a\u52a1\u73b0\u72b6\u5206\u6790\n\n\n\u9700\u6c42\uff1a\u7edf\u8ba1\u4e3b\u7ad9\u6bcf\u4e2a\uff08\u6307\u5b9a\uff09\u8bfe\u7a0b\u8bbf\u95ee\u7684\u5ba2\u6237\u7aef\u3001\u5730\u57df\u4fe1\u606f\u5206\u5e03\n\n\n==\n \u5982\u4e0a\u4e24\u4e2a\u64cd\u4f5c\uff1a\u91c7\u7528\u79bb\u7ebf\uff08spark/mapreduce\uff09\u7684\u65b9\u5f0f\u8fdb\u884c\u7edf\u8ba1\n\n\n\u5b9e\u73b0\u6b65\u9aa4\uff1a\n\n\n\n\n\u8bfe\u7a0b\u7f16\u53f7\uff0cip\u4fe1\u606f\uff0cuser-agent\n\n\n\u8fdb\u884c\u76f8\u5e94\u7684\u7edf\u8ba1\u5206\u6790\u64cd\u4f5c\uff1aMapReduce/Spark\n\n\n\n\n\u9879\u76ee\u67b6\u6784\uff1a\n\n\n\n\n\u65e5\u5fd7\u6536\u96c6\uff1aFlume\n\n\n\u79bb\u7ebf\u5206\u6790\uff1aMapReduce/Spark\n\n\n\u7edf\u8ba1\u7ed3\u679c\u56fe\u5f62\u5316\u5c55\u793a\n\n\n\n\n\u95ee\u9898\uff1a\n\n\n\n\n\u5c0f\u65f6\u7ea7\u522b\n\n\n10\u5206\u949f\n\n\n\u79d2\u7ea7\u522b\n\n\n\n\n\u5b9e\u65f6\u6d41\u5904\u7406\u4ea7\u751f\u80cc\u666f\n\n\n\n\n\u65f6\u6548\u6027\u9ad8\n\n\n\u6570\u636e\u91cf\u5927\n\n\n\n\n\u5b9e\u65f6\u6d41\u5904\u7406\u6982\u8ff0\n\n\nhttps://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101\n\n\n\n\n\u5b9e\u65f6\u8ba1\u7b97 apache storm\n\n\n\u6d41\u5f0f\u8ba1\u7b97\n\n\n\u5b9e\u65f6\u6d41\u5f0f\u8ba1\u7b97\n\n\n\n\n\u79bb\u7ebf\u8ba1\u7b97\u4e0e\u5b9e\u65f6\u8ba1\u7b97\u5bf9\u6bd4\n\n\n\n\n\u6570\u636e\u6765\u6e90\n\n\n\u79bb\u7ebf\uff1a\u6765\u81eaHDFS\u4e0a\u7684\u5386\u53f2\u6570\u636e\uff0c\u6570\u636e\u91cf\u6bd4\u8f83\u5927\n\n\n\u5b9e\u65f6\uff1a\u6765\u81ea\u6d88\u606f\u961f\u5217(Kafka)\uff0c\u662f\u5b9e\u65f6\u65b0\u589e/\u4fee\u6539\u8bb0\u5f55\u8fc7\u6765\u7684\u67d0\u4e00\u7b14\u6570\u636e\n\n\n\n\n\n\n\u5904\u7406\u8fc7\u7a0b\n\n\n\u79bb\u7ebf\uff1aMapReduce, map + reduce\n\n\n\u5b9e\u65f6: Spark(DStream/SS) \n\n\n\n\n\n\n\u5904\u7406\u901f\u5ea6\n\n\n\u79bb\u7ebf\uff1a\u5e54\n\n\n\u5b9e\u65f6\uff1a\u5feb\u901f \n\n\n\n\n\n\n\u8fdb\u7a0b\n\n\n\u79bb\u7ebf\uff1a\u8fdb\u7a0b\u6709\u542f\u52a8+\u9500\u6bc1\u7684\u8fc7\u7a0b\n\n\n\u5b9e\u65f6\uff1a 7*24\u5c0f\u65f6\u8fd0\u884c\n\n\n\n\n\n\n\n\n\u5b9e\u65f6\u6d41\u5904\u7406\u6846\u67b6\u5bf9\u6bd4\n\n\n\n\nApache Storm\n\n\n\n\n\n\nApache Storm is a free and open source distributed \nrealtime\n computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use!\n\n\n\n\n\n\nApache Spark Streaming\n\n\n\n\n\n\n\u5b9e\u9645\u4e0a\u662f\u5fae\u6279\u5904\u7406\uff08\u6279\u5904\u7406\u95f4\u9694\u975e\u5e38\u5c0f)\n\n\n\n\n\n\nApache kafka\n\n\nApache Flink\n\n\n\n\n\n\nApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.\n\n\n\n\n\u5b9e\u65f6\u6d41\u5904\u7406\u67b6\u6784\u548c\u6280\u672f\u9009\u578b\n\n\n\n\n\u52a0\u4e00\u5c42flume\u6d88\u606f\u961f\u5217\uff0c\u4e3b\u8981\u4e3a\u4e86\u51cf\u8f7b\u538b\u529b\uff0c\u8d77\u5230\u7f13\u51b2\u4f5c\u7528\n\n\n\n\n\u5b9e\u65f6\u6d41\u5904\u7406\u5728\u4f01\u4e1a\u4e2d\u7684\u5e94\u7528\n\n\n\n\n\u7535\u4fe1\u884c\u4e1a\uff1a \u4f60\u7684\u624b\u673a\u5957\u9910\u6d41\u91cf\u7528\u5b8c\uff0c\u6536\u5230\u77ed\u4fe1\u63d0\u793a\n\n\n\u7535\u5546\u884c\u4e1a\uff1a\u641c\u7d22\u5546\u54c1\u65f6\uff0c\u8fdb\u884c\u63a8\u8350\n\n\n\n\n2 \u5206\u5e03\u5f0f\u65e5\u5fd7\u6536\u96c6\u6846\u67b6Flume\n\n\nsee detail in Hadoop: definitive Guide, \nChapter 14\n\n\n\u4e1a\u52a1\u73b0\u72b6\u5206\u6790\n\n\nYou have a lot of servers and systems\n\n\n\n\nnetwork devices\n\n\noperating system\n\n\nweb servers\n\n\napplications\n\n\n\n\nAnd they generate large amount of logs and other data.\n\n\nProblem: Since you have a business idea, how to implement the idea?\n\n\nOPTION: You may move logs and data generated to hadoop hdfs directly.\n\n\n\u4f46\u662f\u5b58\u5728\u95ee\u9898\uff1a\n\n\n\n\n\u5982\u4f55\u505a\u76d1\u63a7\n\n\n\u5982\u4f55\u4fdd\u8bc1\u65f6\u6548\u6027\n\n\n\u76f4\u63a5\u4f20\u9001\u6587\u672c\u6570\u636e\uff0c\u5f00\u9500\u592a\u5927\n\n\n\u5bb9\u9519\n\n\n\u8d1f\u8f7d\u5747\u8861\n\n\n\n\nSOLUTION: \u4f7f\u7528Flume\uff0c\u57fa\u672c\u4e0a\u5199\u914d\u7f6e\u6587\u4ef6\u5c31OK\u4e86\uff0cFlume\u81ea\u52a8\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002\n\n\nFlume\u6982\u8ff0\n\n\n\n\nFlume is a distributed, reliable, and available service for efficiently \ncollecting, aggregating, and moving large amounts of log data\n. It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application. [\nApache Flume\n]\n\n\n\n\nFlume\u67b6\u6784\u53ca\u6838\u5fc3\u7ec4\u4ef6\n\n\n\n\nsee detail in Hadoop: definitive Guide, \nChapter 14\n\n\nFlume\u5b9e\u6218\n\n\n\u9700\u6c42\uff1a \u4ece\u6307\u5b9a\u7f51\u7edc\u7aef\u53e3\u91c7\u96c6\u6570\u636e\n\n\n\u4f7f\u7528Flume\u7684\u5173\u952e\u5c31\u662f\u5199\u914d\u7f6e\u6587\u4ef6\n\n\n\n\n\u914d\u7f6eSource, Channel, Sink\n\n\n\u628a\u4ee5\u4e0a\u4e09\u4e2a\u7ec4\u4ef6\u4e32\u8d77\u6765\n\n\n\n\n \nhttp://flume.apache.org/FlumeUserGuide.html#example-2\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 44444\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n\n\n\n\n\nnetcat source\n: A netcat-like source that listens on a given port and turns each line of text into an event. It opens a specified port and listens for data. The expectation is that the supplied data is newline separated text. Each line of text is turned into a Flume event and sent via the connected channel. [\nNetCat TCP Source\n]\n\n\nlogger sink\n: Logs event at INFO level. Typically useful for testing/debugging purpose.  [\nLogger Sink\n]\n\n\nmemory channel\n: The events are stored in an in-memory queue with configurable max size. It\u2019s ideal for flows that need higher throughput and are prepared to lose the staged data in the event of an agent failures. [\nmemory channel\n]\n\n\n\n\n \n## \u542f\u52a8flume\n\n$ flume-ng agent \n\\\n\n--name a1 \n\\ \n \n# agent name\n\n--conf \n$F\nLUME_HOME/conf \n\\ \n# use configs in \nconf\n directory\n\n--conf-file  example.conf \n\\ \n# specify a config file\n\n-Dflume.root.logger\n=\nINFO,console \n# sets a Java system property value\n\n\n\n## \u5728\u53e6\u5916\u4e00\u4e2aterminal\u7528telnet\u6a21\u62df\u6570\u636e\u6e90\n\n$ telnet localhost \n44444\n \nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is \n^]\n.\nhello\nOK\nhellomy\nOK\n\n\n\n\u9700\u6c42\uff1a \u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\u5b9e\u65f6\u91c7\u96c6\u65b0\u589e\u7684\u6570\u636e\u8f93\u51fa\u5230\u63a7\u5236\u53f0\n\n\nAgent\u9009\u578b\uff1a exec source + memory channel + logger sink\n\n\n \n# filename: exec-memeory-logger.conf\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /tmp/data.log\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n\n\n\n\n\nexec source\n runs a given Unix command on start-up and expects that process to continuously produce data on standard out (stderr is simply discarded, unless property logStdErr is set to true). If the process exits for any reason, the source also exits and will produce no further data. This means configurations such as cat [named pipe] or tail -F [file] are going to produce the desired results where as date will probably not - the former two commands produce streams of data where as the latter produces a single event and exits. [\nexec source\n]\n\n\n\n\n\u5c06\u5185\u5bb9\u8f93\u5165\u5230\n/tmp/data.log\n\u6587\u4ef6\u4e2d\uff1a\n\n\n \n$ \necho\n \nhello\n \n data.log\n$ \necho\n \nhello\n \n data.log\n\n\n\n\u9700\u6c42\uff1a \u5c06A\u670d\u52a1\u5668\u4e0a\u7684\u65e5\u5fd7\u5b9e\u65f6\u91c7\u96c6\u5230B\u670d\u52a1\u5668\n\n\n\u65e5\u5fd7\u6536\u96c6\u8fc7\u7a0b\uff1a\n\n\n\n\n\u673a\u56681\u4e0a\u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\uff0c\u5f53\u6211\u4eec\u8bbf\u95ee\u4e3b\u7ad9\u65f6\u4f1a\u6709\u7528\u6237\u884c\u4e3a\u65e5\u5fd7\u8bb0\u5f55\u5230\naccess.log\n\u4e2d\u3002\n\n\navro sink\u628a\u65b0\u4ea7\u751f\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u5bf9\u5e94\u7684avro source\u6307\u5b9a\u7684hostname\u548cport\u4e0a\u3002\n\n\n\u901a\u8fc7avro\u5bf9\u5e94\u7684agent\u5c06\u6211\u4eec\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u63a7\u5236\u53f0\u3002\n\n\n\n\n\n\n\n\navro sink\n: forms one half of Flume\u2019s tiered collection support. Flume events sent to this sink are turned into Avro events and sent to the configured hostname / port pair. [\nAvro sink\n]\n\n\n\n\nExec-Memeory-Avro.conf\n# filename: exec-memeory-avro.conf\n\n# Name the components on this agent\na1.sources = exec-source\na1.sinks = avro-sink\na1.channels = memory-channel\n\n# Describe/configure the source\na1.sources.exec-source.type = exec\na1.sources.exec-source.command = tail -F /tmp/data.log\n\n# Describe the sink\na1.sinks.avro-sink.type = avro\na1.sinks.avro-sink.hostname = localhost\na1.sinks.avro-sink.port = 44444\n\n# Use a channel which buffers events in memory\na1.channels.memory-channel.type = memory\na1.channels.memory-channel.capacity = 1000\na1.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.exec-source.channels = memory-channel\na1.sinks.avro-sink.channel = memory-channel\n\nAvro-Memeory-Logger.conf\n# filename: avro-memeory-logger.conf\n\n# Name the components on this agent\na2.sources = avro-source\na2.sinks = logger-sink\na2.channels = memory-channel\n\n# Describe/configure the source\na2.sources.avro-source.type = avro\na2.sources.avro-source.bind = localhost\na2.sources.avro-source.port = 44444\n\n# Describe the sink\na2.sinks.logger-sink.type = logger\n\n# Use a channel which buffers events in memory\na2.channels.memory-channel.type = memory\na2.channels.memory-channel.capacity = 1000\na2.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na2.sources.avro-source.channels = memory-channel\na2.sinks.logger-sink.channel = memory-channel\n\n\n\n\n\u542f\u52a8flume\uff0c \u6ce8\u610f\u4e24\u4e2aagent\u7684\u542f\u52a8\u987a\u5e8f\n\n\n \n$ flume-ng agent \n\\\n\n--name a2 \n\\\n\n--conf \n$F\nLUME-HOME/conf \n\\\n\n--conf-file avro-memory-logger.conf \n\\\n\n-Dflume.root.logger\n=\nINFO,console\n\n$ flume-ng agent \n\\\n\n--name a1 \n\\\n\n--conf \n$F\nLUME-HOME/conf \n\\\n\n--conf-file exec-memory-avro.conf \n\\\n\n-Dflume.root.logger\n=\nINFO,console\n\n\n\n\u5c06\u5185\u5bb9\u8f93\u5165\u5230\n/tmp/data.log\n\u6587\u4ef6\u4e2d\uff1a\n\n\n \n$ \necho\n \nwelcome\n \n data.log\n$ \necho\n \nwelcome\n \n data.log\n\n\n\n3 \u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217Kafka\n\n\nFirst a few concepts:\n\n\n\n\nKafka is run as a cluster on one or more servers that can span multiple datacenters.\n\n\nThe Kafka cluster stores streams of \nrecords\n in categories called \ntopic\ns.\n\n\nEach record consists of a key, a value, and a timestamp.\n\n\nBroker\ns are the Kafka processes that manage topics and partitions and serve producer and consumer request.\n\n\n\n\n\n\nKafka\u90e8\u7f72\u53ca\u4f7f\u7528\n\n\n\u5355\u8282\u70b9\u5355Broker\u90e8\u7f72\u53ca\u4f7f\u7528\n\n\n \n# \u542f\u52a8Zookeep\n\n$ zkServer.sh start\n\n# \u542f\u52a8kafka\n\n$ kafka-server-start.sh $KAFKA_HOME/config/server.properties\n\n# \u521b\u5efa\u540d\u4e3atest\u7684topic(single partition and only one replica)\n\n$ kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor \n1\n --partitions \n1\n --topic \ntest\n\n\n# \u67e5\u770btopic\n\n$ kafka-topics.sh --list --zookeeper localhost:2181\n\n### \u542f\u52a8\u751f\u4ea7\u8005, 9092\u662fserver\u76d1\u542c\u7aef\u53e3\n\n$ kafka-console-producer.sh --broker-list localhost:9092 --topic \ntest\n\n\n This is a message\n\n This is another message\n\n### \u542f\u52a8\u6d88\u8d39\u8005 --from-beginning\u4ece\u5934\u5f00\u59cb\u63a5\u6536\u6d88\u606f\n\n$ kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic \ntest\n --from-beginning\nThis is a message\nThis is another message\n\n### \u67e5\u770b\u6240\u6709topics\u7684\u8be6\u7ec6\u4fe1\u606f\n\n$ kafka-topics.sh --describe --zookeeper localhost:2181\n\n### \u67e5\u770b\u6307\u5b9atopic\u7684\u8be6\u7ec6\u4fe1\u606f\n\n$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic \ntest\n\n\n\n\n\u5355\u8282\u70b9\u591aBroker\u90e8\u7f72\u53ca\u4f7f\u7528\n\n\n \ncp $KAFKA_HOME/config/server.properties $KAFKA_HOME/config/server-1.properties\ncp $KAFKA_HOME/config/server.properties $KAFKA_HOME/config/server-2.properties\n\n\n\n\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\n\n\n \nconfig/server-1.properties:\n    broker.id=1\n    listeners=PLAINTEXT://:9093\n    log.dirs=/tmp/kafka-logs-1\n\nconfig/server-2.properties:\n    broker.id=2\n    listeners=PLAINTEXT://:9094\n    log.dirs=/tmp/kafka-logs-2\n\n\n\n\u542f\u52a8kafka\n\n\n \n# \u542f\u52a8ZooKeep\n\n$ zkServer.sh start\n\n# \u542f\u52a8kafka server\n\n$ kafka-server-start.sh $KAFKA_HOME/config/server.properties \n\n$ kafka-server-start.sh $KAFKA_HOME/config/server-1.properties \n\n$ kafka-server-start.sh $KAFKA_HOME/config/server-2.properties \n\n\n# \u521b\u5efatopic, 1\u4e2a\u5206\u533a\uff0c\u4e09\u4e2a\u526f\u672c\n\n$ kafka-topics.sh --create --zookeeper localhost:2181 \n\\\n\n    --replication-factor \n3\n --partitions \n1\n --topic my-replicated-topic\n\n# \u67e5\u770btopic\u4fe1\u606f\n\n$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic\nTopic:my-replicated-topic   PartitionCount:1    ReplicationFactor:3 Configs:\n    Topic: my-replicated-topic  Partition: \n0\n    Leader: \n2\n   Replicas: 2,0,1 Isr: 2,0,1\n\n# \u542f\u52a8\u751f\u4ea7\u8005\n\n$ kafka-console-producer.sh --broker-list localhost:9092, localhost:9093, localhost:9094 --topic my-replicated-topic\n\n# \u542f\u52a8\u6d88\u8d39\u8005\n\n$ kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic\n\n\n\nKafka Java \u7f16\u7a0b\n\n\n\u4f7f\u7528\u547d\u4ee4\u884c\u603b\u662f\u4e0d\u65b9\u4fbf\u7684\uff0c\u4e0b\u9762\u6211\u4eec\u5c1d\u8bd5\u7740\u4f7f\u7528Kafka Java API\u7f16\u7a0b\uff0c\u5b9e\u9645\u5185\u5bb9\u548c\u4e0a\u4e00\u8282\u662f\u4e00\u6478\u4e00\u6837\u7684\uff0c\u6240\u4ee5\u76f4\u63a5\u9644\u4e0a\u4ee3\u7801\u4e86\u3002\u6ce8\u610f\u8fd9\u91cc\u4f7f\u7528\u7684API\u662f0.8.2\u7248\u672c\u4ee5\u540e\u7684\uff0c\u4e4b\u524d\u7248\u672c\u4e0e\u4e4b\u540e\u7248\u672c\u7684API\u76f8\u5dee\u975e\u5e38\u5927\u3002\n\n\nProducer\nimport\n \norg.apache.kafka.clients.producer.KafkaProducer\n;\n\n\nimport\n \norg.apache.kafka.clients.producer.ProducerRecord\n;\n\n\nimport\n \njava.util.Properties\n;\n\n\n/**\n\n\n * Kafka\u751f\u4ea7\u8005\n\n\n * \u89c1\u5b98\u65b9\u6587\u6863\n\n\n * http://kafka.apache.org/20/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html\n\n\n */\n\n\npublic\n \nclass\n \nMyKafkaProducer\n \nimplements\n \nRunnable\n \n{\n\n    \nprivate\n \nString\n \ntopic\n;\n\n    \nprivate\n \nKafkaProducer\nString\n,\n \nString\n \nproducer\n;\n\n\n    \npublic\n \nMyKafkaProducer\n(\nString\n \ntopic\n)\n \n{\n\n        \nthis\n.\ntopic\n \n=\n \ntopic\n;\n\n        \nProperties\n \nprops\n \n=\n \nnew\n \nProperties\n();\n\n        \nprops\n.\nput\n(\nbootstrap.servers\n,\n \nlocalhost:9092\n);\n\n        \nprops\n.\nput\n(\nacks\n,\n \nall\n);\n\n        \nprops\n.\nput\n(\nkey.serializer\n,\n \norg.apache.kafka.common.serialization.StringSerializer\n);\n\n        \nprops\n.\nput\n(\nvalue.serializer\n,\n \norg.apache.kafka.common.serialization.StringSerializer\n);\n\n        \nproducer\n \n=\n \nnew\n \nKafkaProducer\nString\n,\n \nString\n(\nprops\n);\n\n    \n}\n\n\n    \npublic\n \nvoid\n \nrun\n()\n \n{\n\n        \nint\n \nmessageNumber\n \n=\n \n1\n;\n\n        \nwhile\n \n(\ntrue\n)\n \n{\n\n            \nString\n \nmessage\n \n=\n \nmessage\n \n+\n \nmessageNumber\n;\n\n            \nproducer\n.\nsend\n(\nnew\n \nProducerRecord\nString\n,\n \nString\n(\ntopic\n,\n \nmessage\n));\n\n            \nmessageNumber\n++;\n\n            \ntry\n{\n\n                \nThread\n.\nsleep\n(\n5000\n);\n\n            \n}\n \ncatch\n \n(\nException\n \nex\n)\n \n{\n\n                \nex\n.\nprintStackTrace\n();\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n}\n\n\nConsumer\nimport\n \norg.apache.kafka.clients.consumer.ConsumerRecord\n;\n\n\nimport\n \norg.apache.kafka.clients.consumer.ConsumerRecords\n;\n\n\nimport\n \norg.apache.kafka.clients.consumer.KafkaConsumer\n;\n\n\n\nimport\n \njava.time.Duration\n;\n\n\nimport\n \njava.util.Arrays\n;\n\n\nimport\n \njava.util.List\n;\n\n\nimport\n \njava.util.Properties\n;\n\n\nimport\n \njava.util.concurrent.atomic.AtomicBoolean\n;\n\n\n\n/**\n\n\n * Kafka\u6d88\u8d39\u8005\n\n\n * \u5b98\u65b9\u6587\u6863\n\n\n * http://kafka.apache.org/20/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html\n\n\n */\n\n\npublic\n \nclass\n \nMyKafkaConsumer\n \nimplements\n \nRunnable\n \n{\n\n    \nprivate\n \nfinal\n \nAtomicBoolean\n \nclosed\n \n=\n \nnew\n \nAtomicBoolean\n(\nfalse\n);\n\n    \nprivate\n \nString\n \ntopic\n;\n\n    \nprivate\n \nKafkaConsumer\nString\n,\n \nString\n \nconsumer\n;\n\n    \nprivate\n \nConsumerRecords\nString\n,\n \nString\n \nrecords\n;\n\n\n    \npublic\n \nMyKafkaConsumer\n(\nString\n \ntopic\n)\n \n{\n\n        \nthis\n.\ntopic\n \n=\n \ntopic\n;\n\n        \nProperties\n \nprops\n \n=\n \nnew\n \nProperties\n();\n\n        \n// connect to cluster\n\n        \nprops\n.\nput\n(\nbootstrap.servers\n,\n \nlocalhost:9092\n);\n\n        \n//  subscribing to the topics- test\n\n        \nprops\n.\nput\n(\ngroup.id\n,\n \ntest\n);\n\n        \n//  offsets are committed automatically\n\n        \nprops\n.\nput\n(\nenable.auto.commit\n,\n \ntrue\n);\n\n        \n// specify how to turn bytes into objects\n\n        \nprops\n.\nput\n(\nkey.deserializer\n,\n \norg.apache.kafka.common.serialization.StringDeserializer\n);\n\n        \nprops\n.\nput\n(\nvalue.deserializer\n,\n \norg.apache.kafka.common.serialization.StringDeserializer\n);\n\n        \nconsumer\n \n=\n \nnew\n \nKafkaConsumer\n(\nprops\n);\n\n\n    \n}\n\n\n    \npublic\n \nvoid\n \nrun\n()\n \n{\n\n        \ntry\n \n{\n\n            \n// subsribes to topic\n\n            \nconsumer\n.\nsubscribe\n(\nArrays\n.\nasList\n(\ntopic\n));\n\n            \nwhile\n \n(!\nclosed\n.\nget\n())\n \n{\n\n                \nrecords\n \n=\n \nconsumer\n.\npoll\n(\nDuration\n.\nofMillis\n(\n10000\n));\n\n                \nfor\n \n(\nConsumerRecord\nString\n,\n \nString\n \nrecord\n \n:\n \nrecords\n)\n\n                    \nSystem\n.\nout\n.\nprintf\n(\noffset = %d, key = %s, value = %s%n\n,\n \nrecord\n.\noffset\n(),\n \nrecord\n.\nkey\n(),\n \nrecord\n.\nvalue\n());\n\n            \n}\n\n        \n}\n \ncatch\n \n(\nException\n \ne\n)\n \n{\n\n            \n// Ignore exception if closing\n\n            \nif\n \n(!\nclosed\n.\nget\n())\n \nthrow\n \ne\n;\n\n        \n}\n \nfinally\n \n{\n\n            \nconsumer\n.\nclose\n();\n\n        \n}\n\n\n    \n}\n\n\n    \n// Shutdown hook which can be called from a separate thread\n\n    \npublic\n \nvoid\n \nshutdown\n()\n \n{\n\n        \nclosed\n.\nset\n(\ntrue\n);\n\n        \nconsumer\n.\nwakeup\n();\n\n    \n}\n\n\n\n}\n\n\nClientapp\npublic\n \nclass\n \nClientApp\n \n{\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \n{\n\n        \nThread\n \njob\n \n=\n \nnew\n \nThread\n(\nnew\n \nMyKafkaProducer\n(\ntest\n));\n\n        \njob\n.\nstart\n();\n\n        \nThread\n \njob2\n \n=\n \nnew\n \nThread\n(\nnew\n \nMyKafkaConsumer\n(\ntest\n));\n\n        \njob2\n.\nstart\n();\n\n    \n}\n\n\n}\n\n\n\n\n\n\u6574\u5408Flume\u548cKafka\u5b8c\u6210\u5b9e\u65f6\u6570\u636e\u91c7\u96c6\n\n\n\u4e3a\u4e86\u5c06Flume\u7684\u8f93\u51fa\u5230Kafka\uff0c\u53ef\u4ee5\u5c06agent2\u7684logger sink\u66ff\u6362\u6210Kafka Sink\u3002\u7136\u540e\u542f\u52a8\u4e00\u4e2aKafka consumer\u4eceKafka sink\u8ba2\u9605\u6d88\u606f\u3002\n\n\n\n\n\n\nkafka sink\n can publish data to a Kafka topic. One of the objective is to integrate Flume with Kafka so that pull based processing systems can process the data coming through various Flume sources. [\nKafka Sink\n]\n\n\n\n\n\u4e0b\u9762\u662fagent2\u5bf9\u5e94\u7684Kafka\u914d\u7f6e\u6587\u4ef6\uff0c\u5728\u8fd9\u91ccagent2\u6539\u540d\u4e3a\navro-memory-kafka\n\u3002\n\n\n \n# filename: avro-memeory-kafka.conf\n\n# Name the components on this agent\navro-memory-kafka.sources = avro-source\navro-memory-kafka.sinks = kafka-sink\navro-memory-kafka.channels = memory-channel\n\n# Describe/configure the source\navro-memory-kafka.sources.avro-source.type = avro\navro-memory-kafka.sources.avro-source.bind = localhost\navro-memory-kafka.sources.avro-source.port = 44444\n\n# Describe the sink\navro-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink\navro-memory-kafka.sinks.kafka-sink.kafka.bootstrap.servers = localhost:9092\navro-memory-kafka.sinks.kafka-sink.kafka.topic = test\n\n\n# Use a channel which buffers events in memory\navro-memory-kafka.channels.memory-channel.type = memory\navro-memory-kafka.channels.memory-channel.capacity = 1000\navro-memory-kafka.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\navro-memory-kafka.sources.avro-source.channels = memory-channel\navro-memory-kafka.sinks.kafka-sink.channel = memory-channel\n\n\n\n\u4e0b\u9762\u662f\u5177\u4f53\u7684\u64cd\u4f5c\u6d41\u7a0b\uff0c\u540c\u6837\u9700\u8981\u6ce8\u610f\u4e24\u4e2aagent\u7684\u542f\u52a8\u987a\u5e8f\uff1a\n\n\n \n## \u542f\u52a8zookeeper, kafka\uff0c\u7701\u7565\n\n\n## \u542f\u52a8agent\n\n$ flume-ng agent \n\\\n\n--name avro-memory-kafka \n\\\n\n--conf \n$F\nLUME-HOME/conf \n\\\n\n--conf-file avro-memory-kafka.conf \n\\\n\n-Dflume.root.logger\n=\nINFO,console\n\n$ flume-ng agent \n\\\n\n--name a1 \n\\\n\n--conf \n$F\nLUME-HOME/conf \n\\\n\n--conf-file exec-memory-avro.conf \n\\\n\n-Dflume.root.logger\n=\nINFO,console\n\n\n## \u542f\u52a8\u6d88\u8d39\u8005 \n\n$ kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic \ntest\n\n\n\n\n4 Spark Streaming \u5165\u95e8\n\n\n\n\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like \nmap\n, \nreduce\n, \njoin\n and \nwindow\n. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark\u2019s machine learning and graph processing algorithms on data streams. \nref\n\n\n\n\n\n\n\n\nSpark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.", 
            "title": "SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#spark-streaming", 
            "text": "\u8be5\u9879\u76ee\u4ece\u5b9e\u65f6\u6570\u636e\u4ea7\u751f\u548c\u6d41\u5411\u7684\u4e0d\u540c\u73af\u8282\u51fa\u53d1\uff0c\u901a\u8fc7\u96c6\u6210\u4e3b\u6d41\u7684\u5206\u5e03\u5f0f\u65e5\u5fd7\u6536\u96c6\u6846\u67b6Flume\u3001\u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217Kafka\u3001\u5206\u5e03\u5f0f\u5217\u5f0f\u6570\u636e\u5e93HBase\u3001\u4ee5\u53caSpark Streaming\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5904\u7406\u3002", 
            "title": "Spark Streaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#1", 
            "text": "", 
            "title": "1 \u521d\u8bc6\u5b9e\u65f6\u6d41\u5904\u7406"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_1", 
            "text": "\u9700\u6c42\uff1a\u7edf\u8ba1\u4e3b\u7ad9\u6bcf\u4e2a\uff08\u6307\u5b9a\uff09\u8bfe\u7a0b\u8bbf\u95ee\u7684\u5ba2\u6237\u7aef\u3001\u5730\u57df\u4fe1\u606f\u5206\u5e03  ==  \u5982\u4e0a\u4e24\u4e2a\u64cd\u4f5c\uff1a\u91c7\u7528\u79bb\u7ebf\uff08spark/mapreduce\uff09\u7684\u65b9\u5f0f\u8fdb\u884c\u7edf\u8ba1  \u5b9e\u73b0\u6b65\u9aa4\uff1a   \u8bfe\u7a0b\u7f16\u53f7\uff0cip\u4fe1\u606f\uff0cuser-agent  \u8fdb\u884c\u76f8\u5e94\u7684\u7edf\u8ba1\u5206\u6790\u64cd\u4f5c\uff1aMapReduce/Spark   \u9879\u76ee\u67b6\u6784\uff1a   \u65e5\u5fd7\u6536\u96c6\uff1aFlume  \u79bb\u7ebf\u5206\u6790\uff1aMapReduce/Spark  \u7edf\u8ba1\u7ed3\u679c\u56fe\u5f62\u5316\u5c55\u793a   \u95ee\u9898\uff1a   \u5c0f\u65f6\u7ea7\u522b  10\u5206\u949f  \u79d2\u7ea7\u522b", 
            "title": "\u4e1a\u52a1\u73b0\u72b6\u5206\u6790"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_2", 
            "text": "\u65f6\u6548\u6027\u9ad8  \u6570\u636e\u91cf\u5927", 
            "title": "\u5b9e\u65f6\u6d41\u5904\u7406\u4ea7\u751f\u80cc\u666f"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_3", 
            "text": "https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101   \u5b9e\u65f6\u8ba1\u7b97 apache storm  \u6d41\u5f0f\u8ba1\u7b97  \u5b9e\u65f6\u6d41\u5f0f\u8ba1\u7b97", 
            "title": "\u5b9e\u65f6\u6d41\u5904\u7406\u6982\u8ff0"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_4", 
            "text": "\u6570\u636e\u6765\u6e90  \u79bb\u7ebf\uff1a\u6765\u81eaHDFS\u4e0a\u7684\u5386\u53f2\u6570\u636e\uff0c\u6570\u636e\u91cf\u6bd4\u8f83\u5927  \u5b9e\u65f6\uff1a\u6765\u81ea\u6d88\u606f\u961f\u5217(Kafka)\uff0c\u662f\u5b9e\u65f6\u65b0\u589e/\u4fee\u6539\u8bb0\u5f55\u8fc7\u6765\u7684\u67d0\u4e00\u7b14\u6570\u636e    \u5904\u7406\u8fc7\u7a0b  \u79bb\u7ebf\uff1aMapReduce, map + reduce  \u5b9e\u65f6: Spark(DStream/SS)     \u5904\u7406\u901f\u5ea6  \u79bb\u7ebf\uff1a\u5e54  \u5b9e\u65f6\uff1a\u5feb\u901f     \u8fdb\u7a0b  \u79bb\u7ebf\uff1a\u8fdb\u7a0b\u6709\u542f\u52a8+\u9500\u6bc1\u7684\u8fc7\u7a0b  \u5b9e\u65f6\uff1a 7*24\u5c0f\u65f6\u8fd0\u884c", 
            "title": "\u79bb\u7ebf\u8ba1\u7b97\u4e0e\u5b9e\u65f6\u8ba1\u7b97\u5bf9\u6bd4"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_5", 
            "text": "Apache Storm    Apache Storm is a free and open source distributed  realtime  computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use!    Apache Spark Streaming    \u5b9e\u9645\u4e0a\u662f\u5fae\u6279\u5904\u7406\uff08\u6279\u5904\u7406\u95f4\u9694\u975e\u5e38\u5c0f)    Apache kafka  Apache Flink    Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.", 
            "title": "\u5b9e\u65f6\u6d41\u5904\u7406\u6846\u67b6\u5bf9\u6bd4"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_6", 
            "text": "\u52a0\u4e00\u5c42flume\u6d88\u606f\u961f\u5217\uff0c\u4e3b\u8981\u4e3a\u4e86\u51cf\u8f7b\u538b\u529b\uff0c\u8d77\u5230\u7f13\u51b2\u4f5c\u7528", 
            "title": "\u5b9e\u65f6\u6d41\u5904\u7406\u67b6\u6784\u548c\u6280\u672f\u9009\u578b"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_7", 
            "text": "\u7535\u4fe1\u884c\u4e1a\uff1a \u4f60\u7684\u624b\u673a\u5957\u9910\u6d41\u91cf\u7528\u5b8c\uff0c\u6536\u5230\u77ed\u4fe1\u63d0\u793a  \u7535\u5546\u884c\u4e1a\uff1a\u641c\u7d22\u5546\u54c1\u65f6\uff0c\u8fdb\u884c\u63a8\u8350", 
            "title": "\u5b9e\u65f6\u6d41\u5904\u7406\u5728\u4f01\u4e1a\u4e2d\u7684\u5e94\u7528"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#2-flume", 
            "text": "see detail in Hadoop: definitive Guide,  Chapter 14", 
            "title": "2 \u5206\u5e03\u5f0f\u65e5\u5fd7\u6536\u96c6\u6846\u67b6Flume"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_8", 
            "text": "You have a lot of servers and systems   network devices  operating system  web servers  applications   And they generate large amount of logs and other data.  Problem: Since you have a business idea, how to implement the idea?  OPTION: You may move logs and data generated to hadoop hdfs directly.  \u4f46\u662f\u5b58\u5728\u95ee\u9898\uff1a   \u5982\u4f55\u505a\u76d1\u63a7  \u5982\u4f55\u4fdd\u8bc1\u65f6\u6548\u6027  \u76f4\u63a5\u4f20\u9001\u6587\u672c\u6570\u636e\uff0c\u5f00\u9500\u592a\u5927  \u5bb9\u9519  \u8d1f\u8f7d\u5747\u8861   SOLUTION: \u4f7f\u7528Flume\uff0c\u57fa\u672c\u4e0a\u5199\u914d\u7f6e\u6587\u4ef6\u5c31OK\u4e86\uff0cFlume\u81ea\u52a8\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002", 
            "title": "\u4e1a\u52a1\u73b0\u72b6\u5206\u6790"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#flume", 
            "text": "Flume is a distributed, reliable, and available service for efficiently  collecting, aggregating, and moving large amounts of log data . It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application. [ Apache Flume ]", 
            "title": "Flume\u6982\u8ff0"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#flume_1", 
            "text": "see detail in Hadoop: definitive Guide,  Chapter 14", 
            "title": "Flume\u67b6\u6784\u53ca\u6838\u5fc3\u7ec4\u4ef6"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#flume_2", 
            "text": "\u9700\u6c42\uff1a \u4ece\u6307\u5b9a\u7f51\u7edc\u7aef\u53e3\u91c7\u96c6\u6570\u636e  \u4f7f\u7528Flume\u7684\u5173\u952e\u5c31\u662f\u5199\u914d\u7f6e\u6587\u4ef6   \u914d\u7f6eSource, Channel, Sink  \u628a\u4ee5\u4e0a\u4e09\u4e2a\u7ec4\u4ef6\u4e32\u8d77\u6765     http://flume.apache.org/FlumeUserGuide.html#example-2\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 44444\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1   netcat source : A netcat-like source that listens on a given port and turns each line of text into an event. It opens a specified port and listens for data. The expectation is that the supplied data is newline separated text. Each line of text is turned into a Flume event and sent via the connected channel. [ NetCat TCP Source ]  logger sink : Logs event at INFO level. Typically useful for testing/debugging purpose.  [ Logger Sink ]  memory channel : The events are stored in an in-memory queue with configurable max size. It\u2019s ideal for flows that need higher throughput and are prepared to lose the staged data in the event of an agent failures. [ memory channel ]     ## \u542f\u52a8flume \n$ flume-ng agent  \\ \n--name a1  \\    # agent name \n--conf  $F LUME_HOME/conf  \\  # use configs in  conf  directory \n--conf-file  example.conf  \\  # specify a config file \n-Dflume.root.logger = INFO,console  # sets a Java system property value  ## \u5728\u53e6\u5916\u4e00\u4e2aterminal\u7528telnet\u6a21\u62df\u6570\u636e\u6e90 \n$ telnet localhost  44444  \nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is  ^] .\nhello\nOK\nhellomy\nOK  \u9700\u6c42\uff1a \u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\u5b9e\u65f6\u91c7\u96c6\u65b0\u589e\u7684\u6570\u636e\u8f93\u51fa\u5230\u63a7\u5236\u53f0  Agent\u9009\u578b\uff1a exec source + memory channel + logger sink    # filename: exec-memeory-logger.conf\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /tmp/data.log\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1   exec source  runs a given Unix command on start-up and expects that process to continuously produce data on standard out (stderr is simply discarded, unless property logStdErr is set to true). If the process exits for any reason, the source also exits and will produce no further data. This means configurations such as cat [named pipe] or tail -F [file] are going to produce the desired results where as date will probably not - the former two commands produce streams of data where as the latter produces a single event and exits. [ exec source ]   \u5c06\u5185\u5bb9\u8f93\u5165\u5230 /tmp/data.log \u6587\u4ef6\u4e2d\uff1a    $  echo   hello    data.log\n$  echo   hello    data.log  \u9700\u6c42\uff1a \u5c06A\u670d\u52a1\u5668\u4e0a\u7684\u65e5\u5fd7\u5b9e\u65f6\u91c7\u96c6\u5230B\u670d\u52a1\u5668  \u65e5\u5fd7\u6536\u96c6\u8fc7\u7a0b\uff1a   \u673a\u56681\u4e0a\u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\uff0c\u5f53\u6211\u4eec\u8bbf\u95ee\u4e3b\u7ad9\u65f6\u4f1a\u6709\u7528\u6237\u884c\u4e3a\u65e5\u5fd7\u8bb0\u5f55\u5230 access.log \u4e2d\u3002  avro sink\u628a\u65b0\u4ea7\u751f\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u5bf9\u5e94\u7684avro source\u6307\u5b9a\u7684hostname\u548cport\u4e0a\u3002  \u901a\u8fc7avro\u5bf9\u5e94\u7684agent\u5c06\u6211\u4eec\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u63a7\u5236\u53f0\u3002     avro sink : forms one half of Flume\u2019s tiered collection support. Flume events sent to this sink are turned into Avro events and sent to the configured hostname / port pair. [ Avro sink ]   Exec-Memeory-Avro.conf # filename: exec-memeory-avro.conf\n\n# Name the components on this agent\na1.sources = exec-source\na1.sinks = avro-sink\na1.channels = memory-channel\n\n# Describe/configure the source\na1.sources.exec-source.type = exec\na1.sources.exec-source.command = tail -F /tmp/data.log\n\n# Describe the sink\na1.sinks.avro-sink.type = avro\na1.sinks.avro-sink.hostname = localhost\na1.sinks.avro-sink.port = 44444\n\n# Use a channel which buffers events in memory\na1.channels.memory-channel.type = memory\na1.channels.memory-channel.capacity = 1000\na1.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.exec-source.channels = memory-channel\na1.sinks.avro-sink.channel = memory-channel Avro-Memeory-Logger.conf # filename: avro-memeory-logger.conf\n\n# Name the components on this agent\na2.sources = avro-source\na2.sinks = logger-sink\na2.channels = memory-channel\n\n# Describe/configure the source\na2.sources.avro-source.type = avro\na2.sources.avro-source.bind = localhost\na2.sources.avro-source.port = 44444\n\n# Describe the sink\na2.sinks.logger-sink.type = logger\n\n# Use a channel which buffers events in memory\na2.channels.memory-channel.type = memory\na2.channels.memory-channel.capacity = 1000\na2.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na2.sources.avro-source.channels = memory-channel\na2.sinks.logger-sink.channel = memory-channel  \u542f\u52a8flume\uff0c \u6ce8\u610f\u4e24\u4e2aagent\u7684\u542f\u52a8\u987a\u5e8f    $ flume-ng agent  \\ \n--name a2  \\ \n--conf  $F LUME-HOME/conf  \\ \n--conf-file avro-memory-logger.conf  \\ \n-Dflume.root.logger = INFO,console\n\n$ flume-ng agent  \\ \n--name a1  \\ \n--conf  $F LUME-HOME/conf  \\ \n--conf-file exec-memory-avro.conf  \\ \n-Dflume.root.logger = INFO,console  \u5c06\u5185\u5bb9\u8f93\u5165\u5230 /tmp/data.log \u6587\u4ef6\u4e2d\uff1a    $  echo   welcome    data.log\n$  echo   welcome    data.log", 
            "title": "Flume\u5b9e\u6218"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#3-kafka", 
            "text": "First a few concepts:   Kafka is run as a cluster on one or more servers that can span multiple datacenters.  The Kafka cluster stores streams of  records  in categories called  topic s.  Each record consists of a key, a value, and a timestamp.  Broker s are the Kafka processes that manage topics and partitions and serve producer and consumer request.", 
            "title": "3 \u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217Kafka"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#kafka", 
            "text": "\u5355\u8282\u70b9\u5355Broker\u90e8\u7f72\u53ca\u4f7f\u7528    # \u542f\u52a8Zookeep \n$ zkServer.sh start # \u542f\u52a8kafka \n$ kafka-server-start.sh $KAFKA_HOME/config/server.properties # \u521b\u5efa\u540d\u4e3atest\u7684topic(single partition and only one replica) \n$ kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor  1  --partitions  1  --topic  test  # \u67e5\u770btopic \n$ kafka-topics.sh --list --zookeeper localhost:2181 ### \u542f\u52a8\u751f\u4ea7\u8005, 9092\u662fserver\u76d1\u542c\u7aef\u53e3 \n$ kafka-console-producer.sh --broker-list localhost:9092 --topic  test   This is a message  This is another message ### \u542f\u52a8\u6d88\u8d39\u8005 --from-beginning\u4ece\u5934\u5f00\u59cb\u63a5\u6536\u6d88\u606f \n$ kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic  test  --from-beginning\nThis is a message\nThis is another message ### \u67e5\u770b\u6240\u6709topics\u7684\u8be6\u7ec6\u4fe1\u606f \n$ kafka-topics.sh --describe --zookeeper localhost:2181 ### \u67e5\u770b\u6307\u5b9atopic\u7684\u8be6\u7ec6\u4fe1\u606f \n$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic  test   \u5355\u8282\u70b9\u591aBroker\u90e8\u7f72\u53ca\u4f7f\u7528    cp $KAFKA_HOME/config/server.properties $KAFKA_HOME/config/server-1.properties\ncp $KAFKA_HOME/config/server.properties $KAFKA_HOME/config/server-2.properties  \u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b    config/server-1.properties:\n    broker.id=1\n    listeners=PLAINTEXT://:9093\n    log.dirs=/tmp/kafka-logs-1\n\nconfig/server-2.properties:\n    broker.id=2\n    listeners=PLAINTEXT://:9094\n    log.dirs=/tmp/kafka-logs-2  \u542f\u52a8kafka    # \u542f\u52a8ZooKeep \n$ zkServer.sh start # \u542f\u52a8kafka server \n$ kafka-server-start.sh $KAFKA_HOME/config/server.properties  \n$ kafka-server-start.sh $KAFKA_HOME/config/server-1.properties  \n$ kafka-server-start.sh $KAFKA_HOME/config/server-2.properties   # \u521b\u5efatopic, 1\u4e2a\u5206\u533a\uff0c\u4e09\u4e2a\u526f\u672c \n$ kafka-topics.sh --create --zookeeper localhost:2181  \\ \n    --replication-factor  3  --partitions  1  --topic my-replicated-topic # \u67e5\u770btopic\u4fe1\u606f \n$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic\nTopic:my-replicated-topic   PartitionCount:1    ReplicationFactor:3 Configs:\n    Topic: my-replicated-topic  Partition:  0     Leader:  2    Replicas: 2,0,1 Isr: 2,0,1 # \u542f\u52a8\u751f\u4ea7\u8005 \n$ kafka-console-producer.sh --broker-list localhost:9092, localhost:9093, localhost:9094 --topic my-replicated-topic # \u542f\u52a8\u6d88\u8d39\u8005 \n$ kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic", 
            "title": "Kafka\u90e8\u7f72\u53ca\u4f7f\u7528"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#kafka-java", 
            "text": "\u4f7f\u7528\u547d\u4ee4\u884c\u603b\u662f\u4e0d\u65b9\u4fbf\u7684\uff0c\u4e0b\u9762\u6211\u4eec\u5c1d\u8bd5\u7740\u4f7f\u7528Kafka Java API\u7f16\u7a0b\uff0c\u5b9e\u9645\u5185\u5bb9\u548c\u4e0a\u4e00\u8282\u662f\u4e00\u6478\u4e00\u6837\u7684\uff0c\u6240\u4ee5\u76f4\u63a5\u9644\u4e0a\u4ee3\u7801\u4e86\u3002\u6ce8\u610f\u8fd9\u91cc\u4f7f\u7528\u7684API\u662f0.8.2\u7248\u672c\u4ee5\u540e\u7684\uff0c\u4e4b\u524d\u7248\u672c\u4e0e\u4e4b\u540e\u7248\u672c\u7684API\u76f8\u5dee\u975e\u5e38\u5927\u3002  Producer import   org.apache.kafka.clients.producer.KafkaProducer ;  import   org.apache.kafka.clients.producer.ProducerRecord ;  import   java.util.Properties ;  /**   * Kafka\u751f\u4ea7\u8005   * \u89c1\u5b98\u65b9\u6587\u6863   * http://kafka.apache.org/20/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html   */  public   class   MyKafkaProducer   implements   Runnable   { \n     private   String   topic ; \n     private   KafkaProducer String ,   String   producer ; \n\n     public   MyKafkaProducer ( String   topic )   { \n         this . topic   =   topic ; \n         Properties   props   =   new   Properties (); \n         props . put ( bootstrap.servers ,   localhost:9092 ); \n         props . put ( acks ,   all ); \n         props . put ( key.serializer ,   org.apache.kafka.common.serialization.StringSerializer ); \n         props . put ( value.serializer ,   org.apache.kafka.common.serialization.StringSerializer ); \n         producer   =   new   KafkaProducer String ,   String ( props ); \n     } \n\n     public   void   run ()   { \n         int   messageNumber   =   1 ; \n         while   ( true )   { \n             String   message   =   message   +   messageNumber ; \n             producer . send ( new   ProducerRecord String ,   String ( topic ,   message )); \n             messageNumber ++; \n             try { \n                 Thread . sleep ( 5000 ); \n             }   catch   ( Exception   ex )   { \n                 ex . printStackTrace (); \n             } \n         } \n     }  }  Consumer import   org.apache.kafka.clients.consumer.ConsumerRecord ;  import   org.apache.kafka.clients.consumer.ConsumerRecords ;  import   org.apache.kafka.clients.consumer.KafkaConsumer ;  import   java.time.Duration ;  import   java.util.Arrays ;  import   java.util.List ;  import   java.util.Properties ;  import   java.util.concurrent.atomic.AtomicBoolean ;  /**   * Kafka\u6d88\u8d39\u8005   * \u5b98\u65b9\u6587\u6863   * http://kafka.apache.org/20/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html   */  public   class   MyKafkaConsumer   implements   Runnable   { \n     private   final   AtomicBoolean   closed   =   new   AtomicBoolean ( false ); \n     private   String   topic ; \n     private   KafkaConsumer String ,   String   consumer ; \n     private   ConsumerRecords String ,   String   records ; \n\n     public   MyKafkaConsumer ( String   topic )   { \n         this . topic   =   topic ; \n         Properties   props   =   new   Properties (); \n         // connect to cluster \n         props . put ( bootstrap.servers ,   localhost:9092 ); \n         //  subscribing to the topics- test \n         props . put ( group.id ,   test ); \n         //  offsets are committed automatically \n         props . put ( enable.auto.commit ,   true ); \n         // specify how to turn bytes into objects \n         props . put ( key.deserializer ,   org.apache.kafka.common.serialization.StringDeserializer ); \n         props . put ( value.deserializer ,   org.apache.kafka.common.serialization.StringDeserializer ); \n         consumer   =   new   KafkaConsumer ( props ); \n\n     } \n\n     public   void   run ()   { \n         try   { \n             // subsribes to topic \n             consumer . subscribe ( Arrays . asList ( topic )); \n             while   (! closed . get ())   { \n                 records   =   consumer . poll ( Duration . ofMillis ( 10000 )); \n                 for   ( ConsumerRecord String ,   String   record   :   records ) \n                     System . out . printf ( offset = %d, key = %s, value = %s%n ,   record . offset (),   record . key (),   record . value ()); \n             } \n         }   catch   ( Exception   e )   { \n             // Ignore exception if closing \n             if   (! closed . get ())   throw   e ; \n         }   finally   { \n             consumer . close (); \n         } \n\n     } \n\n     // Shutdown hook which can be called from a separate thread \n     public   void   shutdown ()   { \n         closed . set ( true ); \n         consumer . wakeup (); \n     }  }  Clientapp public   class   ClientApp   { \n     public   static   void   main ( String []   args )   { \n         Thread   job   =   new   Thread ( new   MyKafkaProducer ( test )); \n         job . start (); \n         Thread   job2   =   new   Thread ( new   MyKafkaConsumer ( test )); \n         job2 . start (); \n     }  }", 
            "title": "Kafka Java \u7f16\u7a0b"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#flumekafka", 
            "text": "\u4e3a\u4e86\u5c06Flume\u7684\u8f93\u51fa\u5230Kafka\uff0c\u53ef\u4ee5\u5c06agent2\u7684logger sink\u66ff\u6362\u6210Kafka Sink\u3002\u7136\u540e\u542f\u52a8\u4e00\u4e2aKafka consumer\u4eceKafka sink\u8ba2\u9605\u6d88\u606f\u3002    kafka sink  can publish data to a Kafka topic. One of the objective is to integrate Flume with Kafka so that pull based processing systems can process the data coming through various Flume sources. [ Kafka Sink ]   \u4e0b\u9762\u662fagent2\u5bf9\u5e94\u7684Kafka\u914d\u7f6e\u6587\u4ef6\uff0c\u5728\u8fd9\u91ccagent2\u6539\u540d\u4e3a avro-memory-kafka \u3002    # filename: avro-memeory-kafka.conf\n\n# Name the components on this agent\navro-memory-kafka.sources = avro-source\navro-memory-kafka.sinks = kafka-sink\navro-memory-kafka.channels = memory-channel\n\n# Describe/configure the source\navro-memory-kafka.sources.avro-source.type = avro\navro-memory-kafka.sources.avro-source.bind = localhost\navro-memory-kafka.sources.avro-source.port = 44444\n\n# Describe the sink\navro-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink\navro-memory-kafka.sinks.kafka-sink.kafka.bootstrap.servers = localhost:9092\navro-memory-kafka.sinks.kafka-sink.kafka.topic = test\n\n\n# Use a channel which buffers events in memory\navro-memory-kafka.channels.memory-channel.type = memory\navro-memory-kafka.channels.memory-channel.capacity = 1000\navro-memory-kafka.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\navro-memory-kafka.sources.avro-source.channels = memory-channel\navro-memory-kafka.sinks.kafka-sink.channel = memory-channel  \u4e0b\u9762\u662f\u5177\u4f53\u7684\u64cd\u4f5c\u6d41\u7a0b\uff0c\u540c\u6837\u9700\u8981\u6ce8\u610f\u4e24\u4e2aagent\u7684\u542f\u52a8\u987a\u5e8f\uff1a    ## \u542f\u52a8zookeeper, kafka\uff0c\u7701\u7565  ## \u542f\u52a8agent \n$ flume-ng agent  \\ \n--name avro-memory-kafka  \\ \n--conf  $F LUME-HOME/conf  \\ \n--conf-file avro-memory-kafka.conf  \\ \n-Dflume.root.logger = INFO,console\n\n$ flume-ng agent  \\ \n--name a1  \\ \n--conf  $F LUME-HOME/conf  \\ \n--conf-file exec-memory-avro.conf  \\ \n-Dflume.root.logger = INFO,console ## \u542f\u52a8\u6d88\u8d39\u8005  \n$ kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic  test", 
            "title": "\u6574\u5408Flume\u548cKafka\u5b8c\u6210\u5b9e\u65f6\u6570\u636e\u91c7\u96c6"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#4-spark-streaming", 
            "text": "Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like  map ,  reduce ,  join  and  window . Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark\u2019s machine learning and graph processing algorithms on data streams.  ref     Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.", 
            "title": "4 Spark Streaming \u5165\u95e8"
        }, 
        {
            "location": "/books/", 
            "text": "Books and Materials\n\n\nBooks\n\n\nThe following is the primary reading list of books. Each chapter is organized as a single page; the included sections are noted with major concepts, and summary.\n\n\nHadoop\n\n\n\n\nHadoop: The Definitive Guide, 4th Edition\n\n\nby Tom White\n\n\nSpark\n\n\n\n\nLearning Spark: Lighting-Fast Data Analysis\n\n\nby Tom White", 
            "title": "Books"
        }, 
        {
            "location": "/books/#books-and-materials", 
            "text": "", 
            "title": "Books and Materials"
        }, 
        {
            "location": "/books/#books", 
            "text": "The following is the primary reading list of books. Each chapter is organized as a single page; the included sections are noted with major concepts, and summary.", 
            "title": "Books"
        }, 
        {
            "location": "/books/#hadoop", 
            "text": "Hadoop: The Definitive Guide, 4th Edition  by Tom White", 
            "title": "Hadoop"
        }, 
        {
            "location": "/books/#spark", 
            "text": "Learning Spark: Lighting-Fast Data Analysis  by Tom White", 
            "title": "Spark"
        }
    ]
}