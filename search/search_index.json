{
    "docs": [
        {
            "location": "/hadoop/ch4/", 
            "text": "Hadoop: The Definitive Guide 4 - YARN\n\n\nApache YARN(Yet Another Resource Negotiator) is Hadoop\ns cluster resource management system. YARN provides APIs for requesting and working with cluster resources, but these APIs are not typically used directly by user code. Distributed computing frameworks (MapReduce, Spark, and so on) running as YARN applications on the cluster compute layer (YARN) and the cluster storage layer (HDFS and HBase).\n\n\n\n\n1 Anatomy of a YARN Application Run\n\n\nYARN provides its core services via two types of long-running daemon:\n\n\n\n\na \nresource manager\n (one per cluster) to manage the use of resources across the cluster,\n\n\nnode managers\n running on all the nodes in the cluster to launch and monitor \ncontainers\n.\n\n\n\n\n\n\n\n\nstep1 : To run an application on YARN, a client contacts the resource manager and asks it to run an \napplication master\n process.\n\n\nsteps 2a and 2b: The resource manager then finds a node manager that can launch the application master in a container. It could simply run a computation in the container it is running in and return the result to the client.\n\n\nstep 3: Or it could request more containers from the resource managers\n\n\nsteps 4a and 4b: use them to run a distributed computation.\n\n\n\n\nResource Requests\n\n\nA YARN application can make resource requests at any time while it is running. \n\n\n\n\nSpark starts a fixed number of executors on the cluster (i.e. make all of requests up front). \n\n\nMapReduce, has two phases: the map task containers are requested up front, but the reduce task containers are not started until later. (i.e. take a more dynamic approach whereby it requests more resources dynamically to meet the changing needs of the application).\n\n\n\n\nApplication Lifespan\n\n\nThe lifespan of a YARN application can vary dramatically. Rather than look at how long the application runs for, it\ns useful to categorize applications in terms of how they map to the jobs that users run. \n\n\n\n\nThe simplest case is one application per user job, which is the approach that MapReduce takes.\n\n\nThe second model is to run one application per workflow or user session of (possibly unrelated) jobs, which is the approach that Spark takes. This approach can be more efficient than the first, since containers can be reused between jobs, and there is also the potential to cache intermediate data between jobs.\n\n\nThe third model is a long-running application that is shared by different users, which is the approach that Apache Slider takes.\n\n\n\n\nBuilding YARN Applications\n\n\nWriting a YARN application from scratch is fairly involved, but in many cases is not necessary, as it is often possible to use an existing application that fits the bill.\n\n\n2 YARN Compared to MapReduce 1\n\n\nThe distributed implementation of MapReduce in the original version of Hadoop is sometimes referred to as \nMapReduce 1\n to distinguish it from MapReduce 2, the implementation that uses YARN.\n\n\nA comparison of MapReduce 1 and YARN components:\n\n\n\n\n\n\n\n\nMapReduce1\n\n\nYARN\n\n\n\n\n\n\n\n\n\n\nJobtracker\n\n\nResource manager, application master, timeline server\n\n\n\n\n\n\nTaskTracker\n\n\nNode manager\n\n\n\n\n\n\nSlot\n\n\nContainer\n\n\n\n\n\n\n\n\n3 Scheduling in YARN\n\n\nThe job of the YARN scheduler to allocate resources to applications according to some defined policy. Scheduling in general is a difficult problem and there is \nno one \"best\" policy\n, which is why YARN provides a choice of schedulers and configurable policies.\n\n\nScheduler Options\n\n\nThree schedulers are available in YARN: the FIFO, Capacity, and Fair Schedulers.\n\n\n\n\nthe FIFO: places applications in a queue and runs them in the order of submission (first in, first out)\n\n\nnot suitable for shared clusters, because large applications will use all the resources in a cluster, so each application has to wait its turn. \n\n\n\n\n\n\nCapacity Scheduler: a separate dedicated queue allows the small job to start as soon as it is submitted, since the queue capacity is reserved for jobs in that queue.\n\n\nFair Scheduler: dynamically balance resources between all running jobs, each job is using its fair share of resources.\n\n\nThere is a lag between the time the second job starts and when it receives its fair share, since it has to wait for resources to free up as containers used by the first job complete. After the small job completes and no longer requires resources, the large job goes back to using the full cluster capacity again.", 
            "title": "Chapter 4: YARN"
        }, 
        {
            "location": "/hadoop/ch4/#hadoop-the-definitive-guide-4-yarn", 
            "text": "Apache YARN(Yet Another Resource Negotiator) is Hadoop s cluster resource management system. YARN provides APIs for requesting and working with cluster resources, but these APIs are not typically used directly by user code. Distributed computing frameworks (MapReduce, Spark, and so on) running as YARN applications on the cluster compute layer (YARN) and the cluster storage layer (HDFS and HBase).", 
            "title": "Hadoop: The Definitive Guide 4 - YARN"
        }, 
        {
            "location": "/hadoop/ch4/#1-anatomy-of-a-yarn-application-run", 
            "text": "YARN provides its core services via two types of long-running daemon:   a  resource manager  (one per cluster) to manage the use of resources across the cluster,  node managers  running on all the nodes in the cluster to launch and monitor  containers .     step1 : To run an application on YARN, a client contacts the resource manager and asks it to run an  application master  process.  steps 2a and 2b: The resource manager then finds a node manager that can launch the application master in a container. It could simply run a computation in the container it is running in and return the result to the client.  step 3: Or it could request more containers from the resource managers  steps 4a and 4b: use them to run a distributed computation.", 
            "title": "1 Anatomy of a YARN Application Run"
        }, 
        {
            "location": "/hadoop/ch4/#resource-requests", 
            "text": "A YARN application can make resource requests at any time while it is running.    Spark starts a fixed number of executors on the cluster (i.e. make all of requests up front).   MapReduce, has two phases: the map task containers are requested up front, but the reduce task containers are not started until later. (i.e. take a more dynamic approach whereby it requests more resources dynamically to meet the changing needs of the application).", 
            "title": "Resource Requests"
        }, 
        {
            "location": "/hadoop/ch4/#application-lifespan", 
            "text": "The lifespan of a YARN application can vary dramatically. Rather than look at how long the application runs for, it s useful to categorize applications in terms of how they map to the jobs that users run.    The simplest case is one application per user job, which is the approach that MapReduce takes.  The second model is to run one application per workflow or user session of (possibly unrelated) jobs, which is the approach that Spark takes. This approach can be more efficient than the first, since containers can be reused between jobs, and there is also the potential to cache intermediate data between jobs.  The third model is a long-running application that is shared by different users, which is the approach that Apache Slider takes.", 
            "title": "Application Lifespan"
        }, 
        {
            "location": "/hadoop/ch4/#building-yarn-applications", 
            "text": "Writing a YARN application from scratch is fairly involved, but in many cases is not necessary, as it is often possible to use an existing application that fits the bill.", 
            "title": "Building YARN Applications"
        }, 
        {
            "location": "/hadoop/ch4/#2-yarn-compared-to-mapreduce-1", 
            "text": "The distributed implementation of MapReduce in the original version of Hadoop is sometimes referred to as  MapReduce 1  to distinguish it from MapReduce 2, the implementation that uses YARN.  A comparison of MapReduce 1 and YARN components:     MapReduce1  YARN      Jobtracker  Resource manager, application master, timeline server    TaskTracker  Node manager    Slot  Container", 
            "title": "2 YARN Compared to MapReduce 1"
        }, 
        {
            "location": "/hadoop/ch4/#3-scheduling-in-yarn", 
            "text": "The job of the YARN scheduler to allocate resources to applications according to some defined policy. Scheduling in general is a difficult problem and there is  no one \"best\" policy , which is why YARN provides a choice of schedulers and configurable policies.", 
            "title": "3 Scheduling in YARN"
        }, 
        {
            "location": "/hadoop/ch4/#scheduler-options", 
            "text": "Three schedulers are available in YARN: the FIFO, Capacity, and Fair Schedulers.   the FIFO: places applications in a queue and runs them in the order of submission (first in, first out)  not suitable for shared clusters, because large applications will use all the resources in a cluster, so each application has to wait its turn.     Capacity Scheduler: a separate dedicated queue allows the small job to start as soon as it is submitted, since the queue capacity is reserved for jobs in that queue.  Fair Scheduler: dynamically balance resources between all running jobs, each job is using its fair share of resources.  There is a lag between the time the second job starts and when it receives its fair share, since it has to wait for resources to free up as containers used by the first job complete. After the small job completes and no longer requires resources, the large job goes back to using the full cluster capacity again.", 
            "title": "Scheduler Options"
        }, 
        {
            "location": "/hadoop/ch5/", 
            "text": "Hadoop: The Definitive Guide 5 - Hadoop I/O\n\n\n1 Data Integrity\n\n\nThe usual way of detecting corrupted data is by computing a \nchecksum\n(\u6821\u9a8c\u548c) for the data when it first enters the system, and again whenever it is transmitted across a channel that is unreliable and hence capable of corrupting the data.\n\n\nA commonly used error-detecting code is CRC-32 (32-bit cyclic redundancy check), which computes a 32-bit integer checksum for input of any size. CRC32 is used for checksumming in Hadoop's \nchecksumFileSystem\n, while HDFS uses a more efficient variant called CRC-32C.\n\n\nData Integrity in HDFS\n\n\nHDFS transparently checksums all data written to it and by default verifies checksums when reading data. A separate checksum is created for every \ndfs.bytes-per-checksum\n(default 512) bytes of data.\n\n\nDatanodes are responsible for verifying the data they receive before storing the data and its checksum. When clients read data from datanodes, they verify checksums as well.\n\n\nIn addition to block verification on client reads, each datanode runs a \nDataBlockScanner\n in a background thread that \nperiodically\n verifies all the blocks stored on the datanode.\n\n\nYou can find a file\u2019s checksum with \nhadoop fs -checksum\n.\n\n\nLocalFileSystem\n\n\nThe Hadoop \nLocalFileSystem\n performs client-side checksumming. It is possible to disable checksums, by using \nRawLocalFileSystem\n in place of \nLocalFileSystem\n\n\nChecksumFileSystem\n\n\nLocalFileSystem\n extends \nChecksumFileSystem\n, and \nChecksumFileSystem\n is also a wrapper around \nFileSystem\n (uses decorator pattern here). The general idiom is as follows:\n\n\n \nFileSystem\n \nrawFs\n \n=\n \n...\n\n\nFileSystem\n \nchecksummedFs\n \n=\n \nnew\n \nChecksumFileSystem\n(\nrawFs\n);\n\n\n\n\n\n\n2 Compression\n\n\nFile compression brings two major benefits: it reduces the space needed to store files, and it speeds up data transfer across the network or to or from disk. When dealing with large volumes of data, both of these savings can be significant.\n\n\nA summary of compression formats:\n\n\n\n\n\n\n\n\nCompression format\n\n\nTools\n\n\nAlgorithm\n\n\nFile Extension\n\n\nCompressionCodec\n\n\nSplittable?\n\n\n\n\n\n\n\n\n\n\nDEFLATE\n\n\nN/A\n\n\nDEFLATE\n\n\n.deflate\n\n\nDefaultCodec\n\n\nNo\n\n\n\n\n\n\ngzip\n\n\ngzip\n\n\nDEFLATE\n\n\n.gz\n\n\nGzipCodec\n\n\nNo\n\n\n\n\n\n\nbzip2\n\n\nbzip2\n\n\nbzip2\n\n\n.bz2\n\n\nBZip2Codec\n\n\nYes\n\n\n\n\n\n\nLZO\n\n\nlzop\n\n\nLZO\n\n\n.lzo\n\n\nLzoCodec\n\n\nNo\n\n\n\n\n\n\nSnappy\n\n\nN/A\n\n\nSnappy\n\n\n.snappy\n\n\nSnappyCodec\n\n\nNo\n\n\n\n\n\n\n\n\nAll compression algorithm exhibit a space/time trade-off. Splittable compression formats are especially suitable for MapReduce\n\n\nCodecs\n\n\nA \ncodec\n is the implementation of a compression-decompression algorithm. In Hadoop, a codec is represented by an implementation of the \nCompressionCodec\n interface. So, for example, \nGzipCodec\n encapsulates the compression and decompression algorithm for gzip.\n\n\nCompressing and decompressing streams with CompressionCodec", 
            "title": "Chapter 5: Hadoop I/O"
        }, 
        {
            "location": "/hadoop/ch5/#hadoop-the-definitive-guide-5-hadoop-io", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 5 - Hadoop I/O"
        }, 
        {
            "location": "/hadoop/ch5/#1-data-integrity", 
            "text": "The usual way of detecting corrupted data is by computing a  checksum (\u6821\u9a8c\u548c) for the data when it first enters the system, and again whenever it is transmitted across a channel that is unreliable and hence capable of corrupting the data.  A commonly used error-detecting code is CRC-32 (32-bit cyclic redundancy check), which computes a 32-bit integer checksum for input of any size. CRC32 is used for checksumming in Hadoop's  checksumFileSystem , while HDFS uses a more efficient variant called CRC-32C.", 
            "title": "1 Data Integrity"
        }, 
        {
            "location": "/hadoop/ch5/#data-integrity-in-hdfs", 
            "text": "HDFS transparently checksums all data written to it and by default verifies checksums when reading data. A separate checksum is created for every  dfs.bytes-per-checksum (default 512) bytes of data.  Datanodes are responsible for verifying the data they receive before storing the data and its checksum. When clients read data from datanodes, they verify checksums as well.  In addition to block verification on client reads, each datanode runs a  DataBlockScanner  in a background thread that  periodically  verifies all the blocks stored on the datanode.  You can find a file\u2019s checksum with  hadoop fs -checksum .", 
            "title": "Data Integrity in HDFS"
        }, 
        {
            "location": "/hadoop/ch5/#localfilesystem", 
            "text": "The Hadoop  LocalFileSystem  performs client-side checksumming. It is possible to disable checksums, by using  RawLocalFileSystem  in place of  LocalFileSystem", 
            "title": "LocalFileSystem"
        }, 
        {
            "location": "/hadoop/ch5/#checksumfilesystem", 
            "text": "LocalFileSystem  extends  ChecksumFileSystem , and  ChecksumFileSystem  is also a wrapper around  FileSystem  (uses decorator pattern here). The general idiom is as follows:    FileSystem   rawFs   =   ...  FileSystem   checksummedFs   =   new   ChecksumFileSystem ( rawFs );", 
            "title": "ChecksumFileSystem"
        }, 
        {
            "location": "/hadoop/ch5/#2-compression", 
            "text": "File compression brings two major benefits: it reduces the space needed to store files, and it speeds up data transfer across the network or to or from disk. When dealing with large volumes of data, both of these savings can be significant.  A summary of compression formats:     Compression format  Tools  Algorithm  File Extension  CompressionCodec  Splittable?      DEFLATE  N/A  DEFLATE  .deflate  DefaultCodec  No    gzip  gzip  DEFLATE  .gz  GzipCodec  No    bzip2  bzip2  bzip2  .bz2  BZip2Codec  Yes    LZO  lzop  LZO  .lzo  LzoCodec  No    Snappy  N/A  Snappy  .snappy  SnappyCodec  No     All compression algorithm exhibit a space/time trade-off. Splittable compression formats are especially suitable for MapReduce", 
            "title": "2 Compression"
        }, 
        {
            "location": "/hadoop/ch5/#codecs", 
            "text": "A  codec  is the implementation of a compression-decompression algorithm. In Hadoop, a codec is represented by an implementation of the  CompressionCodec  interface. So, for example,  GzipCodec  encapsulates the compression and decompression algorithm for gzip.  Compressing and decompressing streams with CompressionCodec", 
            "title": "Codecs"
        }
    ]
}