{
    "docs": [
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/", 
            "text": "Spark Streaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee\n\n\n\u9879\u76ee\u6982\u8ff0\n\n\n1 \u521d\u8bc6\u5b9e\u65f6\u6d41\u5904\u7406\n\n\n\u4e1a\u52a1\u73b0\u72b6\u5206\u6790\n\n\n\u9700\u6c42\uff1a\u7edf\u8ba1\u4e3b\u7ad9\u6bcf\u4e2a\uff08\u6307\u5b9a\uff09\u8bfe\u7a0b\u8bbf\u95ee\u7684\u5ba2\u6237\u7aef\u3001\u5730\u57df\u4fe1\u606f\u5206\u5e03\n==\n \u5982\u4e0a\u4e24\u4e2a\u64cd\u4f5c\uff1a\u91c7\u7528\u79bb\u7ebf\uff08spark/mapreduce\uff09\u7684\u65b9\u5f0f\u8fdb\u884c\u7edf\u8ba1\n\n\n\u5b9e\u73b0\u6b65\u9aa4\uff1a\n\n\n\n\n\u8bfe\u7a0b\u7f16\u53f7\uff0cip\u4fe1\u606f\uff0cuser-agent\n\n\n\u8fdb\u884c\u76f8\u5e94\u7684\u7edf\u8ba1\u5206\u6790\u64cd\u4f5c\uff1aMapReduce/Spark\n\n\n\n\n\u9879\u76ee\u67b6\u6784\uff1a\n\n\n\n\n\u65e5\u5fd7\u6536\u96c6\uff1aFlume\n\n\n\u79bb\u7ebf\u5206\u6790\uff1aMapReduce/Spark\n\n\n\u7edf\u8ba1\u7ed3\u679c\u56fe\u5f62\u5316\u5c55\u793a\n\n\n\n\n\u95ee\u9898\uff1a\n\n\n\n\n\u5c0f\u65f6\u7ea7\u522b\n\n\n10\u5206\u949f\n\n\n\u79d2\u7ea7\u522b\n\n\n\n\n\u5b9e\u65f6\u6d41\u5904\u7406\u4ea7\u751f\u80cc\u666f\n\n\n\n\n\u65f6\u6548\u6027\u9ad8\n\n\n\u6570\u636e\u91cf\u5927\n\n\n\n\n\u5b9e\u65f6\u6d41\u5904\u7406\u6982\u8ff0\n\n\nhttps://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101\n\n\n\n\n\u5b9e\u65f6\u8ba1\u7b97 apache storm\n\n\n\u6d41\u5f0f\u8ba1\u7b97\n\n\n\u5b9e\u65f6\u6d41\u5f0f\u8ba1\u7b97\n\n\n\n\n\u79bb\u7ebf\u8ba1\u7b97\u4e0e\u5b9e\u65f6\u8ba1\u7b97\u5bf9\u6bd4\n\n\n\n\n\u6570\u636e\u6765\u6e90\n\n\n\u79bb\u7ebf\uff1a\u6765\u81eaHDFS\u4e0a\u7684\u5386\u53f2\u6570\u636e\uff0c\u6570\u636e\u91cf\u6bd4\u8f83\u5927\n\n\n\u5b9e\u65f6\uff1a\u6765\u81ea\u6d88\u606f\u961f\u5217(Kafka)\uff0c\u662f\u5b9e\u65f6\u65b0\u589e/\u4fee\u6539\u8bb0\u5f55\u8fc7\u6765\u7684\u67d0\u4e00\u7b14\u6570\u636e\n\n\n\n\n\n\n\u5904\u7406\u8fc7\u7a0b\n\n\n\u79bb\u7ebf\uff1aMapReduce, map + reduce\n\n\n\u5b9e\u65f6: Spark(DStream/SS) \n\n\n\n\n\n\n\u5904\u7406\u901f\u5ea6\n\n\n\u79bb\u7ebf\uff1a\u5e54\n\n\n\u5b9e\u65f6\uff1a\u5feb\u901f \n\n\n\n\n\n\n\u8fdb\u7a0b\n\n\n\u79bb\u7ebf\uff1a\u8fdb\u7a0b\u6709\u542f\u52a8+\u9500\u6bc1\u7684\u8fc7\u7a0b\n\n\n\u5b9e\u65f6\uff1a 7*24\u5c0f\u65f6\u8fd0\u884c\n\n\n\n\n\n\n\n\n\u5b9e\u65f6\u6d41\u5904\u7406\u6846\u67b6\u5bf9\u6bd4\n\n\n\n\nApache Storm\n\n\n\n\n\n\nApache Storm is a free and open source distributed \nrealtime\n computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use!\n\n\n\n\n\n\nApache Spark Streaming\n\n\n\n\n\n\n\u5b9e\u9645\u4e0a\u662f\u5fae\u6279\u5904\u7406\uff08\u6279\u5904\u7406\u95f4\u9694\u975e\u5e38\u5c0f)\n\n\n\n\n\n\nApache kafka\n\n\nApache Flink\n\n\n\n\n\n\nApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.\n\n\n\n\n\u5b9e\u65f6\u6d41\u5904\u7406\u67b6\u6784\u548c\u6280\u672f\u9009\u578b\n\n\n\n\n\u52a0\u4e00\u5c42flume\u6d88\u606f\u961f\u5217\uff0c\u4e3b\u8981\u4e3a\u4e86\u51cf\u8f7b\u538b\u529b\uff0c\u8d77\u5230\u7f13\u51b2\u4f5c\u7528\n\n\n\n\n\u5b9e\u65f6\u6d41\u5904\u7406\u5728\u4f01\u4e1a\u4e2d\u7684\u5e94\u7528\n\n\n\n\n\u7535\u4fe1\u884c\u4e1a\uff1a \u4f60\u7684\u624b\u673a\u5957\u9910\u6d41\u91cf\u7528\u5b8c\uff0c\u6536\u5230\u77ed\u4fe1\u63d0\u793a\n\n\n\u7535\u5546\u884c\u4e1a\uff1a\u641c\u7d22\u5546\u54c1\u65f6\uff0c\u8fdb\u884c\u63a8\u8350\n\n\n\n\n2 \u5206\u5e03\u5f0f\u65e5\u5fd7\u6536\u96c6\u6846\u67b6Flume\n\n\nsee detail in Hadoop: definitive Guide, \nChapter 14\n\n\n\u4e1a\u52a1\u73b0\u72b6\u5206\u6790\n\n\nYou have a lot of servers and systems\n\n\n\n\nnetwork devices\n\n\noperating system\n\n\nweb servers\n\n\napplications\n\n\n\n\nAnd they generate large amount of logs and other data.\n\n\nProblem: Since you have a business idea, how to implement the idea?\n\n\nOPTION: You may move logs and data generated to hadoop hdfs directly.\n\n\n\u4f46\u662f\u5b58\u5728\u95ee\u9898\uff1a\n\n\n\n\n\u5982\u4f55\u505a\u76d1\u63a7\n\n\n\u5982\u4f55\u4fdd\u8bc1\u65f6\u6548\u6027\n\n\n\u76f4\u63a5\u4f20\u9001\u6587\u672c\u6570\u636e\uff0c\u5f00\u9500\u592a\u5927\n\n\n\u5bb9\u9519\n\n\n\u8d1f\u8f7d\u5747\u8861\n\n\n\n\nSOLUTION: \u4f7f\u7528Flume\uff0c\u57fa\u672c\u4e0a\u5199\u914d\u7f6e\u6587\u4ef6\u5c31OK\u4e86\uff0cFlume\u81ea\u52a8\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002\n\n\nFlume\u6982\u8ff0\n\n\n\n\nFlume is a distributed, reliable, and available service for efficiently \ncollecting, aggregating, and moving large amounts of log data\n. It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application. \nApache Flume\n\n\n\n\nFlume\u67b6\u6784\u53ca\u6838\u5fc3\u7ec4\u4ef6\n\n\n\n\nsee detail in Hadoop: definitive Guide, \nChapter 14\n\n\nFlume\u5b9e\u6218\n\n\n\u9700\u6c42\uff1a \u4ece\u6307\u5b9a\u7f51\u7edc\u7aef\u53e3\u91c7\u96c6\u6570\u636e\n\n\n\u4f7f\u7528Flume\u7684\u5173\u952e\u5c31\u662f\u5199\u914d\u7f6e\u6587\u4ef6\n\n\n\n\n\u914d\u7f6eSource, Channel, Sink\n\n\n\u628a\u4ee5\u4e0a\u4e09\u4e2a\u7ec4\u4ef6\u4e32\u8d77\u6765\n\n\n\n\n \nhttp://flume.apache.org/FlumeUserGuide.html#example-2\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 44444\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n\n\n\n\n\nnetcat source\n: A netcat-like source that listens on a given port and turns each line of text into an event. It opens a specified port and listens for data. The expectation is that the supplied data is newline separated text. Each line of text is turned into a Flume event and sent via the connected channel. \nNetCat TCP Source\n\n\nlogger sink\n: Logs event at INFO level. Typically useful for testing/debugging purpose.  \nLogger Sink\n\n\nmemory channel\n: The events are stored in an in-memory queue with configurable max size. It\u2019s ideal for flows that need higher throughput and are prepared to lose the staged data in the event of an agent failures. \nmemory channel\n\n\n\n\n \n## \u542f\u52a8flume\n\n$ flume-ng agent \n\\\n\n--name a1 \n\\ \n \n# agent name\n\n--conf \n$F\nLUME_HOME/conf \n\\ \n# use configs in \nconf\n directory\n\n--conf-file  example.conf \n\\ \n# specify a config file\n\n-Dflume.root.logger\n=\nINFO,console \n# sets a Java system property value\n\n\n\n## \u5728\u53e6\u5916\u4e00\u4e2aterminal\u7528telnet\u6a21\u62df\u6570\u636e\u6e90\n\n$ telnet localhost \n44444\n \nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is \n^]\n.\nhello\nOK\nhellomy\nOK\n\n\n\n\u9700\u6c42\uff1a \u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\u5b9e\u65f6\u91c7\u96c6\u65b0\u589e\u7684\u6570\u636e\u8f93\u51fa\u5230\u63a7\u5236\u53f0\n\n\nAgent\u9009\u578b\uff1a exec source + memory channel + logger sink\n\n\n \n# filename: exec-memeory-logger.conf\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /tmp/data.log\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n\n\n\n\n\nexec source\n runs a given Unix command on start-up and expects that process to continuously produce data on standard out (stderr is simply discarded, unless property logStdErr is set to true). If the process exits for any reason, the source also exits and will produce no further data. This means configurations such as cat [named pipe] or tail -F [file] are going to produce the desired results where as date will probably not - the former two commands produce streams of data where as the latter produces a single event and exits. \nexec source\n\n\n\n\n\u5c06\u5185\u5bb9\u8f93\u5165\u5230\n/tmp/data.log\n\u6587\u4ef6\u4e2d\uff1a\n\n\n \n$ \necho\n \nhello\n \n data.log\n$ \necho\n \nhello\n \n data.log\n\n\n\n\u9700\u6c42\uff1a \u5c06A\u670d\u52a1\u5668\u4e0a\u7684\u65e5\u5fd7\u5b9e\u65f6\u91c7\u96c6\u5230B\u670d\u52a1\u5668\n\n\n\u65e5\u5fd7\u6536\u96c6\u8fc7\u7a0b\uff1a\n\n\n\n\n\u673a\u56681\u4e0a\u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\uff0c\u5f53\u6211\u4eec\u8bbf\u95ee\u4e3b\u7ad9\u65f6\u4f1a\u6709\u7528\u6237\u884c\u4e3a\u65e5\u5fd7\u8bb0\u5f55\u5230\naccess.log\n\u4e2d\u3002\n\n\navro sink\u628a\u65b0\u4ea7\u751f\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u5bf9\u5e94\u7684avro source\u6307\u5b9a\u7684hostname\u548cport\u4e0a\u3002\n\n\n\u901a\u8fc7avro\u5bf9\u5e94\u7684agent\u5c06\u6211\u4eec\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u63a7\u5236\u53f0\u3002\n\n\n\n\n\n\n\n\navro sink\n: forms one half of Flume\u2019s tiered collection support. Flume events sent to this sink are turned into Avro events and sent to the configured hostname / port pair.\nAvro sink\n\n\n\n\nExec-Memeory-Avro.conf\n# filename: exec-memeory-avro.conf\n\n# Name the components on this agent\na1.sources = exec-source\na1.sinks = avro-sink\na1.channels = memory-channel\n\n# Describe/configure the source\na1.sources.exec-source.type = exec\na1.sources.exec-source.command = tail -F /tmp/data.log\n\n# Describe the sink\na1.sinks.avro-sink.type = avro\na1.sinks.avro-sink.hostname = localhost\na1.sinks.avro-sink.port = 44444\n\n# Use a channel which buffers events in memory\na1.channels.memory-channel.type = memory\na1.channels.memory-channel.capacity = 1000\na1.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.exec-source.channels = memory-channel\na1.sinks.avro-sink.channel = memory-channel\n\nAvro-Memeory-Logger.conf\n# filename: avro-memeory-logger.conf\n\n# Name the components on this agent\na2.sources = avro-source\na2.sinks = logger-sink\na2.channels = memory-channel\n\n# Describe/configure the source\na2.sources.avro-source.type = avro\na2.sources.avro-source.bind = localhost\na2.sources.avro-source.port = 44444\n\n# Describe the sink\na2.sinks.logger-sink.type = logger\n\n# Use a channel which buffers events in memory\na2.channels.memory-channel.type = memory\na2.channels.memory-channel.capacity = 1000\na2.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na2.sources.avro-source.channels = memory-channel\na2.sinks.logger-sink.channel = memory-channel\n\n\n\n\n\u542f\u52a8flum\uff0c \u6ce8\u610f\u4e24\u4e2aagent\u7684\u542f\u52a8\u987a\u5e8f\n\n\n \n$ flume-ng agent \n\\\n\n--name a2 \n\\\n\n--conf \n$F\nLUME-HOME/conf \n\\\n\n--conf-file avro-memory-logger.conf \n\\\n\n-Dflume.root.logger\n=\nINFO,console\n\n$ flume-ng agent \n\\\n\n--name a1 \n\\\n\n--conf \n$F\nLUME-HOME/conf \n\\\n\n--conf-file exec-memory-avro.conf \n\\\n\n-Dflume.root.logger\n=\nINFO,console\n\n\n\n\u5c06\u5185\u5bb9\u8f93\u5165\u5230\n/tmp/data.log\n\u6587\u4ef6\u4e2d\uff1a\n\n\n \n$ \necho\n \nwelcome\n \n data.log\n$ \necho\n \nwelcome\n \n data.log", 
            "title": "SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#spark-streaming", 
            "text": "\u9879\u76ee\u6982\u8ff0", 
            "title": "Spark Streaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#1", 
            "text": "", 
            "title": "1 \u521d\u8bc6\u5b9e\u65f6\u6d41\u5904\u7406"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_1", 
            "text": "\u9700\u6c42\uff1a\u7edf\u8ba1\u4e3b\u7ad9\u6bcf\u4e2a\uff08\u6307\u5b9a\uff09\u8bfe\u7a0b\u8bbf\u95ee\u7684\u5ba2\u6237\u7aef\u3001\u5730\u57df\u4fe1\u606f\u5206\u5e03\n==  \u5982\u4e0a\u4e24\u4e2a\u64cd\u4f5c\uff1a\u91c7\u7528\u79bb\u7ebf\uff08spark/mapreduce\uff09\u7684\u65b9\u5f0f\u8fdb\u884c\u7edf\u8ba1  \u5b9e\u73b0\u6b65\u9aa4\uff1a   \u8bfe\u7a0b\u7f16\u53f7\uff0cip\u4fe1\u606f\uff0cuser-agent  \u8fdb\u884c\u76f8\u5e94\u7684\u7edf\u8ba1\u5206\u6790\u64cd\u4f5c\uff1aMapReduce/Spark   \u9879\u76ee\u67b6\u6784\uff1a   \u65e5\u5fd7\u6536\u96c6\uff1aFlume  \u79bb\u7ebf\u5206\u6790\uff1aMapReduce/Spark  \u7edf\u8ba1\u7ed3\u679c\u56fe\u5f62\u5316\u5c55\u793a   \u95ee\u9898\uff1a   \u5c0f\u65f6\u7ea7\u522b  10\u5206\u949f  \u79d2\u7ea7\u522b", 
            "title": "\u4e1a\u52a1\u73b0\u72b6\u5206\u6790"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_2", 
            "text": "\u65f6\u6548\u6027\u9ad8  \u6570\u636e\u91cf\u5927", 
            "title": "\u5b9e\u65f6\u6d41\u5904\u7406\u4ea7\u751f\u80cc\u666f"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_3", 
            "text": "https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101   \u5b9e\u65f6\u8ba1\u7b97 apache storm  \u6d41\u5f0f\u8ba1\u7b97  \u5b9e\u65f6\u6d41\u5f0f\u8ba1\u7b97", 
            "title": "\u5b9e\u65f6\u6d41\u5904\u7406\u6982\u8ff0"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_4", 
            "text": "\u6570\u636e\u6765\u6e90  \u79bb\u7ebf\uff1a\u6765\u81eaHDFS\u4e0a\u7684\u5386\u53f2\u6570\u636e\uff0c\u6570\u636e\u91cf\u6bd4\u8f83\u5927  \u5b9e\u65f6\uff1a\u6765\u81ea\u6d88\u606f\u961f\u5217(Kafka)\uff0c\u662f\u5b9e\u65f6\u65b0\u589e/\u4fee\u6539\u8bb0\u5f55\u8fc7\u6765\u7684\u67d0\u4e00\u7b14\u6570\u636e    \u5904\u7406\u8fc7\u7a0b  \u79bb\u7ebf\uff1aMapReduce, map + reduce  \u5b9e\u65f6: Spark(DStream/SS)     \u5904\u7406\u901f\u5ea6  \u79bb\u7ebf\uff1a\u5e54  \u5b9e\u65f6\uff1a\u5feb\u901f     \u8fdb\u7a0b  \u79bb\u7ebf\uff1a\u8fdb\u7a0b\u6709\u542f\u52a8+\u9500\u6bc1\u7684\u8fc7\u7a0b  \u5b9e\u65f6\uff1a 7*24\u5c0f\u65f6\u8fd0\u884c", 
            "title": "\u79bb\u7ebf\u8ba1\u7b97\u4e0e\u5b9e\u65f6\u8ba1\u7b97\u5bf9\u6bd4"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_5", 
            "text": "Apache Storm    Apache Storm is a free and open source distributed  realtime  computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use!    Apache Spark Streaming    \u5b9e\u9645\u4e0a\u662f\u5fae\u6279\u5904\u7406\uff08\u6279\u5904\u7406\u95f4\u9694\u975e\u5e38\u5c0f)    Apache kafka  Apache Flink    Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.", 
            "title": "\u5b9e\u65f6\u6d41\u5904\u7406\u6846\u67b6\u5bf9\u6bd4"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_6", 
            "text": "\u52a0\u4e00\u5c42flume\u6d88\u606f\u961f\u5217\uff0c\u4e3b\u8981\u4e3a\u4e86\u51cf\u8f7b\u538b\u529b\uff0c\u8d77\u5230\u7f13\u51b2\u4f5c\u7528", 
            "title": "\u5b9e\u65f6\u6d41\u5904\u7406\u67b6\u6784\u548c\u6280\u672f\u9009\u578b"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_7", 
            "text": "\u7535\u4fe1\u884c\u4e1a\uff1a \u4f60\u7684\u624b\u673a\u5957\u9910\u6d41\u91cf\u7528\u5b8c\uff0c\u6536\u5230\u77ed\u4fe1\u63d0\u793a  \u7535\u5546\u884c\u4e1a\uff1a\u641c\u7d22\u5546\u54c1\u65f6\uff0c\u8fdb\u884c\u63a8\u8350", 
            "title": "\u5b9e\u65f6\u6d41\u5904\u7406\u5728\u4f01\u4e1a\u4e2d\u7684\u5e94\u7528"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#2-flume", 
            "text": "see detail in Hadoop: definitive Guide,  Chapter 14", 
            "title": "2 \u5206\u5e03\u5f0f\u65e5\u5fd7\u6536\u96c6\u6846\u67b6Flume"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#_8", 
            "text": "You have a lot of servers and systems   network devices  operating system  web servers  applications   And they generate large amount of logs and other data.  Problem: Since you have a business idea, how to implement the idea?  OPTION: You may move logs and data generated to hadoop hdfs directly.  \u4f46\u662f\u5b58\u5728\u95ee\u9898\uff1a   \u5982\u4f55\u505a\u76d1\u63a7  \u5982\u4f55\u4fdd\u8bc1\u65f6\u6548\u6027  \u76f4\u63a5\u4f20\u9001\u6587\u672c\u6570\u636e\uff0c\u5f00\u9500\u592a\u5927  \u5bb9\u9519  \u8d1f\u8f7d\u5747\u8861   SOLUTION: \u4f7f\u7528Flume\uff0c\u57fa\u672c\u4e0a\u5199\u914d\u7f6e\u6587\u4ef6\u5c31OK\u4e86\uff0cFlume\u81ea\u52a8\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\u3002", 
            "title": "\u4e1a\u52a1\u73b0\u72b6\u5206\u6790"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#flume", 
            "text": "Flume is a distributed, reliable, and available service for efficiently  collecting, aggregating, and moving large amounts of log data . It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application.  Apache Flume", 
            "title": "Flume\u6982\u8ff0"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#flume_1", 
            "text": "see detail in Hadoop: definitive Guide,  Chapter 14", 
            "title": "Flume\u67b6\u6784\u53ca\u6838\u5fc3\u7ec4\u4ef6"
        }, 
        {
            "location": "/projects/SparkStreaming\u5b9e\u65f6\u6d41\u5904\u7406\u9879\u76ee/#flume_2", 
            "text": "\u9700\u6c42\uff1a \u4ece\u6307\u5b9a\u7f51\u7edc\u7aef\u53e3\u91c7\u96c6\u6570\u636e  \u4f7f\u7528Flume\u7684\u5173\u952e\u5c31\u662f\u5199\u914d\u7f6e\u6587\u4ef6   \u914d\u7f6eSource, Channel, Sink  \u628a\u4ee5\u4e0a\u4e09\u4e2a\u7ec4\u4ef6\u4e32\u8d77\u6765     http://flume.apache.org/FlumeUserGuide.html#example-2\n# example.conf: A single-node Flume configuration\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 44444\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1   netcat source : A netcat-like source that listens on a given port and turns each line of text into an event. It opens a specified port and listens for data. The expectation is that the supplied data is newline separated text. Each line of text is turned into a Flume event and sent via the connected channel.  NetCat TCP Source  logger sink : Logs event at INFO level. Typically useful for testing/debugging purpose.   Logger Sink  memory channel : The events are stored in an in-memory queue with configurable max size. It\u2019s ideal for flows that need higher throughput and are prepared to lose the staged data in the event of an agent failures.  memory channel     ## \u542f\u52a8flume \n$ flume-ng agent  \\ \n--name a1  \\    # agent name \n--conf  $F LUME_HOME/conf  \\  # use configs in  conf  directory \n--conf-file  example.conf  \\  # specify a config file \n-Dflume.root.logger = INFO,console  # sets a Java system property value  ## \u5728\u53e6\u5916\u4e00\u4e2aterminal\u7528telnet\u6a21\u62df\u6570\u636e\u6e90 \n$ telnet localhost  44444  \nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is  ^] .\nhello\nOK\nhellomy\nOK  \u9700\u6c42\uff1a \u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\u5b9e\u65f6\u91c7\u96c6\u65b0\u589e\u7684\u6570\u636e\u8f93\u51fa\u5230\u63a7\u5236\u53f0  Agent\u9009\u578b\uff1a exec source + memory channel + logger sink    # filename: exec-memeory-logger.conf\n\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /tmp/data.log\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1   exec source  runs a given Unix command on start-up and expects that process to continuously produce data on standard out (stderr is simply discarded, unless property logStdErr is set to true). If the process exits for any reason, the source also exits and will produce no further data. This means configurations such as cat [named pipe] or tail -F [file] are going to produce the desired results where as date will probably not - the former two commands produce streams of data where as the latter produces a single event and exits.  exec source   \u5c06\u5185\u5bb9\u8f93\u5165\u5230 /tmp/data.log \u6587\u4ef6\u4e2d\uff1a    $  echo   hello    data.log\n$  echo   hello    data.log  \u9700\u6c42\uff1a \u5c06A\u670d\u52a1\u5668\u4e0a\u7684\u65e5\u5fd7\u5b9e\u65f6\u91c7\u96c6\u5230B\u670d\u52a1\u5668  \u65e5\u5fd7\u6536\u96c6\u8fc7\u7a0b\uff1a   \u673a\u56681\u4e0a\u76d1\u63a7\u4e00\u4e2a\u6587\u4ef6\uff0c\u5f53\u6211\u4eec\u8bbf\u95ee\u4e3b\u7ad9\u65f6\u4f1a\u6709\u7528\u6237\u884c\u4e3a\u65e5\u5fd7\u8bb0\u5f55\u5230 access.log \u4e2d\u3002  avro sink\u628a\u65b0\u4ea7\u751f\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u5bf9\u5e94\u7684avro source\u6307\u5b9a\u7684hostname\u548cport\u4e0a\u3002  \u901a\u8fc7avro\u5bf9\u5e94\u7684agent\u5c06\u6211\u4eec\u7684\u65e5\u5fd7\u8f93\u51fa\u5230\u63a7\u5236\u53f0\u3002     avro sink : forms one half of Flume\u2019s tiered collection support. Flume events sent to this sink are turned into Avro events and sent to the configured hostname / port pair. Avro sink   Exec-Memeory-Avro.conf # filename: exec-memeory-avro.conf\n\n# Name the components on this agent\na1.sources = exec-source\na1.sinks = avro-sink\na1.channels = memory-channel\n\n# Describe/configure the source\na1.sources.exec-source.type = exec\na1.sources.exec-source.command = tail -F /tmp/data.log\n\n# Describe the sink\na1.sinks.avro-sink.type = avro\na1.sinks.avro-sink.hostname = localhost\na1.sinks.avro-sink.port = 44444\n\n# Use a channel which buffers events in memory\na1.channels.memory-channel.type = memory\na1.channels.memory-channel.capacity = 1000\na1.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.exec-source.channels = memory-channel\na1.sinks.avro-sink.channel = memory-channel Avro-Memeory-Logger.conf # filename: avro-memeory-logger.conf\n\n# Name the components on this agent\na2.sources = avro-source\na2.sinks = logger-sink\na2.channels = memory-channel\n\n# Describe/configure the source\na2.sources.avro-source.type = avro\na2.sources.avro-source.bind = localhost\na2.sources.avro-source.port = 44444\n\n# Describe the sink\na2.sinks.logger-sink.type = logger\n\n# Use a channel which buffers events in memory\na2.channels.memory-channel.type = memory\na2.channels.memory-channel.capacity = 1000\na2.channels.memory-channel.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na2.sources.avro-source.channels = memory-channel\na2.sinks.logger-sink.channel = memory-channel  \u542f\u52a8flum\uff0c \u6ce8\u610f\u4e24\u4e2aagent\u7684\u542f\u52a8\u987a\u5e8f    $ flume-ng agent  \\ \n--name a2  \\ \n--conf  $F LUME-HOME/conf  \\ \n--conf-file avro-memory-logger.conf  \\ \n-Dflume.root.logger = INFO,console\n\n$ flume-ng agent  \\ \n--name a1  \\ \n--conf  $F LUME-HOME/conf  \\ \n--conf-file exec-memory-avro.conf  \\ \n-Dflume.root.logger = INFO,console  \u5c06\u5185\u5bb9\u8f93\u5165\u5230 /tmp/data.log \u6587\u4ef6\u4e2d\uff1a    $  echo   welcome    data.log\n$  echo   welcome    data.log", 
            "title": "Flume\u5b9e\u6218"
        }
    ]
}