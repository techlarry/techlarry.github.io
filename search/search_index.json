{
    "docs": [
        {
            "location": "/hadoop/ch5/", 
            "text": "Hadoop: The Definitive Guide 5 - Hadoop I/O\n\n\n1 Data Integrity\n\n\nThe usual way of detecting corrupted data is by computing a \nchecksum\n(\u6821\u9a8c\u548c) for the data when it first enters the system, and again whenever it is transmitted across a channel that is unreliable and hence capable of corrupting the data.\n\n\nA commonly used error-detecting code is CRC-32 (32-bit cyclic redundancy check), which computes a 32-bit integer checksum for input of any size. CRC32 is used for checksumming in Hadoop's \nchecksumFileSystem\n, while HDFS uses a more efficient variant called CRC-32C.\n\n\nData Integrity in HDFS\n\n\nHDFS transparently checksums all data written to it and by default verifies checksums when reading data. A separate checksum is created for every \nChecksumFileSystem.bytesPerChecksum\n(default 512) bytes of data.\n\n\nDatanodes are responsible for verifying the data they receive before storing the data and its checksum. When clients read data from datanodes, they verify checksums as well.\n\n\nIn addition to block verification on client reads, each datanode runs a \nDataBlockScanner\n in a background thread that \nperiodically\n verifies all the blocks stored on the datanode.\n\n\nYou can find a file\u2019s checksum with \nhadoop fs -checksum\n.\n\n\nLocalFileSystem\n\n\nThe Hadoop \nLocalFileSystem\n performs client-side checksumming. It is possible to disable checksums, by using \nRawLocalFileSystem\n in place of \nLocalFileSystem\n.\n\n\nChecksumFileSystem\n\n\nLocalFileSystem\n extends \nChecksumFileSystem\n, and \nChecksumFileSystem\n is also a wrapper around \nFileSystem\n (uses \ndecorator pattern\n here). The general idiom is as follows:\n\n\n \nFileSystem\n \nrawFs\n \n=\n \n...\n\n\nFileSystem\n \nchecksummedFs\n \n=\n \nnew\n \nChecksumFileSystem\n(\nrawFs\n);\n\n\n\n\n\n\n2 Compression\n\n\nFile compression brings two major benefits: it \nreduces the space\n needed to store files, and it \nspeeds up data transfer\n across the network or to or from disk. When dealing with large volumes of data, both of these savings can be significant.\n\n\nA summary of compression formats:\n\n\n\n\n\n\n\n\nCompression format\n\n\nTools\n\n\nAlgorithm\n\n\nFile Extension\n\n\nCompressionCodec\n\n\nSplittable?\n\n\n\n\n\n\n\n\n\n\nDEFLATE\n\n\nN/A\n\n\nDEFLATE\n\n\n.deflate\n\n\nDefaultCodec\n\n\nNo\n\n\n\n\n\n\ngzip\n\n\ngzip\n\n\nDEFLATE\n\n\n.gz\n\n\nGzipCodec\n\n\nNo\n\n\n\n\n\n\nbzip2\n\n\nbzip2\n\n\nbzip2\n\n\n.bz2\n\n\nBZip2Codec\n\n\nYes\n\n\n\n\n\n\nLZO\n\n\nlzop\n\n\nLZO\n\n\n.lzo\n\n\nLzoCodec\n\n\nNo\n\n\n\n\n\n\nSnappy\n\n\nN/A\n\n\nSnappy\n\n\n.snappy\n\n\nSnappyCodec\n\n\nNo\n\n\n\n\n\n\n\n\nAll compression algorithm exhibit a space/time trade-off. Splittable compression formats are especially suitable for MapReduce.\n\n\nCodecs\n\n\nA \ncodec\n is the implementation of a compression-decompression algorithm. In Hadoop, a codec is represented by an implementation of the \nCompressionCodec\n interface. So, for example, \nGzipCodec\n encapsulates the compression and decompression algorithm for gzip.\n\n\nCompressing and decompressing streams with CompressionCodec\n\n\nInterface CompressionCodec \n has two methods that allow you to easily compress or decompress data. \n\n\n\n\nTo compress data being written to an output stream, use the \ncreateOutputStream(OutputStream out)\n method to create a \nCompressionOutputStream\n\n\nConversely, to decompress data being read from an input stream, call \ncreateInputStream(InputStream in)\n to obtain a \nCompressionInputStream\n.\n\n\n\n\nThe code below illustrates how to use the API to compress data read from standard input and write it to standard output.\n\n\n \nimport\n \norg.apache.hadoop.conf.Configuration\n;\n\n\nimport\n \norg.apache.hadoop.io.IOUtils\n;\n\n\nimport\n \norg.apache.hadoop.io.compress.CompressionCodec\n;\n\n\nimport\n \norg.apache.hadoop.io.compress.CompressionOutputStream\n;\n\n\nimport\n \norg.apache.hadoop.util.ReflectionUtils\n;\n\n\n\n// vv StreamCompressor\n\n\npublic\n \nclass\n \nStreamCompressor\n \n{\n\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \nthrows\n \nException\n \n{\n\n        \nString\n \ncodecClassname\n \n=\n \nargs\n[\n0\n];\n\n        \nClass\n?\n \ncodecClass\n \n=\n \nClass\n.\nforName\n(\ncodecClassname\n);\n\n        \nConfiguration\n \nconf\n \n=\n \nnew\n \nConfiguration\n();\n\n        \nCompressionCodec\n \ncodec\n \n=\n \n(\nCompressionCodec\n)\n\n                \nReflectionUtils\n.\nnewInstance\n(\ncodecClass\n,\n \nconf\n);\n\n\n        \nCompressionOutputStream\n \nout\n \n=\n \ncodec\n.\ncreateOutputStream\n(\nSystem\n.\nout\n);\n\n        \nIOUtils\n.\ncopyBytes\n(\nSystem\n.\nin\n,\n \nout\n,\n \n4096\n,\n \nfalse\n);\n\n        \nout\n.\nfinish\n();\n\n    \n}\n\n\n}\n\n\n\n\nWe can try it out with the following command line, which compresses the string \u201cText\u201d using the \nStreamCompressor\n program with the \nGzipCodec\n, then decompresses it from standard input using \ngunzip\n:\n\n\n \nexport\n \nHADOOP_CLASSPATH\n=\n/Users/larry/JavaProject/out/artifacts/StreamCompressor/StreamCompressor.jar\n\necho\n \nText\n \n|\n hadoop com.definitivehadoop.compression.StreamCompressor org.apache.hadoop.io.compress.GzipCodec \n|\n gunzip \n\n\n\nInferring CompressionCodecs using CompressionCodecFactory\n\n\nCompressionCodecFactory\n provides a way of mapping a filename extension to a \nCompressionCodec\n using its \ngetCodec()\n method,\n\n\nCodecPool\n\n\nIf you are using a native library and you are doing a lot of compression or decompression in your application, consider using \nCodecPool\n, which allows you to reuse compressors and decompressors, thereby amortizing the cost of creating these objects.\n\n\nCompression and Input Splits\n\n\nIf a compressed file using a format that does not support splitting, say gzip format, MapReduce will not try to split the gzipped file, at the expense of locality: a single map will process all blocks containing the file, most of which will not be local to the map.\n\n\nFor an LZO file, in spite of not supporting splitting, it is possible to preprocess LZO files using an indexer tool that comes with the Hadoop LZO libraries. \n\n\nUsing Compression in MapReduce\n\n\nIn order to compress the output of a MapReduce, job you can use the static convenience methods on \nFileOutputFormat\n to set properties.\n\n\nApplication to run the maximum temperature job producing compressed output:\n\n\nMaxtemperaturewithcompression\npublic\n \nclass\n \nMaxTemperatureWithCompression\n \n{\n\n    \npublic\n \nstatic\n \nvoid\n \nmain\n(\nString\n[]\n \nargs\n)\n \nthrows\n \nException\n \n{\n\n        \nif\n \n(\nargs\n.\nlength\n \n!=\n \n2\n)\n \n{\n\n            \nSystem\n.\nerr\n.\nprintln\n(\nUsage: MaxTemperatureWithCompression \ninput path\n \n \n+\n\n                    \noutput path\n);\n\n            \nSystem\n.\nexit\n(-\n1\n);\n\n        \n}\n\n\n        \nJob\n \njob\n \n=\n  \nJob\n.\ngetInstance\n();\n\n        \njob\n.\nsetJarByClass\n(\ncom\n.\ndefinitivehadoop\n.\nweatherdata\n.\nMaxTemperature\n.\nclass\n);\n\n\n        \nFileInputFormat\n.\naddInputPath\n(\njob\n,\n \nnew\n \nPath\n(\nargs\n[\n0\n]));\n\n        \nFileOutputFormat\n.\nsetOutputPath\n(\njob\n,\n \nnew\n \nPath\n(\nargs\n[\n1\n]));\n\n\n        \njob\n.\nsetOutputKeyClass\n(\nText\n.\nclass\n);\n\n        \njob\n.\nsetOutputValueClass\n(\nIntWritable\n.\nclass\n);\n\n\n        \n/*[*/\n\n        \nFileOutputFormat\n.\nsetCompressOutput\n(\njob\n,\n \ntrue\n);\n\n        \nFileOutputFormat\n.\nsetOutputCompressorClass\n(\njob\n,\n \nGzipCodec\n.\nclass\n);\n/*]*/\n\n\n\njob\n.\nsetMapperClass\n(\ncom\n.\ndefinitivehadoop\n.\nweatherdata\n.\nMaxTemperatureMapper\n.\nclass\n);\n\n\njob\n.\nsetCombinerClass\n(\ncom\n.\ndefinitivehadoop\n.\nweatherdata\n.\nMaxTemperatureReducer\n.\nclass\n);\n\n\njob\n.\nsetReducerClass\n(\ncom\n.\ndefinitivehadoop\n.\nweatherdata\n.\nMaxTemperatureReducer\n.\nclass\n);\n\n\n        \nSystem\n.\nexit\n(\njob\n.\nwaitForCompletion\n(\ntrue\n)\n \n?\n \n0\n \n:\n \n1\n);\n\n    \n}\n\n\n}\n\n\n//^^ MaxTemperatureWithCompression\n\n\nUsage\n$ \nexport\n \nHADOOP_CLASSPATH\n=\n/Users/larry/JavaProject/out/artifacts/MaxTemperatureWithCompression/MaxTemperatureWithCompression.jar\n$ hadoop com.definitivehadoop.compression.MaxTemperatureWithCompression /Users/larry/JavaProject/resources/HadoopBook/ncdc/sample.txt output\n\n\n\n\n3 Serialization\n\n\nSee concepts of serialization and deserialization in Head First Java \nChapter 14\n.\n\n\nSerialization is used in two quite distinct areas of distributed data processing: for interprocess communication and for persistent storage.\n\n\nIn Hadoop, interprocess communication between nodes in the system is implemented using remote procedure calls (RPCs). The RPC protocol uses serialization to render the message into a binary stream to be sent to the remote node, which then deserializes the binary stream into the original message. In general, four desirable properties are crucial for  an RPC serialization and persistent storage:\n\n\n\n\n\n\n\n\nProperties\n\n\nPRC Serialization\n\n\nPersistent Storage\n\n\n\n\n\n\n\n\n\n\nCompact\n\n\nmakes the best use of network bandwidth\n\n\nmake efficient use of storage space\n\n\n\n\n\n\nFast\n\n\nlittle performance overhead\n\n\nlittle overhead in reading or writing\n\n\n\n\n\n\nExtensible\n\n\nmeet new requirements\n\n\ntransparently read data of older formats\n\n\n\n\n\n\nInteroperable\n\n\nsupport clients written in different languages\n\n\nread/write using different languages\n\n\n\n\n\n\n\n\nHadoop uses its own serialization format, \nWritables\n, which is certainly compact and fast, but not so easy to extend or use from languages other than Java. Avro, a serialization system that was designed to overcome some of the limitations of \nWritables\n, is covered in \nChapter 12\n.\n\n\nThe Writable Interface\n\n\nWritable Classes\n\n\n\n\nImplementing a Custom Writable\n\n\nSerialization Frameworks\n\n\n4 File-Based Data Structures", 
            "title": "Chapter 5: Hadoop I/O"
        }, 
        {
            "location": "/hadoop/ch5/#hadoop-the-definitive-guide-5-hadoop-io", 
            "text": "", 
            "title": "Hadoop: The Definitive Guide 5 - Hadoop I/O"
        }, 
        {
            "location": "/hadoop/ch5/#1-data-integrity", 
            "text": "The usual way of detecting corrupted data is by computing a  checksum (\u6821\u9a8c\u548c) for the data when it first enters the system, and again whenever it is transmitted across a channel that is unreliable and hence capable of corrupting the data.  A commonly used error-detecting code is CRC-32 (32-bit cyclic redundancy check), which computes a 32-bit integer checksum for input of any size. CRC32 is used for checksumming in Hadoop's  checksumFileSystem , while HDFS uses a more efficient variant called CRC-32C.", 
            "title": "1 Data Integrity"
        }, 
        {
            "location": "/hadoop/ch5/#data-integrity-in-hdfs", 
            "text": "HDFS transparently checksums all data written to it and by default verifies checksums when reading data. A separate checksum is created for every  ChecksumFileSystem.bytesPerChecksum (default 512) bytes of data.  Datanodes are responsible for verifying the data they receive before storing the data and its checksum. When clients read data from datanodes, they verify checksums as well.  In addition to block verification on client reads, each datanode runs a  DataBlockScanner  in a background thread that  periodically  verifies all the blocks stored on the datanode.  You can find a file\u2019s checksum with  hadoop fs -checksum .", 
            "title": "Data Integrity in HDFS"
        }, 
        {
            "location": "/hadoop/ch5/#localfilesystem", 
            "text": "The Hadoop  LocalFileSystem  performs client-side checksumming. It is possible to disable checksums, by using  RawLocalFileSystem  in place of  LocalFileSystem .", 
            "title": "LocalFileSystem"
        }, 
        {
            "location": "/hadoop/ch5/#checksumfilesystem", 
            "text": "LocalFileSystem  extends  ChecksumFileSystem , and  ChecksumFileSystem  is also a wrapper around  FileSystem  (uses  decorator pattern  here). The general idiom is as follows:    FileSystem   rawFs   =   ...  FileSystem   checksummedFs   =   new   ChecksumFileSystem ( rawFs );", 
            "title": "ChecksumFileSystem"
        }, 
        {
            "location": "/hadoop/ch5/#2-compression", 
            "text": "File compression brings two major benefits: it  reduces the space  needed to store files, and it  speeds up data transfer  across the network or to or from disk. When dealing with large volumes of data, both of these savings can be significant.  A summary of compression formats:     Compression format  Tools  Algorithm  File Extension  CompressionCodec  Splittable?      DEFLATE  N/A  DEFLATE  .deflate  DefaultCodec  No    gzip  gzip  DEFLATE  .gz  GzipCodec  No    bzip2  bzip2  bzip2  .bz2  BZip2Codec  Yes    LZO  lzop  LZO  .lzo  LzoCodec  No    Snappy  N/A  Snappy  .snappy  SnappyCodec  No     All compression algorithm exhibit a space/time trade-off. Splittable compression formats are especially suitable for MapReduce.", 
            "title": "2 Compression"
        }, 
        {
            "location": "/hadoop/ch5/#codecs", 
            "text": "A  codec  is the implementation of a compression-decompression algorithm. In Hadoop, a codec is represented by an implementation of the  CompressionCodec  interface. So, for example,  GzipCodec  encapsulates the compression and decompression algorithm for gzip.  Compressing and decompressing streams with CompressionCodec  Interface CompressionCodec   has two methods that allow you to easily compress or decompress data.    To compress data being written to an output stream, use the  createOutputStream(OutputStream out)  method to create a  CompressionOutputStream  Conversely, to decompress data being read from an input stream, call  createInputStream(InputStream in)  to obtain a  CompressionInputStream .   The code below illustrates how to use the API to compress data read from standard input and write it to standard output.    import   org.apache.hadoop.conf.Configuration ;  import   org.apache.hadoop.io.IOUtils ;  import   org.apache.hadoop.io.compress.CompressionCodec ;  import   org.apache.hadoop.io.compress.CompressionOutputStream ;  import   org.apache.hadoop.util.ReflectionUtils ;  // vv StreamCompressor  public   class   StreamCompressor   { \n\n     public   static   void   main ( String []   args )   throws   Exception   { \n         String   codecClassname   =   args [ 0 ]; \n         Class ?   codecClass   =   Class . forName ( codecClassname ); \n         Configuration   conf   =   new   Configuration (); \n         CompressionCodec   codec   =   ( CompressionCodec ) \n                 ReflectionUtils . newInstance ( codecClass ,   conf ); \n\n         CompressionOutputStream   out   =   codec . createOutputStream ( System . out ); \n         IOUtils . copyBytes ( System . in ,   out ,   4096 ,   false ); \n         out . finish (); \n     }  }   We can try it out with the following command line, which compresses the string \u201cText\u201d using the  StreamCompressor  program with the  GzipCodec , then decompresses it from standard input using  gunzip :    export   HADOOP_CLASSPATH = /Users/larry/JavaProject/out/artifacts/StreamCompressor/StreamCompressor.jar echo   Text   |  hadoop com.definitivehadoop.compression.StreamCompressor org.apache.hadoop.io.compress.GzipCodec  |  gunzip   Inferring CompressionCodecs using CompressionCodecFactory  CompressionCodecFactory  provides a way of mapping a filename extension to a  CompressionCodec  using its  getCodec()  method,  CodecPool  If you are using a native library and you are doing a lot of compression or decompression in your application, consider using  CodecPool , which allows you to reuse compressors and decompressors, thereby amortizing the cost of creating these objects.", 
            "title": "Codecs"
        }, 
        {
            "location": "/hadoop/ch5/#compression-and-input-splits", 
            "text": "If a compressed file using a format that does not support splitting, say gzip format, MapReduce will not try to split the gzipped file, at the expense of locality: a single map will process all blocks containing the file, most of which will not be local to the map.  For an LZO file, in spite of not supporting splitting, it is possible to preprocess LZO files using an indexer tool that comes with the Hadoop LZO libraries.", 
            "title": "Compression and Input Splits"
        }, 
        {
            "location": "/hadoop/ch5/#using-compression-in-mapreduce", 
            "text": "In order to compress the output of a MapReduce, job you can use the static convenience methods on  FileOutputFormat  to set properties.  Application to run the maximum temperature job producing compressed output:  Maxtemperaturewithcompression public   class   MaxTemperatureWithCompression   { \n     public   static   void   main ( String []   args )   throws   Exception   { \n         if   ( args . length   !=   2 )   { \n             System . err . println ( Usage: MaxTemperatureWithCompression  input path     + \n                     output path ); \n             System . exit (- 1 ); \n         } \n\n         Job   job   =    Job . getInstance (); \n         job . setJarByClass ( com . definitivehadoop . weatherdata . MaxTemperature . class ); \n\n         FileInputFormat . addInputPath ( job ,   new   Path ( args [ 0 ])); \n         FileOutputFormat . setOutputPath ( job ,   new   Path ( args [ 1 ])); \n\n         job . setOutputKeyClass ( Text . class ); \n         job . setOutputValueClass ( IntWritable . class ); \n\n         /*[*/ \n         FileOutputFormat . setCompressOutput ( job ,   true ); \n         FileOutputFormat . setOutputCompressorClass ( job ,   GzipCodec . class ); /*]*/  job . setMapperClass ( com . definitivehadoop . weatherdata . MaxTemperatureMapper . class );  job . setCombinerClass ( com . definitivehadoop . weatherdata . MaxTemperatureReducer . class );  job . setReducerClass ( com . definitivehadoop . weatherdata . MaxTemperatureReducer . class ); \n\n         System . exit ( job . waitForCompletion ( true )   ?   0   :   1 ); \n     }  }  //^^ MaxTemperatureWithCompression  Usage $  export   HADOOP_CLASSPATH = /Users/larry/JavaProject/out/artifacts/MaxTemperatureWithCompression/MaxTemperatureWithCompression.jar\n$ hadoop com.definitivehadoop.compression.MaxTemperatureWithCompression /Users/larry/JavaProject/resources/HadoopBook/ncdc/sample.txt output", 
            "title": "Using Compression in MapReduce"
        }, 
        {
            "location": "/hadoop/ch5/#3-serialization", 
            "text": "See concepts of serialization and deserialization in Head First Java  Chapter 14 .  Serialization is used in two quite distinct areas of distributed data processing: for interprocess communication and for persistent storage.  In Hadoop, interprocess communication between nodes in the system is implemented using remote procedure calls (RPCs). The RPC protocol uses serialization to render the message into a binary stream to be sent to the remote node, which then deserializes the binary stream into the original message. In general, four desirable properties are crucial for  an RPC serialization and persistent storage:     Properties  PRC Serialization  Persistent Storage      Compact  makes the best use of network bandwidth  make efficient use of storage space    Fast  little performance overhead  little overhead in reading or writing    Extensible  meet new requirements  transparently read data of older formats    Interoperable  support clients written in different languages  read/write using different languages     Hadoop uses its own serialization format,  Writables , which is certainly compact and fast, but not so easy to extend or use from languages other than Java. Avro, a serialization system that was designed to overcome some of the limitations of  Writables , is covered in  Chapter 12 .", 
            "title": "3 Serialization"
        }, 
        {
            "location": "/hadoop/ch5/#the-writable-interface", 
            "text": "", 
            "title": "The Writable Interface"
        }, 
        {
            "location": "/hadoop/ch5/#writable-classes", 
            "text": "", 
            "title": "Writable Classes"
        }, 
        {
            "location": "/hadoop/ch5/#implementing-a-custom-writable", 
            "text": "", 
            "title": "Implementing a Custom Writable"
        }, 
        {
            "location": "/hadoop/ch5/#serialization-frameworks", 
            "text": "", 
            "title": "Serialization Frameworks"
        }, 
        {
            "location": "/hadoop/ch5/#4-file-based-data-structures", 
            "text": "", 
            "title": "4 File-Based Data Structures"
        }
    ]
}