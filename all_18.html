<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  techlarry
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">HomePage</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_blank" href="wiki">WIKI</a></li>
        
        <li id=""><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        <li id=""><a target="_blank" href="imlarry.coding.me/imlarry">BOOK NOTES</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">HomePage</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_blank" href="wiki">WIKI</a></li>
        
        <li><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        
        <li><a target="_blank" href="imlarry.coding.me/imlarry">BOOK NOTES</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="Leetcode.html">Leetcode</a></li>
        
            <li><a href="programming_language.html">编程语言</a></li>
        
            <li><a href="data_structure_and_algorithm.html">数据结构和算法</a></li>
        
            <li><a href="Python%E7%89%B9%E6%80%A7.html">Python特性</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html">Python科学计算三维可视化</a></li>
        
            <li><a href="English.html">English</a></li>
        
            <li><a href="Computer%20System.html">Computer System</a></li>
        
            <li><a href="Deep%20Learning.html">Deep Learning</a></li>
        
            <li><a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html">Linux 系统编程</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html">数据库</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="Big%20Data.html">Big Data</a></li>
        
            <li><a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html">文献阅读</a></li>
        
            <li><a href="Tools.html">Tools</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="convolutional-neural-networks.html">
                
                  <h1>Convolutional Neural Networks</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Architecture</a>
<ul>
<li>
<ul>
<li>
<a href="#toc_1">Conv layers</a>
</li>
<li>
<a href="#toc_2">Pooling</a>
</li>
<li>
<a href="#toc_3">Fully-connected layers</a>
</li>
<li>
<a href="#toc_4">Layer Patterns</a>
</li>
<li>
<a href="#toc_5">Layer Sizing Patterns</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">Computational Considerations</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_7">The memory bottleneck is the largest bottleneck when constructing ConvNet architectures.</a>


<h2 id="toc_0">Architecture</h2>

<p>Three main types of layers to build ConvNet architectures: <strong>Convolutional Layer, Pooling Layer and Fully-Connected Layer</strong>.</p>

<p>The layers of a ConvNet have neurons arranged in 3 dimensions: <strong>width, height, depth</strong>.</p>

<p><img src="media/15042285789723/15043413926666.jpg" alt="convolutional"/></p>

<p>A simple ConvNet for <a href="https://www.cs.toronto.edu/%7Ekriz/cifar.html">CIFAR-10</a> classification could have the architecture as follows:</p>

<ul>
<li><strong>INPUT</strong> [width x height x color channel] will hold the raw pixel values of the image, in this case an image of width, height, and with three color channels R,G,B.</li>
<li><strong>CONV layer</strong> will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume.</li>
<li><strong>RELU layer</strong> will apply an elementwise activation function, such as the <code>max(0,x)</code> thresholding at zero. </li>
<li><strong>POOL layer</strong> will perform a downsampling operation along the spatial dimensions (width, height).</li>
<li><strong>FC</strong> (i.e. fully-connected) layer will compute the class scores, where each score corresponding to the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.</li>
</ul>

<h4 id="toc_1">Conv layers</h4>

<p>Four hyperparameters <strong>depth</strong>(K), <strong>spatial extent</strong>(\(F\)), <strong>stride</strong>(\(S\)) and <strong>zero-padding</strong>(\(P\)) control the size of the output volume from the input volume (\(W\)).</p>

<p><strong>Summary</strong>. To summarize, the Conv Layer:</p>

<ul>
<li>Accepts a volume of size \(W_1 \times H_1 \times D_1\)</li>
<li>Requires four hyperparameters: 
<ul>
<li>Number of filters \(K\), </li>
<li>their spatial extent \(F\), </li>
<li>the stride \(S\), </li>
<li>the amount of zero padding \(P\).</li>
</ul></li>
<li>Produces a volume of size \(W_2 \times H_2 \times D_2\) where:
<ul>
<li>\(W_2 = (W_1 - F + 2P)/S + 1\)</li>
<li>\(H_2 = (H_1 - F + 2P)/S + 1\) (i.e. width and height are computed equally by symmetry)</li>
<li>\(D_2 = K\)</li>
</ul></li>
<li>With parameter sharing, it introduces \(F \cdot F \cdot D_1\) weights per filter, for a total of \((F \cdot F \cdot D_1) \cdot K\) weights and \(K\) biases.</li>
<li>In the output volume, the \(d\)-th depth slice (of size \(W_2 \times H_2\)) is the result of performing a valid convolution of the \(d\)-th filter over the input volume with a stride of \(S\), and then offset by \(d\)-th bias.</li>
</ul>

<p><strong>parameter sharing</strong>: the neurons in each depth slice(i.e \(K\)) to use the same weights and bias. </p>

<h4 id="toc_2">Pooling</h4>

<p><code>max pooling</code> is the most common function performed on the pooling units, others like <code>average pooling</code> or <code>L2-norm pooling</code> is work worse in practice. And Many people dislike the pooling operation and think that we can get away without it.</p>

<h4 id="toc_3">Fully-connected layers</h4>

<p>Neurons in a fully connected layer have full connections to all activations in the previous layer.</p>

<h4 id="toc_4">Layer Patterns</h4>

<p>The most common form of a ConvNet architecture stacks a few CONV-RELU layers, follows them with POOL layers, and <strong>repeats this pattern until the image has been merged spatially to a small size</strong>. At some point, it is common to transition to fully-connected layers. The last fully-connected layer holds the output, such as the class scores. In other words, the most common ConvNet architecture follows the pattern:</p>

<p>\[INPUT \rightarrow  [[CONV \rightarrow  RELU]*N \rightarrow  POOL?]*M \rightarrow [FC -&gt; RELU]*K \rightarrow  FC\]</p>

<p>where the <code>*</code> indicates repetition, and the <code>POOL?</code> indicates an optional pooling layer. Moreover, <code>N &gt;= 0</code> (and usually <code>N &lt;= 3</code>), <code>M &gt;= 0</code>, <code>K &gt;= 0</code> (and usually <code>K &lt; 3</code>). For example, here are some common ConvNet architectures you may see that follow this pattern:</p>

<p><strong>Note</strong>: <code>INPUT -&gt; FC</code>, implements a linear classifier. Here <code>N = M = K = 0</code>.<br/>
<strong>Note</strong>: Prefer a stack of small filter CONV to one large receptive field CONV layer, because of few parameters needed and expressing more powerful features of the input.</p>

<h4 id="toc_5">Layer Sizing Patterns</h4>

<ul>
<li><strong>input layer</strong>:  Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512.</li>
<li><strong>conv layers</strong>: small filters(\(3\times3\) or \(5\times5\)), with \(S=1\). Crucially, <em>padding the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input.</em> </li>
<li><strong>pool layers</strong>: use max-pooling with \(F=2, S=2\) or \(F=3, S=2\)</li>
</ul>

<h3 id="toc_6">Computational Considerations</h3>

<h1 id="toc_7">The memory bottleneck is the largest bottleneck when constructing ConvNet architectures.</h1>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/2</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Deep%20Learning.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="virtualenv.html">
                
                  <h1>Python `virtualenv` on mac</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">Install</h2>

<p>Install <code>virtualenv</code> using <code>conda</code> instead of <code>pip</code>, because it might raise error (see on <a href="virtualenv%20--no-site-packages%20venv">StackOverflow</a>)</p>

<pre><code class="language-bash">conda install virtualenv
</code></pre>

<h2 id="toc_1">create your environment</h2>

<p>Now you can create your python environment for your particular programs. For example, under the folder <code>your project</code>, you create an environment called <code>.venv</code> by:</p>

<pre><code class="language-bash">virtualenv --no-site-packages .venv
</code></pre>

<p>The command <code>--no-site-packages</code> requires the environment should not access to global site-packages (as default now).</p>

<p>Before running your program in your created environment, you need to activate it:</p>

<pre><code class="language-python">source .venv/bin/activate
</code></pre>

<p>And remember to deactivate it whenever you are done.:</p>

<pre><code class="language-text">deactivate
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/2</span>
                    
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15042359761425.html">
                
                  <h1>Transfer Learning</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p><code>Transfer learning</code> focuses on storing knowledge gained while solving one problem and applying it to a different but related problem.</p>

<p>Generally, transfer learning will work only well if the inputs have similar low-level features.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/1</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="cross-validation.html">
                
                  <h1>Cross-Validation</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p><code>cross-validation</code> splits the training set into complementary subsets, and each model is trained against a different combination of these subsets and validates against the remaining parts. Once the model type and hyperparameters on the full training set, and the generalized error is measured on the test set.</p>

<p><img src="media/15038841395536/15042349862270.jpg" alt="demo of cross-validation"/></p>

<p>A great convenient is to use Scikit-Learn&#39;s <code>cross-validation</code> feature. The following code performs <code>K-fold cross-validation</code>: it randomly splits the training set into 10 distinct subsets called <code>folds</code>, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds.</p>

<pre><code class="language-python">from sklearn.model_selection import cross_val_score
scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
                             scoring=&quot;neg_mean_squared_error&quot;, cv=10)
    rmse_scores = np.sqrt(-scores)
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/8/28</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15025954300059.html">
                
                  <h1>CS231n:</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p><code>score function</code> that maps the raw data to class scores. For visual recognition, score function maps the pixel values of an image to confidence scores for each class.</p>

<p><code>loss function</code> that quantifies the agreement between the predicated scores and the ground truth labels.</p>

<p><strong>Linear classifier.</strong> We will start out with arguably the simplest possible function( a <code>score function</code>), a linear mapping:</p>

<p>\[<br/>
f(x_i, W, b) =  W x_i + b<br/>
\]</p>

<p>Convolutional Neural Networks will map image pixels to scores exactly as shown above, but the mapping ( \(f \)) will be more complex and will contain more parameters.</p>

<p><strong>Bias trick</strong>: A commonly used trick is to combine the two sets of parameters (\(W, b\)) into a single matrix that holds both of them by extending the vector \(x_i\) with one additional dimension that always holds the constant \(1\) - a default <em>bias dimension</em>. With the extra dimension, the new <code>score function</code> will simplify to a single matrix multiply:</p>

<p>\[<br/>
f(x_i, W) =  W x_i<br/>
\]</p>

<p><img src="media/15025954300059/15041865704757.jpg" alt=""/></p>

<p><strong>Loss Function</strong>: We want to set parameters W so that the predicted class scores are consistent with the ground truth labels in the training data. We are going to measure our unhappiness with outcomes with a <code>loss function</code> (or sometimes also referred to as the <code>cost function</code> or the <code>objective</code>). Intuitively, the loss will be high if we’re doing a poor job of classifying the training data, and it will be low if we’re doing well.</p>

<ul>
<li><code>Multiclass Support Vector Machine</code> (SVM): The <code>score function</code> computes the vector \(f(x_i, W)\) of class scores, which we will abbreviate to \(s\) (short for scores). For example, the score for the j-th class is the j-th element: \(s_j=f(x_i,W)_j\). The Multiclass SVM loss for the i-th example is then formalized as follows:</li>
</ul>

<p>\[<br/>
L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)<br/>
\]</p>

<p>The SVM loss function wants the score of the correct class \(y_i\) to be larger than the incorrect class scores by at least by \(\Delta\) (delta). If this is not the case, we will accumulate loss.  Hyperparameter \(\Delta\) can safely be set to \(\Delta=1.0\) in all cases</p>

<p><strong><code>hinge loss</code></strong>: The function <code>max(0,1-t)$ is called the</code>hinge loss` function.</p>

<p><img src="media/15025954300059/Hinge%20Loss%20Function.png" alt="Hinge Loss Function"/></p>

<p><strong>Regularization</strong>:</p>

<p>Including the regularization penalty completes the full Multiclass Support Vector Machine loss, which is made up of two components: the <strong>data loss</strong> (which is the average loss \(L_i\) over all examples) and the <strong>regularization loss</strong>. That is, the full Multiclass SVM loss becomes:</p>

<p>\[<br/>
L =  \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{ \lambda R(W) }_\text{regularization loss} \\\\<br/>
\]<br/>
Or expanding this out in its full form:</p>

<p>\[<br/>
L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2<br/>
\]</p>

<p>Where \(N\) is the number of training examples.</p>

<p>Code. Here is the loss function (without regularization) implemented in Python, in both unvectorized and half-vectorized form:</p>

<p><strong>Softmax classifier</strong>: Another commonly seen classifiers besides <code>SVM classifier</code> is the <code>Softmax classifier</code>, which has a <code>cross-entropy loss</code>:</p>

<p>\[<br/>
L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}<br/>
\]</p>

<p>where we are using the notation \(f_j\) to mean the j-th element of the vector of class scores \(f\). As before, the full loss for the dataset is the mean of \(L_i\) over all training examples together with a regularization term \(R(W)\). The function \(f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}} \) is called the <strong>softmax function</strong>: It takes a vector of arbitrary real-valued scores (in \(z\)) and squashes it to a vector of values between zero and one that sum to one.</p>

<p>The difference between <code>SVM classifiers</code> and <code>Soft-max classifiers</code>:</p>

<p><img src="media/15025954300059/15041909839867.png" alt=""/></p>

<h2 id="toc_0">Reference</h2>

<ul>
<li>CS231n Convolutional Neural Network</li>
<li>Aurelien Geron. Hands-On Machine Learning with Scikit-Learn &amp; TensorFlow</li>
</ul>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/8/13</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Deep%20Learning.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all_17.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_19.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">他山之石，可以攻玉</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Leetcode.html"><strong>Leetcode</strong></a>
        
            <a href="programming_language.html"><strong>编程语言</strong></a>
        
            <a href="data_structure_and_algorithm.html"><strong>数据结构和算法</strong></a>
        
            <a href="Python%E7%89%B9%E6%80%A7.html"><strong>Python特性</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>Python科学计算三维可视化</strong></a>
        
            <a href="English.html"><strong>English</strong></a>
        
            <a href="Computer%20System.html"><strong>Computer System</strong></a>
        
            <a href="Deep%20Learning.html"><strong>Deep Learning</strong></a>
        
            <a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html"><strong>Linux 系统编程</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html"><strong>数据库</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="Big%20Data.html"><strong>Big Data</strong></a>
        
            <a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html"><strong>文献阅读</strong></a>
        
            <a href="Tools.html"><strong>Tools</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="exceptional_control_flow.html">CSAPP - 异常控制流</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="introduction_to_computer_system_CMU.html">CMU 15-213 Introduction to Computer Systems</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os-concepts-os-structures.html">Operating System Concepts 2 - Operating System structures</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os-concets-processes.html">Operating System Concepts 3 - Processes</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os_concepts_synchronization_examples.html">Operating System Concepts 7 - Synchronization Examples</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
