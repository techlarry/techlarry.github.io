<!doctype html>
<html class="no-js" lang="en">

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?fdc936c9f5a3b72177541183cdeb8cb3";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();

</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-112743284-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-112743284-1');
</script>

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  机器学习 - techlarry
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">HomePage</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_self" href="wiki">WIKI</a></li>
        
        <li id=""><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">HomePage</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_self" href="wiki">WIKI</a></li>
        
        <li><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="Leetcode.html">Leetcode</a></li>
        
            <li><a href="C/C++.html">C/C++</a></li>
        
            <li><a href="Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html">Python数据结构与算法</a></li>
        
            <li><a href="Course.html">Course</a></li>
        
            <li><a href="Python%E7%89%B9%E6%80%A7.html">Python特性</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html">Python科学计算三维可视化</a></li>
        
            <li><a href="English.html">English</a></li>
        
            <li><a href="Computer%20System.html">Computer System</a></li>
        
            <li><a href="Deep%20Learning.html">Deep Learning</a></li>
        
            <li><a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html">Linux 系统编程</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html">数据库</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="Big%20Data.html">Big Data</a></li>
        
            <li><a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html">文献阅读</a></li>
        
            <li><a href="Tools.html">Tools</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="PCA.html">
                
                  <h1>PCA基础及在Spark中的应用</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>主成分分析(Principal Component Analysis, PCA)主要是用来可视化数据和数据压缩。</p>

<p>PCA的目的是找到一组可以代表原始数据的低维数据。设原始数据为\(X\in \mathbf{R}^{n\times d}\)，有\(d\)个特征。压缩后的数据为\(\mathbf{Z=XP} \in \mathbf{R}^{n\times k}\), 有\(k\)个特征(feature)。</p>

<p>那么找到的这\(k\)个特征，有什么好的约束呢？<br/>
（Variance/Covariance constraints）</p>

<ul>
<li>这k个特征无关，也就是说协相关矩阵除对角线以外的元素全部为0</li>
<li>应该根据特征的方差(variance)选择，越大越好。因为variance越大，越能代表feature。也就是说应该选择协相关矩阵对角元素大的feature。</li>
</ul>

<p>所以 \(\mathbf{P}\)应该等于X的协方差矩阵的最大\(k\)个特征向量。</p>

<p>协方差矩阵的分布式求解方法见本博客<a href="http://larryim.cc/covariance_spark.html">Covariance Matrix and its solution in Spark</a></p>

<p>下面是特征分解的基础知识，可以在线性代数课本上找到：</p>

<p>特征分解(Eigendecomposition)是将矩阵分解为由特征值和特征向量表示矩阵之积的方法。</p>

<p>令 \(A\)是一个\(N\times N\)的方阵，且有\(N\)个线性无关的特征向量\(q_i(i=1,...,N)\)。这样，A可以被分解为<br/>
\[\mathbf{A=Q\Lambda Q}^{-1}\]<br/>
其中\(\mathbf{Q}\)是\(N\times N\)的方阵，且其第\(i\)列为\(A\)的特征向量\(q_i\)。\(\mathbf{\Lambda}\)是对角矩阵，其对角线上的元素为对应的特征值，也即\(\Lambda_{ii}=\lambda_i\).</p>

<p>那么，怎么样选择\(k\)的大小呢？一般认为保留99%或95%的variance. 由于协方差矩阵的对角元素就是对应特征向量的特征值，又是variance，所以我们选择k个对角元素，保留所要求的variance:</p>

<p>\[\text{find} \min k \quad \text{s.t.} \frac{\sum^k_{i=1}\lambda_i}{\sum^k_{i=1}\lambda_i} &gt; 99\%\]</p>

<h2 id="toc_0">PCA in Spark</h2>

<p>在python 中使用 <code>numpy.linalg.eigh</code>计算特征值和特征向量.</p>

<p>基本步骤如下：</p>

<ul>
<li>计算协方差矩阵 <code>estimateCovariance()</code></li>
<li>计算主成分和对应方差 <code>pca()</code></li>
<li>计算保留的成分比例<code>varianceExplained()</code></li>
</ul>

<pre><code class="language-Python">def estimateCovariance(data):
    &quot;&quot;&quot;Compute the covariance matrix for a given rdd.

    Args:
        data (RDD of np.ndarray):  An `RDD` consisting of NumPy arrays.

    Returns:
        np.ndarray: A multi-dimensional array where the number of rows and columns both equal the
            length of the arrays in the input `RDD`.
    &quot;&quot;&quot;
    mean = data.mean()
    normalized = data.map(lambda x: x-mean)
    return normalized.map(lambda x: np.outer(x, x)).sum()/data.count()
    
def pca(data, k=2):
    &quot;&quot;&quot;Computes the top `k` principal components, corresponding scores, and all eigenvalues.


    Args:
        data (RDD of np.ndarray): An `RDD` consisting of NumPy arrays.
        k (int): The number of principal components to return.

    Returns:
        tuple of (np.ndarray, RDD of np.ndarray, np.ndarray): A tuple of (eigenvectors, `RDD` of
            scores, eigenvalues).  Eigenvectors is a multi-dimensional array where the number of
            rows equals the length of the arrays in the input `RDD` and the number of columns equals
            `k`.  The `RDD` of scores has the same number of rows as `data` and consists of arrays
            of length `k`.  Eigenvalues is an array of length d (the number of features).
    &quot;&quot;&quot;
    cov = estimateCovariance(data)
    eigVals, eigVecs = eigh(cov)
    inds = np.argsort(eigVals)[::-1]
    topkComponent = eigVecs[:,inds[0:k]]
    scores = data.map(lambda x: np.dot(x, topkComponent))
    # Return the `k` principal components, `k` scores, and all eigenvalues
    return (topkComponent, scores, eigVals[inds])
    
    
def varianceExplained(data, k=1):
    &quot;&quot;&quot;Calculate the fraction of variance explained by the top `k` eigenvectors.

    Args:
        data (RDD of np.ndarray): An RDD that contains NumPy arrays which store the
            features for an observation.
        k: The number of principal components to consider.

    Returns:
        float: A number between 0 and 1 representing the percentage of variance explained
            by the top `k` eigenvectors.
    &quot;&quot;&quot;
    components, scores, eigenvalues = pca(data)
    return sum(eigenvalues[0:k])/sum(eigenvalues)
</code></pre>

<h2 id="toc_1">应用</h2>

<p>斑马鱼(zebrafish)大脑的响应<a href="http://larryim.cc/ML_lab5_pca_student.html">可视化</a>。</p>

<h2 id="toc_2">Reference</h2>

<ol>
<li>Machine Learning Course from Coursera. Andrew Ng</li>
<li>CS190: Scalable Machine Learning</li>
</ol>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/11/28</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="covariance_spark.html">
                
                  <h1>Covariance Matrix and its solution in Spark</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>A <code>covariance matrix</code>(协方差矩阵, 离差矩阵) is a matrix whose element in the \(i,j\) position is the covariance between the \(i^{th}\) and \(j^{th}\) elements of a <code>random vector</code>(随机向量,多元随机变量). Each element of a random vector is a scalar <code>random variable</code>(随机变量).</p>

<p>Because the covariance of the \(i^{th}\) random variable with itself is simply that random variable&#39;s variance, each element on the principal diagonal of the covariance matrix is the variance of one of the random variables:</p>

<p>The covariance between random variables \(X, Y\):</p>

<p>\[\text{cov}(X,Y)=\text{E}[(X-E[X])(Y-E[Y])]\]</p>

<p>Then the covariance matrix \(\sum\) is the matrix whose \((i,j)\) is the covariance</p>

<p>\[\sum(i,j) =\text{cov}(X_i,X_j)=\text{E}[(X_i-E[X_i])(Y_i-E[Y_i])]\]</p>

<p>where \(X=[X_1,...,X_n]\)</p>

<h2 id="toc_0">Distributed Computing: Spark</h2>

<p>Setup: Raw data \(\mathbf{P} \in \mathbb{R}^{n \times d}\)</p>

<ul>
<li>Step 1: Zero Mean Data \(X=\text{E}(P)\)

<ul>
<li>Compute \(d\) feature means, \(m\in \mathbf{R}^d\)</li>
<li>Communication \(m\) to all workers</li>
<li>Subtract \(m\) from each data point</li>
</ul></li>
<li>Step 2: Compute Covariance Matrix \( \mathbf{C}_{\mathbf X} = \frac{1}{n} \mathbf{X}^\top \mathbf{X} \,.\)

<ul>
<li>compute the outer product of each data point, </li>
<li>add together these outer products, and divide by the number of data points</li>
</ul></li>
</ul>

<p><img src="media/15124807146530/compute_covariance_matrix.png" alt="compute_covariance_matrix"/></p>

<p><u>Spark Code in Python</u>:</p>

<pre><code class="language-python">def estimateCovariance(data):
    &quot;&quot;&quot;Compute the covariance matrix for a given rdd.
   
    Args:
        data (RDD of np.ndarray):  An `RDD` consisting of NumPy arrays.

    Returns:
        np.ndarray: A multi-dimensional array where the number of rows and columns both equal the length of the arrays in the input `RDD`.
    &quot;&quot;&quot;
    mean = data.mean()
    normalized = data.map(lambda x: x-mean)
    return normalized.map(lambda x: np.outer(x, x)).sum()/data.count()
</code></pre>

<h2 id="toc_1">Reference</h2>

<ol>
<li><a href="https://en.wikipedia.org/wiki/Covariance_matrix">Covariance matrix from wikipedia</a></li>
</ol>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/11/25</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="data_science_competition.html">
                
                  <h1>Data Science Competition</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">real-world application v. s.competition</h2>

<p><img src="media/15114480120681/real%20machine%20learning.png" alt="real machine learning"/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/11/23</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="scalable%20machine%20learning.html">
                
                  <h1>CS190: Scalable Machine Learning</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>see <a href="http://larryim.cc/notebook.html">Labs here</a></p>

<h3 id="toc_0">Distributing Computing</h3>

<p>Need more hardware to store/process modern data</p>

<p>Scale-up(one big machine)</p>

<ul>
<li>Can be very fast for medium scale problems</li>
<li>Expensive, specialized hardware</li>
<li>Eventually hit a wall</li>
</ul>

<p>Scale-out (many small machine, i.e., distributed)</p>

<ul>
<li>Commodity hardware, scales to massive problems</li>
<li>Need to deal with network communication</li>
<li>Added software complexity</li>
</ul>

<p>Apache Spark is a general, open-source cluster computing engine.</p>

<p>Well-suited for machine learning</p>

<ul>
<li>Fast iterative computations</li>
<li>Efficient communication primitives</li>
<li>Simple and expressive: APIs in Scala, Java, Python, R</li>
<li>Integrated higher-level libraries (<code>MLlib</code>, Spark SQL, Spark Streaming, GraphX)</li>
</ul>

<h3 id="toc_1">Typical Supervised Learning Pipeline</h3>

<ul>
<li>Obtain Raw Data</li>
<li>Feature Extraction</li>
<li>Supervised Learning</li>
<li>Evaluation</li>
<li>Predict</li>
</ul>

<p><img src="media/15108151409015/Typical%20Supervised%20Learning%20Pipeline.png" alt="Typical Supervised Learning Pipeline"/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/11/16</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				  
          					    <span class="posted-in"><a href='Course.html'>Course</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="machine_learning_techniques_SVM.html">
                
                  <h1>Machine Learning Techniques: Support Vector Machine (SVM)</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Linear SVM</a>
<ul>
<li>
<a href="#toc_1">Standard Large-Margin Problem</a>
</li>
<li>
<a href="#toc_2">Support Vector Machine</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">Dual Support Vector Machine</a>
<ul>
<li>
<a href="#toc_4">Lagrange Dual Problem</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">Linear SVM</h2>

<p><img src="media/15088454152744/large_margin.png" alt="large_margin"/></p>

<p>Our goal is to find <code>largest-margin</code> separating hyperplane.</p>

<p><strong>Distance to Hyperplane</strong>: distance(\(x,b,w) = \frac{1}{\lVert w\rVert}|w^Tx+b|\)</p>

<h3 id="toc_1">Standard Large-Margin Problem</h3>

<p>Now the problem becomes:<br/>
<img src="media/15088454152744/hyperplane%20distance.png" alt="hyperplane distance"/></p>

<p><img src="media/15088454152744/standard%20problem.png" alt="standard proble"/></p>

<h3 id="toc_2">Support Vector Machine</h3>

<p><img src="media/15088454152744/svm%20with%20qp%20solver.png" alt="svm with qp solve"/></p>

<h2 id="toc_3">Dual Support Vector Machine</h2>

<p>\[\text{SVM} \equiv \min\limits_{b,w} (\max\limits_{\text{all}\,\alpha_n \ge 0} \mathcal{L}(b,w,\alpha))\]</p>

<h3 id="toc_4">Lagrange Dual Problem</h3>

<p>for any fixed \(\alpha&#39;\)  with all \(\alpha&#39;_n\ge\) 0,<br/>
\[\min\limits_{b,w} (\max\limits_{\text{all}\,\alpha_n \ge 0} \mathcal{L}(b,w,\alpha)) \ge \min\limits_{b,w}\mathcal{L}(b,w,\alpha&#39;)\]</p>

<p>for best \(\alpha&#39;\ge 0\) on RHS,</p>

<p>\[\min\limits_{b,w} (\max\limits_{\text{all}\,\alpha_n \ge 0} \mathcal{L}(b,w,\alpha)) \ge \max\limits_{\text{all}\,\alpha_n \ge 0}(\min\limits_{b,w}  \mathcal{L}(b,w,\alpha))\]</p>

<p>Strong duality of Quadratic Programming<br/>
\[\min\limits_{b,w} (\max\limits_{\text{all}\,\alpha_n \ge 0} \mathcal{L}(b,w,\alpha)) \ge \max\limits_{\text{all}\,\alpha_n \ge 0}(\min\limits_{b,w}  \mathcal{L}(b,w,\alpha))\]</p>

<ul>
<li>&#39;&gt;=&#39;: weak duality</li>
<li>&#39;=&#39;: strong duality, true for QP if

<ul>
<li>convex primal</li>
<li>feasible primal</li>
<li>linear constraints</li>
</ul></li>
</ul>

<p>Dual Formulation of Support Vector Machine:</p>

<p><img src="media/15088454152744/Screen%20Shot%202017-10-25%20at%201.43.46%20PM.png" alt="Screen Shot 2017-10-25 at 1.43.46 P"/></p>

<p><img src="media/15088454152744/Screen%20Shot%202017-10-25%20at%201.46.05%20PM.png" alt="Screen Shot 2017-10-25 at 1.46.05 P"/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/10/24</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="机器学习_1.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">他山之石，可以攻玉</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Leetcode.html"><strong>Leetcode</strong></a>
        
            <a href="C/C++.html"><strong>C/C++</strong></a>
        
            <a href="Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html"><strong>Python数据结构与算法</strong></a>
        
            <a href="Course.html"><strong>Course</strong></a>
        
            <a href="Python%E7%89%B9%E6%80%A7.html"><strong>Python特性</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>Python科学计算三维可视化</strong></a>
        
            <a href="English.html"><strong>English</strong></a>
        
            <a href="Computer%20System.html"><strong>Computer System</strong></a>
        
            <a href="Deep%20Learning.html"><strong>Deep Learning</strong></a>
        
            <a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html"><strong>Linux 系统编程</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html"><strong>数据库</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="Big%20Data.html"><strong>Big Data</strong></a>
        
            <a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html"><strong>文献阅读</strong></a>
        
            <a href="Tools.html"><strong>Tools</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15187866094632.html">技巧：Coursera下载Jupyter notebook相关文件</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15171345711813.html">用于训练的深度学习硬件</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15166008990128.html">网络编程</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="overhead.html">overhead</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="memory_alignment.html">内存对齐</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
