<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  techlarry
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">HomePage</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_blank" href="wiki">WIKI</a></li>
        
        <li id=""><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">HomePage</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_blank" href="wiki">WIKI</a></li>
        
        <li><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="Leetcode.html">Leetcode</a></li>
        
            <li><a href="programming_language.html">编程语言</a></li>
        
            <li><a href="data_structure_and_algorithm.html">数据结构和算法</a></li>
        
            <li><a href="Course.html">Course</a></li>
        
            <li><a href="Python%E7%89%B9%E6%80%A7.html">Python特性</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html">Python科学计算三维可视化</a></li>
        
            <li><a href="English.html">English</a></li>
        
            <li><a href="Computer%20System.html">Computer System</a></li>
        
            <li><a href="Deep%20Learning.html">Deep Learning</a></li>
        
            <li><a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html">Linux 系统编程</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html">数据库</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="Big%20Data.html">Big Data</a></li>
        
            <li><a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html">文献阅读</a></li>
        
            <li><a href="Tools.html">Tools</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="15050120680855.html">
                
                  <h1>Image Captioning</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Vanilla RNN</a>
</li>
<li>
<a href="#toc_1">LSTM</a>
</li>
<li>
<a href="#toc_2">COCO</a>
</li>
<li>
<a href="#toc_3">Reference</a>
</li>
</ul>


<h2 id="toc_0">Vanilla RNN</h2>

<p><img src="media/15050120680855/Screen%20Shot%202017-09-10%20at%206.52.08%20PM.png" alt="Vanilla RNN gradient flow"/></p>

<h2 id="toc_1">LSTM</h2>

<p>Long Short Term Memory (LSTM)</p>

<p>Backward flow of gradients in RNN can explode or vanish.</p>

<p>Exploding is controlled with gradient clipping. Vanishing is controlled with additive interactions (LSTM)</p>

<p><img src="media/15050120680855/Screen%20Shot%202017-09-10%20at%206.52.27%20PM.png" alt="LSTM"/></p>

<h2 id="toc_2">COCO</h2>

<p><code>COCO</code>（Common Objects in Context） is a large-scale <code>object detection</code>(物体检测), <code>semantic segmentation</code>(语义分割), and captioning dataset. <code>COCO</code> has several main features: <code>Object segmentation</code>, <code>Recognition in context</code>.</p>

<h2 id="toc_3">Reference</h2>

<ul>
<li>Tsung Y L, Michael M, Serge B, et al. Microsoft COCO: Common Objects in Context.  arXiv:<a href="https://arxiv.org/abs/1405.0312">1405.0312</a></li>
<li>Karpathy et al. Deep Visual-Semantic Alignments for Generating Image Descriptions, CVPR 2015</li>
</ul>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/10</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Deep%20Learning.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="large_scale_machine_learning.html">
                
                  <h1>Machine Learning (10): Large Scale Machine Learning</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Learning with Large Datasets</a>
</li>
<li>
<a href="#toc_1">Stochastic Gradient Descent</a>
<ul>
<li>
<a href="#toc_2">Mini-Batch Gradient Descent</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">Stochastic Gradient Descent Convergence</a>
</li>
<li>
<a href="#toc_4">Online Learning</a>
</li>
<li>
<a href="#toc_5">Map Reduce and Data Parallelism</a>
</li>
</ul>


<h2 id="toc_0">Learning with Large Datasets</h2>

<p>We mainly benefit from a very large dataset when our algorithm has high variance when m is small. Recall that if our algorithm has high bias, more data will not have any benefit.</p>

<p>Datasets can often approach such sizes as m = 100,000,000. In this case, our gradient descent step will have to make a summation over all one hundred million examples. We will want to try to avoid this -- the approaches for doing so are described below.</p>

<h2 id="toc_1">Stochastic Gradient Descent</h2>

<p>Stochastic gradient descent is an alternative to classic (or batch) gradient descent and is more efficient and scalable to large data sets.</p>

<p>Stochastic gradient descent is written out in a different but similar way:</p>

<p>\(cost(\theta,(x^{(i)}, y^{(i)})) = \dfrac{1}{2}(h_{\theta}(x^{(i)}) - y^{(i)})^2\)</p>

<p>The only difference in the above cost function is the elimination of the m constant within \(\dfrac{1}{2}\).</p>

<p>\(J_{train}(\theta) = \dfrac{1}{m} \displaystyle \sum_{i=1}^m cost(\theta, (x^{(i)}, y^{(i)}))\)</p>

<p>\(J_{train}\) is now just the average of the cost applied to all of our training examples.</p>

<p>The algorithm is as follows</p>

<ol>
<li><p>Randomly &#39;shuffle&#39; the dataset</p></li>
<li><p>For \(i = 1\dots m\)</p></li>
</ol>

<p>\(\Theta_j := \Theta_j - \alpha (h_{\Theta}(x^{(i)}) - y^{(i)}) \cdot x^{(i)}_j\)</p>

<p>This algorithm will only try to fit one training example at a time. This way we can make progress in gradient descent without having to scan all m training examples first. Stochastic gradient descent will be unlikely to converge at the global minimum and will instead wander around it randomly, but usually yields a result that is close enough. Stochastic gradient descent will usually take 1-10 passes through your data set to get near the global minimum.</p>

<h3 id="toc_2">Mini-Batch Gradient Descent</h3>

<p>Mini-batch gradient descent can sometimes be even faster than stochastic gradient descent. Instead of using all m examples as in batch gradient descent, and instead of using only 1 example as in stochastic gradient descent, we will use some in-between number of examples b.</p>

<p>Typical values for b range from 2-100 or so.</p>

<p>For example, with b=10 and m=1000:</p>

<p>Repeat:</p>

<p>For \(i = 1,11,21,31,\dots,991\)</p>

<p>\(\theta_j := \theta_j - \alpha \dfrac{1}{10} \displaystyle \sum_{k=i}^{i+9} (h_\theta(x^{(k)}) - y^{(k)})x_j^{(k)}\)</p>

<p>We&#39;re simply summing over ten examples at a time. The advantage of computing more than one example at a time is that we can use vectorized implementations over the b examples.</p>

<h2 id="toc_3">Stochastic Gradient Descent Convergence</h2>

<p>How do we choose the learning rate α for stochastic gradient descent? Also, how do we debug stochastic gradient descent to make sure it is getting as close as possible to the global optimum?</p>

<p>One strategy is to plot the average cost of the hypothesis applied to every 1000 or so training examples. We can compute and save these costs during the gradient descent iterations.</p>

<p>With a smaller learning rate, it is <strong>possible</strong> that you may get a slightly better solution with stochastic gradient descent. That is because stochastic gradient descent will oscillate and jump around the global minimum, and it will make smaller random jumps with a smaller learning rate.</p>

<p>If you increase the number of examples you average over to plot the performance of your algorithm, the plot&#39;s line will become smoother.</p>

<p>With a very small number of examples for the average, the line will be too noisy and it will be difficult to find the trend.</p>

<p>One strategy for trying to actually converge at the global minimum is to <strong>slowly decrease α over time</strong> . For example \(\alpha = \dfrac{\text{const1}}{\text{iterationNumber + const2}}\)</p>

<p>However, this is not often done because people don&#39;t want to have to fiddle with even more parameters.</p>

<h2 id="toc_4">Online Learning</h2>

<p>With a continuous stream of users to a website, we can run an endless loop that gets (x,y), where we collect some user actions for the features in x to predict some behavior y.</p>

<p>You can update θ for each individual (x,y) pair as you collect them. This way, you can adapt to new pools of users, since you are continuously updating theta.</p>

<h2 id="toc_5">Map Reduce and Data Parallelism</h2>

<p>We can divide up batch gradient descent and dispatch the cost function for a subset of the data to many different machines so that we can train our algorithm in parallel.</p>

<p>You can split your training set into z subsets corresponding to the number of machines you have. On each of those machines calculate \(\displaystyle \sum_{i=p}^{q}(h_{\theta}(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}\), where we&#39;ve split the data starting at p and ending at q.</p>

<p>MapReduce will take all these dispatched (or &#39;mapped&#39;) jobs and &#39;reduce&#39; them by calculating:</p>

<p>\(\Theta_j := \Theta_j - \alpha \dfrac{1}{z}(temp_j^{(1)} + temp_j^{(2)} + \cdots + temp_j^{(z)})\)</p>

<p>For all \(j = 0, \dots, n\).</p>

<p>This is simply taking the computed cost from all the machines, calculating their average, multiplying by the learning rate, and updating theta.</p>

<p>Your learning algorithm is MapReduceable if it can be <u>expressed as computing sums of functions over the training set</u> . Linear regression and logistic regression are easily parallelizable.</p>

<p>For neural networks, you can compute forward propagation and back propagation on subsets of your data on many machines. Those machines can report their derivatives back to a &#39;master&#39; server that will combine them.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/9</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="batch_normalization.html">
                
                  <h1>Batch Normalization</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>The idea is relatively straightforward. Machine learning methods tend to work better when their input data consists of uncorrelated features with <strong>zero mean</strong> and <strong>unit variance</strong>. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features; this will ensure that the first layer of the network sees data that follows a nice distribution. However even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.</p>

<h2 id="toc_0">Covariate Shift</h2>

<p>While stochastic gradient is simple and effective, it requires careful tuning of the model hyper-parameters, specifically the learning rate used in optimization, as well as the initial values for the model parameters. The train- ing is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers – so that small changes to the network parameters amplify as the network becomes deeper.</p>

<p>The change in the distribution of layers&#39; inputs presents a problem because the layers need to continuously adapt to the new distribution. When the input distribution to a learning system changes, it is said to experience <code>covariate shift</code>. This is typically handled via domain adaption.</p>

<p><code>Internal Covariate Shift</code> refers to the change in the distribution of internal nodes of a deep network due to change in network parameters, in the course of training. <code>Batch Normalization</code>, that takes a step towards reducing internal covariance shift, and in doing so dramatically accelerates the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. It also has a beneficial effect on the dependence of gradients on the scale of the parameters or of their initial values. This allows us to use much higher learning rates without the risk of divergence. </p>

<h2 id="toc_1">Algorithm</h2>

<h3 id="toc_2">Forward</h3>

<p>Given some input values \(\mathcal{B} = x^{(1)},...,x^{(m)}\) over a mini-batch in the layer \(l\) of neural network; </p>

<p>\[\mu_{\mathcal{B}} = \frac{1}{m}\sum_i^m x_i  \text{     (mini-batch mean)}\\<br/>
\sigma^2=\frac{1}{m}\sum_i(x_i-\mu_{\mathcal{B}})^2  \text{     (mini-batch variance)}\\<br/>
\hat{x}_i=\frac{x_i-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{\mathcal{B}}}^2+\varepsilon}} \text{     (normalize)}\\<br/>
y_i=\gamma \hat{x}_i+\beta \text{     (scale and shift)}<br/>
\]</p>

<p>At each iteration, we update the running averages for mean and variance using an exponential decay based on the momentum parameter:</p>

<pre><code class="language-text">running_mean = momentum * running_mean + (1 - momentum) * sample_mean
running_var = momentum * running_var + (1 - momentum) * sample_var
</code></pre>

<h2 id="toc_3">Backward</h2>

<p><img src="media/15048405877529/Screen%20Shot%202017-09-08%20at%203.11.45%20PM.png" alt="batch-normalization-backward"/></p>

<h3 id="toc_4">Test time</h3>

<p>Using trained <code>runing_mean</code> and <code>running_var</code> to take forward step.</p>

<h2 id="toc_5">TensorFlow</h2>

<p>Applying <code>Batch Normalization</code> in TensorFlow Model is very convenient. Just add one line of code in TensorFlow: <code>tf.nn.batch-normalization</code>.</p>

<h2 id="toc_6">Reference</h2>

<p>Sergey Ioffe, Christian Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv:<a href="https://arxiv.org/abs/1502.03167v3">1501.02167v3</a></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/8</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Deep%20Learning.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15046649572570.html">
                
                  <h1>Pandas</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>To convert a pandas <code>dataframe</code> (df) to a numpy <code>ndarray</code>, use this code:</p>

<pre><code class="language-text">df=df.values
</code></pre>

<p>df now becomes a numpy <code>ndarray</code>.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/6</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Python%E7%89%B9%E6%80%A7.html'>Python特性</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="tensorflow_operation.html">
                
                  <h1>TensorFlow(4):TensorFlow Operation</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">1 Visualize with TensorBoard</a>
<ul>
<li>
<a href="#toc_1">Explicitly name operation, variable</a>
</li>
</ul>
</li>
<li>
<a href="#toc_2">2 Constant types</a>
<ul>
<li>
<a href="#toc_3">Tensors filled with a specific value</a>
</li>
<li>
<a href="#toc_4">Constants as sequences</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">3 Math Operations</a>
</li>
<li>
<a href="#toc_6">4 TensorFlow data types:</a>
<ul>
<li>
<a href="#toc_7">Python Native Types</a>
</li>
<li>
<a href="#toc_8">TensorFlow Native Types</a>
</li>
<li>
<a href="#toc_9">Numpy Data Types</a>
</li>
<li>
<a href="#toc_10">Constant</a>
</li>
<li>
<a href="#toc_11">Variables</a>
<ul>
<li>
<a href="#toc_12">Each session maintains its own copy of variable</a>
</li>
<li>
<a href="#toc_13">Use a variable to initialize another variables</a>
</li>
<li>
<a href="#toc_14">Session vs InteractiveSession,</a>
</li>
</ul>
</li>
<li>
<a href="#toc_15">Placeholder</a>
</li>
<li>
<a href="#toc_16">Lazy loading</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">1 Visualize with TensorBoard</h2>

<pre><code class="language-python">import tensorflow as tf
a = tf.constant(2, name=&#39;a&#39;)
b = tf.constant(3, name=&#39;b&#39;)
x = tf.add(a, b, name=&#39;add&#39;)
with tf.Session() as sess:
    writer = tf.summary.FileWriter(&#39;./graphs&#39;, sess.graph)
    print(sess.run(x))media/15033836099887
writer.close() # close the writer when you&#39;re done using it.
</code></pre>

<pre><code class="language-text">5
</code></pre>

<p>Bash command (to view TensorBoard):</p>

<pre><code class="language-bash"> tensorboard --logdir=&#39;./graphs&#39; --port 6006
 # open http://localhost:6006/#graphs in your browser
</code></pre>

<h3 id="toc_1">Explicitly name operation, variable</h3>

<pre><code class="language-python">a = tf.constant(2, name=&#39;a&#39;)
b = tf.constant(3, name=&#39;b&#39;)
x = tf.add(a,b,name=&#39;add&#39;)
with tf.Session() as sess:
    writer = tf.summary.FileWriter(&#39;./graphs&#39;, sess.graph)
    print(sess.run(x))
writer.close() # close the writer when you&#39;re done using it.
</code></pre>

<pre><code class="language-text">5
</code></pre>

<p>The figure produced by TensorBoard is as follows:</p>

<p><img src="media/15033836099887/explicit_name.png" alt=""/></p>

<p><strong>Note</strong>:  Learn to use TensorBoard well and often. It will help a lot when you build complicated models.</p>

<h2 id="toc_2">2 Constant types</h2>

<h3 id="toc_3">Tensors filled with a specific value</h3>

<p>Using <code>tensorflow.zeros</code> to fill tensor with zeros, which is similar to <code>Numpy</code>:</p>

<pre><code class="language-python">tf.zeros(shape, dtype=tf.float32, name=None)
</code></pre>

<p>For example,</p>

<pre><code class="language-python">x = tf.zeros([2,3], tf.int32)
with tf.Session() as sess:
    print(sess.run(x))
</code></pre>

<pre><code class="language-text">[[0 0 0]
 [0 0 0]]
</code></pre>

<p><code>tensorflow.zeros_like</code> return an tensor of zeros with the same shape and type as a given tensor. For example, we may want to have a tensor filled with zeros, with the same shape as <code>x</code>:</p>

<pre><code class="language-python">y = tf.zeros_like(x)
with tf.Session() as sess:
    print(sess.run(y))
</code></pre>

<pre><code class="language-text">[[0 0 0]
 [0 0 0]]
</code></pre>

<p>There are other command to fill tensor with a specific value, such as <code>tensorflow.ones</code>, <code>tensorflow.ones_like</code>, which of usage is similar to <code>tensorflow.zeros</code>, <code>tensorflow.zeros_like</code>.</p>

<p><code>tensorflow.fill</code> creates a tensor filled with a scalar value:</p>

<pre><code class="language-python">tf.fill(dims, value, name=None)
</code></pre>

<pre><code class="language-python">z = tf.fill([3,4],3)
with tf.Session() as sess:
    print(sess.run(z))
</code></pre>

<pre><code class="language-text">[[3 3 3 3]
 [3 3 3 3]
 [3 3 3 3]]
</code></pre>

<h3 id="toc_4">Constants as sequences</h3>

<p>You can create constants that are sequences, using <code>tf.linspace</code>, <code>tf.range</code>:</p>

<pre><code class="language-python">tf.linspace(start, stop, num, name=None)

# create a sequence of num evenly-spaced values are generated beginning at  start. If num &gt; 1, the values in the sequence increase by stop - start / num - 1, so that the last one is exactly stop.
# start, stop, num must be scalars
# comparable to but slightly different from numpy.linspace
# numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)

tf.range(start, limit=None, delta=1, dtype=None, name=&#39;range&#39;)
# create a sequence of numbers that begins at start and extends by increments of delta up to but not including limit
# slight different from range in Python
</code></pre>

<pre><code class="language-python">x = tf.linspace(10.0, 13.0, 4, name=&#39;linspace&#39;)
y = tf.range(3, 18)
z= tf.range(3, 18, 3)
with tf.Session() as sess:
    print(sess.run(x))
    print(sess.run(y))
    print(sess.run(z))
</code></pre>

<pre><code class="language-text">[ 10.  11.  12.  13.]
[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
[ 3  6  9 12 15]
</code></pre>

<h2 id="toc_5">3 Math Operations</h2>

<p><img src="media/15033836099887/math_operations.png" alt=""/></p>

<pre><code class="language-python">a = tf.constant([[3,6],[0,0]])
b = tf.constant([[0,0],[2,2]])
x1 = tf.add(a, b)
x2 = tf.add_n([a,b,b]) # &gt;&gt; [7 10]. Equivalent to a + b + b
x3 = tf.multiply(a, b) # &gt;&gt; [6 12] because mul is element wise
x4 = tf.matmul(a, b) # &gt;&gt; ValueError
x5 = tf.matmul(tf.reshape(a, [4, 1]), tf.reshape(b, [1, 4])) # &gt;&gt; [[18]]

with tf.Session() as sess:
    sess.run(a)
    sess.run(b)
    print(&#39;x1:\n&#39;, sess.run(x1))
    print(&#39;x2:\n&#39;, sess.run(x2))
    print(&#39;x3:\n&#39;, sess.run(x3))
    print(&#39;x4:\n&#39;, sess.run(x4))
    print(&#39;x5:\n&#39;, sess.run(x5))
</code></pre>

<pre><code class="language-text">x1:
 [[3 6]
 [2 2]]
x2:
 [[3 6]
 [4 4]]
x3:
 [[0 0]
 [0 0]]
x4:
 [[12 12]
 [ 0  0]]
x5:
 [[ 0  0  6  6]
 [ 0  0 12 12]
 [ 0  0  0  0]
 [ 0  0  0  0]]
</code></pre>

<h2 id="toc_6">4 TensorFlow data types:</h2>

<h3 id="toc_7">Python Native Types</h3>

<p>TensorFlow takes Python natives types: <code>boolean</code>, <code>numeric</code> (<code>int</code>, <code>float</code>), <code>strings</code></p>

<p>TensorFlow takes in Python native types such as Python boolean values, numeric values (integers, floats), and strings. Single values will be converted to 0-d tensors (or scalars), lists of values will be converted to 1-d tensors (vectors), lists of lists of values will be converted to 2-d tensors (matrices), and so on.</p>

<pre><code class="language-python">tf.InteractiveSession() # open tensorflow interactivesession
t_0 = 19   # Treated as a 0-d tensor, or &quot;scalar&quot; 
print(&#39;t_0:&#39;,t_0)
print(tf.zeros_like(t_0))   # ==&gt; 0
print(tf.ones_like(t_0))   # ==&gt; 1
t_1 = [b&quot;apple&quot; ,  b&quot;peach&quot; ,  b&quot;grape&quot;]   # treated as a 1-d tensor, or &quot;vector&quot; 
print(&#39;t_1:&#39;,t_1)
print(tf.zeros_like(t_1))   # ==&gt; [&#39;&#39; &#39;&#39; &#39;&#39;]
t_2= [[ True, False, False],  [False, False, True], [False, True ,   False ]]   # treated as a 2-d tensor, or &quot;matrix&quot;
print(&#39;t_2:&#39;,t_2)
print(tf.zeros_like(t_2))   # ==&gt; 2x2 tensor, all elements are False 
print(tf.ones_like(t_2))   # ==&gt; 2x2 tensor, all elements are True
</code></pre>

<pre><code class="language-text">t_0: 19
Tensor(&quot;zeros_like_32:0&quot;, shape=(), dtype=int32)
Tensor(&quot;ones_like_20:0&quot;, shape=(), dtype=int32)
t_1: [b&#39;apple&#39;, b&#39;peach&#39;, b&#39;grape&#39;]
Tensor(&quot;zeros_like_33:0&quot;, shape=(3,), dtype=string)
t_2: [[True, False, False], [False, False, True], [False, True, False]]
Tensor(&quot;zeros_like_34:0&quot;, shape=(3, 3), dtype=bool)
Tensor(&quot;ones_like_21:0&quot;, shape=(3, 3), dtype=bool)
</code></pre>

<p><strong>Note: Do not use Python native types for tensors because TensorFlow has to infer Python type.</strong></p>

<h3 id="toc_8">TensorFlow Native Types</h3>

<p>Like <code>NumPy</code>, <code>TensorFlow</code> also its own data types such as <code>tf.int32</code>, <code>tf.float32</code>. Below is a list of current TensorFlow data types.</p>

<p><img src="media/15033836099887/tensorflow_data_types.png" alt=""/></p>

<h3 id="toc_9">Numpy Data Types</h3>

<p>By now, you’ve probably noticed the similarity between <code>NumPy</code> and <code>TensorFlow</code>. <code>TensorFlow</code> was designed to integrate seamlessly with <code>Numpy</code>, the package that has become the  lingua franca of data science.</p>

<p>TensorFlow’s data types are based on those of NumPy; in fact, <code>np.int32 == tf.int32</code> returns <code>True</code>. You can pass <code>NumPy</code> types to <code>TensorFlow</code> ops.</p>

<p>Example:</p>

<pre><code class="language-python">import numpy as np
tf.ones([2, 2],  np.float32)
</code></pre>

<pre><code class="language-text">&lt;tf.Tensor &#39;ones:0&#39; shape=(2, 2) dtype=float32&gt;
</code></pre>

<pre><code class="language-python">x = np.zeros((2,2))
tf.ones_like(x)
</code></pre>

<pre><code class="language-text">&lt;tf.Tensor &#39;ones_like_22:0&#39; shape=(2, 2) dtype=float64&gt;
</code></pre>

<h3 id="toc_10">Constant</h3>

<p>Constants are stored in the graph definition. This makes loading graphs expensive when constants are big. <strong>Only use constants for primitive types, use variables or readers for more data that requires more memory</strong>.</p>

<pre><code class="language-python">g = tf.Graph() # to add operators to a graph, set it as default:
with g.as_default():
    my_const = tf.constant([1.0, 2.0], name=&quot;my_const&quot;)
    with tf.Session() as sess:
        print(sess.graph.as_graph_def())
</code></pre>

<pre><code class="language-text">node {
  name: &quot;my_const&quot;
  op: &quot;Const&quot;
  attr {
    key: &quot;dtype&quot;
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: &quot;value&quot;
    value {
      tensor {
        dtype: DT_FLOAT
        tensor_shape {
          dim {
            size: 2
          }
        }
        tensor_content: &quot;\000\000\200?\000\000\000@&quot;
      }
    }
  }
}
versions {
  producer: 24
}
</code></pre>

<h3 id="toc_11">Variables</h3>

<p><code>tf.constant</code> is an operation, but <code>tf.Variable</code> is a class. <code>tf.Variables</code> holds several operations:</p>

<pre><code class="language-python">tf.InteractiveSession()
xx = tf.Variable(23, name=&#39;scalar&#39;)
xx.initializer # init op
xx.value() # read op
assign_op = xx.assign(5)

</code></pre>

<p>You have to initialize <code>variables</code>, The easiest way is initializing all variables at once:</p>

<pre><code class="language-python">init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    print(xx.eval())
    sess.run(assign_op)
    print(xx.eval())
</code></pre>

<pre><code class="language-text">23
5
</code></pre>

<h4 id="toc_12">Each session maintains its own copy of variable</h4>

<pre><code class="language-python">W = tf.Variable(10, name=&#39;W&#39;)
sess1 = tf.Session()
sess2 = tf.Session()
sess1.run(W.initializer)
sess2.run(W.initializer)
print(sess1.run(W.assign_add(10)))
print(sess2.run(W.assign_sub(2))) # not 18!

sess1.close()
sess2.close()
</code></pre>

<pre><code class="language-text">20
8
</code></pre>

<h4 id="toc_13">Use a variable to initialize another variables</h4>

<pre><code class="language-python"># want to declare U = 2*W
# W is random tensor
W = tf.Variable(tf.truncated_normal([4, 2]))
U = tf.Variable(2*W.initialized_value())
with tf.Session() as sess:
    sess.run(U.initializer)
    print(U.eval())
</code></pre>

<pre><code class="language-text">[[ 1.11442947 -3.1675539 ]
 [ 3.02267933 -0.81786388]
 [ 2.57613969 -0.98440802]
 [ 0.6298722  -0.38194153]]
</code></pre>

<h4 id="toc_14">Session vs InteractiveSession,</h4>

<p>You sometimes see InteractiveSession instead of Session. The only difference is an InteractiveSession makes itself the default.</p>

<pre><code class="language-python">sess = tf.InteractiveSession()
a = tf.constant(5.0)
b = tf.constant(6.0)
c = a*b
# We can just use `c.eval()` with out specifying the context `sess`
print(c.eval())
sess.close()
</code></pre>

<pre><code class="language-text">30.0
</code></pre>

<h3 id="toc_15">Placeholder</h3>

<p>A TensorFlow program often has 2 phases:</p>

<ol>
<li>Assemble a graph</li>
<li>Use a session to execute operations in the graph</li>
</ol>

<p>\(\rightarrow\) can assemble the graph without knowing the values needed for computation</p>

<p><strong>Analogy</strong>: Can define the function \(f(x,y) = x*2+y\) without knowing value of \(x\) or \(y\).</p>

<p>So using <code>placeholders</code>, we can later supply their data when they needed to execute the computation.</p>

<pre><code class="language-text">tf.placeholder(dtype, shape=None, name=None)
</code></pre>

<p><code>shape=None</code> means that tensor of nay shape will be accepted as value for placeholder. Note: <strong><code>shape=None</code> is easy to construct graphs, but nightmarish for debugging</strong>.</p>

<p>To make <code>shape</code>  flexible, <code>None</code> can be used in the <code>shape</code> argument:</p>

<pre><code class="language-text">    X = tf.placeholder(dtype=tf.float32, shape=[n_x, None], name=&#39;X&#39;)
</code></pre>

<h3 id="toc_16">Lazy loading</h3>

<p><code>Lazy loading</code> means defer creating/initializing an object until it is needed. In the context of TensorFlow, it means you defer creating an op until you need to compute it. </p>

<p>Normal loading:</p>

<pre><code class="language-python">g = tf.Graph()
with g.as_default():
    x = tf.Variable(10, name=&#39;x&#39;)
    y = tf.Variable(20, name=&#39;y&#39;)
    z = tf.add(x,y) # you create the node for add node before executing the graph

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for _ in range(10):
            sess.run(z)

</code></pre>

<p>Lazy loading:</p>

<pre><code class="language-python">g = tf.Graph()
with g.as_default():
    x = tf.Variable(10, name=&#39;x&#39;)
    y = tf.Variable(20, name=&#39;y&#39;)

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        writer = tf.summary.FileWriter(&#39;./my_graph/12&#39;, sess.graph)
        for _ in range(10):
            sess.run(tf.add(x,y)) # someone decides to be clever to save one line of code
        writer.close()
</code></pre>

<p>Note: In Lazy loading, Node <code>ADD</code> added 10 times to the graph definition. Image you want to compute an operations thousands of times, you graph gets bloated slow to load, and expensive to pass around.</p>

<p><strong>Solution</strong>: </p>

<ol>
<li>Separate definition of ops from computing/running ops</li>
<li>Use Python property to ensure function is also loaded once the first time it is called.</li>
</ol>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/8/20</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Tensorflow.html'>Tensorflow</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all_15.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_17.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">他山之石，可以攻玉</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Leetcode.html"><strong>Leetcode</strong></a>
        
            <a href="programming_language.html"><strong>编程语言</strong></a>
        
            <a href="data_structure_and_algorithm.html"><strong>数据结构和算法</strong></a>
        
            <a href="Course.html"><strong>Course</strong></a>
        
            <a href="Python%E7%89%B9%E6%80%A7.html"><strong>Python特性</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>Python科学计算三维可视化</strong></a>
        
            <a href="English.html"><strong>English</strong></a>
        
            <a href="Computer%20System.html"><strong>Computer System</strong></a>
        
            <a href="Deep%20Learning.html"><strong>Deep Learning</strong></a>
        
            <a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html"><strong>Linux 系统编程</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html"><strong>数据库</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="Big%20Data.html"><strong>Big Data</strong></a>
        
            <a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html"><strong>文献阅读</strong></a>
        
            <a href="Tools.html"><strong>Tools</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="csapp-internet-programming.html">CSAPP - 网络编程</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os_concepts_threads_and_concurrency.html">Operating System Concepts 4 - Threads & Concurrency</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="head-first_java_note.html">Head first Java</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="cpp_by_diessection.html">[NOTE] C++ By Dissection</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os-concets-processes.html">Operating System Concepts 3 - Processes</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
