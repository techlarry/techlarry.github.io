<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Zhenhua Wang">
        <link rel="canonical" href="http://larryim.cc/note-big-data/projects/SparkStreaming实时流处理项目/">
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>SparkStreaming实时流处理 - Zhenhua's Notes - Big Data</title>
        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome-4.5.0.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="../../css/highlight.css">
        <link href="../../extra_css/custom.css" rel="stylesheet">
        <link href="../../extra_css/custom.js" rel="stylesheet">
        <link href="../../extra_css/friendly.css" rel="stylesheet">
        <link href="../../extra_css/theme.css" rel="stylesheet">
        <link href="../../extra_css/mkdocs/js/lunr-0.5.7.min.js" rel="stylesheet">
        <link href="../../extra_css/mkdocs/js/mustache.min.js" rel="stylesheet">
        <link href="../../extra_css/mkdocs/js/require.js" rel="stylesheet">
        <link href="../../extra_css/mkdocs/js/search.js" rel="stylesheet">
        <link href="../../extra_css/mkdocs/js/text.js" rel="stylesheet">
        <link href="../../extra_css/code-tab.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        <script src="../../js/jquery-1.10.2.min.js"></script>
        <script src="../../js/bootstrap-3.0.3.min.js"></script>
        <script src="../../js/highlight.pack.js"></script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="../..">Zhenhua's Notes - Big Data</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                    <li >
                        <a href="../..">Home</a>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">HADOOP <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../hadoop/">Contents</a>
</li>
                            
<li >
    <a href="../../hadoop/ch1/">Chapter 1: Meet Hadoop</a>
</li>
                            
<li >
    <a href="../../hadoop/ch2/">Chapter 2: MapReduce</a>
</li>
                            
<li >
    <a href="../../hadoop/ch3/">Chapter 3: The Hadoop Distributed FileSystem</a>
</li>
                            
<li >
    <a href="../../hadoop/ch4/">Chapter 4: YARN</a>
</li>
                            
<li >
    <a href="../../hadoop/ch5/">Chapter 5: Hadoop I/O</a>
</li>
                            
<li >
    <a href="../../hadoop/ch6/">Chapter 6: Developing a MapReduce Application</a>
</li>
                            
<li >
    <a href="../../hadoop/ch7/">Chapter 7: How MapReduce Works</a>
</li>
                            
<li >
    <a href="../../hadoop/ch8/">Chapter 8: MapReduce Types and Formats</a>
</li>
                            
<li >
    <a href="../../hadoop/ch9/">Chapter 9: MapReduce Features</a>
</li>
                            
<li >
    <a href="../../hadoop/ch10/">Chapter 10: Setting Up a Hadoop Cluster</a>
</li>
                            
<li >
    <a href="../../hadoop/ch11/">Chapter 11: Adminstering Hadoop</a>
</li>
                            
<li >
    <a href="../../hadoop/ch12/">Chapter 12: Avro</a>
</li>
                            
<li >
    <a href="../../hadoop/ch13/">Chapter 13: Parquet</a>
</li>
                            
<li >
    <a href="../../hadoop/ch14/">Chapter 14: Flume</a>
</li>
                            
<li >
    <a href="../../hadoop/ch15/">Chapter 15: Sqoop</a>
</li>
                            
<li >
    <a href="../../hadoop/ch16/">Chapter 16: Pig</a>
</li>
                            
<li >
    <a href="../../hadoop/ch17/">Chapter 17: Hive</a>
</li>
                            
<li >
    <a href="../../hadoop/ch18/">Chapter 18: Crunch</a>
</li>
                            
<li >
    <a href="../../hadoop/ch19/">Chapter 19: Spark</a>
</li>
                            
<li >
    <a href="../../hadoop/ch20/">Chapter 20: HBase</a>
</li>
                            
<li >
    <a href="../../hadoop/ch21/">Chapter 21: ZooKeeper</a>
</li>
                            
<li >
    <a href="../../hadoop/ch22/">Chapter 22: Composable Data at Center</a>
</li>
                            
<li >
    <a href="../../hadoop/ch23/">Chapter 23: Biological Data Science: Saving Lives with Software</a>
</li>
                            
<li >
    <a href="../../hadoop/ch24/">Chapter 24: Cascading</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">GDM <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../gdm/">Contents</a>
</li>
                            
<li >
    <a href="../../gdm/ch1/">Chapter 1: 简介</a>
</li>
                            
<li >
    <a href="../../gdm/ch2/">Chapter 2: 推荐系统入门</a>
</li>
                            
<li >
    <a href="../../gdm/ch3/">Chapter 3: 隐式评价和基于物品的过滤算法</a>
</li>
                            
<li >
    <a href="../../gdm/ch4/">Chapter 4: 分类</a>
</li>
                            
<li >
    <a href="../../gdm/ch5/">Chapter 5: 进一步探索分类</a>
</li>
                            
<li >
    <a href="../../gdm/ch6/">Chapter 6: 概率和朴素贝叶斯</a>
</li>
                            
<li >
    <a href="../../gdm/ch7/">Chapter 7: 朴素贝叶斯和文本数据</a>
</li>
                            
<li >
    <a href="../../gdm/ch8/">Chapter 8: 聚类</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">DataMining <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../../datamining/">Contents</a>
</li>
                            
<li >
    <a href="../../datamining/guideToDataMining/">面向程序员的数据挖掘指南</a>
</li>
                        </ul>
                    </li>
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Projects <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                            
<li >
    <a href="../">Contents</a>
</li>
                            
<li class="active">
    <a href="./">SparkStreaming实时流处理</a>
</li>
                        </ul>
                    </li>
                    <li >
                        <a href="../../books/">Books</a>
                    </li>
                </ul>

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                    <li >
                        <a rel="next" href="../">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../../books/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
            </ul>
        </div>
    </div>
</div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#spark-streaming">Spark Streaming实时流处理项目</a></li>
        <li class="main "><a href="#1">1 初识实时流处理</a></li>
            <li><a href="#_1">业务现状分析</a></li>
            <li><a href="#_2">实时流处理产生背景</a></li>
            <li><a href="#_3">实时流处理概述</a></li>
            <li><a href="#_4">离线计算与实时计算对比</a></li>
            <li><a href="#_5">实时流处理框架对比</a></li>
            <li><a href="#_6">实时流处理架构和技术选型</a></li>
            <li><a href="#_7">实时流处理在企业中的应用</a></li>
        <li class="main "><a href="#2-flume">2 分布式日志收集框架Flume</a></li>
            <li><a href="#_8">业务现状分析</a></li>
            <li><a href="#flume">Flume概述</a></li>
            <li><a href="#flume_1">Flume架构及核心组件</a></li>
            <li><a href="#flume_2">Flume实战</a></li>
        <li class="main "><a href="#3-kafka">3 分布式消息队列Kafka</a></li>
            <li><a href="#kafka">Kafka部署及使用</a></li>
            <li><a href="#kafka-java">Kafka Java 编程</a></li>
            <li><a href="#flumekafka">整合Flume和Kafka完成实时数据采集</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h3 id="spark-streaming"><strong>Spark Streaming实时流处理项目</strong><a class="headerlink" href="#spark-streaming" title="Permanent link">&para;</a></h3>
<p>该项目从实时数据产生和流向的不同环节出发，通过集成主流的分布式日志收集框架Flume、分布式消息队列Kafka、分布式列式数据库HBase、以及Spark Streaming实现实时流处理。</p>
<h3 id="1">1 初识实时流处理<a class="headerlink" href="#1" title="Permanent link">&para;</a></h3>
<h4 id="_1">业务现状分析<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h4>
<p>需求：统计主站每个（指定）课程访问的客户端、地域信息分布</p>
<p>==&gt; 如上两个操作：采用离线（spark/mapreduce）的方式进行统计</p>
<p>实现步骤：</p>
<ul>
<li>课程编号，ip信息，user-agent</li>
<li>进行相应的统计分析操作：MapReduce/Spark</li>
</ul>
<p>项目架构：</p>
<ul>
<li>日志收集：Flume</li>
<li>离线分析：MapReduce/Spark</li>
<li>统计结果图形化展示</li>
</ul>
<p>问题：</p>
<ul>
<li>小时级别</li>
<li>10分钟</li>
<li>秒级别</li>
</ul>
<h4 id="_2">实时流处理产生背景<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h4>
<ul>
<li>时效性高</li>
<li>数据量大</li>
</ul>
<h4 id="_3">实时流处理概述<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h4>
<p>https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101</p>
<ul>
<li>实时计算 apache storm</li>
<li>流式计算</li>
<li>实时流式计算</li>
</ul>
<h4 id="_4">离线计算与实时计算对比<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h4>
<ul>
<li>数据来源<ul>
<li>离线：来自HDFS上的历史数据，数据量比较大</li>
<li>实时：来自消息队列(Kafka)，是实时新增/修改记录过来的某一笔数据</li>
</ul>
</li>
<li>处理过程<ul>
<li>离线：MapReduce, map + reduce</li>
<li>实时: Spark(DStream/SS) </li>
</ul>
</li>
<li>处理速度<ul>
<li>离线：幔</li>
<li>实时：快速 </li>
</ul>
</li>
<li>进程<ul>
<li>离线：进程有启动+销毁的过程</li>
<li>实时： 7*24小时运行</li>
</ul>
</li>
</ul>
<h4 id="_5">实时流处理框架对比<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h4>
<ul>
<li><a href="http://storm.apache.org/">Apache Storm</a></li>
</ul>
<blockquote>
<p>Apache Storm is a free and open source distributed <strong>realtime</strong> computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use!</p>
</blockquote>
<ul>
<li><a href="https://spark.apache.org/streaming/">Apache Spark Streaming</a></li>
</ul>
<blockquote>
<p>实际上是微批处理（批处理间隔非常小)</p>
</blockquote>
<ul>
<li><a href="http://kafka.apache.org/">Apache kafka</a></li>
<li><a href="https://flink.apache.org/">Apache Flink</a></li>
</ul>
<blockquote>
<p>Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.</p>
</blockquote>
<h4 id="_6">实时流处理架构和技术选型<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h4>
<p><img alt="实时流处理架构和技术选型" src="../figures/实时流处理架构和技术选型.jpg" /></p>
<p>加一层flume消息队列，主要为了减轻压力，起到缓冲作用</p>
<p><img alt="" src="../figures/15370800175689.jpg" /></p>
<h4 id="_7">实时流处理在企业中的应用<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h4>
<ul>
<li>电信行业： 你的手机套餐流量用完，收到短信提示</li>
<li>电商行业：搜索商品时，进行推荐</li>
</ul>
<h3 id="2-flume">2 分布式日志收集框架Flume<a class="headerlink" href="#2-flume" title="Permanent link">&para;</a></h3>
<p>see detail in Hadoop: definitive Guide, <a href="../../hadoop/ch14/">Chapter 14</a></p>
<h4 id="_8">业务现状分析<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h4>
<p>You have a lot of servers and systems</p>
<ul>
<li>network devices</li>
<li>operating system</li>
<li>web servers</li>
<li>applications</li>
</ul>
<p>And they generate large amount of logs and other data.</p>
<p>Problem: Since you have a business idea, how to implement the idea?</p>
<p>OPTION: You may move logs and data generated to hadoop hdfs directly.</p>
<p>但是存在问题：</p>
<ul>
<li>如何做监控</li>
<li>如何保证时效性</li>
<li>直接传送文本数据，开销太大</li>
<li>容错</li>
<li>负载均衡</li>
</ul>
<p>SOLUTION: 使用Flume，基本上写配置文件就OK了，Flume自动解决以上问题。</p>
<h4 id="flume">Flume概述<a class="headerlink" href="#flume" title="Permanent link">&para;</a></h4>
<blockquote>
<p>Flume is a distributed, reliable, and available service for efficiently <strong>collecting, aggregating, and moving large amounts of log data</strong>. It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application. [<a href="http://flume.apache.org/">Apache Flume</a>]</p>
</blockquote>
<h4 id="flume_1">Flume架构及核心组件<a class="headerlink" href="#flume_1" title="Permanent link">&para;</a></h4>
<p><img alt="" src="../figures/apacheFlumeDemo.png" /></p>
<p>see detail in Hadoop: definitive Guide, <a href="../../hadoop/ch14/">Chapter 14</a></p>
<h4 id="flume_2">Flume实战<a class="headerlink" href="#flume_2" title="Permanent link">&para;</a></h4>
<p><hh>需求： 从指定网络端口采集数据<hh></p>
<p>使用Flume的关键就是写配置文件</p>
<ul>
<li>配置Source, Channel, Sink</li>
<li>把以上三个组件串起来</li>
</ul>
<p> <div class=codehilite><pre>http://flume.apache.org/FlumeUserGuide.html#example-2
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
</pre></div></p>
<blockquote>
<p><CB>netcat source</CB>: A netcat-like source that listens on a given port and turns each line of text into an event. It opens a specified port and listens for data. The expectation is that the supplied data is newline separated text. Each line of text is turned into a Flume event and sent via the connected channel. [<a href="http://flume.apache.org/FlumeUserGuide.html#netcat-tcp-source">NetCat TCP Source</a>]</p>
<p><CB>logger sink</CB>: Logs event at INFO level. Typically useful for testing/debugging purpose.  [<a href="http://flume.apache.org/FlumeUserGuide.html#logger-sink">Logger Sink</a>]</p>
<p><CB>memory channel</CB>: The events are stored in an in-memory queue with configurable max size. It’s ideal for flows that need higher throughput and are prepared to lose the staged data in the event of an agent failures. [<a href="http://flume.apache.org/FlumeUserGuide.html#memory-channel0">memory channel</a>]</p>
</blockquote>
<p> <div class=codehilite><pre><span class=c1>## 启动flume</span>
$ flume-ng agent <span class=se>\</span>
--name a1 <span class=se>\ </span> <span class=c1># agent name</span>
--conf <span class=nv>$F</span>LUME_HOME/conf <span class=se>\ </span><span class=c1># use configs in &lt;conf&gt; directory</span>
--conf-file  example.conf <span class=se>\ </span><span class=c1># specify a config file</span>
-Dflume.root.logger<span class=o>=</span>INFO,console <span class=c1># sets a Java system property value</span>

<span class=c1>## 在另外一个terminal用telnet模拟数据源</span>
$ telnet localhost <span class=m>44444</span> 
Trying 127.0.0.1...
Connected to localhost.
Escape character is <span class=s1>&#39;^]&#39;</span>.
hello
OK
hellomy
OK
</pre></div></p>
<p><hh>需求： 监控一个文件实时采集新增的数据输出到控制台<hh></p>
<p>Agent选型： exec source + memory channel + logger sink</p>
<p> <div class=codehilite><pre># filename: exec-memeory-logger.conf

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /tmp/data.log

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
</pre></div></p>
<blockquote>
<p><CB>exec source</CB> runs a given Unix command on start-up and expects that process to continuously produce data on standard out (stderr is simply discarded, unless property logStdErr is set to true). If the process exits for any reason, the source also exits and will produce no further data. This means configurations such as cat [named pipe] or tail -F [file] are going to produce the desired results where as date will probably not - the former two commands produce streams of data where as the latter produces a single event and exits. [<a href="http://flume.apache.org/FlumeUserGuide.html#exec-source">exec source</a>]</p>
</blockquote>
<p>将内容输入到<code class="codehilite">/tmp/data.log</code>文件中：</p>
<p> <div class=codehilite><pre>$ <span class=nb>echo</span> <span class=s2>&quot;hello&quot;</span> &gt; data.log
$ <span class=nb>echo</span> <span class=s2>&quot;hello&quot;</span> &gt; data.log
</pre></div></p>
<p><hh>需求： 将A服务器上的日志实时采集到B服务器<hh></p>
<p>日志收集过程：</p>
<ul>
<li>机器1上监控一个文件，当我们访问主站时会有用户行为日志记录到<code class="codehilite">access.log</code>中。</li>
<li>avro sink把新产生的日志输出到对应的avro source指定的hostname和port上。</li>
<li>通过avro对应的agent将我们的日志输出到控制台。</li>
</ul>
<p><img alt="" src="../figures/UsingAvroSink.png" /></p>
<blockquote>
<p><CB>avro sink</CB>: forms one half of Flume’s tiered collection support. Flume events sent to this sink are turned into Avro events and sent to the configured hostname / port pair. [<a href="http://flume.apache.org/FlumeUserGuide.html#avro-sink">Avro sink</a>]</p>
</blockquote>
<div class=md-fenced-code-tabs id=tab-tab-group-4><input name=tab-group-4 type=radio id=tab-group-4-0_text checked=checked class=code-tab data-lang=text aria-controls=tab-group-4-0_text-panel role=tab><label for=tab-group-4-0_text class=code-tab-label data-lang=text id=tab-group-4-0_text-label>Exec-Memeory-Avro.conf</label><div class=code-tabpanel role=tabpanel data-lang=text id=tab-group-4-0_text-panel aria-labelledby=tab-group-4-0_text-label><div class=codehilite><pre># filename: exec-memeory-avro.conf

# Name the components on this agent
a1.sources = exec-source
a1.sinks = avro-sink
a1.channels = memory-channel

# Describe/configure the source
a1.sources.exec-source.type = exec
a1.sources.exec-source.command = tail -F /tmp/data.log

# Describe the sink
a1.sinks.avro-sink.type = avro
a1.sinks.avro-sink.hostname = localhost
a1.sinks.avro-sink.port = 44444

# Use a channel which buffers events in memory
a1.channels.memory-channel.type = memory
a1.channels.memory-channel.capacity = 1000
a1.channels.memory-channel.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.exec-source.channels = memory-channel
a1.sinks.avro-sink.channel = memory-channel
</pre></div></div><input name=tab-group-4 type=radio id=tab-group-4-1_text class=code-tab data-lang=text aria-controls=tab-group-4-1_text-panel role=tab><label for=tab-group-4-1_text class=code-tab-label data-lang=text id=tab-group-4-1_text-label>Avro-Memeory-Logger.conf</label><div class=code-tabpanel role=tabpanel data-lang=text id=tab-group-4-1_text-panel aria-labelledby=tab-group-4-1_text-label><div class=codehilite><pre># filename: avro-memeory-logger.conf

# Name the components on this agent
a2.sources = avro-source
a2.sinks = logger-sink
a2.channels = memory-channel

# Describe/configure the source
a2.sources.avro-source.type = avro
a2.sources.avro-source.bind = localhost
a2.sources.avro-source.port = 44444

# Describe the sink
a2.sinks.logger-sink.type = logger

# Use a channel which buffers events in memory
a2.channels.memory-channel.type = memory
a2.channels.memory-channel.capacity = 1000
a2.channels.memory-channel.transactionCapacity = 100

# Bind the source and sink to the channel
a2.sources.avro-source.channels = memory-channel
a2.sinks.logger-sink.channel = memory-channel
</pre></div></div></div>

<p>启动flume， 注意两个agent的启动顺序</p>
<p> <div class=codehilite><pre>$ flume-ng agent <span class=se>\</span>
--name a2 <span class=se>\</span>
--conf <span class=nv>$F</span>LUME-HOME/conf <span class=se>\</span>
--conf-file avro-memory-logger.conf <span class=se>\</span>
-Dflume.root.logger<span class=o>=</span>INFO,console

$ flume-ng agent <span class=se>\</span>
--name a1 <span class=se>\</span>
--conf <span class=nv>$F</span>LUME-HOME/conf <span class=se>\</span>
--conf-file exec-memory-avro.conf <span class=se>\</span>
-Dflume.root.logger<span class=o>=</span>INFO,console
</pre></div></p>
<p>将内容输入到<code class="codehilite">/tmp/data.log</code>文件中：</p>
<p> <div class=codehilite><pre>$ <span class=nb>echo</span> <span class=s2>&quot;welcome&quot;</span> &gt; data.log
$ <span class=nb>echo</span> <span class=s2>&quot;welcome&quot;</span> &gt; data.log
</pre></div></p>
<h3 id="3-kafka">3 分布式消息队列Kafka<a class="headerlink" href="#3-kafka" title="Permanent link">&para;</a></h3>
<p>First a few concepts:</p>
<ul>
<li>Kafka is run as a cluster on one or more servers that can span multiple datacenters.</li>
<li>The Kafka cluster stores streams of <em>records</em> in categories called <strong><em>topic</em></strong>s.</li>
<li>Each record consists of a key, a value, and a timestamp.</li>
<li><strong><em>Broker</em></strong>s are the Kafka processes that manage topics and partitions and serve producer and consumer request.</li>
</ul>
<p><img alt="" src="../figures/kafkademo.jpg" /></p>
<h4 id="kafka">Kafka部署及使用<a class="headerlink" href="#kafka" title="Permanent link">&para;</a></h4>
<p><hh>单节点单Broker部署及使用</hh></p>
<p> <div class=codehilite><pre><span class=c1># 启动Zookeep</span>
$ zkServer.sh start
<span class=c1># 启动kafka</span>
$ kafka-server-start.sh $KAFKA_HOME/config/server.properties
<span class=c1># 创建名为test的topic(single partition and only one replica)</span>
$ kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor <span class=m>1</span> --partitions <span class=m>1</span> --topic <span class=nb>test</span>
<span class=c1># 查看topic</span>
$ kafka-topics.sh --list --zookeeper localhost:2181
<span class=c1>### 启动生产者, 9092是server监听端口</span>
$ kafka-console-producer.sh --broker-list localhost:9092 --topic <span class=nb>test</span>
&gt; This is a message
&gt; This is another message
<span class=c1>### 启动消费者 --from-beginning从头开始接收消息</span>
$ kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <span class=nb>test</span> --from-beginning
This is a message
This is another message
<span class=c1>### 查看所有topics的详细信息</span>
$ kafka-topics.sh --describe --zookeeper localhost:2181
<span class=c1>### 查看指定topic的详细信息</span>
$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic <span class=nb>test</span>
</pre></div></p>
<p><hh>单节点多Broker部署及使用</hh></p>
<p> <div class=codehilite><pre>cp $KAFKA_HOME/config/server.properties $KAFKA_HOME/config/server-1.properties
cp $KAFKA_HOME/config/server.properties $KAFKA_HOME/config/server-2.properties
</pre></div></p>
<p>修改配置文件如下</p>
<p> <div class=codehilite><pre>config/server-1.properties:
    broker.id=1
    listeners=PLAINTEXT://:9093
    log.dirs=/tmp/kafka-logs-1

config/server-2.properties:
    broker.id=2
    listeners=PLAINTEXT://:9094
    log.dirs=/tmp/kafka-logs-2
</pre></div></p>
<p>启动kafka</p>
<p> <div class=codehilite><pre><span class=c1># 启动ZooKeep</span>
$ zkServer.sh start
<span class=c1># 启动kafka server</span>
$ kafka-server-start.sh $KAFKA_HOME/config/server.properties <span class=p>&amp;</span>
$ kafka-server-start.sh $KAFKA_HOME/config/server-1.properties <span class=p>&amp;</span>
$ kafka-server-start.sh $KAFKA_HOME/config/server-2.properties <span class=p>&amp;</span>
<span class=c1># 创建topic, 1个分区，三个副本</span>
$ kafka-topics.sh --create --zookeeper localhost:2181 <span class=se>\</span>
    --replication-factor <span class=m>3</span> --partitions <span class=m>1</span> --topic my-replicated-topic
<span class=c1># 查看topic信息</span>
$ kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
Topic:my-replicated-topic   PartitionCount:1    ReplicationFactor:3 Configs:
    Topic: my-replicated-topic  Partition: <span class=m>0</span>    Leader: <span class=m>2</span>   Replicas: 2,0,1 Isr: 2,0,1
<span class=c1># 启动生产者</span>
$ kafka-console-producer.sh --broker-list localhost:9092, localhost:9093, localhost:9094 --topic my-replicated-topic
<span class=c1># 启动消费者</span>
$ kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic
</pre></div></p>
<h4 id="kafka-java">Kafka Java 编程<a class="headerlink" href="#kafka-java" title="Permanent link">&para;</a></h4>
<p>使用命令行总是不方便的，下面我们尝试着使用Kafka Java API编程，实际内容和上一节是一摸一样的，所以直接附上代码了。注意这里使用的API是0.8.2版本以后的，之前版本与之后版本的API相差非常大。</p>
<div class=md-fenced-code-tabs id=tab-tab-group-11><input name=tab-group-11 type=radio id=tab-group-11-0_java checked=checked class=code-tab data-lang=java aria-controls=tab-group-11-0_java-panel role=tab><label for=tab-group-11-0_java class=code-tab-label data-lang=java id=tab-group-11-0_java-label>Producer</label><div class=code-tabpanel role=tabpanel data-lang=java id=tab-group-11-0_java-panel aria-labelledby=tab-group-11-0_java-label><div class=codehilite><pre><span class=kn>import</span> <span class=nn>org.apache.kafka.clients.producer.KafkaProducer</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.kafka.clients.producer.ProducerRecord</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>java.util.Properties</span><span class=o>;</span>
<span class=cm>/**</span>
<span class=cm> * Kafka生产者</span>
<span class=cm> * 见官方文档</span>
<span class=cm> * http://kafka.apache.org/20/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html</span>
<span class=cm> */</span>
<span class=kd>public</span> <span class=kd>class</span> <span class=nc>MyKafkaProducer</span> <span class=kd>implements</span> <span class=n>Runnable</span> <span class=o>{</span>
    <span class=kd>private</span> <span class=n>String</span> <span class=n>topic</span><span class=o>;</span>
    <span class=kd>private</span> <span class=n>KafkaProducer</span><span class=o>&lt;</span><span class=n>String</span><span class=o>,</span> <span class=n>String</span><span class=o>&gt;</span> <span class=n>producer</span><span class=o>;</span>

    <span class=kd>public</span> <span class=nf>MyKafkaProducer</span><span class=o>(</span><span class=n>String</span> <span class=n>topic</span><span class=o>)</span> <span class=o>{</span>
        <span class=k>this</span><span class=o>.</span><span class=na>topic</span> <span class=o>=</span> <span class=n>topic</span><span class=o>;</span>
        <span class=n>Properties</span> <span class=n>props</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Properties</span><span class=o>();</span>
        <span class=n>props</span><span class=o>.</span><span class=na>put</span><span class=o>(</span><span class=s>&quot;bootstrap.servers&quot;</span><span class=o>,</span> <span class=s>&quot;localhost:9092&quot;</span><span class=o>);</span>
        <span class=n>props</span><span class=o>.</span><span class=na>put</span><span class=o>(</span><span class=s>&quot;acks&quot;</span><span class=o>,</span> <span class=s>&quot;all&quot;</span><span class=o>);</span>
        <span class=n>props</span><span class=o>.</span><span class=na>put</span><span class=o>(</span><span class=s>&quot;key.serializer&quot;</span><span class=o>,</span> <span class=s>&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span><span class=o>);</span>
        <span class=n>props</span><span class=o>.</span><span class=na>put</span><span class=o>(</span><span class=s>&quot;value.serializer&quot;</span><span class=o>,</span> <span class=s>&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span><span class=o>);</span>
        <span class=n>producer</span> <span class=o>=</span> <span class=k>new</span> <span class=n>KafkaProducer</span><span class=o>&lt;</span><span class=n>String</span><span class=o>,</span> <span class=n>String</span><span class=o>&gt;(</span><span class=n>props</span><span class=o>);</span>
    <span class=o>}</span>

    <span class=kd>public</span> <span class=kt>void</span> <span class=nf>run</span><span class=o>()</span> <span class=o>{</span>
        <span class=kt>int</span> <span class=n>messageNumber</span> <span class=o>=</span> <span class=mi>1</span><span class=o>;</span>
        <span class=k>while</span> <span class=o>(</span><span class=kc>true</span><span class=o>)</span> <span class=o>{</span>
            <span class=n>String</span> <span class=n>message</span> <span class=o>=</span> <span class=s>&quot;message&quot;</span> <span class=o>+</span> <span class=n>messageNumber</span><span class=o>;</span>
            <span class=n>producer</span><span class=o>.</span><span class=na>send</span><span class=o>(</span><span class=k>new</span> <span class=n>ProducerRecord</span><span class=o>&lt;</span><span class=n>String</span><span class=o>,</span> <span class=n>String</span><span class=o>&gt;(</span><span class=n>topic</span><span class=o>,</span> <span class=n>message</span><span class=o>));</span>
            <span class=n>messageNumber</span><span class=o>++;</span>
            <span class=k>try</span><span class=o>{</span>
                <span class=n>Thread</span><span class=o>.</span><span class=na>sleep</span><span class=o>(</span><span class=mi>5000</span><span class=o>);</span>
            <span class=o>}</span> <span class=k>catch</span> <span class=o>(</span><span class=n>Exception</span> <span class=n>ex</span><span class=o>)</span> <span class=o>{</span>
                <span class=n>ex</span><span class=o>.</span><span class=na>printStackTrace</span><span class=o>();</span>
            <span class=o>}</span>
        <span class=o>}</span>
    <span class=o>}</span>
<span class=o>}</span>
</pre></div></div><input name=tab-group-11 type=radio id=tab-group-11-1_java class=code-tab data-lang=java aria-controls=tab-group-11-1_java-panel role=tab><label for=tab-group-11-1_java class=code-tab-label data-lang=java id=tab-group-11-1_java-label>Consumer</label><div class=code-tabpanel role=tabpanel data-lang=java id=tab-group-11-1_java-panel aria-labelledby=tab-group-11-1_java-label><div class=codehilite><pre><span class=kn>import</span> <span class=nn>org.apache.kafka.clients.consumer.ConsumerRecord</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.kafka.clients.consumer.ConsumerRecords</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.kafka.clients.consumer.KafkaConsumer</span><span class=o>;</span>

<span class=kn>import</span> <span class=nn>java.time.Duration</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>java.util.Arrays</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>java.util.List</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>java.util.Properties</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>java.util.concurrent.atomic.AtomicBoolean</span><span class=o>;</span>

<span class=cm>/**</span>
<span class=cm> * Kafka消费者</span>
<span class=cm> * 官方文档</span>
<span class=cm> * http://kafka.apache.org/20/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html</span>
<span class=cm> */</span>
<span class=kd>public</span> <span class=kd>class</span> <span class=nc>MyKafkaConsumer</span> <span class=kd>implements</span> <span class=n>Runnable</span> <span class=o>{</span>
    <span class=kd>private</span> <span class=kd>final</span> <span class=n>AtomicBoolean</span> <span class=n>closed</span> <span class=o>=</span> <span class=k>new</span> <span class=n>AtomicBoolean</span><span class=o>(</span><span class=kc>false</span><span class=o>);</span>
    <span class=kd>private</span> <span class=n>String</span> <span class=n>topic</span><span class=o>;</span>
    <span class=kd>private</span> <span class=n>KafkaConsumer</span><span class=o>&lt;</span><span class=n>String</span><span class=o>,</span> <span class=n>String</span><span class=o>&gt;</span> <span class=n>consumer</span><span class=o>;</span>
    <span class=kd>private</span> <span class=n>ConsumerRecords</span><span class=o>&lt;</span><span class=n>String</span><span class=o>,</span> <span class=n>String</span><span class=o>&gt;</span> <span class=n>records</span><span class=o>;</span>

    <span class=kd>public</span> <span class=nf>MyKafkaConsumer</span><span class=o>(</span><span class=n>String</span> <span class=n>topic</span><span class=o>)</span> <span class=o>{</span>
        <span class=k>this</span><span class=o>.</span><span class=na>topic</span> <span class=o>=</span> <span class=n>topic</span><span class=o>;</span>
        <span class=n>Properties</span> <span class=n>props</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Properties</span><span class=o>();</span>
        <span class=c1>// connect to cluster</span>
        <span class=n>props</span><span class=o>.</span><span class=na>put</span><span class=o>(</span><span class=s>&quot;bootstrap.servers&quot;</span><span class=o>,</span> <span class=s>&quot;localhost:9092&quot;</span><span class=o>);</span>
        <span class=c1>//  subscribing to the topics- test</span>
        <span class=n>props</span><span class=o>.</span><span class=na>put</span><span class=o>(</span><span class=s>&quot;group.id&quot;</span><span class=o>,</span> <span class=s>&quot;test&quot;</span><span class=o>);</span>
        <span class=c1>//  offsets are committed automatically</span>
        <span class=n>props</span><span class=o>.</span><span class=na>put</span><span class=o>(</span><span class=s>&quot;enable.auto.commit&quot;</span><span class=o>,</span> <span class=s>&quot;true&quot;</span><span class=o>);</span>
        <span class=c1>// specify how to turn bytes into objects</span>
        <span class=n>props</span><span class=o>.</span><span class=na>put</span><span class=o>(</span><span class=s>&quot;key.deserializer&quot;</span><span class=o>,</span> <span class=s>&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span><span class=o>);</span>
        <span class=n>props</span><span class=o>.</span><span class=na>put</span><span class=o>(</span><span class=s>&quot;value.deserializer&quot;</span><span class=o>,</span> <span class=s>&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span><span class=o>);</span>
        <span class=n>consumer</span> <span class=o>=</span> <span class=k>new</span> <span class=n>KafkaConsumer</span><span class=o>&lt;&gt;(</span><span class=n>props</span><span class=o>);</span>

    <span class=o>}</span>

    <span class=kd>public</span> <span class=kt>void</span> <span class=nf>run</span><span class=o>()</span> <span class=o>{</span>
        <span class=k>try</span> <span class=o>{</span>
            <span class=c1>// subsribes to topic</span>
            <span class=n>consumer</span><span class=o>.</span><span class=na>subscribe</span><span class=o>(</span><span class=n>Arrays</span><span class=o>.</span><span class=na>asList</span><span class=o>(</span><span class=n>topic</span><span class=o>));</span>
            <span class=k>while</span> <span class=o>(!</span><span class=n>closed</span><span class=o>.</span><span class=na>get</span><span class=o>())</span> <span class=o>{</span>
                <span class=n>records</span> <span class=o>=</span> <span class=n>consumer</span><span class=o>.</span><span class=na>poll</span><span class=o>(</span><span class=n>Duration</span><span class=o>.</span><span class=na>ofMillis</span><span class=o>(</span><span class=mi>10000</span><span class=o>));</span>
                <span class=k>for</span> <span class=o>(</span><span class=n>ConsumerRecord</span><span class=o>&lt;</span><span class=n>String</span><span class=o>,</span> <span class=n>String</span><span class=o>&gt;</span> <span class=n>record</span> <span class=o>:</span> <span class=n>records</span><span class=o>)</span>
                    <span class=n>System</span><span class=o>.</span><span class=na>out</span><span class=o>.</span><span class=na>printf</span><span class=o>(</span><span class=s>&quot;offset = %d, key = %s, value = %s%n&quot;</span><span class=o>,</span> <span class=n>record</span><span class=o>.</span><span class=na>offset</span><span class=o>(),</span> <span class=n>record</span><span class=o>.</span><span class=na>key</span><span class=o>(),</span> <span class=n>record</span><span class=o>.</span><span class=na>value</span><span class=o>());</span>
            <span class=o>}</span>
        <span class=o>}</span> <span class=k>catch</span> <span class=o>(</span><span class=n>Exception</span> <span class=n>e</span><span class=o>)</span> <span class=o>{</span>
            <span class=c1>// Ignore exception if closing</span>
            <span class=k>if</span> <span class=o>(!</span><span class=n>closed</span><span class=o>.</span><span class=na>get</span><span class=o>())</span> <span class=k>throw</span> <span class=n>e</span><span class=o>;</span>
        <span class=o>}</span> <span class=k>finally</span> <span class=o>{</span>
            <span class=n>consumer</span><span class=o>.</span><span class=na>close</span><span class=o>();</span>
        <span class=o>}</span>

    <span class=o>}</span>

    <span class=c1>// Shutdown hook which can be called from a separate thread</span>
    <span class=kd>public</span> <span class=kt>void</span> <span class=nf>shutdown</span><span class=o>()</span> <span class=o>{</span>
        <span class=n>closed</span><span class=o>.</span><span class=na>set</span><span class=o>(</span><span class=kc>true</span><span class=o>);</span>
        <span class=n>consumer</span><span class=o>.</span><span class=na>wakeup</span><span class=o>();</span>
    <span class=o>}</span>

<span class=o>}</span>
</pre></div></div><input name=tab-group-11 type=radio id=tab-group-11-2_java class=code-tab data-lang=java aria-controls=tab-group-11-2_java-panel role=tab><label for=tab-group-11-2_java class=code-tab-label data-lang=java id=tab-group-11-2_java-label>Clientapp</label><div class=code-tabpanel role=tabpanel data-lang=java id=tab-group-11-2_java-panel aria-labelledby=tab-group-11-2_java-label><div class=codehilite><pre><span class=kd>public</span> <span class=kd>class</span> <span class=nc>ClientApp</span> <span class=o>{</span>
    <span class=kd>public</span> <span class=kd>static</span> <span class=kt>void</span> <span class=nf>main</span><span class=o>(</span><span class=n>String</span><span class=o>[]</span> <span class=n>args</span><span class=o>)</span> <span class=o>{</span>
        <span class=n>Thread</span> <span class=n>job</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Thread</span><span class=o>(</span><span class=k>new</span> <span class=n>MyKafkaProducer</span><span class=o>(</span><span class=s>&quot;test&quot;</span><span class=o>));</span>
        <span class=n>job</span><span class=o>.</span><span class=na>start</span><span class=o>();</span>
        <span class=n>Thread</span> <span class=n>job2</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Thread</span><span class=o>(</span><span class=k>new</span> <span class=n>MyKafkaConsumer</span><span class=o>(</span><span class=s>&quot;test&quot;</span><span class=o>));</span>
        <span class=n>job2</span><span class=o>.</span><span class=na>start</span><span class=o>();</span>
    <span class=o>}</span>
<span class=o>}</span>
</pre></div></div></div>

<h4 id="flumekafka">整合Flume和Kafka完成实时数据采集<a class="headerlink" href="#flumekafka" title="Permanent link">&para;</a></h4>
<p>为了将Flume的输出到Kafka，可以将agent2的logger sink替换成Kafka Sink。然后启动一个Kafka consumer从Kafka sink订阅消息。</p>
<p><img alt="" src="../figures/FlumeKafkaCombined.png" /></p>
<blockquote>
<p><CB>kafka sink</CB> can publish data to a Kafka topic. One of the objective is to integrate Flume with Kafka so that pull based processing systems can process the data coming through various Flume sources. [<a href="http://flume.apache.org/FlumeUserGuide.html#kafka-sink">Kafka Sink</a>]</p>
</blockquote>
<p>下面是agent2对应的Kafka配置文件，在这里agent2改名为<code class="codehilite">avro-memory-kafka</code>。</p>
<p> <div class=codehilite><pre># filename: avro-memeory-kafka.conf

# Name the components on this agent
avro-memory-kafka.sources = avro-source
avro-memory-kafka.sinks = kafka-sink
avro-memory-kafka.channels = memory-channel

# Describe/configure the source
avro-memory-kafka.sources.avro-source.type = avro
avro-memory-kafka.sources.avro-source.bind = localhost
avro-memory-kafka.sources.avro-source.port = 44444

# Describe the sink
avro-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink
avro-memory-kafka.sinks.kafka-sink.kafka.bootstrap.servers = localhost:9092
avro-memory-kafka.sinks.kafka-sink.kafka.topic = test


# Use a channel which buffers events in memory
avro-memory-kafka.channels.memory-channel.type = memory
avro-memory-kafka.channels.memory-channel.capacity = 1000
avro-memory-kafka.channels.memory-channel.transactionCapacity = 100

# Bind the source and sink to the channel
avro-memory-kafka.sources.avro-source.channels = memory-channel
avro-memory-kafka.sinks.kafka-sink.channel = memory-channel
</pre></div></p>
<p>下面是具体的操作流程，同样需要注意两个agent的启动顺序：</p>
<p> <div class=codehilite><pre><span class=c1>## 启动zookeeper, kafka，省略</span>
<span class=c1>## 启动agent</span>
$ flume-ng agent <span class=se>\</span>
--name avro-memory-kafka <span class=se>\</span>
--conf <span class=nv>$F</span>LUME-HOME/conf <span class=se>\</span>
--conf-file avro-memory-kafka.conf <span class=se>\</span>
-Dflume.root.logger<span class=o>=</span>INFO,console

$ flume-ng agent <span class=se>\</span>
--name a1 <span class=se>\</span>
--conf <span class=nv>$F</span>LUME-HOME/conf <span class=se>\</span>
--conf-file exec-memory-avro.conf <span class=se>\</span>
-Dflume.root.logger<span class=o>=</span>INFO,console

<span class=c1>## 启动消费者 </span>
$ kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <span class=nb>test</span>
</pre></div></p></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>var base_url = '../..';</script>
        <script src="../../js/base.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <script src="../../extra_javascript/tabhack.js"></script>
        <script src="../../search/require.js"></script>
        <script src="../../search/search.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td><kbd>&larr;</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td><kbd>&rarr;</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>


    </body>
</html>
