<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  techlarry
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        <li id=""><a target="_self" href="category.html">Category</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">Home</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        
        <li><a target="_self" href="category.html">Category</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="Leetcode.html">Leetcode</a></li>
        
            <li><a href="Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html">Python数据结构与算法</a></li>
        
            <li><a href="Python%E7%89%B9%E6%80%A7.html">Python特性</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html">Python科学计算三维可视化</a></li>
        
            <li><a href="English.html">English</a></li>
        
            <li><a href="%20%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA.html"> 理解计算机</a></li>
        
            <li><a href="Deep%20Learning.html">Deep Learning</a></li>
        
            <li><a href="Latex.html">Latex</a></li>
        
            <li><a href="%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html">操作系统</a></li>
        
            <li><a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html">Linux 系统编程</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html">数据库</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="%E5%B7%A5%E4%BD%9C%E4%B8%8E%E5%AD%A6%E4%B9%A0.html">工作与学习</a></li>
        
            <li><a href="Data%20Science.html">Data Science</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="15050120680855.html">
                
                  <h1>Image Captioning</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Vanilla RNN</a>
</li>
<li>
<a href="#toc_1">LSTM</a>
</li>
<li>
<a href="#toc_2">COCO</a>
</li>
<li>
<a href="#toc_3">Reference</a>
</li>
</ul>


<h2 id="toc_0">Vanilla RNN</h2>

<p><img src="media/15050120680855/Screen%20Shot%202017-09-10%20at%206.52.08%20PM.png" alt="Vanilla RNN gradient flow"/></p>

<h2 id="toc_1">LSTM</h2>

<p>Long Short Term Memory (LSTM)</p>

<p>Backward flow of gradients in RNN can explode or vanish.</p>

<p>Exploding is controlled with gradient clipping. Vanishing is controlled with additive interactions (LSTM)</p>

<p><img src="media/15050120680855/Screen%20Shot%202017-09-10%20at%206.52.27%20PM.png" alt="LSTM"/></p>

<h2 id="toc_2">COCO</h2>

<p><code>COCO</code>（Common Objects in Context） is a large-scale <code>object detection</code>(物体检测), <code>semantic segmentation</code>(语义分割), and captioning dataset. <code>COCO</code> has several main features: <code>Object segmentation</code>, <code>Recognition in context</code>.</p>

<h2 id="toc_3">Reference</h2>

<ul>
<li>Tsung Y L, Michael M, Serge B, et al. Microsoft COCO: Common Objects in Context.  arXiv:<a href="https://arxiv.org/abs/1405.0312">1405.0312</a></li>
<li>Karpathy et al. Deep Visual-Semantic Alignments for Generating Image Descriptions, CVPR 2015</li>
</ul>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/10</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Deep%20Learning.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15049402475273.html">
                
                  <h1>Machine Learning (10): Large Scale Machine Learning</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Learning with Large Datasets</a>
</li>
<li>
<a href="#toc_1">Stochastic Gradient Descent</a>
<ul>
<li>
<a href="#toc_2">Mini-Batch Gradient Descent</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">Stochastic Gradient Descent Convergence</a>
</li>
<li>
<a href="#toc_4">Online Learning</a>
</li>
<li>
<a href="#toc_5">Map Reduce and Data Parallelism</a>
</li>
</ul>


<h2 id="toc_0">Learning with Large Datasets</h2>

<p>We mainly benefit from a very large dataset when our algorithm has high variance when m is small. Recall that if our algorithm has high bias, more data will not have any benefit.</p>

<p>Datasets can often approach such sizes as m = 100,000,000. In this case, our gradient descent step will have to make a summation over all one hundred million examples. We will want to try to avoid this -- the approaches for doing so are described below.</p>

<h2 id="toc_1">Stochastic Gradient Descent</h2>

<p>Stochastic gradient descent is an alternative to classic (or batch) gradient descent and is more efficient and scalable to large data sets.</p>

<p>Stochastic gradient descent is written out in a different but similar way:</p>

<p>\(cost(\theta,(x^{(i)}, y^{(i)})) = \dfrac{1}{2}(h_{\theta}(x^{(i)}) - y^{(i)})^2\)</p>

<p>The only difference in the above cost function is the elimination of the m constant within \(\dfrac{1}{2}\).</p>

<p>\(J_{train}(\theta) = \dfrac{1}{m} \displaystyle \sum_{i=1}^m cost(\theta, (x^{(i)}, y^{(i)}))\)</p>

<p>\(J_{train}\) is now just the average of the cost applied to all of our training examples.</p>

<p>The algorithm is as follows</p>

<ol>
<li><p>Randomly &#39;shuffle&#39; the dataset</p></li>
<li><p>For \(i = 1\dots m\)</p></li>
</ol>

<p>\(\Theta_j := \Theta_j - \alpha (h_{\Theta}(x^{(i)}) - y^{(i)}) \cdot x^{(i)}_j\)</p>

<p>This algorithm will only try to fit one training example at a time. This way we can make progress in gradient descent without having to scan all m training examples first. Stochastic gradient descent will be unlikely to converge at the global minimum and will instead wander around it randomly, but usually yields a result that is close enough. Stochastic gradient descent will usually take 1-10 passes through your data set to get near the global minimum.</p>

<h3 id="toc_2">Mini-Batch Gradient Descent</h3>

<p>Mini-batch gradient descent can sometimes be even faster than stochastic gradient descent. Instead of using all m examples as in batch gradient descent, and instead of using only 1 example as in stochastic gradient descent, we will use some in-between number of examples b.</p>

<p>Typical values for b range from 2-100 or so.</p>

<p>For example, with b=10 and m=1000:</p>

<p>Repeat:</p>

<p>For \(i = 1,11,21,31,\dots,991\)</p>

<p>\(\theta_j := \theta_j - \alpha \dfrac{1}{10} \displaystyle \sum_{k=i}^{i+9} (h_\theta(x^{(k)}) - y^{(k)})x_j^{(k)}\)</p>

<p>We&#39;re simply summing over ten examples at a time. The advantage of computing more than one example at a time is that we can use vectorized implementations over the b examples.</p>

<h2 id="toc_3">Stochastic Gradient Descent Convergence</h2>

<p>How do we choose the learning rate α for stochastic gradient descent? Also, how do we debug stochastic gradient descent to make sure it is getting as close as possible to the global optimum?</p>

<p>One strategy is to plot the average cost of the hypothesis applied to every 1000 or so training examples. We can compute and save these costs during the gradient descent iterations.</p>

<p>With a smaller learning rate, it is <strong>possible</strong> that you may get a slightly better solution with stochastic gradient descent. That is because stochastic gradient descent will oscillate and jump around the global minimum, and it will make smaller random jumps with a smaller learning rate.</p>

<p>If you increase the number of examples you average over to plot the performance of your algorithm, the plot&#39;s line will become smoother.</p>

<p>With a very small number of examples for the average, the line will be too noisy and it will be difficult to find the trend.</p>

<p>One strategy for trying to actually converge at the global minimum is to <strong>slowly decrease α over time</strong> . For example \(\alpha = \dfrac{\text{const1}}{\text{iterationNumber + const2}}\)</p>

<p>However, this is not often done because people don&#39;t want to have to fiddle with even more parameters.</p>

<h2 id="toc_4">Online Learning</h2>

<p>With a continuous stream of users to a website, we can run an endless loop that gets (x,y), where we collect some user actions for the features in x to predict some behavior y.</p>

<p>You can update θ for each individual (x,y) pair as you collect them. This way, you can adapt to new pools of users, since you are continuously updating theta.</p>

<h2 id="toc_5">Map Reduce and Data Parallelism</h2>

<p>We can divide up batch gradient descent and dispatch the cost function for a subset of the data to many different machines so that we can train our algorithm in parallel.</p>

<p>You can split your training set into z subsets corresponding to the number of machines you have. On each of those machines calculate \(\displaystyle \sum_{i=p}^{q}(h_{\theta}(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}\), where we&#39;ve split the data starting at p and ending at q.</p>

<p>MapReduce will take all these dispatched (or &#39;mapped&#39;) jobs and &#39;reduce&#39; them by calculating:</p>

<p>\(\Theta_j := \Theta_j - \alpha \dfrac{1}{z}(temp_j^{(1)} + temp_j^{(2)} + \cdots + temp_j^{(z)})\)</p>

<p>For all \(j = 0, \dots, n\).</p>

<p>This is simply taking the computed cost from all the machines, calculating their average, multiplying by the learning rate, and updating theta.</p>

<p>Your learning algorithm is MapReduceable if it can be <u>expressed as computing sums of functions over the training set</u> . Linear regression and logistic regression are easily parallelizable.</p>

<p>For neural networks, you can compute forward propagation and back propagation on subsets of your data on many machines. Those machines can report their derivatives back to a &#39;master&#39; server that will combine them.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/9</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="batch_normalization.html">
                
                  <h1>Batch Normalization</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>The idea is relatively straightforward. Machine learning methods tend to work better when their input data consists of uncorrelated features with <strong>zero mean</strong> and <strong>unit variance</strong>. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features; this will ensure that the first layer of the network sees data that follows a nice distribution. However even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.</p>

<h2 id="toc_0">Covariate Shift</h2>

<p>While stochastic gradient is simple and effective, it requires careful tuning of the model hyper-parameters, specifically the learning rate used in optimization, as well as the initial values for the model parameters. The train- ing is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers – so that small changes to the network parameters amplify as the network becomes deeper.</p>

<p>The change in the distribution of layers&#39; inputs presents a problem because the layers need to continuously adapt to the new distribution. When the input distribution to a learning system changes, it is said to experience <code>covariate shift</code>. This is typically handled via domain adaption.</p>

<p><code>Internal Covariate Shift</code> refers to the change in the distribution of internal nodes of a deep network due to change in network parameters, in the course of training. <code>Batch Normalization</code>, that takes a step towards reducing internal covariance shift, and in doing so dramatically accelerates the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. It also has a beneficial effect on the dependence of gradients on the scale of the parameters or of their initial values. This allows us to use much higher learning rates without the risk of divergence. </p>

<h2 id="toc_1">Algorithm</h2>

<h3 id="toc_2">Forward</h3>

<p>Given some input values \(\mathcal{B} = x^{(1)},...,x^{(m)}\) over a mini-batch in the layer \(l\) of neural network; </p>

<p>\[\mu_{\mathcal{B}} = \frac{1}{m}\sum_i^m x_i  \text{     (mini-batch mean)}\\<br/>
\sigma^2=\frac{1}{m}\sum_i(x_i-\mu_{\mathcal{B}})^2  \text{     (mini-batch variance)}\\<br/>
\hat{x}_i=\frac{x_i-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{\mathcal{B}}}^2+\varepsilon}} \text{     (normalize)}\\<br/>
y_i=\gamma \hat{x}_i+\beta \text{     (scale and shift)}<br/>
\]</p>

<p>At each iteration, we update the running averages for mean and variance using an exponential decay based on the momentum parameter:<br/>
<code><br/>
running_mean = momentum * running_mean + (1 - momentum) * sample_mean<br/>
running_var = momentum * running_var + (1 - momentum) * sample_var<br/>
</code></p>

<h2 id="toc_3">Backward</h2>

<p><img src="media/15048405877529/Screen%20Shot%202017-09-08%20at%203.11.45%20PM.png" alt="batch-normalization-backward"/></p>

<h3 id="toc_4">Test time</h3>

<p>Using trained <code>runing_mean</code> and <code>running_var</code> to take forward step.</p>

<h2 id="toc_5">TensorFlow</h2>

<p>Applying <code>Batch Normalization</code> in TensorFlow Model is very convenient. Just add one line of code in TensorFlow: <code>tf.nn.batch-normalization</code>.</p>

<h2 id="toc_6">Reference</h2>

<p>Sergey Ioffe, Christian Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv:<a href="https://arxiv.org/abs/1502.03167v3">1501.02167v3</a></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/8</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Deep%20Learning.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15046658203135.html">
                
                  <h1>TensorFlow Q</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h3 id="toc_0">Q1: what&#39;s the difference between <code>tf.placeholder</code> and <code>tf.Variable</code></h3>

<p>In general, we use <code>tf.placeholder</code> to feed actual training examples, while using <code>tf.Variable</code> for parameters such as weights (\(W\)) and biases (\(b\)) for models.</p>

<p>With <code>tf.Variable</code>, we have to provide an initial value when declaring it. And we don&#39;t have to provide an initial value until running time with a <code>feed_dict</code>.</p>

<h3 id="toc_1">Q2: what&#39;s the difference between <code>tf.random_normal</code> and <code>tf.trucated_normal</code></h3>

<p><code>tf.trucated_normal</code> generates values following a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.</p>

<pre><code>import tensorflow as tf
import matplotlib.pyplot as plt
%matplotlib inline
Session = tf.InteractiveSession()
</code></pre>

<pre><code>mean = 0.0
std = 1.0
shape = (10000,)
hist_range = (-5, 5)

A = tf.truncated_normal(shape, mean, std)
B = tf.random_normal(shape, mean, std)
a, b = Session.run([A, B])
</code></pre>

<pre><code>f, (ax1, ax2) = plt.subplots(2,1)
f1 = ax1.hist(a, bins= 100, range=hist_range)
f2 = ax2.hist(b, bins= 100, range=hist_range)
</code></pre>

<p><img src="media/15046658203135/output_2_0.png" alt="output_2_0"/></p>

<h3 id="toc_2">Q3: What&#39;s the difference between <code>tf.Variable</code> and <code>tf.get_variable</code></h3>

<p><code>tf.get_variable</code> will make it way easier to refactor code if you need to share variables at any time.</p>

<pre><code class="language-python">import tensorflow as tf

with tf.variable_scope(&quot;scope1&quot;):
    w1 = tf.get_variable(&quot;w1&quot;, shape=[2,3])
    w2 = tf.Variable(1.0, name=&quot;w2&quot;)
with tf.variable_scope(&quot;scope1&quot;, reuse=True):
    w1_p = tf.get_variable(&quot;w1&quot;, shape=[2,3])
    w2_p = tf.Variable(1.0, name=&quot;w2&quot;)

print(w1 is w1_p, w2 is w2_p)

#Output: True  False
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/6</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Tensorflow.html'>Tensorflow</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15046649572570.html">
                
                  <h1>Pandas</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>To convert a pandas <code>dataframe</code> (df) to a numpy <code>ndarray</code>, use this code:</p>

<pre><code>df=df.values
</code></pre>

<p>df now becomes a numpy <code>ndarray</code>.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/6</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Data%20Science.html'>Data Science</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all_3.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_5.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">他山之石，可以攻玉</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Leetcode.html"><strong>Leetcode</strong></a>
        
            <a href="Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html"><strong>Python数据结构与算法</strong></a>
        
            <a href="Python%E7%89%B9%E6%80%A7.html"><strong>Python特性</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>Python科学计算三维可视化</strong></a>
        
            <a href="English.html"><strong>English</strong></a>
        
            <a href="%20%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA.html"><strong> 理解计算机</strong></a>
        
            <a href="Deep%20Learning.html"><strong>Deep Learning</strong></a>
        
            <a href="Latex.html"><strong>Latex</strong></a>
        
            <a href="%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html"><strong>操作系统</strong></a>
        
            <a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html"><strong>Linux 系统编程</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html"><strong>数据库</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="%E5%B7%A5%E4%BD%9C%E4%B8%8E%E5%AD%A6%E4%B9%A0.html"><strong>工作与学习</strong></a>
        
            <a href="Data%20Science.html"><strong>Data Science</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15088177470889.html">Machine Learning Foundations (8): The noise and error</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15088177317059.html">Machine Learning Foundations (6): The VC dimension</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15088176831066.html">Machine Learning Foundations (6): Theory of generalization</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15088159325268.html">Machine Learning Foundations (5): Training versus Testing</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="active_learning_intro.html">主动学习</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
