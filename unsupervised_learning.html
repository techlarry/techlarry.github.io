<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  Machine Learning(8): Unsupervised Learning - techlarry
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">HomePage</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_blank" href="wiki">WIKI</a></li>
        
        <li id=""><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        <li id=""><a target="_blank" href="note">NOTE</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">HomePage</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_blank" href="wiki">WIKI</a></li>
        
        <li><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        
        <li><a target="_blank" href="note">NOTE</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="Leetcode.html">Leetcode</a></li>
        
            <li><a href="programming_language.html">编程语言</a></li>
        
            <li><a href="data_structure_and_algorithm.html">数据结构和算法</a></li>
        
            <li><a href="Python%E7%89%B9%E6%80%A7.html">Python特性</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html">Python科学计算三维可视化</a></li>
        
            <li><a href="English.html">English</a></li>
        
            <li><a href="Computer%20System.html">Computer System</a></li>
        
            <li><a href="Deep%20Learning.html">Deep Learning</a></li>
        
            <li><a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html">Linux 系统编程</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html">数据库</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="Big%20Data.html">Big Data</a></li>
        
            <li><a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html">文献阅读</a></li>
        
            <li><a href="Tools.html">Tools</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
  $(function(){
    $('#menu_item_index').addClass('is_active');
  });
</script>
<div class="row">
  <div class="large-8 medium-8 columns">
      <div class="markdown-body article-wrap">
       <div class="article">
          
          <h1>Machine Learning(8): Unsupervised Learning</h1>
     
        <div class="read-more clearfix">
          <span class="date">2017/9/8</span>

          <span>posted in&nbsp;</span> 
          
              <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
           
         
          <span class="comments">
            

            
          </span>

        </div>
      </div><!-- article -->

      <div class="article-content">
      <ul>
<li>
<a href="#toc_0">K-means Clustering</a>
<ul>
<li>
<a href="#toc_1">Explaination</a>
</li>
<li>
<a href="#toc_2">Choosing the Number of Clusters</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">Principal Component Analysis</a>
<ul>
<li>
<a href="#toc_4">Choosing the Number of Principal Components</a>
</li>
<li>
<a href="#toc_5">Advice for Applying PCA</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">K-means Clustering</h2>

<p>The K-Means Algorithm is the most popular and widely used algorithm for automatically grouping data into coherent subsets.</p>

<ol>
<li><p>Randomly initialize two points in the dataset called the <u>cluster centroids</u> .</p></li>
<li><p>_Cluster assignment_: assign all examples into one of two groups based on which cluster centroid the example is closest to.</p></li>
<li><p>_Move centroid_: compute the averages for all the points inside each of the two cluster centroid groups, then move the cluster centroid points to those averages.</p></li>
<li><p>Re-run (2) and (3) until we have found our clusters.</p></li>
</ol>

<p>Main variables are:</p>

<ul>
<li><p>K (number of clusters)</p></li>
<li><p>Training set \({x^{(1)}, x^{(2)}, \dots,x^{(m)}}\)</p></li>
<li><p>Where \(x^{(i)} \in \mathbb{R}^n\)</p></li>
</ul>

<p><strong>The algorithm:</strong></p>

<pre><code class="language-text">Randomly initialize K cluster centroids mu(1), mu(2), ..., mu(K)
Repeat:
   for i = 1 to m:
      c(i):= index (from 1 to K) of cluster centroid closest to x(i)
   for k = 1 to K:
      mu(k):= average (mean) of points assigned to cluster k
</code></pre>

<h3 id="toc_1">Explaination</h3>

<p>The <strong>first for-loop</strong> is the <u>Cluster Assignment</u> step. We make a vector <u>c</u> where <u>c(i)</u> represents the centroid assigned to example <u>x(i)</u> .</p>

<p>We can write the operation of the Cluster Assignment step more mathematically as follows:</p>

<p>\(c^{(i)} = \arg \min_k\ ||x^{(i)} - \mu_k||^2\)</p>

<p>That is, each \(c^{(i)}\) contains the index of the centroid that has minimal distance to \(x^{(i)}\).</p>

<p>By convention, we square the right-hand-side, which makes the function we are trying to minimize more sharply increasing. It is mostly just a convention. But a convention that helps reduce the computation load because the Euclidean distance requires a square root but it is canceled.</p>

<p>Without the square:</p>

<p>\(||x^{(i)} - \mu_k|| = ||\quad\sqrt{(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + ...}\quad||\)</p>

<p>With the square:</p>

<p>\(||x^{(i)} - \mu_k||^2 = ||\quad(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + ...\quad||\)</p>

<p>...so the square convention serves two purposes, minimize more sharply and less computation.</p>

<p>The <strong>second for-loop</strong> is the &#39;Move Centroid&#39; step where we move each centroid to the average of its group.</p>

<p>More formally, the equation for this loop is as follows:</p>

<p>\(\mu_k = \dfrac{1}{n}[x^{(k_1)} + x^{(k_2)} + \dots + x^{(k_n)}] \in \mathbb{R}^n\)</p>

<p>Where each of \(x^{(k_1)}, x^{(k_2)}, \dots, x^{(k_n)}\) are the training examples assigned to group \(mμ_k\).</p>

<p>If you have a cluster centroid with <strong>0 points</strong> assigned to it, you can randomly <strong>re-initialize</strong> that centroid to a new point. You can also simply <strong>eliminate</strong> that cluster group.</p>

<p>After a number of iterations the algorithm will <u><strong>converge</strong></u> , where new iterations do not affect the clusters.</p>

<p>Note on non-separated clusters: some datasets have no real inner separation or natural structure. K-means can still evenly segment your data into K subsets, so can still be useful in this case.</p>

<h3 id="toc_2">Choosing the Number of Clusters</h3>

<p>Choosing K can be quite arbitrary and ambiguous.</p>

<p><strong>The elbow method</strong>: plot the cost J and the number of clusters K. The cost function should reduce as we increase the number of clusters, and then flatten out. Choose K at the point where the cost function starts to flatten out.</p>

<p>However, fairly often, the curve is <strong>very gradual</strong> , so there&#39;s no clear elbow.</p>

<p><strong>Note:</strong> J will <strong>always</strong> decrease as K is increased. The one exception is if k-means gets stuck at a bad local optimum.</p>

<p>Another way to choose K is to observe how well k-means performs on a <strong>downstream purpose</strong> . In other words, you choose K that proves to be most useful for some goal you&#39;re trying to achieve from using these clusters.</p>

<p><img src="media/15101207367271/elbow%20method.png" alt="elbow method"/></p>

<h2 id="toc_3">Principal Component Analysis</h2>

<p>The most popular dimensionality reduction algorithm is <u>Principal Component Analysis</u> (PCA)</p>

<p>Before applying PCA, there is a data pre-processing step we must perform:</p>

<p><strong>Data preprocessing</strong></p>

<ul>
<li>  Given training set: \(x(1),x(2),…,x(m)\)</li>
<li><p>Preprocess (feature scaling/mean normalization): \(\mu_j = \dfrac{1}{m}\sum^m_{i=1}x_j^{(i)}\)</p></li>
<li><p>Replace each \(x_j^{(i)}\) with \(x_j^{(i)} - \mu_j\)</p></li>
<li><p>If different features on different scales (e.g., \(x_1\) = size of house, \(x_2\) = number of bedrooms), scale features to have comparable range of values.</p></li>
</ul>

<p>Above, we first subtract the mean of each feature from the original feature. Then we scale all the features \(x_j^{(i)} = \dfrac{x_j^{(i)} - \mu_j}{s_j}\).</p>

<p>We can define specifically what it means to reduce from 2d to 1d data as follows:</p>

<p>\(\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T\)</p>

<p>The z values are all real numbers and are the projections of our features onto \(u^{(1)}\).</p>

<p>So, PCA has two tasks: figure out \(u^{(1)},\dots,u^{(k)}\) and also to find \(z_1, z_2, \dots, z_m\).</p>

<p>The mathematical proof for the following procedure is complicated and beyond the scope of this course.</p>

<p><strong>1. Compute &quot;covariance matrix&quot;</strong></p>

<p>\(\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T\)</p>

<p>This can be vectorized in Octave as:</p>

<p>\[Sigma = (1/m) * X&#39; * X;\]</p>

<p>We denote the covariance matrix with a capital sigma (which happens to be the same symbol for summation, confusingly---they represent entirely different things).</p>

<p>Note that \(x^{(i)}\) is an n×1 vector, \((x^{(i)})^T\) is an 1×n vector and X is a m×n matrix (row-wise stored examples). The product of those will be an n×n matrix, which are the dimensions of Σ.</p>

<p><strong>2. Compute &quot;eigenvectors&quot; of covariance matrix Σ</strong></p>

<pre><code class="language-octave">[U,S,V] = svd(Sigma);
</code></pre>

<p>svd() is the &#39;singular value decomposition&#39;, a built-in Octave function.</p>

<p>What we actually want out of svd() is the &#39;U&#39; matrix of the Sigma covariance matrix: \(U \in \mathbb{R}^{n \times n}\). U contains \(u^{(1)},\dots,u^{(n)}\), which is exactly what we want.</p>

<p><strong>3. Take the first k columns of the U matrix and compute z</strong></p>

<p>We&#39;ll assign the first k columns of U to a variable called <code>Ureduce</code>. This will be an n×k matrix. We compute z with:</p>

<p>\(z^{(i)} = \text{Ureduce}^T \cdot x^{(i)}\)</p>

<p>\(\text{Ureduce}Z^T\) will have dimensions k×n while x(i) will have dimensions n×1. The product \(\text{Ureduce}^T \cdot x^{(i)}\) will have dimensions k×1.</p>

<p>To summarize, the whole algorithm in <code>octave</code> is roughly:</p>

<pre><code class="language-octave">Sigma = (1/m) * X&#39; * X; % compute the covariance matrix
[U,S,V] = svd(Sigma);   % compute our projected directions
Ureduce = U(:,1:k);     % take the first k directions
Z = X * Ureduce;        % compute the projected data points
</code></pre>

<h3 id="toc_4">Choosing the Number of Principal Components</h3>

<p>How do we choose <u>number of principal components</u> \(k\)? </p>

<p>One way to choose k is by using the following formula:</p>

<ul>
<li><p>Given the average squared projection error: \(\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2\)</p></li>
<li><p>Also given the total variation in the data: \(\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2\)</p></li>
<li><p>Choose k to be the smallest value such that: \(\dfrac{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2}{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2} \leq 0.01\)</p></li>
</ul>

<p>In other words, the squared projection error divided by the total variation should be less than one percent, so that <strong>99% of the variance is retained</strong> .</p>

<p><strong>Algorithm for choosing k</strong></p>

<ol>
<li><p>Try PCA with k=1,2,…</p></li>
<li><p>Compute \(U_{reduce}, z, x\)</p></li>
<li><p>Check the formula given above that _99% of the variance is retained_. If not, go to step one and increase k.</p></li>
</ol>

<p>This procedure would actually be horribly inefficient. In Octave, we will call svd:</p>

<pre>[U,S,V] = svd(Sigma)
</pre>

<p>Which gives us a matrix S. We can actually check for 99% of retained variance using the S matrix as follows:</p>

<p>\(\dfrac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \geq 0.99\)</p>

<h3 id="toc_5">Advice for Applying PCA</h3>

<p>The most common use of PCA is to speed up supervised learning.</p>

<p>Given a training set with a large number of features (e.g. \(x^{(1)},\dots,x^{(m)} \in \mathbb{R}^{10000}\) ) we can use PCA to reduce the number of features in each example of the training set (e.g. \(z^{(1)},\dots,z^{(m)} \in \mathbb{R}^{1000}\)).</p>

<p>Note that we should define the PCA reduction from \(x^{(i)}\) to \(z^{(i)}\) only on the training set and not on the cross-validation or test sets. You can apply the mapping z(i) to your cross-validation and test sets after it is defined on the training set.</p>


    

      </div>

      <div class="row">
        <div class="large-6 columns">
        <p class="text-left" style="padding:15px 0px;">
      
          <a href="neural_networks.html" 
          title="Previous Post: Machine Learning (5): Neural Networks">&laquo; Machine Learning (5): Neural Networks</a>
      
        </p>
        </div>
        <div class="large-6 columns">
      <p class="text-right" style="padding:15px 0px;">
      
          <a  href="support-vector-machine.html" 
          title="Next Post: Machine Learning(7): Support Vector Machines">Machine Learning(7): Support Vector Machines &raquo;</a>
      
      </p>
        </div>
      </div>
      <div class="comments-wrap">
        <div class="share-comments">
          <div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://techlarry-1.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                                

          

          
        </div>
      </div>
    </div><!-- article-wrap -->
  </div><!-- large 8 -->




 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">他山之石，可以攻玉</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Leetcode.html"><strong>Leetcode</strong></a>
        
            <a href="programming_language.html"><strong>编程语言</strong></a>
        
            <a href="data_structure_and_algorithm.html"><strong>数据结构和算法</strong></a>
        
            <a href="Python%E7%89%B9%E6%80%A7.html"><strong>Python特性</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>Python科学计算三维可视化</strong></a>
        
            <a href="English.html"><strong>English</strong></a>
        
            <a href="Computer%20System.html"><strong>Computer System</strong></a>
        
            <a href="Deep%20Learning.html"><strong>Deep Learning</strong></a>
        
            <a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html"><strong>Linux 系统编程</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html"><strong>数据库</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="Big%20Data.html"><strong>Big Data</strong></a>
        
            <a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html"><strong>文献阅读</strong></a>
        
            <a href="Tools.html"><strong>Tools</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="head-first_java_note.html">Head first Java</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="exceptional_control_flow.html">CSAPP - 异常控制流</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="introduction_to_computer_system_CMU.html">CMU 15-213 Introduction to Computer Systems</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os-concepts-os-structures.html">Operating System Concepts 2 - Operating System structures</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os-concets-processes.html">Operating System Concepts 3 - Processes</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
