<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  techlarry
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">HomePage</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_blank" href="wiki">WIKI</a></li>
        
        <li id=""><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">HomePage</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_blank" href="wiki">WIKI</a></li>
        
        <li><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href=".html">Leetcode</a></li>
        
            <li><a href=".html">C/C++</a></li>
        
            <li><a href=".html">PythonÊï∞ÊçÆÁªìÊûÑ‰∏éÁÆóÊ≥ï</a></li>
        
            <li><a href=".html">Course</a></li>
        
            <li><a href=".html">PythonÁâπÊÄß</a></li>
        
            <li><a href=".html">Êú∫Âô®Â≠¶‰π†</a></li>
        
            <li><a href=".html">PythonÁßëÂ≠¶ËÆ°ÁÆó‰∏âÁª¥ÂèØËßÜÂåñ</a></li>
        
            <li><a href=".html">English</a></li>
        
            <li><a href=".html">Computer System</a></li>
        
            <li><a href=".html">Deep Learning</a></li>
        
            <li><a href=".html">Linux Á≥ªÁªüÁºñÁ®ã</a></li>
        
            <li><a href=".html">Êï∞ÊçÆÂ∫ì</a></li>
        
            <li><a href=".html">Tensorflow</a></li>
        
            <li><a href=".html">Big Data</a></li>
        
            <li><a href=".html">ÊñáÁåÆÈòÖËØª</a></li>
        
            <li><a href=".html">Tools</a></li>
        
            <li><a href=".html">Â§ßÊï∞ÊçÆ</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="15108229432607.html">
                
                  <h1>`Filter`, `Map`, `Reduce`</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<pre><code class="language-python">&gt;&gt;&gt; foo = [2, 18, 9, 22, 17, 24, 8, 12, 27]
&gt;&gt;&gt; print filter(lambda x: x % 3 == 0, foo)
[18, 9, 24, 12, 27]
&gt;&gt;&gt; 
&gt;&gt;&gt; print map(lambda x: x * 2 + 10, foo)
[14, 46, 28, 54, 44, 58, 26, 34, 64]
&gt;&gt;&gt; 
&gt;&gt;&gt; print reduce(lambda x, y: x + y, foo)
139
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/11/16</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Python%E7%89%B9%E6%80%A7.html'>PythonÁâπÊÄß</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15035780907547.html">
                
                  <h1>TensorFlow(5): Vector and Matrix Product in Numpy and TensorFlow</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">Numpy</h2>

<p>Following are common vector and matrix product operations in Numpy, they are quite simple and straightforward:</p>

<ul>
<li><p>Inner Product   \(\quad a^Tb\quad \):  <code>np.inner(a,b)</code></p></li>
<li><p>Outer Product  \(\quad ab^T\quad \):  <code>np.outer(a,b)</code></p></li>
<li><p>Dot Product  \(\quad a \cdot b = \sum a_ib_i\quad \): <code>np.dot(a,b)</code></p></li>
<li><p>Elementwise Product  \(\quad c_i = a_ib_i\quad \): <code>np.multiply()</code> or <code>c=a*b</code></p></li>
</ul>

<p>Note: inner product is defined on vector spaces over a field ùïÇ (finite or infinite dimensional). Dot product refers specifically to the product of vectors in \(‚Ñù^n\)</p>

<p>The difference between the following implementations of the dot/inner/outer/elementwise product are demonstrated as follows:</p>

<pre><code class="language-python">import numpy as np

W = np.ones((2, 7), dtype=&#39;float32&#39;)
x1 = np.array([9, 2, 5, 0, 0, 7, 5])
x2 = np.array([9, 2, 2, 9, 0, 9, 2])
print(&#39;vector dot product&#39;, np.dot(x1,x2)) # dot product
print(&#39;inner&#39;, np.inner(x1,x2)) # inner product
print(&#39;outter&#39;, np.outer(x1,x2)) # outter product
print(&#39;element-wsie&#39;, np.multiply(x1,x2)) # Element-wise product
print(&#39;element-wsie&#39;, x1*x2) # Element-wise product
print(&#39;matrix dot product&#39;, np.dot(W, x1)) # dot product
</code></pre>

<pre><code class="language-text">vector dot product 168
inner 168
outter [[81 18 18 81  0 81 18]
 [18  4  4 18  0 18  4]
 [45 10 10 45  0 45 10]
 [ 0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0]
 [63 14 14 63  0 63 14]
 [45 10 10 45  0 45 10]]
element-wsie [81  4 10  0  0 63 10]
element-wsie [81  4 10  0  0 63 10]
matrix dot product [ 28.  28.]
</code></pre>

<p>For detailed explanation, see <a href="">here</a>;</p>

<h2 id="toc_1">TensorFlow</h2>

<p>Vector inner/outer Product are a bit complex in TensorFlow. </p>

<pre><code class="language-python">import tensorflow as tf
import numpy as np

x = tf.Variable([[1, -2, 3]], tf.float32, name=&#39;x&#39;)
y = tf.Variable([[-1, 2, -3]], tf.float32, name=&#39;y&#39;)

## inner product
inner_product1 = tf.reduce_sum(tf.multiply(x, y))
inner_product2 = tf.matmul(x, y, transpose_a=False, transpose_b= True) # different from inner_product1

## outer product
outer_product2 = tf.matmul(x, y, transpose_a= True)

## matrix dot product
X = tf.constant(np.random.randn(3,3), name=&#39;X&#39;)
W = tf.constant(np.random.randn(3,3), name=&#39;W&#39;)
matrix_product = tf.matmul(W, X)

sess = tf.InteractiveSession()
init_op = tf.global_variables_initializer()

# run
sess.run(init_op)
print(sess.run(inner_product1))
print(sess.run(inner_product2))
print(sess.run(outer_product2))
print(sess.run(matrix_product))
</code></pre>

<pre><code class="language-text">-14
[[-14]]
[[-1  2 -3]
 [ 2 -4  6]
 [-3  6 -9]]
[[-0.88722509 -0.94128018 -2.1999658 ]
 [-0.67967623  1.33193446 -0.75612585]
 [ 0.31741623  1.3271727  -0.04311113]]
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/8/24</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Tensorflow.html'>Tensorflow</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15108128842368.html">
                
                  <h1>`MLlib`: Machine Learning in Apache Spark</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>[<a href="https://arxiv.org/pdf/1505.06807.pdf">https://arxiv.org/pdf/1505.06807.pdf</a>]</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/11/16</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html'>ÊñáÁåÆÈòÖËØª</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15088244818278.html">
                
                  <h1>Machine Learning Foundations - Mathematical Foundations</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">(1) The Learning Problem</a>
<ul>
<li>
<a href="#toc_1">Key Essence of Machine Learning</a>
</li>
<li>
<a href="#toc_2">Formalize the Learning Problem</a>
</li>
<li>
<a href="#toc_3">Machine Learning and other Fields</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">(2) Learning to answer Yes or No</a>
<ul>
<li>
<a href="#toc_5">Perceptron Hypothesis Set</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">(3) Types of Learning</a>
<ul>
<li>
<a href="#toc_7">Learning with Different Output Space</a>
</li>
<li>
<a href="#toc_8">Learning with Different Data Label</a>
</li>
<li>
<a href="#toc_9">Learning with Different Protocol</a>
</li>
<li>
<a href="#toc_10">Learning with Different Input Space</a>
</li>
</ul>
</li>
<li>
<a href="#toc_11">(4) Feasibility of Learning</a>
<ul>
<li>
<a href="#toc_12">Learning outside training examples</a>
</li>
<li>
<a href="#toc_13">In and out of sample error</a>
</li>
<li>
<a href="#toc_14">Connection to Real Learning</a>
<ul>
<li>
<a href="#toc_15">Bound of BAD data</a>
</li>
<li>
<a href="#toc_16">The Statistical Learning Flow</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_17">(5) Training versus Testing</a>
<ul>
<li>
<a href="#toc_18">Dichotomy</a>
</li>
<li>
<a href="#toc_19">Growth function</a>
</li>
<li>
<a href="#toc_20">Shatter and Break point</a>
</li>
</ul>
</li>
<li>
<a href="#toc_21">(6) Theory of generalization</a>
<ul>
<li>
<a href="#toc_22">Bounding Function</a>
</li>
<li>
<a href="#toc_23">VC Bound</a>
</li>
</ul>
</li>
<li>
<a href="#toc_24">(7) VC dimension</a>
<ul>
<li>
<a href="#toc_25">Interpreting VC Dimension</a>
</li>
</ul>
</li>
<li>
<a href="#toc_26">(8): Noise and error</a>
<ul>
<li>
<a href="#toc_27">Error Measure</a>
</li>
</ul>
</li>
</ul>


<p>The course, Machine Learning Foundations - Mathematical Foundations(Êú∫Âô®Â≠¶‰π†Âü∫Áü≥), is taught by Hsuan-Tien Lin on Coursera (<a href="https://www.coursera.org/learn/ntumlone-mathematicalfoundations/home/info">course info page</a>).</p>

<p>As the name of the course indicates, the course teaches the most fundamental algorithmic, theoretical, mathematical background of machine learning.</p>

<p>The article summaries the key concepts and conclusions in the course. Each section in the article corresponding to one chapter in the course.</p>

<h2 id="toc_0">(1) The Learning Problem</h2>

<h3 id="toc_1">Key Essence of Machine Learning</h3>

<p>Machine Learning: improving some <strong>performance measure</strong> with experience <strong>computed</strong> from <strong>data</strong></p>

<ul>
<li>exists some &#39;underlying pattern&#39; to be learned ‚Äî so &#39;performance measure&#39; can be improved</li>
<li>but no programmable (easy) definition ‚Äî so &#39;ML&#39; is needed</li>
<li>somehow there is data about the pattern ‚Äî so ML has some &#39;inputs&#39; to learn from</li>
</ul>

<h3 id="toc_2">Formalize the Learning Problem</h3>

<p><strong>Basic Notations</strong>:</p>

<ul>
<li>input: \(x\in\mathcal{X}\)</li>
<li>output: \(y\in\mathcal{Y}\)</li>
<li>unknown pattern to be learned \(\Leftrightarrow\) target function:<br/>
\(f:\mathcal{X}\rightarrow\mathcal{Y}\) </li>
<li>data \(\Leftrightarrow\) training examples: \(\mathcal{D}={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}\)</li>
<li>hypothesis \(\Leftrightarrow\) skill with hopefully good performance: \(g: \mathcal{X}\rightarrow \mathcal{Y}\)</li>
<li>final hypothesis: \(\mathcal{g}\approx f\)</li>
</ul>

<p><strong>Machine Learning: use data \(\mathcal{D}\) to compute hypothesis \(g\) that approximates target \(f\).</strong></p>

<p><img src="media/15088244818278/Screen%20Shot%202017-11-07%20at%2012.35.17%20PM.png" alt="Screen Shot 2017-11-07 at 12.35.17 P"/></p>

<h3 id="toc_3">Machine Learning and other Fields</h3>

<ul>
<li>Machine Learning: use data to compute hypothesis \(g\) that approximates target \(f\)</li>
<li>Data Mining: Use (huge) data to find property that is interesting</li>
</ul>

<p>\(\Longleftrightarrow\)If &#39;interesting property&#39; same as &#39;hypothesis that approximate target&#39;, Machine Learning = Data Mining<br/>
\(\Longleftrightarrow\) If &#39;interesting property&#39; related to &#39;hypothesis that approximate target&#39;, data mining can help machine learning.<br/>
\(\Longleftrightarrow\) traditional data mining also focuses on efficient computation in large database.</p>

<h2 id="toc_4">(2) Learning to answer Yes or No</h2>

<h3 id="toc_5">Perceptron Hypothesis Set</h3>

<p>Vector Form of Perceptron Hypothesis:</p>

<p>\[h(x) = \text{sign}(\sum\limits_{i=0}^dw_ix_i)=\text{sign}(W^Tx)\]</p>

<p><strong>Perceptron Learning Algorithm</strong>(PLA):</p>

<p><img src="media/15088244818278/PLA.png" alt="PLA"/></p>

<p><strong>Pocket Algorithm</strong>: modify <code>PLA</code> algorithm by keeping best weights in pocket</p>

<p><img src="media/15088244818278/pocket.png" alt="pocket"/></p>

<h2 id="toc_6">(3) Types of Learning</h2>

<h3 id="toc_7">Learning with Different Output Space</h3>

<ul>
<li>binary classification: \(\mathcal{Y}=\{-1,+1\}\)</li>
<li>multiclass classification: \(\mathcal{Y}=\{1, 2,..., K\}\)</li>
<li>regression: \(\mathcal {Y} =\mathbb{R}\)</li>
<li>structured learning: \(\mathcal{Y}=\) structures</li>
</ul>

<h3 id="toc_8">Learning with Different Data Label</h3>

<ul>
<li>supervised: all \(y_n\)</li>
<li>unsupervised: no \(y_n\)</li>
<li>semi-supervised: some \(y_n\)</li>
<li>reinforcement: implicit \(y_n\) by goodness (\(\hat y_n\))</li>
</ul>

<h3 id="toc_9">Learning with Different Protocol</h3>

<p>Protocol \(\Longleftrightarrow\) Learning Philosophy.</p>

<ul>
<li>batch: all known data</li>
<li>online: sequential (passive) data</li>
<li>active: strategically-observed data</li>
</ul>

<h3 id="toc_10">Learning with Different Input Space</h3>

<ul>
<li>concrete: sophisticated (and related) physical meaning e.g. user features(age, sex, education level)</li>
<li>raw: simple physical meaning e.g. image features</li>
<li>abstract: no (or little) physical meaning e.g. user/image IDs</li>
</ul>

<h2 id="toc_11">(4) Feasibility of Learning</h2>

<h3 id="toc_12">Learning outside training examples</h3>

<p><strong>No Free Lunch</strong>: Learning from \(D\) (\(D\): training examples) (to infer something outside \(D\)) is doomed to fail if any unknown \(f\) can happen.</p>

<ul>
<li>\(g\approx f\) inside \(D\): Possible!</li>
<li>\(g\approx f\) outside \(D\): Impossible!</li>
</ul>

<h3 id="toc_13">In and out of sample error</h3>

<p>For any fixed \(h\), in &#39;big&#39; data (N large), <code>in-sample error</code> \(E_{in}(h)\) is probably close to <code>out-of-sample error</code> \(E_{out}(h)\) (within \(\epsilon\)):</p>

<p>\(P(|E_{in}(h)-E_{out}(h)|&gt; \epsilon) \le 2\exp(-2\epsilon^2N)\)</p>

<p>The equation above is called <code>Hoeffding&#39;s Inequality</code>. The statement \(E_{in}(h)=E_{out}(h)\) is <code>probably approximately correct</code> (<strong>PAC</strong>, Â§ßÊ¶ÇËøë‰ººÊ≠£Á°Æ).</p>

<p>If large \(N\), we can <strong>probably</strong> infer unknown \(E_{out}(h)\) by known \(E_{in}(h)\).</p>

<p><code>in-sample error</code> \(E_{in}(h)\) denotes orange fraction in sample, <code>out-of-sample error</code> \(E_{out}(h)\) denotes orange probability in bin, When we want to infer the orange probability in bin.</p>

<p><img src="media/15088244818278/Ein_Eout.png" alt="Ein_Eout"/></p>

<p><img src="media/15088244818278/Connection%20to%20Learning.png" alt="Connection to Learning"/></p>

<h3 id="toc_14">Connection to Real Learning</h3>

<p><strong>Bad data for One \(h\)</strong>: </p>

<p>\(E_{out}(h)\) and \(E_{in}(h)\) far away: e.g. \(E_{out}\) big (far from \(f\)), but \(E_{in}\) small (correct on most examples).</p>

<p><strong>Bad data for many \(h\)</strong>:</p>

<p>\(\Leftrightarrow\) no &#39;freedom of choices&#39; by learning algorithm \(\mathcal{A}\)<br/>
\(\Leftrightarrow\) there exists some \(h\) such that \(E_{out}(h)\) and \(E_{in}(h)\) far away</p>

<h4 id="toc_15">Bound of BAD data</h4>

<p>Form \(M\) hypotheses, what is the bound of \(\mathbb{P}_{\mathcal{D}} [\text{BAD} \; \mathcal{D}]\)</p>

<p><img src="media/15088244818278/bound_of_bad_data.png" alt="bound of bad data"/></p>

<p>The <code>union bound</code> <a href="https://en.wikipedia.org/wiki/Boole%27s_inequality">WIKI</a> (Â∏ÉÂ∞î‰∏çÁ≠âÂºè), also known as <code>Boole&#39;s inequality</code>, says that for any finite or countable set of events, the probability that at least one of the events happens is no greater than the sum of the probabilities of the individual events.</p>

<h4 id="toc_16">The Statistical Learning Flow</h4>

<p>If \(|\mathcal{H}|=M\) finite, \(N\) large enough, for whatever \(g\) picked by \(\mathcal{A}\), \(E_{out}(g)\approx E_{in}(g)\)</p>

<p>if \(\mathcal{A}\) finds one \(g\) with \(E_{in}(g)\approx 0\),<br/>
PAC gaurantee for \(E_{out}(g)\approx 0\) \(\rightarrow\) learning possilbe!</p>

<h2 id="toc_17">(5) Training versus Testing</h2>

<p>For batch and supervised binary classification, \(g\approx f\Longleftrightarrow E_{out}(g)\approx 0\) achieved through \(E_{out}(g)\approx E_{in}(g)\) and \(E_{in}(g)\approx 0\).</p>

<p>In order to achieve two conditions above, learning split to two central questions:</p>

<ul>
<li>Can we make sure that \(E_{out}(g)\) is close enough to \(E_{in}(g)\)?</li>
<li>Can we make \(E_{in}(g)\) small enough?</li>
</ul>

<p>In order to understand trade-off for &#39;right&#39; \(\mathcal{H}\), we establish a finite quantity that replace \(M\),</p>

<p>\[\mathbb{P}[|E_{in}9g)-E_{out}(g)|&gt;\epsilon] \le 2\cdot m_\mathcal{H} \cdot \exp(-2\epsilon^2 N)\]</p>

<h3 id="toc_18">Dichotomy</h3>

<p>Define hypothesis set \(\mathcal{H}\):</p>

<p>\(\mathcal{H}=\{\text{hypothesis}\quad h: \mathcal{X}\rightarrow{\text{x,o}}\}\)</p>

<p>And we call hypothesis &#39;limited&#39; to the eyes of \(x_1, x_2,..., x_N\) a dichotomy:</p>

<p>\[\mathcal{H}(x_1,x_2,...,x_N)=\{(h(x_1),h(x_2),...,h(x_N)) \quad | \quad h \in \mathcal{H} \}\]</p>

<p>One can think of the dichotomies \(\mathcal{H}(x_1,x_2,...,x_N)\) as a set of hypotheses just like \(\mathcal{H}\)  is, except that the hypotheses are seen through the eyes of \(N\) points only.</p>

<p><strong>Note</strong>: hypotheses \(\mathcal{H}\) and dichotomy \(\mathcal{H}(x_1,x_2,...,x_N)\) are different!</p>

<h3 id="toc_19">Growth function</h3>

<p>Growth function is <strong>the maximum number of dichotomies</strong> that can be generated by \(\mathcal{H}\) on <strong>any</strong> \(N\) points: </p>

<p>\[m_\mathcal{H}(N) = \max\limits_{x_1,x_2,...,x_N\in\mathcal{X}}|\mathcal{H}(x_1,x_2,...,x_N)|\]</p>

<p>Growth function is finite, its upper-bound is \(2^N\).</p>

<p>Growth functions in different situation:</p>

<ul>
<li>positive rays: \(m_\mathcal{H}(N)=N+1\)</li>
<li>positive intervals: \(m_\mathcal{H}(N)=\frac{1}{2}N^2+\frac{1}{2}N+1\)</li>
<li>convex sets: \(m_\mathcal{H}(N)=2^N \)</li>
<li>2D perceptrons: \(m_\mathcal{H}(N)=2^N \text{in some cases}\)</li>
</ul>

<h3 id="toc_20">Shatter and Break point</h3>

<p><strong>If no \(k\) inputs can be shattered by \(\mathcal{H}\) , call \(k\) a break point for \(\mathcal{H}\).</strong></p>

<p>if \(m_H(K) &lt; 2^k\), call k a <strong>break point</strong> for \(\mathcal{H}\).</p>

<ul>
<li>since \(k\) is a break point, \(k+1, k+2, k+3,...,\) also break points</li>
</ul>

<p>minimum break points for different \(\mathcal{H}\):</p>

<ul>
<li>positive rays: k=2</li>
<li>positive intervals: k=3</li>
<li>convex sets: none</li>
<li>2D perceptrons: k=4</li>
</ul>

<h2 id="toc_21">(6) Theory of generalization</h2>

<h3 id="toc_22">Bounding Function</h3>

<p>Bounding Function \(B(N,k)\) is maximum possible \(m_\mathcal{H}(N)\) when break point \(=k\).</p>

<p>\[B(N,k) \le \sum\limits_{i=0}^{k-1} \left(\stackrel N i\right)\]</p>

<p><u>For fixed</u> \(k, B(N,k)\) <u>upper bounded by</u> \(ploy(N)\), \(\rightarrow m_{\mathcal{H}}(N)\) is \(poly(N)\) _if break point exists_.</p>

<h3 id="toc_23">VC Bound</h3>

<p>When N large enough, </p>

<p>\[P[\exists h\in \mathcal{H} \quad s.t.\quad  |E_{in}(h)-E_{out}(h)|&gt; \epsilon]\le 4 m_\mathcal{H}(2N)\exp(-\frac{1}{8}\epsilon^2N)\]</p>

<p>The above equation called _<strong>Vapnik-Chervonenkis (VC) bound</strong>_.</p>

<h2 id="toc_24">(7) VC dimension</h2>

<p>VC dimension, the formal name of <strong>maximum non-break point</strong>, denoted \(d_{VC}(\mathcal{H})\), is </p>

<p>\[\text{largest N for which } m_{\mathcal{H}}(N)=2^N\]</p>

<ul>
<li>positive rays: \(d_{VC}=1\)</li>
<li>positive intervals: \(d_{VC}=2\)</li>
<li>convex sets: \(d_{VC}= \infty \)</li>
<li>2D perceptrons: \(d_{VC}=3\)</li>
<li>d dimension Perceptrons: \(d_{VC}=d+1\)</li>
</ul>

<h3 id="toc_25">Interpreting VC Dimension</h3>

<p><img src="media/15088244818278/the%20vc%20message.png" alt="the vc message"/><br/>
<img src="media/15088244818278/the%20vc%20message2.png" alt="the vc message2"/></p>

<h2 id="toc_26">(8): Noise and error</h2>

<p><strong>Noise</strong> in \(x\) and \(y\):</p>

<p><img src="media/15088244818278/Noises.png" alt="Noises"/></p>

<p>NOTE: <strong>VC holds for \(x \stackrel{i.i.d}{\backsim} P(x), y \stackrel{i.i.d}{\backsim} P(y|x)\)</strong></p>

<p><img src="media/15088244818278/New%20Learning%20Law.png" alt="New Learning Law"/></p>

<h3 id="toc_27">Error Measure</h3>

<p>Two Important Pointwise Error Measures</p>

<p><strong>0/1 error</strong>(often for classification): err(\(\tilde y,y)=[\tilde y\ne y]\) <br/>
<strong>squared error</strong>(often for regression): err(\(\tilde y,y)=(\tilde y\ne y)^2\)</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/10/24</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>Êú∫Âô®Â≠¶‰π†</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="anomaly-detection_and_recommender_system.html">
                
                  <h1>Machine Learning(9): Anomaly Detection AND Recommender System</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">[1] Anomaly Detection</a>
<ul>
<li>
<a href="#toc_1">Problem Motivation</a>
</li>
<li>
<a href="#toc_2">Gaussian Distribution</a>
</li>
<li>
<a href="#toc_3">Algorithm</a>
</li>
<li>
<a href="#toc_4">Developing and Evaluating an Anomaly Detection System</a>
</li>
<li>
<a href="#toc_5">Anomaly Detection vs. Supervised Learning</a>
</li>
<li>
<a href="#toc_6">Choosing What Features to Use</a>
</li>
<li>
<a href="#toc_7">Multivariate Gaussian Distribution</a>
</li>
<li>
<a href="#toc_8">Anomaly Detection using the Multivariate Gaussian Distribution</a>
</li>
</ul>
</li>
<li>
<a href="#toc_9">[2] Recommender Systems</a>
<ul>
<li>
<a href="#toc_10">Problem Formulation</a>
</li>
<li>
<a href="#toc_11">Content Based Recommendations</a>
</li>
<li>
<a href="#toc_12">Feature Finder</a>
</li>
<li>
<a href="#toc_13">Collaborative Filtering Algorithm</a>
</li>
<li>
<a href="#toc_14">Vectorization: Low Rank Matrix Factorization</a>
</li>
<li>
<a href="#toc_15">Implementation Detail: Mean Normalization</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">[1] Anomaly Detection</h2>

<h3 id="toc_1">Problem Motivation</h3>

<p>Just like in other learning problems, we are given a dataset \({x^{(1)}, x^{(2)},\dots,x^{(m)}}\).</p>

<p>We are then given a new example, \(x_{test}\), and we want to know whether this new example is abnormal/anomalous.</p>

<p>We define a &quot;model&quot; p(x) that tells us the probability the example is not anomalous. We also use a threshold œµ (epsilon) as a dividing line so we can say which examples are anomalous and which are not.</p>

<p>A very common application of anomaly detection is detecting fraud:</p>

<ul>
<li><p>\(x^{(i)} =\) features of user i&#39;s activities</p></li>
<li><p>Model p(x) from the data.</p></li>
<li><p>Identify unusual users by checking which have p(x)&lt;œµ.</p></li>
</ul>

<p>If our anomaly detector is flagging <strong>too many</strong> anomalous examples, then we need to <strong>decrease</strong> our threshold œµ</p>

<h3 id="toc_2">Gaussian Distribution</h3>

<p>The Gaussian Distribution is a familiar bell-shaped curve that can be described by a function \(\mathcal{N}(\mu,\sigma^2)\)</p>

<p>Let x‚àà‚Ñù. If the probability distribution of x is Gaussian with mean Œº, variance \(\sigma^2\), then:</p>

<p>\(x \sim \mathcal{N}(\mu, \sigma^2)\)</p>

<p>The little ‚àº or &#39;tilde&#39; can be read as &quot;distributed as.&quot;</p>

<p>The Gaussian Distribution is parameterized by a mean and a variance.</p>

<p>Mu, or Œº, describes the center of the curve, called the mean. The width of the curve is described by sigma, or œÉ, called the standard deviation.</p>

<p>The full function is as follows:</p>

<p>\(\large p(x;\mu,\sigma^2) = \dfrac{1}{\sigma\sqrt{(2\pi)}}e^{-\dfrac{1}{2}(\dfrac{x - \mu}{\sigma})^2}\)</p>

<p>We can estimate the parameter Œº from a given dataset by simply taking the average of all the examples:</p>

<p>\(\mu = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x^{(i)}\)</p>

<p>We can estimate the other parameter, \(\sigma^2\), with our familiar squared error formula:</p>

<p>\(\sigma^2 = \dfrac{1}{m}\displaystyle \sum_{i=1}^m(x^{(i)} - \mu)^2\)</p>

<p><img src="media/15045011015887/15045018120355.png" alt="Gaussian distribution"/></p>

<h3 id="toc_3">Algorithm</h3>

<p>Given a training set of examples, \(\lbrace x^{(1)},\dots,x^{(m)}\rbrace\) where each example is a vector, \(x \in \mathbb{R}^n\).</p>

<p>\(p(x) = p(x_1;\mu_1,\sigma_1^2)p(x_2;\mu_2,\sigma^2_2)\cdots p(x_n;\mu_n,\sigma^2_n)\)</p>

<p>In statistics, this is called an &quot;independence assumption&quot; on the values of the features inside training example x.</p>

<p>More compactly, the above expression can be written as follows:</p>

<p>\(= \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2)\)</p>

<p><strong>The algorithm</strong></p>

<p>Choose features \(x_i\) that you think might be indicative of anomalous examples.</p>

<p>Fit parameters \(\mu_1,\dots,\mu_n,\sigma_1^2,\dots,\sigma_n^2\)</p>

<p>Calculate \(\mu_j = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x_j^{(i)}\)</p>

<p>Calculate \(\sigma^2_j = \dfrac{1}{m}\displaystyle \sum_{i=1}^m(x_j^{(i)} - \mu_j)^2\)</p>

<p>Given a new example x, compute p(x):</p>

<p>\(p(x) = \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2) = \prod\limits^n_{j=1} \dfrac{1}{\sqrt{2\pi}\sigma_j}exp(-\dfrac{(x_j - \mu_j)^2}{2\sigma^2_j})\)</p>

<p>Anomaly if p(x)&lt;œµ</p>

<p>A vectorized version of the calculation for Œº is \(\mu = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x^{(i)}\). You can vectorize \(\sigma^2\) similarly.</p>

<h3 id="toc_4">Developing and Evaluating an Anomaly Detection System</h3>

<p>To evaluate our learning algorithm, we take some labeled data, categorized into anomalous and non-anomalous examples ( y = 0 if normal, y = 1 if anomalous).</p>

<p>Among that data, take a large proportion of <strong>good</strong> , non-anomalous data for the training set on which to train p(x).</p>

<p>Then, take a smaller proportion of mixed anomalous and non-anomalous examples (you will usually have many more non-anomalous examples) for your cross-validation and test sets.</p>

<p>For example, we may have a set where 0.2% of the data is anomalous. We take 60% of those examples, all of which are good (y=0) for the training set. We then take 20% of the examples for the cross-validation set (with 0.1% of the anomalous examples) and another 20% from the test set (with another 0.1% of the anomalous).</p>

<p>In other words, we split the data 60/20/20 training/CV/test and then split the anomalous examples 50/50 between the CV and test sets.</p>

<p><strong>Algorithm evaluation:</strong></p>

<p>Fit model p(x) on training set \(\lbrace x^{(1)},\dots,x^{(m)} \rbrace\)</p>

<p>On a cross validation/test example x, predict:</p>

<p>If p(x) &lt; œµ ( <strong>anomaly</strong> ), then y=1</p>

<p>If p(x) ‚â• œµ ( <strong>normal</strong> ), then y=0</p>

<p>Possible evaluation metrics (see &quot;Machine Learning System Design&quot; section):</p>

<ul>
<li><p>True positive, false positive, false negative, true negative.</p></li>
<li><p>Precision/recall</p></li>
<li><p>\(F_1\) score</p></li>
</ul>

<p>Note that we use the cross-validation set to choose parameter œµ</p>

<h3 id="toc_5">Anomaly Detection vs. Supervised Learning</h3>

<p>When do we use anomaly detection and when do we use supervised learning?</p>

<p>Use anomaly detection when...</p>

<ul>
<li><p>We have a very small number of positive examples (y=1 ... 0-20 examples is common) and a large number of negative (y=0) examples.</p></li>
<li><p>We have many different &quot;types&quot; of anomalies and it is hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous examples we&#39;ve seen so far.</p></li>
</ul>

<p>Use supervised learning when...</p>

<ul>
<li><p>We have a large number of both positive and negative examples. In other words, the training set is more evenly divided into classes.</p></li>
<li><p>We have enough positive examples for the algorithm to get a sense of what new positives examples look like. The future positive examples are likely similar to the ones in the training set.</p></li>
</ul>

<h3 id="toc_6">Choosing What Features to Use</h3>

<p>The features will greatly affect how well your anomaly detection algorithm works.</p>

<p>We can check that our features are <strong>gaussian</strong> by plotting a histogram of our data and checking for the bell-shaped curve.</p>

<p>Some <strong>transforms</strong> we can try on an example feature x that does not have the bell-shaped curve are:</p>

<ul>
<li><p>log(x)</p></li>
<li><p>log(x+1)</p></li>
<li><p>log(x+c) for some constant</p></li>
<li><p>\(\sqrt{x}\)</p></li>
<li><p>\(x^{1/3}\)</p></li>
</ul>

<p>We can play with each of these to try and achieve the gaussian shape in our data.</p>

<p>There is an <strong>error analysis procedure</strong> for anomaly detection that is very similar to the one in supervised learning.</p>

<p>Our goal is for p(x) to be large for normal examples and small for anomalous examples.</p>

<p>One common problem is when p(x) is similar for both types of examples. In this case, you need to examine the anomalous examples that are giving high probability in detail and try to figure out new features that will better distinguish the data.</p>

<p>In general, choose features that might take on unusually large or small values in the event of an anomaly.</p>

<h3 id="toc_7">Multivariate Gaussian Distribution</h3>

<p>The multivariate gaussian distribution is an extension of anomaly detection and may (or may not) catch more anomalies.</p>

<p>Instead of modeling \(p(x_1),p(x_2),\dots\) separately, we will model p(x) all in one go. Our parameters will be: \(\mu \in \mathbb{R}^n\) and \(\Sigma \in \mathbb{R}^{n \times n}\)</p>

<p>\(p(x;\mu,\Sigma) = \dfrac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} exp(-1/2(x-\mu)^T\Sigma^{-1}(x-\mu))\)</p>

<p>The important effect is that we can model oblong gaussian contours, allowing us to better fit data that might not fit into the normal circular contours.</p>

<p>Varying Œ£ changes the shape, width, and orientation of the contours. Changing Œº will move the center of the distribution.</p>

<p>Check also:</p>

<ul>
<li>  <a href="http://cs229.stanford.edu/section/gaussians.pdf">The Multivariate Gaussian Distribution</a> <a href="http://cs229.stanford.edu/section/gaussians.pdf">http://cs229.stanford.edu/section/gaussians.pdf</a> Chuong B. Do, October 10, 2008.</li>
</ul>

<p>Following examples illustrate the basic meaning of parameters in multivariable gaussian distribution:</p>

<pre><code class="language-text">mean = [0, 0]
cov = [[1, 0], [0, 1]]  # diagonal covariance
# Draw random samples from a multivariate normal distribution
x, y = np.random.multivariate_normal(mean, cov, 5000).T
plt.plot(x, y, &#39;x&#39;,color=&#39;y&#39;)
plt.axis(&#39;equal&#39;)
plt.hold

# change mean
mean = [0, 10]
x, y = np.random.multivariate_normal(mean, cov, 5000).T
plt.plot(x, y, &#39;.&#39;, color=&#39;b&#39;)

# change variances
mean = [10, 10]
cov = [[1, 0], [0, 10]]  # diagonal covariance
x, y = np.random.multivariate_normal(mean, cov, 5000).T
plt.plot(x, y, &#39;-&#39;, color=&#39;r&#39;)
plt.show()
</code></pre>

<p><img src="media/15045011015887/15045091983941.jpg" alt="demo of multivarible gaussian"/></p>

<h3 id="toc_8">Anomaly Detection using the Multivariate Gaussian Distribution</h3>

<p>When doing anomaly detection with multivariate gaussian distribution, we compute Œº and Œ£ normally. We then compute p(x) using the new formula in the previous section and flag an anomaly if p(x) &lt; œµ.</p>

<p>The original model for p(x) corresponds to a multivariate Gaussian where the contours of \(p(x;\mu,\Sigma)\) are axis-aligned.</p>

<p>The multivariate Gaussian model can automatically capture correlations between different features of x.</p>

<p>However, the original model maintains some advantages: it is computationally cheaper (no matrix to invert, which is costly for large number of features) and it performs well even with small training set size (in multivariate Gaussian model, it should be greater than the number of features for Œ£ to be invertible).</p>

<h2 id="toc_9">[2] Recommender Systems</h2>

<h3 id="toc_10">Problem Formulation</h3>

<p>Recommendation is currently a very popular application of machine learning.</p>

<p>Say we are trying to recommend movies to customers. We can use the following definitions</p>

<ul>
<li><p>\(n_u =\) number of users</p></li>
<li><p>\(n_m =\) number of movies</p></li>
<li><p>\(r(i,j) = 1\) if user \(j\) has rated movie \(i\)</p></li>
<li><p>\(y(i,j) =\) rating given by user \(j\) to movie \(i\) (defined only if \(r(i,j)=1\))</p></li>
</ul>

<h3 id="toc_11">Content Based Recommendations</h3>

<p>We can introduce two features, \(x_1\) and \(x_2\) which represents how much romance or how much action a movie may have (on a scale of 0‚àí1).</p>

<p>One approach is that we could do linear regression for every single user. For each user \(j\), learn a parameter \(\theta^{(j)} \in \mathbb{R}^3\). _Predict user \(j\) as rating movie \(i\) with \((\theta^{(j)})^Tx^{(i)}\) stars_.</p>

<ul>
<li><p>\(\theta^{(j)} =\) parameter vector for user \(j\)</p></li>
<li><p>\(x^{(i)} =\) feature vector for movie \(i\)</p></li>
</ul>

<p>For user \(j\), movie \(i\), predicted rating: \((\theta^{(j)})^T(x^{(i)})\)</p>

<ul>
<li>  \(m^{(j)} =\) number of movies rated by user \(j\)</li>
</ul>

<p>To learn \(\theta^{(j)}\), we do the following</p>

<p>\(min_{\theta^{(j)}} = \dfrac{1}{2}\displaystyle \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{k=1}^n(\theta_k^{(j)})^2\)</p>

<p>This is our familiar linear regression. The base of the first summation is choosing all \(i\) such that \(r(i,j) = 1\).</p>

<p>To get the parameters for all our users, we do the following:</p>

<p>\(min_{\theta^{(1)},\dots,\theta^{(n_u)}} = \dfrac{1}{2}\displaystyle \sum_{j=1}^{n_u} \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^2\)</p>

<p>We can apply our linear regression gradient descent update using the above cost function.</p>

<p>The only real difference is that we <strong>eliminate the constant</strong> \(\dfrac{1}{m}\).</p>

<h3 id="toc_12">Feature Finder</h3>

<p>It can be very difficult to find features such as &quot;amount of romance&quot; or &quot;amount of action&quot; in a movie. To figure this out, we can use <u>feature finders</u> .</p>

<p>We can let the users tell us how much they like the different genres, providing their parameter vector immediately for us.</p>

<p>To infer the features from given parameters, we use the squared error function with regularization over all the users:</p>

<p>\(min_{x^{(1)},\dots,x^{(n_m)}} \dfrac{1}{2} \displaystyle \sum_{i=1}^{n_m} \sum_{j:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2\)</p>

<p>You can also <strong>randomly guess</strong> the values for theta to guess the features repeatedly. You will actually converge to a good set of features.</p>

<h3 id="toc_13">Collaborative Filtering Algorithm</h3>

<p>To speed things up, we can simultaneously minimize our features and our parameters:</p>

<p><img src="media/15045011015887/Collaborative%20Filtering.png" alt="Collaborative Filtering"/></p>

<p>\(J(x,\theta) = \dfrac{1}{2} \displaystyle \sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2 + \dfrac{\lambda}{2}\sum_{j=1}^{n_u} \sum_{k=1}^{n} (\theta_k^{(j)})^2\)</p>

<p>It looks very complicated, but we&#39;ve only combined the cost function for \(\theta\) and the cost function for \(x\).</p>

<p>Because the algorithm can learn them itself, the bias units where \(x_0=1\) have been removed, therefore \(x\in \mathcal{R}^ n\) and \(\theta \in \mathcal{R} ^n\).</p>

<p>These are three steps in the algorithm:</p>

<ol>
<li><p>Initialize \(x^{(i)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}\) to small random values. This serves to break symmetry and ensures that the algorithm learns features \(x^{(i)},...,x^{(n_m)}\) that are different from each other.</p></li>
<li><p>Minimize \(J(x^{(i)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)})\) using gradient descent (or an advanced optimization algorithm).  E.g. for every \(j=1,...,n_u,i=1,...n_m\):</p>
<p>\(x_k^{(i)} := x_k^{(i)} - \alpha\left (\displaystyle \sum_{j:r(i,j)=1}{((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) \theta_k^{(j)}} + \lambda x_k^{(i)} \right)\)</p>
<p>\(\theta_k^{(j)} := \theta_k^{(j)} - \alpha\left (\displaystyle \sum_{i:r(i,j)=1}{((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) x_k^{(i)}} + \lambda \theta_k^{(j)} \right)\)</p></li>
<li><p>For a user with parameters Œ∏ and a movie with (learned) features x, predict a star rating of \(\theta^Tx\).</p></li>
</ol>

<h3 id="toc_14">Vectorization: Low Rank Matrix Factorization</h3>

<p>Given matrices X (each row containing features of a particular movie) and Œò (each row containing the weights for those features for a given user), then the full matrix Y of all predicted ratings of all movies by all users is given simply by: \(Y = X\Theta^T\).</p>

<p>Predicting how similar two movies i and j are can be done using the distance between their respective feature vectors x. Specifically, we are looking for a small value of \(||x^{(i)} - x^{(j)}||\).</p>

<h3 id="toc_15">Implementation Detail: Mean Normalization</h3>

<p>If the ranking system for movies is used from the previous lectures, then new users (who have watched no movies), will be assigned new movies incorrectly. Specifically, they will be assigned Œ∏ with all components equal to zero due to the minimization of the regularization term. That is, we assume that the new user will rank all movies 0, which does not seem intuitively correct.</p>

<p>We rectify this problem by normalizing the data relative to the mean. First, we use a matrix Y to store the data from previous ratings, where the ith row of Y is the ratings for the ith movie and the jth column corresponds to the ratings for the jth user.</p>

<p>We can now define a vector</p>

<p>\(\mu = [\mu_1, \mu_2, \dots , \mu_{n_m}]\)</p>

<p>such that</p>

<p>\(\mu_i = \frac{\sum_{j:r(i,j)=1}{Y_{i,j}}}{\sum_{j}{r(i,j)}}\)</p>

<p>Which is effectively the mean of the previous ratings for the ith movie (where only movies that have been watched by users are counted). We now can normalize the data by subtracting u, the mean rating, from the actual ratings for each user (column in matrix Y):</p>

<p>As an example, consider the following matrix Y and mean ratings Œº:</p>

<p>\(Y = \begin{bmatrix} 5 &amp; 5 &amp; 0 &amp; 0 \newline 4 &amp; ? &amp; ? &amp; 0 \newline 0 &amp; 0 &amp; 5 &amp; 4 \newline 0 &amp; 0 &amp; 5 &amp; 0 \newline \end{bmatrix}, \quad \mu = \begin{bmatrix} 2.5 \newline 2 \newline 2.25 \newline 1.25 \newline \end{bmatrix}\)</p>

<p>The resulting Y‚Ä≤ vector is:</p>

<p>\(Y&#39; = \begin{bmatrix} 2.5 &amp; 2.5 &amp; -2.5 &amp; -2.5 \newline 2 &amp; ? &amp; ? &amp; -2 \newline -.2.25 &amp; -2.25 &amp; 3.75 &amp; 1.25 \newline -1.25 &amp; -1.25 &amp; 3.75 &amp; -1.25 \end{bmatrix}\)</p>

<p>Now we must slightly modify the linear regression prediction to include the mean normalization term:</p>

<p>\((\theta^{(j)})^T x^{(i)} + \mu_i\)</p>

<p>Now, for a new user, the initial predicted values will be equal to the Œº term instead of simply being initialized to zero, which is more accurate.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/9</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>Êú∫Âô®Â≠¶‰π†</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all_7.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_9.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">‰ªñÂ±±‰πãÁü≥ÔºåÂèØ‰ª•ÊîªÁéâ</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href=".html"><strong>Leetcode</strong></a>
        
            <a href=".html"><strong>C/C++</strong></a>
        
            <a href=".html"><strong>PythonÊï∞ÊçÆÁªìÊûÑ‰∏éÁÆóÊ≥ï</strong></a>
        
            <a href=".html"><strong>Course</strong></a>
        
            <a href=".html"><strong>PythonÁâπÊÄß</strong></a>
        
            <a href=".html"><strong>Êú∫Âô®Â≠¶‰π†</strong></a>
        
            <a href=".html"><strong>PythonÁßëÂ≠¶ËÆ°ÁÆó‰∏âÁª¥ÂèØËßÜÂåñ</strong></a>
        
            <a href=".html"><strong>English</strong></a>
        
            <a href=".html"><strong>Computer System</strong></a>
        
            <a href=".html"><strong>Deep Learning</strong></a>
        
            <a href=".html"><strong>Linux Á≥ªÁªüÁºñÁ®ã</strong></a>
        
            <a href=".html"><strong>Êï∞ÊçÆÂ∫ì</strong></a>
        
            <a href=".html"><strong>Tensorflow</strong></a>
        
            <a href=".html"><strong>Big Data</strong></a>
        
            <a href=".html"><strong>ÊñáÁåÆÈòÖËØª</strong></a>
        
            <a href=".html"><strong>Tools</strong></a>
        
            <a href=".html"><strong>Â§ßÊï∞ÊçÆ</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15317356378033.html">Operating System Concepts - Introduction</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="Linking.html">CSAPP - ÈìæÊé•</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15315844145426.html">Intro to Hadoop and MapReduce</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="exceptional_control_flow.html">CSAPP - ÂºÇÂ∏∏ÊéßÂà∂ÊµÅ</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="overhead.html">Overhead</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
