<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Zhenhua Wang">
        <link rel="canonical" href="http://larryim.cc/note-os/bigdata/hadoop/ch3/">
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Chapter 3: The Hadoop Distributed FileSystem - Zhenhua's Notes</title>
        <link href="../../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/docco.min.css">
        <link href="../../../extra_css/custom.css" rel="stylesheet">
        <link href="../../../extra_css/custom.js" rel="stylesheet">
        <link href="../../../extra_css/friendly.css" rel="stylesheet">
        <link href="../../../extra_css/theme.css" rel="stylesheet">
        <link href="../../../extra_css/mkdocs/js/lunr-0.5.7.min.js" rel="stylesheet">
        <link href="../../../extra_css/mkdocs/js/mustache.min.js" rel="stylesheet">
        <link href="../../../extra_css/mkdocs/js/require.js" rel="stylesheet">
        <link href="../../../extra_css/mkdocs/js/search.js" rel="stylesheet">
        <link href="../../../extra_css/mkdocs/js/text.js" rel="stylesheet">
        <link href="../../../extra_css/code-tab.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Zhenhua's Notes</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="../../..">Home</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Algorithm <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../../algorithm/">Contents</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#">AlgorithmPrinceton</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../algorithm/algorithmPrinceton/">Contents</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmPrinceton/unionfind/">Topic 1: UnionFind</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmPrinceton/stackqueue/">Topic 2: StackQueue</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmPrinceton/sort/">Topic 3: Sort</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmPrinceton/priorityqueue/">Topic 4: PriorityQueues</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmPrinceton/symboltable/">Topic 5: Symbol Tables</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmPrinceton/bst/">Topic 6: Balanced Search Trees</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmPrinceton/hashtable/">Topic 7: Hash Table</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmPrinceton/graph/">Topic 8: Graph</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmPrinceton/mst_shortestpath/">Topic 9: Minimum Spanning Tree and Shortest Path</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmPrinceton/stringsort/">Topic 11: String Sort</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmPrinceton/tries/">Topic 12: Tries</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">AlgorithmStanford</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../algorithm/algorithmStanford/">Contents</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmStanford/dynamicprogramming/">Topic: Dynammic Programming</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmStanford/heap/">Topic: Heap</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmStanford/graph/">Topic: Graph</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmStanford/greedy/">Topic: Greedy Algorithm</a>
</li>
            
<li >
    <a href="../../../algorithm/algorithmStanford/hashtable/">Topic: Hash Table</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">CS61B</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../algorithm/cs61b/">Contents</a>
</li>
            
<li >
    <a href="../../../algorithm/cs61b/Lab1/">Lab1: javac, java, git</a>
</li>
            
<li >
    <a href="../../../algorithm/cs61b/Lab2/">Lab2: Unit Testing with JUnit and IntLists</a>
</li>
            
<li >
    <a href="../../../algorithm/cs61b/Lab3/">Lab3: Unit Testing with JUnit, Debugging</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">SWORD</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../algorithm/sword/">Contents</a>
</li>
            
<li >
    <a href="../../../algorithm/sword/solution/">剑指Offer题解</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">OS <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../../os/">Contents</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#">OSC</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../os/osc/">Contents</a>
</li>
            
<li >
    <a href="../../../os/osc/ch1/">Chapter 1: Introduction </a>
</li>
            
<li >
    <a href="../../../os/osc/ch2/">Chapter 2: Operating System structures</a>
</li>
            
<li >
    <a href="../../../os/osc/ch3/">Chapter 3: Processes</a>
</li>
            
<li >
    <a href="../../../os/osc/ch4/">Chapter 4: Threads and Concurrency</a>
</li>
            
<li >
    <a href="../../../os/osc/ch5/">Chapter 5: CPU Scheduling</a>
</li>
            
<li >
    <a href="../../../os/osc/ch6/">Chapter 6: Synchronization Tools</a>
</li>
            
<li >
    <a href="../../../os/osc/ch7/">Chapter 7: Synchronization Examples</a>
</li>
            
<li >
    <a href="../../../os/osc/ch8/">Chapter 8: Deadlocks</a>
</li>
            
<li >
    <a href="../../../os/osc/ch9/">Chapter 9: Main Memory</a>
</li>
            
<li >
    <a href="../../../os/osc/ch10/">Chapter 10: Virtual Memory</a>
</li>
            
<li >
    <a href="../../../os/osc/ch11/">Chapter 11: Mass-Storage Structure</a>
</li>
            
<li >
    <a href="../../../os/osc/ch13/">Chapter 13: File-System Interfaces</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">CSAPP</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../os/csapp/">Contents</a>
</li>
            
<li >
    <a href="../../../os/csapp/ch1/">Chapter 1: 计算机系统漫游</a>
</li>
            
<li >
    <a href="../../../os/csapp/ch2/">Chapter 2: 信息的表示和处理</a>
</li>
            
<li >
    <a href="../../../os/csapp/ch3/">Chapter 3: 程序的机器级表示</a>
</li>
            
<li >
    <a href="../../../os/csapp/ch4/">Chapter 4: 处理器体系结构</a>
</li>
            
<li >
    <a href="../../../os/csapp/ch5/">Chapter 5: 优化程序性能</a>
</li>
            
<li >
    <a href="../../../os/csapp/ch6/">Chapter 6: 存储器层次结构</a>
</li>
            
<li >
    <a href="../../../os/csapp/ch7/">Chapter 7: 链接</a>
</li>
            
<li >
    <a href="../../../os/csapp/ch8/">Chapter 8: 异常控制流</a>
</li>
            
<li >
    <a href="../../../os/csapp/ch9/">Chapter 9: 虚拟内存</a>
</li>
            
<li >
    <a href="../../../os/csapp/ch10/">Chapter 10: 系统级I/O</a>
</li>
            
<li >
    <a href="../../../os/csapp/ch11/">Chapter 11: 网络编程</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">DataBase <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../../database/">Contents</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#">MySql</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../database/mysql/">Contents</a>
</li>
            
<li >
    <a href="../../../database/mysql/LearningMySQLandMariaDB/">Chapter Learning MySQL and MariaDB</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Java <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../../java/">Contents</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#">HFJ</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../java/hfj/">Contents</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch1/">Chapter 1: Dive in A Quick Dip</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch2/">Chapter 2: Classes and Objects</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch3/">Chapter 3: Primitives and References</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch4/">Chapter 4: Methods use Instance Variables</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch5/">Chapter 5: Writing a Program</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch6/">Chapter 6: Get to Know the Java API</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch7/">Chapter 7: Inheritance and Polymorphism</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch8/">Chapter 8: Interfaces and Abstract Classes</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch9/">Chapter 9: Constructors and Garbage Collection</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch10/">Chapter 10: Numbers and Statics</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch11/">Chapter 11: Exception Handling</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch12/">Chapter 12: Getting GUI</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch13/">Chapter 13: Using Swing</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch14/">Chapter 14: Serialization and File I/O</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch15/">Chapter 15: Networking and Threads</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch16/">Chapter 16: Collections and Generics</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch17/">Chapter 17: Packages, Jars and Deployment</a>
</li>
            
<li >
    <a href="../../../java/hfj/ch18/">Chapter 18: Remote deploy with RMI</a>
</li>
            
<li >
    <a href="../../../java/hfj/Appendix/">Appendix: The Top Ten Topics</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">HFDP</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../java/hfdp/">Contents</a>
</li>
            
<li >
    <a href="../../../java/hfdp/ch1/">Chapter 1: Strategy Pattern </a>
</li>
            
<li >
    <a href="../../../java/hfdp/ch2/">Chapter 2: Observer Pattern</a>
</li>
            
<li >
    <a href="../../../java/hfdp/ch3/">Chapter 3: Decorator Pattern </a>
</li>
            
<li >
    <a href="../../../java/hfdp/ch4/">Chapter 4: Factory Pattern</a>
</li>
            
<li >
    <a href="../../../java/hfdp/ch5/">Chapter 5: Singleton Pattern</a>
</li>
            
<li >
    <a href="../../../java/hfdp/ch6/">Chapter 6: Command Pattern</a>
</li>
            
<li >
    <a href="../../../java/hfdp/ch7/">Chapter 7: Adapter and Facade Patterns</a>
</li>
            
<li >
    <a href="../../../java/hfdp/ch8/">Chapter 8: Template Method Pattern</a>
</li>
            
<li >
    <a href="../../../java/hfdp/ch9/">Chapter 9: Iterator and Composite Patterns</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">MultiThreading</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../java/multithreading/">Contents</a>
</li>
            
<li >
    <a href="../../../java/multithreading/pre1/">序章1 Java线程</a>
</li>
            
<li >
    <a href="../../../java/multithreading/pre2/">序章2 多线程程序的评价标准</a>
</li>
            
<li >
    <a href="../../../java/multithreading/ch1/">第1章 Single Threaded Execution模式</a>
</li>
            
<li >
    <a href="../../../java/multithreading/ch1/">第2章 Immutable模式</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">TIJ</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../java/tij/">Contents</a>
</li>
            
<li >
    <a href="../../../java/tij/ch1/">Chapter 1: Introduction</a>
</li>
            
<li >
    <a href="../../../java/tij/ch2/">Chapter 2: Introduction to Objects</a>
</li>
            
<li >
    <a href="../../../java/tij/ch3/">Chapter 3: Everything is an Object</a>
</li>
            
<li >
    <a href="../../../java/tij/ch4/">Chapter 4: Opertors</a>
</li>
            
<li >
    <a href="../../../java/tij/ch5/">Chapter 5: Controlling Execution</a>
</li>
            
<li >
    <a href="../../../java/tij/ch6/">Chapter 6: Initialization & Cleanup</a>
</li>
            
<li >
    <a href="../../../java/tij/ch7/">Chapter 7: Access Control</a>
</li>
            
<li >
    <a href="../../../java/tij/ch8/">Chapter 8: Reusing Clases</a>
</li>
            
<li >
    <a href="../../../java/tij/ch9/">Chapter 9: Polymorphism</a>
</li>
            
<li >
    <a href="../../../java/tij/ch10/">Chapter 10: Interfaces</a>
</li>
            
<li >
    <a href="../../../java/tij/ch11/">Chapter 11: Inner Classes</a>
</li>
            
<li >
    <a href="../../../java/tij/ch12/">Chapter 12: Holding Your Objects</a>
</li>
            
<li >
    <a href="../../../java/tij/ch13/">Chapter 13: Error Handling with Exceptions</a>
</li>
            
<li >
    <a href="../../../java/tij/ch14/">Chapter 14: Strings</a>
</li>
            
<li >
    <a href="../../../java/tij/ch15/">Chapter 15: Type Information</a>
</li>
            
<li >
    <a href="../../../java/tij/ch16/">Chapter 16: Generics</a>
</li>
            
<li >
    <a href="../../../java/tij/ch17/">Chapter 17: Arrays</a>
</li>
            
<li >
    <a href="../../../java/tij/ch18/">Chapter 18: Containers in Depth</a>
</li>
            
<li >
    <a href="../../../java/tij/ch19/">Chapter 19: I/O</a>
</li>
            
<li >
    <a href="../../../java/tij/ch20/">Chapter 20: Enumerated Types</a>
</li>
            
<li >
    <a href="../../../java/tij/ch21/">Chapter 21: Annotations</a>
</li>
            
<li >
    <a href="../../../java/tij/ch22/">Chapter 22: Concurrency</a>
</li>
            
<li >
    <a href="../../../java/tij/ch23/">Chapter 23: Graphical User Interfaces</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">UJVM</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../../java/ujvm/">Contents</a>
</li>
            
<li >
    <a href="../../../java/ujvm/ch1/">Chapter 1 : 走进Java</a>
</li>
            
<li >
    <a href="../../../java/ujvm/ch2/">Chapter 2 : Java内存区域与内存溢出正常</a>
</li>
            
<li >
    <a href="../../../java/ujvm/ch3/">Chapter 3 : 垃圾收集器与内存分配策略</a>
</li>
            
<li >
    <a href="../../../java/ujvm/ch4/">Chapter 4 : 虚拟机性能监控与故障处理工具</a>
</li>
            
<li >
    <a href="../../../java/ujvm/ch5/">Chapter 5 : 调优案例分析与实战</a>
</li>
            
<li >
    <a href="../../../java/ujvm/ch6/">Chapter 6 : 类文件结构</a>
</li>
            
<li >
    <a href="../../../java/ujvm/ch7/">Chapter 7 : 虚拟机类加载机制</a>
</li>
            
<li >
    <a href="../../../java/ujvm/ch8/">Chapter 8 : 虚拟机字节码执行引擎</a>
</li>
            
<li >
    <a href="../../../java/ujvm/ch9/">Chapter 9 : 类加载及执行子系统的案例与实战</a>
</li>
            
<li >
    <a href="../../../java/ujvm/ch10/">Chapter 10 : 早期(编译期)优化</a>
</li>
            
<li >
    <a href="../../../java/ujvm/ch11/">Chapter 11 : 晚期(运行期)优化</a>
</li>
            
<li >
    <a href="../../../java/ujvm/ch12/">Chapter 12 : Java内存模型与线程</a>
</li>
            
<li >
    <a href="../../../java/ujvm/ch13/">Chapter 13 : 线程安全与锁优化</a>
</li>
            
<li >
    <a href="../../../java/ujvm/AppendixC/">Appendix HotSpot虚拟机主要参数列表</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">BigData <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../">Contents</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#">HADOOP</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../">Contents</a>
</li>
            
<li >
    <a href="../ch1/">Chapter 1: Meet Hadoop</a>
</li>
            
<li >
    <a href="../ch2/">Chapter 2: MapReduce</a>
</li>
            
<li class="active">
    <a href="./">Chapter 3: The Hadoop Distributed FileSystem</a>
</li>
            
<li >
    <a href="../ch4/">Chapter 4: YARN</a>
</li>
            
<li >
    <a href="../ch5/">Chapter 5: Hadoop I/O</a>
</li>
            
<li >
    <a href="../ch6/">Chapter 6: Developing a MapReduce Application</a>
</li>
            
<li >
    <a href="../ch7/">Chapter 7: How MapReduce Works</a>
</li>
            
<li >
    <a href="../ch8/">Chapter 8: MapReduce Types and Formats</a>
</li>
            
<li >
    <a href="../ch9/">Chapter 9: MapReduce Features</a>
</li>
            
<li >
    <a href="../ch10/">Chapter 10: Setting Up a Hadoop Cluster</a>
</li>
            
<li >
    <a href="../ch11/">Chapter 11: Adminstering Hadoop</a>
</li>
            
<li >
    <a href="../ch12/">Chapter 12: Avro</a>
</li>
            
<li >
    <a href="../ch13/">Chapter 13: Parquet</a>
</li>
            
<li >
    <a href="../ch14/">Chapter 14: Flume</a>
</li>
            
<li >
    <a href="../ch15/">Chapter 15: Sqoop</a>
</li>
            
<li >
    <a href="../ch16/">Chapter 16: Pig</a>
</li>
            
<li >
    <a href="../ch17/">Chapter 17: Hive</a>
</li>
            
<li >
    <a href="../ch18/">Chapter 18: Crunch</a>
</li>
            
<li >
    <a href="../ch19/">Chapter 19: Spark</a>
</li>
            
<li >
    <a href="../ch20/">Chapter 20: HBase</a>
</li>
            
<li >
    <a href="../ch21/">Chapter 21: ZooKeeper</a>
</li>
            
<li >
    <a href="../ch22/">Chapter 22: Composable Data at Center</a>
</li>
            
<li >
    <a href="../ch23/">Chapter 23: Biological Data Science: Saving Lives with Software</a>
</li>
            
<li >
    <a href="../ch24/">Chapter 24: Cascading</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">Spark</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../spark/">Contents</a>
</li>
            
<li >
    <a href="../../spark/ch1/">Chapter 1: Introduction to Data Analysis with Spark</a>
</li>
            
<li >
    <a href="../../spark/ch2/">Chapter 2: Downloading Spark and Getting Started</a>
</li>
            
<li >
    <a href="../../spark/ch3/">Chapter 3: Programming with RDDs</a>
</li>
            
<li >
    <a href="../../spark/ch4/">Chapter 4: Working with Key/Value Pairs</a>
</li>
            
<li >
    <a href="../../spark/ch5/">Chapter 5: Loading and Saving Your Data</a>
</li>
            
<li >
    <a href="../../spark/ch6/">Chapter 6: Advanced Spark Programming</a>
</li>
            
<li >
    <a href="../../spark/ch7/">Chapter 7: Running on a Cluster</a>
</li>
            
<li >
    <a href="../../spark/ch8/">Chapter 8: Tuning and Debugging Spark</a>
</li>
            
<li >
    <a href="../../spark/ch9/">Chapter 9: Spark SQL</a>
</li>
            
<li >
    <a href="../../spark/ch10/">Chapter 10: Spark Streaming</a>
</li>
            
<li >
    <a href="../../spark/ch11/">Chapter 11: Machine Learning with MLlib</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">GDM</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../gdm/">Contents</a>
</li>
            
<li >
    <a href="../../gdm/ch1/">Chapter 1: 推荐系统入门</a>
</li>
            
<li >
    <a href="../../gdm/ch2/">Chapter 2: 隐式评价和基于物品的过滤算法</a>
</li>
            
<li >
    <a href="../../gdm/ch3/">Chapter 3: 分类</a>
</li>
            
<li >
    <a href="../../gdm/ch4/">Chapter 4: 进一步探索分类</a>
</li>
            
<li >
    <a href="../../gdm/ch5/">Chapter 5: 概率和朴素贝叶斯</a>
</li>
            
<li >
    <a href="../../gdm/ch6/">Chapter 6: 朴素贝叶斯和文本数据</a>
</li>
            
<li >
    <a href="../../gdm/ch7/">Chapter 7: 聚类</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">MLIA</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../mlia/">Contents</a>
</li>
            
<li >
    <a href="../../mlia/ch1/">Chapter 1: 机器学习基础</a>
</li>
            
<li >
    <a href="../../mlia/ch2/">Chapter 2: k-近邻算法</a>
</li>
            
<li >
    <a href="../../mlia/ch3/">Chapter 3: 决策树</a>
</li>
            
<li >
    <a href="../../mlia/ch4/">Chapter 4: 基于概率论的分类方法：朴素贝叶斯</a>
</li>
            
<li >
    <a href="../../mlia/ch5/">Chapter 5: Logistic回归</a>
</li>
            
<li >
    <a href="../../mlia/ch6/">Chapter 6: 支持向量机</a>
</li>
            
<li >
    <a href="../../mlia/ch7/">Chapter 7: 利用AdaBoost元算法提高分类性能</a>
</li>
            
<li >
    <a href="../../mlia/ch8/">Chapter 8: 预测数值型数据：回归</a>
</li>
            
<li >
    <a href="../../mlia/ch9/">Chapter 9: 树回归</a>
</li>
            
<li >
    <a href="../../mlia/ch10/">Chapter 10: 利用Ｋ-均值聚类算法对未标注数据分组</a>
</li>
            
<li >
    <a href="../../mlia/ch11/">Chapter 11: 使用Apriori算法进行关联分析</a>
</li>
            
<li >
    <a href="../../mlia/ch11/">Chapter 12: 使用FP-growth算法来高效发现频繁项集</a>
</li>
            
<li >
    <a href="../../mlia/ch13/">Chapter 13: 利用PCA来简化数据</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">Crawler</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../crawler/">Contents</a>
</li>
            
<li >
    <a href="../../crawler/ch1/">Chapter 1: 开发环境配置</a>
</li>
            
<li >
    <a href="../../crawler/ch2/">Chapter 2: 爬虫基础</a>
</li>
            
<li >
    <a href="../../crawler/ch3/">Chapter 3: 基本库的使用</a>
</li>
            
<li >
    <a href="../../crawler/ch4/">Chapter 4: 解析库的使用</a>
</li>
            
<li >
    <a href="../../crawler/ch5/">Chapter 5: 数据存储</a>
</li>
            
<li >
    <a href="../../crawler/ch7/">Chapter 7: 动态渲染页面爬取</a>
</li>
            
<li >
    <a href="../../crawler/ch13/">Chapter 13: Scrapy框架的使用</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">Projects</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../projects/">Contents</a>
</li>
            
<li >
    <a href="../../projects/SparkStreaming实时流处理项目/">SparkStreaming实时流处理</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                            <li >
                                <a href="../../../books/">Books</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>

                     <!--
                            <li >
                                <a rel="next" href="../ch2/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../ch4/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    -->
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#hadoop-the-definitive-guide-3-the-hadoop-distributed-filesystem">Hadoop: The Definitive Guide 3 - The Hadoop Distributed FileSystem</a></li>
        <li class="main "><a href="#1-the-design-of-hdfs">1 The Design of HDFS</a></li>
        <li class="main "><a href="#2-hdfs-concepts">2 HDFS Concepts</a></li>
            <li><a href="#blocks">Blocks</a></li>
            <li><a href="#namenodes-and-datanodes">Namenodes and Datanodes</a></li>
            <li><a href="#block-caching">Block Caching</a></li>
            <li><a href="#hdfs-federation">HDFS Federation</a></li>
            <li><a href="#hdfs-high-availability">HDFS High Availability</a></li>
        <li class="main "><a href="#3-the-command-line-interface">3 The Command-Line Interface</a></li>
            <li><a href="#basic-filesystem-operations">Basic Filesystem Operations</a></li>
        <li class="main "><a href="#4-hadoop-filesystems">4 Hadoop Filesystems</a></li>
            <li><a href="#http">HTTP</a></li>
        <li class="main "><a href="#5-the-java-interface">5 The Java Interface</a></li>
            <li><a href="#reading-data-from-a-hadoop-url">Reading Data from a Hadoop URL</a></li>
            <li><a href="#reading-data-using-the-filesystem-api">Reading Data Using the FileSystem API</a></li>
            <li><a href="#fsdatainputstream">FSDataInputStream</a></li>
            <li><a href="#writing-data">Writing Data</a></li>
            <li><a href="#fsdataoutputstream">FSDataOutputStream</a></li>
            <li><a href="#directories">Directories</a></li>
            <li><a href="#querying-the-filesystem">Querying the Filesystem</a></li>
            <li><a href="#deleting-data">Deleting Data</a></li>
        <li class="main "><a href="#6-data-flow">6 Data Flow</a></li>
            <li><a href="#anatomy-of-a-file-read">Anatomy of a File Read</a></li>
            <li><a href="#anatomy-of-a-file-write">Anatomy of a File Write</a></li>
            <li><a href="#coherency-model">Coherency Model</a></li>
        <li class="main "><a href="#7-parallel-copying-with-distcp">7 Parallel Copying with distcp</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h3 id="hadoop-the-definitive-guide-3-the-hadoop-distributed-filesystem"><strong>Hadoop: The Definitive Guide 3 - The Hadoop Distributed FileSystem</strong><a class="headerlink" href="#hadoop-the-definitive-guide-3-the-hadoop-distributed-filesystem" title="Permanent link">&para;</a></h3>
<p>Filesystems that manage the storage across a network of machines are called <strong><em>distributed filesystems</em></strong> . Hadoop comes with a distributed filesystem called HDFS, which stands for <strong><em>Hadoop Distributed Filesystem</em></strong> .</p>
<h3 id="1-the-design-of-hdfs">1 The Design of HDFS<a class="headerlink" href="#1-the-design-of-hdfs" title="Permanent link">&para;</a></h3>
<p>HDFS is a filesystem designed for storing very large files with streaming data access patterns, running on clusters of commodity hardware.</p>
<ul>
<li>Very large files: files that are hundreds of megabytes, gigabytes, or terabytes in size.</li>
<li>Streaming data access: HDFS is built around the idea that the most efficient data processing pattern is a <strong><em>write-once, read-many-times</em></strong> pattern.</li>
<li>Commodity hardware: It’s designed to run on clusters of commodity hardware.</li>
</ul>
<p>These are areas where HDFS is not a good fit today:</p>
<ul>
<li>Low-latency data access</li>
<li>Lots of small files</li>
<li>Multiple writers, arbitrary file modifications</li>
</ul>
<h3 id="2-hdfs-concepts">2 HDFS Concepts<a class="headerlink" href="#2-hdfs-concepts" title="Permanent link">&para;</a></h3>
<h4 id="blocks">Blocks<a class="headerlink" href="#blocks" title="Permanent link">&para;</a></h4>
<p>A disk has a block size, which is the minimum amount of data that it can read or write. Filesystems for a single disk build on this by dealing with data in blocks, which are an integral multiple of the disk block size.</p>
<p>HDFS, too, has the concept of a <strong><em>block</em></strong>, but it is a much larger unit — 128 MB by default (typically a few kilobytes for ordinary file system). Unlike a filesystem for a single disk, a file in HDFS that is smaller than a single block does not occupy a full block’s worth of underlying storage. (For example, a 1 MB file stored with a block size of 128 MB uses 1 MB of disk space, not 128 MB.)</p>
<div class="admonition question">
<p class="admonition-title">Question</p>
<p>WHY IS A BLOCK IN HDFS SO LARGE? To minimize the cost of seeks.</p>
</div>
<p>Having a block abstraction for a distributed filesystem brings several benefits.</p>
<ul>
<li>A file can be larger than any single disk in the network.</li>
<li>Making the unit of abstraction a block rather than a file simplifies the storage subsystem.<ul>
<li>storage management: because blocks are a fixed size, it is easy to calculate how many can be stored on a given disk.</li>
<li>metadata concerns: because blocks are just chunks of data to be stored, file metadata such as permissions information does not need to be stored with the blocks.</li>
</ul>
</li>
<li>Blocks fit well with replication for providing fault tolerance and availability.<ul>
<li>To insure against corrupted blocks and disk and machine failure, each block is replicated to a small number of physically separate machines (typically three).</li>
</ul>
</li>
</ul>
<h4 id="namenodes-and-datanodes">Namenodes and Datanodes<a class="headerlink" href="#namenodes-and-datanodes" title="Permanent link">&para;</a></h4>
<p>An HDFS cluster has two types of nodes: a <strong><em>namenode</em></strong> (the master) and a number of <strong><em>datanodes</em></strong> (workers). </p>
<ul>
<li>The namenode manages the filesystem namespace. It maintains the filesystem tree and the metadata for all the files and directories in the tree. This information is stored persistently on the local disk in the form of two files: the namespace image and the edit log.</li>
<li>The namenode also knows the datanodes on which all the blocks for a given file are located;</li>
<li>Datanodes are the workhorses of the filesystem. They store and retrieve blocks when they are told to (by clients or the namenode), and they report back to the namenode periodically with lists of blocks that they are storing.</li>
</ul>
<p>If the machine running the namenode were obliterated, all the files on the filesystem would be lost since there would be no way of knowing how to reconstruct the files from the blocks on the datanodes. Possible solution:</p>
<ul>
<li>to back up the files that make up the persistent state of the filesystem metadata.</li>
<li>to run a secondary namenode, which keeps a copy of the merged namespace image.</li>
</ul>
<h4 id="block-caching">Block Caching<a class="headerlink" href="#block-caching" title="Permanent link">&para;</a></h4>
<p>For frequently accessed files, the blocks may be <em>explicitly</em> cached in the datanode’s memory, in an off-heap block cache. Users or applications instruct the namenode which files to cache (and for how long) by adding a <em>cache directive</em> to a <em>cache pool</em>.</p>
<h4 id="hdfs-federation">HDFS Federation<a class="headerlink" href="#hdfs-federation" title="Permanent link">&para;</a></h4>
<p>Problem: On very large clusters with many files, memory becomes the limiting factor for scaling, since namenode keeps a reference to every file and block in the filesystem in memory.</p>
<p>For example, a 200-node cluster with 24 TB of disk space per node, a block size of 128 MB, and a replication factor of 3 has room for about 2 million blocks (or more): <span><span class="MathJax_Preview">200\times 24TB⁄(128MB×3)</span><script type="math/tex">200\times 24TB⁄(128MB×3)</script></span>, So in this case, setting the namenode memory to 12,000 MB would be a good starting point.</p>
<p>Solution: HDFS federation, allows a cluster to scale by adding namenodes, each of which manages a portion of the filesystem namespace.</p>
<h4 id="hdfs-high-availability">HDFS High Availability<a class="headerlink" href="#hdfs-high-availability" title="Permanent link">&para;</a></h4>
<p>To remedy a failed namenode, a pair of namenodes in an <strong><em>active-standby</em></strong> configuration is introduced in Hadoop 2. In the event of the failure of the active namenode, the standby takes over its duties to continue servicing client requests <em>without</em> a significant interruption.</p>
<h3 id="3-the-command-line-interface">3 The Command-Line Interface<a class="headerlink" href="#3-the-command-line-interface" title="Permanent link">&para;</a></h3>
<h4 id="basic-filesystem-operations">Basic Filesystem Operations<a class="headerlink" href="#basic-filesystem-operations" title="Permanent link">&para;</a></h4>
<p>Hadoop’s filesystem shell command is <C>fs</C>, which supports a number of subcommands (type <c>hadoop fs -help</C> to get detailed help).</p>
<p>Copying a file from the local filesystem to HDFS:</p>
<p> <div class=codehilite><pre><span class=c1>#The local file is copied tothe HDFS instance running on localhost.</span>
$ hadoop fs -copyFromLocal test.copy /test.copy
<span class=c1># works as the same</span>
$ hadoop fs -copyFromLocal test.copy hdfs://localhost:9000/test2.copy
</pre></div></p>
<p>Copying the file from the HDFS to the local filesystem:</p>
<p> <div class=codehilite><pre>$ hadoop fs -copyToLocal /test.copy test.copy.txt
</pre></div></p>
<h3 id="4-hadoop-filesystems">4 Hadoop Filesystems<a class="headerlink" href="#4-hadoop-filesystems" title="Permanent link">&para;</a></h3>
<p>Hadoop has an abstract notion of filesystems, of which HDFS is just one implementation. The Java abstract class <C>org.apache.hadoop.fs.FileSystem</C> represents the client interface to a filesystem in Hadoop, and there are several concrete implementations.</p>
<table>
<thead>
<tr>
<th>Filesystem</th>
<th>URI scheme</th>
<th>Java implementation</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Local</td>
<td>file</td>
<td>fs.LocalFileSystem</td>
<td>A filesystem for a locally connected disk with client-side checksums</td>
</tr>
<tr>
<td>HDFS</td>
<td>hfs</td>
<td>hdfs.DistributedFileSystem</td>
<td>Hadoop’s distributed filesystem</td>
</tr>
<tr>
<td>WebHDFS</td>
<td>webhdfs</td>
<td>hdfs.web.WebHdfsFileSystem</td>
<td>Providing authenticated read/write access to HDFS over HTTP.</td>
</tr>
<tr>
<td>Secure WebHDFS</td>
<td>swebhdfs</td>
<td>hdfs.web.SWebHdfsFileSystem</td>
<td>The HTTPS version of WebHDFS.</td>
</tr>
</tbody>
</table>
<p>When you are processing large volumes of data you should choose a distributed filesystem that has the data locality optimization, notably HDFS.</p>
<h4 id="http">HTTP<a class="headerlink" href="#http" title="Permanent link">&para;</a></h4>
<p>The HTTP REST API exposed by the WebHDFS protocol makes it easier for other languages to interact with HDFS. Note that the HTTP interface is slower than the native Java client, so should be avoided for very large data transfers if possible.</p>
<p>There are two ways of accessing HDFS over HTTP:</p>
<ul>
<li>Directly, where the HDFS daemons serve HTTP requests to clients;</li>
<li>Via a proxy (or proxies), which accesses HDFS on the client’s behalf using the usual DistributedFileSystem API.</li>
</ul>
<p><img alt="" src="../figures/AccessingHdfsOverHttpOrHdfsProxies.jpg" /></p>
<p>HDFS proxy allows for stricter firewall and bandwidth-limiting policies to be put in place. It’s common to use a proxy for transfers between Hadoop clusters located in different data centers, or when accessing a Hadoop cluster running in the cloud from an external network.</p>
<h3 id="5-the-java-interface">5 The Java Interface<a class="headerlink" href="#5-the-java-interface" title="Permanent link">&para;</a></h3>
<p>Hadoop <C>FileSystem</C> class is the API for interacting with one of Hadoop’s filesystems. In general you should strive to <strong><em>write your code against the <C>FileSystem</C> abstract class</em></strong> , to retain portability across filesystems. This is very useful when testing your program, for example, because you can rapidly run tests using data stored on the local filesystem.</p>
<h4 id="reading-data-from-a-hadoop-url">Reading Data from a Hadoop URL<a class="headerlink" href="#reading-data-from-a-hadoop-url" title="Permanent link">&para;</a></h4>
<p>NOT recommended, because <C>setURLStreamHandlerFactory()</C> method can be called only once per JVM, which means that if some other part of your program sets it, you won't be able to use.</p>
<h4 id="reading-data-using-the-filesystem-api">Reading Data Using the FileSystem API<a class="headerlink" href="#reading-data-using-the-filesystem-api" title="Permanent link">&para;</a></h4>
<p>A file in a Hadoop filesystem is represented by a Hadoop <C>Path</C> object(<C>org.apache.hadoop.fs.Path</C>, not <C>java.io.File</C>). You can think of a <C>Path</C>  as a Hadoop filesystem URI, such as <code class="codehilite">hdfs://localhost/user/tom/test.copy</code></p>
<p>Since <C>FileSystem</C> is a general filesystem API, so the first step is to retrieve an instance for the filesystem we want. There are several static factory methods for getting a <C>FileSystem</C> instance:</p>
<p> <div class=codehilite><pre><span class=c1>// Returns the default filesystem</span>
<span class=kd>public</span> <span class=kd>static</span> <span class=n>FileSystem</span> <span class=nf>get</span><span class=o>(</span><span class=n>Configuration</span> <span class=n>conf</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span> 
<span class=c1>// Uses the given URI’s scheme and authority to determine the filesystem to use</span>
<span class=kd>public</span> <span class=kd>static</span> <span class=n>FileSystem</span> <span class=nf>get</span><span class=o>(</span><span class=n>URI</span> <span class=n>uri</span><span class=o>,</span> <span class=n>Configuration</span> <span class=n>conf</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span> 
<span class=c1>// Retrieves the filesystem as the given user</span>
<span class=kd>public</span> <span class=kd>static</span> <span class=n>FileSystem</span> <span class=nf>get</span><span class=o>(</span><span class=n>URI</span> <span class=n>uri</span><span class=o>,</span> <span class=n>Configuration</span> <span class=n>conf</span><span class=o>,</span> <span class=n>String</span> <span class=n>user</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
<span class=c1>// Retrieves a local filesystem instance</span>
<span class=kd>public</span> <span class=kd>static</span> <span class=n>LocalFileSystem</span> <span class=nf>getLocal</span><span class=o>(</span><span class=n>Configuration</span> <span class=n>conf</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
</pre></div></p>
<p>A <C>Configuration</C> object encapsulates a client or server's configuration, which is set using configuration files read from the classpath, such as <code class="codehilite">etc/hadoop/core-site.xml</code>.</p>
<p>With a <C>FileSystem</C> instance in hand, we invoke an <C>open()</C> method to get the input stream for a file:</p>
<p> <div class=codehilite><pre><span class=c1>// Uses a default buffer size of 4 KB</span>
<span class=kd>public</span> <span class=n>FSDataInputStream</span> <span class=nf>open</span><span class=o>(</span><span class=n>Path</span> <span class=n>f</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
<span class=c1>// Uses a buffer size of bufferSize</span>
<span class=kd>public</span> <span class=kd>abstract</span> <span class=n>FSDataInputStream</span> <span class=nf>open</span><span class=o>(</span><span class=n>Path</span> <span class=n>f</span><span class=o>,</span> <span class=kt>int</span> <span class=n>bufferSize</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
</pre></div></p>
<p>Displaying files from a Hadoop filesystem on standard output by using the FileSystem directly:</p>
<p> <div class=codehilite><pre><span class=c1>// $ hdfs://localhost:9000/test2.copy</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.conf.Configuration</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.fs.FileSystem</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.fs.Path</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.io.IOUtils</span><span class=o>;</span>

<span class=kn>import</span> <span class=nn>java.io.InputStream</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>java.net.URI</span><span class=o>;</span>

<span class=kd>public</span> <span class=kd>class</span> <span class=nc>FileSystemCat</span> <span class=o>{</span>
    <span class=kd>public</span> <span class=kd>static</span> <span class=kt>void</span> <span class=nf>main</span><span class=o>(</span><span class=n>String</span><span class=o>[]</span> <span class=n>args</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>Exception</span> <span class=o>{</span>
        <span class=n>String</span> <span class=n>uri</span> <span class=o>=</span> <span class=n>args</span><span class=o>[</span><span class=mi>0</span><span class=o>];</span>
        <span class=n>Configuration</span> <span class=n>conf</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Configuration</span><span class=o>();</span>
        <span class=n>FileSystem</span> <span class=n>fs</span> <span class=o>=</span> <span class=n>FileSystem</span><span class=o>.</span><span class=na>get</span><span class=o>(</span><span class=n>URI</span><span class=o>.</span><span class=na>create</span><span class=o>(</span><span class=n>uri</span><span class=o>),</span> <span class=n>conf</span><span class=o>);</span>
        <span class=n>InputStream</span> <span class=n>in</span> <span class=o>=</span> <span class=kc>null</span><span class=o>;</span>
        <span class=k>try</span> <span class=o>{</span>
            <span class=n>in</span> <span class=o>=</span> <span class=n>fs</span><span class=o>.</span><span class=na>open</span><span class=o>(</span><span class=k>new</span> <span class=n>Path</span><span class=o>(</span><span class=n>uri</span><span class=o>));</span>
            <span class=n>IOUtils</span><span class=o>.</span><span class=na>copyBytes</span><span class=o>(</span><span class=n>in</span><span class=o>,</span> <span class=n>System</span><span class=o>.</span><span class=na>out</span><span class=o>,</span> <span class=mi>4096</span><span class=o>,</span> <span class=kc>false</span><span class=o>);</span>
        <span class=o>}</span> <span class=k>finally</span> <span class=o>{</span>
            <span class=n>IOUtils</span><span class=o>.</span><span class=na>closeStream</span><span class=o>(</span><span class=n>in</span><span class=o>);</span>
        <span class=o>}</span>
    <span class=o>}</span>
<span class=o>}</span>
</pre></div></p>
<h4 id="fsdatainputstream">FSDataInputStream<a class="headerlink" href="#fsdatainputstream" title="Permanent link">&para;</a></h4>
<p>The <C>open()</C> method on <C>FileSystem</C> actually returns an <C>FSDataInputStream</C> rather than a standard java.io class. This class is a specialization of <C>java.io.DataInputStream</C> with support for random access, so you can read from any part of the stream:</p>
<p><img alt="FSDataInputStrea" src="../figures/FSDataInputStream.png" /></p>
<p>The <C>Seekable</C> interface permits seeking to a position in the file and provides a query method for the <em>current</em> offset from the start of the file (<C>getPos()</C>):</p>
<p> <div class=codehilite><pre><span class=kd>public</span> <span class=kd>interface</span> <span class=nc>Seekable</span> <span class=o>{</span> 
    <span class=kt>void</span> <span class=nf>seek</span><span class=o>(</span><span class=kt>long</span> <span class=n>pos</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span><span class=o>;</span> 
    <span class=kt>long</span> <span class=nf>getPos</span><span class=o>()</span> <span class=kd>throws</span> <span class=n>IOException</span><span class=o>;</span> <span class=o>}</span>
</pre></div></p>
<p>Displaying files from a Hadoop filesystem on standard output twice, by using <C>seek()</C>:</p>
<p> <div class=codehilite><pre><span class=c1>// hdfs://localhost:9000/test2.copy</span>

<span class=kn>import</span> <span class=nn>org.apache.hadoop.conf.Configuration</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.fs.FSDataInputStream</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.fs.FileSystem</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.fs.Path</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.io.IOUtils</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>java.net.URI</span><span class=o>;</span>

<span class=kd>public</span> <span class=kd>class</span> <span class=nc>FileSystemDoubleCat</span> <span class=o>{</span>
    <span class=kd>public</span> <span class=kd>static</span> <span class=kt>void</span> <span class=nf>main</span><span class=o>(</span><span class=n>String</span><span class=o>[]</span> <span class=n>args</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>Exception</span> <span class=o>{</span>
        <span class=n>String</span> <span class=n>uri</span> <span class=o>=</span> <span class=n>args</span><span class=o>[</span><span class=mi>0</span><span class=o>];</span>
        <span class=n>Configuration</span> <span class=n>conf</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Configuration</span><span class=o>();</span>
        <span class=n>FileSystem</span> <span class=n>fs</span> <span class=o>=</span> <span class=n>FileSystem</span><span class=o>.</span><span class=na>get</span><span class=o>(</span><span class=n>URI</span><span class=o>.</span><span class=na>create</span><span class=o>(</span><span class=n>uri</span><span class=o>),</span> <span class=n>conf</span><span class=o>);</span>
        <span class=n>FSDataInputStream</span> <span class=n>in</span> <span class=o>=</span> <span class=kc>null</span><span class=o>;</span>
        <span class=k>try</span> <span class=o>{</span>
            <span class=n>in</span> <span class=o>=</span> <span class=n>fs</span><span class=o>.</span><span class=na>open</span><span class=o>(</span><span class=k>new</span> <span class=n>Path</span><span class=o>(</span><span class=n>uri</span><span class=o>));</span>
            <span class=n>IOUtils</span><span class=o>.</span><span class=na>copyBytes</span><span class=o>(</span><span class=n>in</span><span class=o>,</span> <span class=n>System</span><span class=o>.</span><span class=na>out</span><span class=o>,</span> <span class=mi>4096</span><span class=o>,</span> <span class=kc>false</span><span class=o>);</span>
            <span class=n>in</span><span class=o>.</span><span class=na>seek</span><span class=o>(</span><span class=mi>0</span><span class=o>);</span> <span class=c1>// go back to the start of the file</span>
            <span class=n>IOUtils</span><span class=o>.</span><span class=na>copyBytes</span><span class=o>(</span><span class=n>in</span><span class=o>,</span> <span class=n>System</span><span class=o>.</span><span class=na>out</span><span class=o>,</span> <span class=mi>4096</span><span class=o>,</span> <span class=kc>false</span><span class=o>);</span>
        <span class=o>}</span> <span class=k>catch</span> <span class=o>(</span><span class=n>Exception</span> <span class=n>ex</span><span class=o>)</span> <span class=o>{</span>
            <span class=n>ex</span><span class=o>.</span><span class=na>printStackTrace</span><span class=o>();</span>
        <span class=o>}</span> <span class=k>finally</span> <span class=o>{</span>
            <span class=n>IOUtils</span><span class=o>.</span><span class=na>closeStream</span><span class=o>(</span><span class=n>in</span><span class=o>);</span>
        <span class=o>}</span>
    <span class=o>}</span>
<span class=o>}</span>
</pre></div></p>
<h4 id="writing-data">Writing Data<a class="headerlink" href="#writing-data" title="Permanent link">&para;</a></h4>
<p>The <C>FileSystem class</C> has a number of methods for creating a file. </p>
<p> <div class=codehilite><pre><span class=c1>// takes a Path object for the file to be created and returns an output stream to write to</span>
<span class=kd>public</span> <span class=n>FSDataOutputStream</span> <span class=nf>create</span><span class=o>(</span><span class=n>Path</span> <span class=n>f</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
<span class=c1>// appends to an existing file</span>
<span class=kd>public</span> <span class=n>FSDataOutputStream</span> <span class=nf>append</span><span class=o>(</span><span class=n>Path</span> <span class=n>f</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
</pre></div></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The <C>create()</C> methods create any parent directories of the file to be written that don’t already exist.</p>
</div>
<p>There’s an overloaded method of <create()> for passing a callback interface, <C>Progressable&lt;&gt;/C, so your application can be notified of the progress of the data being written to the datanodes:</p>
<p> <div class=codehilite><pre>public interface Progressable { 
    public void progress(); 
}
</pre></div></p>
<p>Here, we illustrate progress by printing a period every time the <C>progress()</C> method is called by Hadoop, which is after each 64 KB packet of data is written to the datanode pipeline.</p>
<p> <div class=codehilite><pre><span class=c1>// args: /Users/larry/test.copy hdfs://localhost:9000/test4.copy</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.conf.Configuration</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.fs.FileSystem</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.fs.Path</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.io.IOUtils</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>org.apache.hadoop.util.Progressable</span><span class=o>;</span>

<span class=kn>import</span> <span class=nn>java.io.*</span><span class=o>;</span>
<span class=kn>import</span> <span class=nn>java.net.URI</span><span class=o>;</span>

<span class=c1>// Copying a local file to a Hadoop filesystem</span>
<span class=kd>public</span> <span class=kd>class</span> <span class=nc>FileCopyWithProgress</span> <span class=o>{</span>

    <span class=kd>public</span> <span class=kd>static</span> <span class=kt>void</span> <span class=nf>main</span><span class=o>(</span><span class=n>String</span><span class=o>[]</span> <span class=n>args</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>Exception</span> <span class=o>{</span>
        <span class=n>String</span> <span class=n>localsrc</span> <span class=o>=</span> <span class=n>args</span><span class=o>[</span><span class=mi>0</span><span class=o>];</span>
        <span class=n>String</span> <span class=n>dstsrc</span> <span class=o>=</span> <span class=n>args</span><span class=o>[</span><span class=mi>1</span><span class=o>];</span>
        <span class=n>BufferedInputStream</span> <span class=n>in</span> <span class=o>=</span> <span class=k>new</span> <span class=n>BufferedInputStream</span><span class=o>(</span><span class=k>new</span> <span class=n>FileInputStream</span><span class=o>(</span><span class=n>localsrc</span><span class=o>));</span>

        <span class=n>Configuration</span> <span class=n>conf</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Configuration</span><span class=o>();</span>
        <span class=n>FileSystem</span> <span class=n>fs</span> <span class=o>=</span> <span class=n>FileSystem</span><span class=o>.</span><span class=na>get</span><span class=o>(</span><span class=n>URI</span><span class=o>.</span><span class=na>create</span><span class=o>(</span><span class=n>dstsrc</span><span class=o>),</span> <span class=n>conf</span><span class=o>);</span>
        <span class=k>try</span> <span class=o>{</span>
            <span class=n>OutputStream</span> <span class=n>out</span> <span class=o>=</span> <span class=n>fs</span><span class=o>.</span><span class=na>create</span><span class=o>(</span><span class=k>new</span> <span class=n>Path</span><span class=o>(</span><span class=n>dstsrc</span><span class=o>),</span> <span class=k>new</span> <span class=n>Progressable</span><span class=o>()</span> <span class=o>{</span>
                <span class=nd>@Override</span>
                <span class=kd>public</span> <span class=kt>void</span> <span class=nf>progress</span><span class=o>()</span> <span class=o>{</span>
                    <span class=n>System</span><span class=o>.</span><span class=na>out</span><span class=o>.</span><span class=na>println</span><span class=o>(</span><span class=s>&quot;.&quot;</span><span class=o>);</span>
                <span class=o>}</span>
            <span class=o>});</span>
            <span class=n>IOUtils</span><span class=o>.</span><span class=na>copyBytes</span><span class=o>(</span><span class=n>in</span><span class=o>,</span> <span class=n>out</span><span class=o>,</span> <span class=mi>4096</span><span class=o>,</span> <span class=kc>true</span><span class=o>);</span>

        <span class=o>}</span> <span class=k>finally</span> <span class=o>{</span>
            <span class=n>IOUtils</span><span class=o>.</span><span class=na>closeStream</span><span class=o>(</span><span class=n>in</span><span class=o>);</span>
        <span class=o>}</span> <span class=c1>//end try</span>
    <span class=o>}</span><span class=c1>// end main</span>
<span class=o>}</span>
</pre></div></p>
<h4 id="fsdataoutputstream">FSDataOutputStream<a class="headerlink" href="#fsdataoutputstream" title="Permanent link">&para;</a></h4>
<p>The <C>create()</C> method on <C>FileSystem</C> returns an <C>FSDataOutputStream</C>, which, like <C>FSDataInputStream</C>, has a method for querying the current position in the file:</p>
<p> <div class=codehilite><pre><span class=kd>public</span> <span class=kd>class</span> <span class=nc>FSDataOutputStream</span> <span class=kd>extends</span> <span class=n>DataOutputStream</span> <span class=kd>implements</span> <span class=n>Syncable</span> <span class=o>{</span>
    <span class=kd>public</span> <span class=kt>long</span> <span class=nf>getPos</span><span class=o>()</span> <span class=kd>throws</span> <span class=n>IOException</span> <span class=o>{</span> 
        <span class=c1>// implementation elided </span>
    <span class=o>}</span><span class=c1>// implementation elided</span>
<span class=o>}</span>
</pre></div></p>
<p>However, because HDFS allows only sequential writes to an open file or appends to an already written file, <C>FSDataOutputStream</C> does not permit seeking.</p>
<h4 id="directories">Directories<a class="headerlink" href="#directories" title="Permanent link">&para;</a></h4>
<p><C>FileSystem</C> provides a method to create a directory:</p>
<p> <div class=codehilite><pre><span class=kd>public</span> <span class=kt>boolean</span> <span class=nf>mkdirs</span><span class=o>(</span><span class=n>Path</span> <span class=n>f</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
</pre></div></p>
<p>This method creates all of the necessary parent directories if they don’t already exist.</p>
<h4 id="querying-the-filesystem">Querying the Filesystem<a class="headerlink" href="#querying-the-filesystem" title="Permanent link">&para;</a></h4>
<p><hh>File metadata: FileStatus</hh>
<hh>Listing files</hh>
<hh>File patterns</hh></p>
<h4 id="deleting-data">Deleting Data<a class="headerlink" href="#deleting-data" title="Permanent link">&para;</a></h4>
<p>Use the <C>delete()</C> method on <C>FileSystem</C> to permanently remove files or directories:</p>
<p> <div class=codehilite><pre><span class=kd>public</span> <span class=kt>boolean</span> <span class=nf>delete</span><span class=o>(</span><span class=n>Path</span> <span class=n>f</span><span class=o>,</span> <span class=kt>boolean</span> <span class=n>recursive</span><span class=o>)</span> <span class=kd>throws</span> <span class=n>IOException</span>
</pre></div></p>
<p>If <C>f</C> is a file or an empty directory, the value of <C>recursive</C> is ignored.</p>
<h3 id="6-data-flow">6 Data Flow<a class="headerlink" href="#6-data-flow" title="Permanent link">&para;</a></h3>
<h4 id="anatomy-of-a-file-read">Anatomy of a File Read<a class="headerlink" href="#anatomy-of-a-file-read" title="Permanent link">&para;</a></h4>
<p>The figure below shows the main sequence of events when reading a file.</p>
<p><img alt="" src="../figures/AClientReadingDataFromHDFS.jpg" /></p>
<ul>
<li>step 1: The client opens the file it wishes to read by calling <C>open()</C> on the <C>FileSystem</C> object, which for HDFS is an instance of <C>DistributedFileSystem</C>. </li>
<li>step 2: <C>DistributedFileSystem</C> calls the namenode, using remote procedure calls (RPCs), to determine the locations of the first few blocks in the file. </li>
<li>step 3: For each block, the namenode returns the addresses of the datanodes that have a copy of that block. Furthermore, the datanodes are sorted according to their proximity to the client. <ul>
<li>If the client is itself a datanode, the client will read from the local datanode if that datanode hosts a copy of the block.</li>
<li>The <C>DistributedFileSystem</C> returns an <C>FSDataInputStream</C> to the client for it to read data from. <C>FSDataInputStream</C> in turn wraps a <C>DFSInputStream</C>, which manages the datanode and namenode I/O.</li>
<li>The client then calls <C>read()</C> on the stream. </li>
</ul>
</li>
<li>step 4: <C>DFSInputStream</C>, which has stored the datanode addresses for the first few blocks in the file, then connects to the first (closest) datanode for the first block in the file. Data is streamed from the datanode back to the client, which calls <C>read()</C> repeatedly on the stream. </li>
<li>step 5: When the end of the block is reached, <C>DFSInputStream</C> will close the connection to the datanode, then find the best datanode for the next block. </li>
<li>step 6: This happens transparently to the client, which from its point of view is just reading a continuous stream.<ul>
<li>Blocks are read in order, with the <C>DFSInputStream</C> opening new connections to datanodes as the client reads through the stream. </li>
<li>It will also call the namenode to retrieve the datanode locations for the next batch of blocks as needed. When the client has finished reading, it calls <C>close()</C> on the <C>FSDataInputStream</C>.</li>
</ul>
</li>
</ul>
<h4 id="anatomy-of-a-file-write">Anatomy of a File Write<a class="headerlink" href="#anatomy-of-a-file-write" title="Permanent link">&para;</a></h4>
<p>The figure below illustrates the case of creating a new file, writing data to it, then closing the file.</p>
<p><img alt="" src="../figures/AClientWritingDataToHDFS.jpg" /></p>
<ul>
<li>step 1: The client creates the file by calling <C>create()</C> on <C>DistributedFileSystem</C>. </li>
<li>step 2: <C>DistributedFileSystem</C> makes an RPC call to the namenode to create a new file in the filesystem’s namespace, with no blocks associated with it. <ul>
<li>The namenode performs various checks to make sure the file doesn’t already exist and that the client has the right permissions to create the file. </li>
<li>If these checks pass, the namenode makes a record of the new file; otherwise, file creation fails and the client is thrown an <C>IOException</C>. </li>
<li>The <C>DistributedFileSystem</C> returns an <C>FSDataOutputStream</C> for the client to start writing data to. Just as in the read case, <C>FSDataOutputStream</C> wraps a <C>DFSOutputStream</C>, which handles communication with the datanodes and namenode.</li>
</ul>
</li>
<li>step 3: As the client writes data, the <C>DFSOutputStream</C> splits it into packets, which it writes to an internal queue called the <strong><em>data queue</em></strong> . The data queue is consumed by the <C>DataStreamer</C>, which is responsible for asking the namenode to allocate new blocks by picking a list of suitable datanodes to store the replicas. </li>
<li>step 4: The list of datanodes forms a pipeline, and here we’ll assume the replication level is three, so there are three nodes in the pipeline. The <C>DataStreamer</C> streams the packets to the first datanode in the pipeline, which stores each packet and forwards it to the second datanode in the pipeline. Similarly, the second datanode stores the packet and forwards it to the third (and last) datanode in the pipeline .</li>
<li>step 5: The <C>DFSOutputStream</C> also maintains an internal queue of packets that are waiting to be acknowledged by datanodes, called the <strong><em>ack queue</em></strong>. A packet is removed from the ack queue only when it has been acknowledged by all the datanodes in the pipeline.</li>
<li>step 6: When the client has finished writing data, it calls <C>close()</C> on the stream. This action flushes all the remaining packets to the datanode pipeline.</li>
<li>step 7:  It waits for acknowledgments before contacting the namenode to signal that the file is complete. </li>
</ul>
<h4 id="coherency-model">Coherency Model<a class="headerlink" href="#coherency-model" title="Permanent link">&para;</a></h4>
<p>A coherency model for a filesystem describes the data visibility of reads and writes for a file.</p>
<ul>
<li>Any content written to the file is not guaranteed to be visible, even if the stream is flushed.</li>
<li>Once more than a block’s worth of data has been written, the first block will be visible to new readers.</li>
<li>The <C>FSDataOutputStream.hflush()</C> method force all buffers to be flushed to the datanodes.<ul>
<li>The <C>hflush()</T> guarantees that the data written up to that point in the file has reached all the datanodes in the write pipeline and is visible to all new readers.</li>
<li>But it does not guarantee that the datanodes have written the data to disk, only that it’s in the datanodes’ memory.</li>
<li>Closing a file in HDFS performs an implicit <C>hflush()</C>.</li>
</ul>
</li>
<li>The <C>hsync()</C> method syncs to disk for a file descriptor.</li>
</ul>
<p> <div class=codehilite><pre><span class=n>FileOutputStream</span> <span class=n>out</span> <span class=o>=</span> <span class=k>new</span> <span class=n>FileOutputStream</span><span class=o>(</span><span class=n>localFile</span><span class=o>);</span> <span class=n>out</span><span class=o>.</span><span class=na>write</span><span class=o>(</span><span class=s>&quot;content&quot;</span><span class=o>.</span><span class=na>getBytes</span><span class=o>(</span><span class=s>&quot;UTF-8&quot;</span><span class=o>));</span> 
<span class=n>out</span><span class=o>.</span><span class=na>flush</span><span class=o>();</span> <span class=c1>// flush to operating system </span>
<span class=n>out</span><span class=o>.</span><span class=na>getFD</span><span class=o>().</span><span class=na>sync</span><span class=o>();</span> <span class=c1>// sync to disk </span>
<span class=n>assertThat</span><span class=o>(</span><span class=n>localFile</span><span class=o>.</span><span class=na>length</span><span class=o>(),</span> <span class=n>is</span><span class=o>(((</span><span class=kt>long</span><span class=o>)</span> <span class=s>&quot;content&quot;</span><span class=o>.</span><span class=na>length</span><span class=o>())));</span>
</pre></div></p>
<p><hh>Consequences for application design</hh></p>
<p>You should call <C>hflush()</C> at suitable points, such as after writing a certain number of records or number of bytes.</p>
<h3 id="7-parallel-copying-with-distcp">7 Parallel Copying with distcp<a class="headerlink" href="#7-parallel-copying-with-distcp" title="Permanent link">&para;</a></h3>
<p>The program <C>distcp</C> copys data to and from Hadoop filesystems in parallel.</p>
<p> <div class=codehilite><pre>$ hadoop distcp file1 file2
</pre></div></p>
<p><C>distcp</C> is implemented as a MapReduce job where the work of copying is done by the maps that run in parallel across the cluster, with no reducers.</p></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"search": 83, "next": 78, "help": 191, "previous": 80};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
        <script src="../../../extra_javascript/tabhack.js" defer></script>
        <script src="../../../extra_javascript/baidu.js" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
