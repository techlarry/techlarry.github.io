{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"osc/ch10/","text":"Operating System Concepts 10 - Virtual Memory 1 Background Various memory-management strategies discussed in Chapter 9 have the same goal to keep many processes in memory simultaneously to allow multiprogramming. PROBLEM: However, they tend to require that an entire process be in memory before it can execute. In many cases, the entire program is not needed: Programs often have code to handle unusual error conditions. This code is almost never executed. Arrays, lists, and tables are often allocated more memory than they actually needed. Certain options and features of a program may be used rarely. SOLUTION: Virtual memory allows the execution of processes that are not completely in memory. ADVANTAGE: Programs can be larger than physical memory. Virtual memory abstracts main memory as viewed by the programmer from physical memory. It frees programmers from the concerns of memory-storage limitations. Virtual memory allows processes to share files and memory through page sharing . 2 Demand Paging QUESTION: How an executable program might be loaded from secondary storage into memory? OPTION: One option is to load the entire program in physical memory at program execution time. PROBLEM: We may not initially need the entire program in memory. (e.g. a program starts with a list of available options from which the user is to select). SOLUTION: Demand paging (\u6309\u9700\u8c03\u9875) loads pages in memory only when they are needed. Note A demand-paging system is similar to a paging system with swapping where processes reside in secondary memory. Basic Concepts Since demand paging loads pages in memory only when they are needed, some pages will be in memory and some will be in secondary storage. Thus, we need some form of hardware support to distinguish between the two. The valid bit is set to ensure that the page is in the logical address space of the process and is currently in secondary storage. Access to a page marked invalid causes a page fault (\u7f3a\u9875). The procedure for handling this page fault is straightforward: We check page tables (usually kept with the process control block) for this process to determine whether the reference was a valid or an invalid memory access. If the reference was invalid, we terminate the process. If it was valid but we have not yet brought in that page, we now page it in. We find a free frame (by taking one from the free-frame list, for example). We schedule a secondary storage operation to read the desired page into the newly allocated frame. When the storage read is complete, we modify the internal table kept with the process and the page table to indicate that the page is now in memory. We restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. In the extreme case, we can start executing a process with no pages in memory. This scheme is pure demand paging : never bring a page into memory until it is required . Free-Frame List Most operating system maintain a free-frame list , a pool of free frames for satisfying requests, that bring the desired pages from secondary storage into main memory. Operating system typically allocate free frames using a technique known as zero-fill-on-demand , which \"zero-out\" frames before it is allocated (i.e. erasing previous contents). Performance of Demand Paging Let p(0\\le p \\le 1) p(0\\le p \\le 1) be page-fault rage (\u7f3a\u9875\u7387), the probability of a page fault. We would expect p p to be close to zero. The effective access time, is \\text{effective access time} = (1-p)\\times ma + p\\times \\text{page fault time} \\text{effective access time} = (1-p)\\times ma + p\\times \\text{page fault time} where ma ma denotes the memory-access time. With an average page-fault service time of 8 milliseconds and a memory access time of 200 nanoseconds, the effective access time in nanoseconds is 200+7,999,800\\times p 200+7,999,800\\times p . The effective access time is directly proportional to the page-fault rate p p , so it is important to keep the page-fault rate low in a demand-paging system. An additional aspect of demand paging is the handling and overall use of swap space. I/O to swap space is generally faster than that to the file system, because swap space is allocated in much larger blocks, and file lookups an indirect allocation methods are not used ( see details in Chapter 11 ). One OPTION: copying an entire file image into the swap space at process startup and then performing demand paging from the swap space. Second OPTION: practiced by several operating system, including Linux and Windows \u2014\u2014\u2014 to demand-page from the file system initially but to write the pages to swap space as they are replaced. Mobile operating system typically do not support swapping [ Chapter 9 ]. Instead, these systems demand-page from the file system and reclaim read-only pages (such as code) from applications if memory becomes constrained. 3 Copy-on-Write Copy-on-write (\u5199\u65f6\u590d\u5236) works by allowing the parent and child processes initially to share the same pages, and if either process writes to a shared page, a copy of the shared page is created. 4 Page Replacement When we increase degree of multiprogramming, over-allocating of memory results in page faults. The operating system determines where the desired page is residing on secondary storage but to find that there are no free frames on the free-frame list (i.e. all memory is in use). Most operating systems now combine swapping pages with page replacement (\u9875\u9762\u7f6e\u6362). Basic Page Replacement If no frame is free, we find one that is not currently being used and free it. When we select a page for replacement, We examine its modify bit (or dirty bit , see Example of Core i7 ). The modify bit for a page is set by the hardware whenever any byte in the page is written into, indicating that the page has been modified. If the bit is set, we must write the page to storage. Otherwise, we need not write the memory page to storage: it is already there. We must solve two major problems to implement demand paging: frame-allocation algorithm : decide how many frames allocate to each process page-replacement algorithm : select the frames that are to be replaced. In general, we want the one with the lowest page-fault rate. FIFO Page Replacement The simplest page-replacement algorithm is a first-in, first-out(FIFO) algorithm. We can create a FIFO queue to hold all pages in memory. We replace the page at the head of the queue. When a page is brought into memory, we insert it at the tail of the queue. DISADVANTAGE: Its performance is not always good. Belady\u2019s anomaly : for some page-replacement algorithms, the page-fault rate may increase as the number of allocated frames increases. Figure below shows the curve of page faults for the reference string 1,2,3,4,1,2,5,1,2,3,4,5 versus the number of available frames, with a FIFO page-replacement algorithm. Optimal Page Replacement, OPT Optimal page-replacement (OPT, \u6700\u4f73\u9875\u9762\u7f6e\u6362) algorithm replaces the page that will not be used for the longest period of time . It has the lowest page-fault rate of all algorithms. It will never suffer from Belady's anomaly. It is different to implement, because it requires future knowledge of the reference string. It is used mainly for comparison studies. LRU Page Replacement, LRU Least recently used (LRU, \u6700\u8fd1\u6700\u5c11\u4f7f\u7528) algorithm chooses the page that has not been used for the longest period of time. We can think of this strategy as the optimal page-replacement algorithm looking backward in time. ( If we let S^R S^R be the reverse of a reference string S S , then the page-fault rate for the OPT algorithm on S S is the same as the page-fault rate of OPT algorithm on S^R S^R ). The LRU policy is often used as a page-replacement algorithm and is considered to be good. Best way to implement LRU replacement with a stack of page numbers is using a doubly linked list with a head pointer and a tail pointer. Note Implementation of LRU would be not conceivable without hardware assistance beyond the standard TLB registers. The updating of stack must be done for every memory reference. If we were to use an interrupt for every reference to allow software to update such data structures, it would slow every memory reference by a factor of at least ten, hence slowing every process by a factor of ten. Few systems could tolerate that level of overhead for memory management. LRU-Approximation Page Replacement In fact, some systems provide no hardware support, using the form of a reference bit instead. The reference bit for a page is set by the hardware whenever that page is referenced(either a read or a write to any byte in the page). Additional-Reference-Bits Algorithm We can gain additional ordering information by recording the reference bits at regular intervals. We can keep an 8-bit byte for each page in a table in memory. At regular intervals (say, every 100 milliseconds), a timer interrupt transfers control to the operating system. The operating system shifts the reference bit for each page into the high-order bit of its 8-bit byte, shifting the other bits right by 1 bit and discarding the low-order bit. These 8-bit shift registers contain the history of page use for the last eight time periods. If we interpret these 8-bit bytes as unsigned integers, the page with the lowest number is the LRU page, and it can be replaced. Second-Chance Algorithm The number of bits of history included in the shift register can be varied, the number can be reduced to zero, leaving only the reference bit itself. When a page has been selected, we inspect its reference bit. If the value is 0, we proceed to replace this page; but if the reference bit is set to 1, we give the page a second chance and move on to select the next FIFO page. When a page gets a second chance, its reference bit is cleared, and its arrival time is reset to the current time. Thus, a page that is given a second chance will not be replaced until all other pages have been replaced (or given second chances). I n addition, if a page is used often enough to keep its reference bit set, it will never be replaced. One way to implement the second-chance algorithm (sometimes referred to as the clock algorithm) is as a circular queue. A pointer (that is, a hand on the clock) indicates which page is to be replaced next. When a frame is needed, the pointer advances until it finds a page with a 0 reference bit. As it advances, it clears the reference bits. Once a victim page is found, the page is replaced, and the new page is inserted in the circular queue in that position. A simple example is illustrated by the figure below, in which small blue digits denotes the reference bit and green arrow denotes the pointer. 5 Allocation of Frames Minimum Number of Frames We must allocate at least a minimum number of frames. One reason is performance. Obviously, as the number of frames allocated to each process decreases, the page-fault rate increases. Another reason is that when a page fault occurs before an executing instruction complete, the instruction must be restarted. So we must have enough frames to hold all the different pages that any single instruction can reference. The minimum number of frames is defined by the computer architecture. Allocation Algorithms Equal Allocation The easiest way to split m m frames among n n processes is to give everyone an equal share, m/n m/n frames (ignoring frames needed by the operating system for the moment). Proportional Allocation In proportional allocation, we allocate available memory to each process according to its size. Let the size of the virtual memory for process p_i p_i be s_i s_i , and define S=\\sum s_i S=\\sum s_i . Then, if the total number of available frames is m m , we allocate a_i a_i frames to process p_i p_i , where a_i a_i is approximately a_i=s_i/S\\times m a_i=s_i/S\\times m . Global versus Local Allocation Global replacement allows a process to select a replacement frame from the set of all frames, even if that frame is currently allocated to some other process; that is, one process can take a frame from another. Local replacement requires that each process select from only its own set of allocated frames. Local replacement might hinder a process, however, by not making available to it other, less used pages of memory. Thus, global replacement generally results in greater system throughput. It is therefore the more commonly used method. Global Page-Replacement Policy Rather than waiting for the free-frame list to drop to zero before we begin selecting pages for replacement, we trigger page replacement when the list falls below a certain threshold . It attempts to ensure there is always sufficient free memory to satisfy new requests. When the amount of free memory drops below minimum threshold, a kernel routine ( reapers ) is triggered that begins reclaiming pages from all processes in the system. When the amount of free memory reaches the maximum threshold, the reaper routine is suspended. The kernel reaper routine typically uses some form of LRU approximation. 6 Thrashing Cause of Thrashing Thrashing may be caused by programs or workloads that present insufficient locality of reference (also principle of locality, \u8bbf\u95ee\u5c40\u90e8\u6027): if the working set (\u5de5\u4f5c\u96c6) of a program or a workload cannot be effectively held within physical memory, then constant data swapping, i.e., thrashing, may occur. Consider the following scenario, which is based on the actual behavior of early paging systems. The operating system monitors CPU utilization. If CPU utilizition is too low, we increase the degree of multiprogramming by introducing a new process to the system. A global page-replacement algorithm is used; it replaces pages without regard to the process to which they belong. Now suppose that a process enters a new phase in its execution and needs more frames. It starts faulting and taking frames away from other processes. These processes need those pages, however, and so they also fault, taking frames from other processes. These faulting processes must use the paging device to swap pages in and out. As they queue up for the paging device, the ready queue empties. As processes wait for the paging device, CPU utilization decreases. The CPU scheduler sees the decreasing CPU utilization and increases the degree of multiprogramming as a result. The new process tries to get started by taking frames from running processes, causing more page faults and a longer queue for the paging device. As a result, CPU utilization drops even further, and the CPU scheduler tries to increase the degree of multiprogramming even more. Thrashing has occurred, and system throughput plunges. QUESTION: To prevent thrashing, we must provide a process with as many frames as it needs. But how do we know how many frames it \"needs\"? The locality model of process execution, states that, as a process executes, it moves from locality to locality. A locality is a set of pages that are actively used together. A running program is generally composed of several different localities, which may overlap. If we do not allocate enough frames to accommodate the size of the current locality, the process will thrash, since it cannot keep in memory all the pages that it is actively using. Example Figure below illustrates the concept of locality and how a process\u2019s locality changes over time. At time (a), the locality is the set of pages {18, 19, 20, 21, 22, 23, 24, 29, 30, 33}. At time (b), the locality changes to {18, 19, 20, 24, 25, 26, 27, 28, 29, 31, 32, 33}. Notice the overlap, as some pages (for example, 18, 19, and 20) are part of both localities. Working-Set Model The group of physical memory pages currently dedicated to a specific process is known as the Working set (WS, \u5de5\u4f5c\u96c6) for that process. Example For example, the working set at time t_1 t_1 is {1, 2, 5, 6, 7}. By time t_2 t_2 , the working set has changed to {3, 4}. If we compute the working-set size WSS_i WSS_i for each process in the system, the total demand for frames D D is D=\\sum WSS_i D=\\sum WSS_i . If the total demand D D is greater than the total number of available frames ( D\\gt m D\\gt m ), thrashing will occur, because some processes will not have enough frames. WORKING SET SOLUTION TO THRASHING: The operating system monitors the working set of each process and allocates to that working set enough frames to provide it with its working-set size. If there are enough extra frames, another process can be initiated. If the sum of the working-set sizes increases, exceeding the total number of available frames, the operating system selects a process to suspend. The process\u2019s pages are written out (swapped), and its frames are reallocated to other processes. The suspended process can be restarted later. Page-Fault Frequency The working-set model is successful but seems a clumsy way to control thrashing. A strategy that uses the page-fault frequency (PFF) takes a more direct approach: Thrashing has a high page-fault rate. Thus, we control the page-fault rate. When the page fault rate is too high, we know that the process needs more frames. Conversely, if it too low, then the process may have too many frames. We can establish upper and lower bounds on the desired page-fault rate. If the actual page-fault rate exceeds the upper limit, we allocate the process. another frame. If the page-fault rate falls below the lower limit, we remove a frame from the process. 7 Memory Compression When the number of free frames falls below a certain threshold that would triggers page replacement, rather than paging out modified frames to swap space, we compress several frames into a single frame ( memory compression , \u5185\u5b58\u538b\u7f29), enabling the system to reduce memory usage without resorting to swapping pages. Example For example, the free-frame list contains six frames: 7,2,9,21,27,16 , and the modified frame list contains four frames: 15, 3, 35, 26 . In Figure below, frame 7 is removed from the free-frame list. Frames 15, 3, and 35 are compressed and stored in frame 7, which is then stored in the list of compressed frames. The frames 15, 3, and 35 can now be moved to the free-frame list. If one of the three compressed frames is later referenced, a page fault occurs, and the compressed frame is decompressed, restoring the three pages 15, 3, and 35 in memory. 8 Allocating Kernel Memory Kernel memory is often allocated from a free-memory pool different from the list used to satisfy ordinary user-mode processes discussed before. There are two primary reasons for this: The kernel requests memory for data structures of varying sizes, some of which are less than a page in size. As a result, the kernel must use memory conservatively and attempt to minimize waste due to fragmentation. Certain hardware devices interact directly with physical memory \u2014\u2014 without the benefit of a virtual memory interface \u2014\u2014 and consequently may require memory residing in physically contiguous pages. Buddy System The buddy system allocates memory from a fixed-size segment consisting of physically contiguous pages. Memory is allocated from this segment using a power-of-2 allocator, which satisfies requests in units sized as a power of 2 (4 KB, 8 KB, 16 KB, and so forth). A request in units not appropriately sized is rounded up to the next highest power of 2. Pro and Cons: An advantage of the buddy system is how quickly adjacent buddies can be combined to form larger segments using a technique known as coalescing (illustrated below in the Example section). Rounding up to the next highest power of 2 is very likely to cause internal fragmentation. Example Assume the size of a memory segment is initially 256 KB and the kernel requests 21 KB of memory. The segment is initially divided into two buddies\u2014which we will call A_L A_L and A_R A_R \u2014\u2014 each 128 KB in size. One of these buddies is further divided into two 64-KB buddies\u2014 B_L B_L and B_R B_R . However, the next-highest power of 2 from 21 KB is 32 KB so either B_L B_L or B_R B_R is again divided into two 32-KB buddies, C_L C_L and C_R C_R . One of these buddies is used to satisfy the 21-KB request. This scheme is illustrated in Figure below, where C_L C_L is the segment allocated to the 21-KB request. when the kernel releases the C_L C_L unit it was allocated, the system can coalesce C_L C_L and C_R C_R into a 64-KB segment. This segment, B_L B_L , can in turn be coalesced with its buddy B_R B_R to form a 128-KB segment. Ultimately, we can end up with the original 256-KB segment. Slab Allocations A second strategy for allocating kernel memory is known as slab allocation . A slab is made up of one or more physically contiguous pages. A cache consists of one or more slabs. Each of caches stores a different type of object. There is one cache per object type.(e.g. a separate cache for the data structure representing process descriptors, a separate cache for file objects). Each cache is populated with objects that are instantiations of the kernel data structure the cache represents.(e.g. the cache representing semaphores stores instances of semaphore objects). Example Linux kernel adopted the slab allocator after Version 2.2. Each slab contains some number of objects, which are the data structures being cached. Each slab is in one of three states: full , partial , or empty . A full slab has no free objects. (All objects in the slab are allocated.) An empty slab has no allocated objects. (All objects in the slab are free.) A partial slab has some allocated objects and some free objects. When some part of the kernel requests a new object, the request is satisfied from a partial slab, if one exists. Otherwise, the request is satisfied from an empty slab. The slab allocator provides two main benefits: No memory is wasted due to fragmentation. Each unique kernel data structure has an associated cache, and each cache is made up of one or more slabs that are divided into chunks the size of the objects being represented. Memory requests can be satisfied quickly . Objects are created in advance and thus can be quickly allocated from the cache. When the kernel has finished with an object and releases it, it is marked as free and returned to its cache, thus making it immediately available for subsequent requests from the kernel.","title":"Chapter 10: Virtual Memory"},{"location":"osc/ch10/#operating-system-concepts-10-virtual-memory","text":"","title":"Operating System Concepts 10 - Virtual Memory"},{"location":"osc/ch10/#1-background","text":"Various memory-management strategies discussed in Chapter 9 have the same goal to keep many processes in memory simultaneously to allow multiprogramming. PROBLEM: However, they tend to require that an entire process be in memory before it can execute. In many cases, the entire program is not needed: Programs often have code to handle unusual error conditions. This code is almost never executed. Arrays, lists, and tables are often allocated more memory than they actually needed. Certain options and features of a program may be used rarely. SOLUTION: Virtual memory allows the execution of processes that are not completely in memory. ADVANTAGE: Programs can be larger than physical memory. Virtual memory abstracts main memory as viewed by the programmer from physical memory. It frees programmers from the concerns of memory-storage limitations. Virtual memory allows processes to share files and memory through page sharing .","title":"1 Background"},{"location":"osc/ch10/#2-demand-paging","text":"QUESTION: How an executable program might be loaded from secondary storage into memory? OPTION: One option is to load the entire program in physical memory at program execution time. PROBLEM: We may not initially need the entire program in memory. (e.g. a program starts with a list of available options from which the user is to select). SOLUTION: Demand paging (\u6309\u9700\u8c03\u9875) loads pages in memory only when they are needed. Note A demand-paging system is similar to a paging system with swapping where processes reside in secondary memory.","title":"2 Demand Paging"},{"location":"osc/ch10/#basic-concepts","text":"Since demand paging loads pages in memory only when they are needed, some pages will be in memory and some will be in secondary storage. Thus, we need some form of hardware support to distinguish between the two. The valid bit is set to ensure that the page is in the logical address space of the process and is currently in secondary storage. Access to a page marked invalid causes a page fault (\u7f3a\u9875). The procedure for handling this page fault is straightforward: We check page tables (usually kept with the process control block) for this process to determine whether the reference was a valid or an invalid memory access. If the reference was invalid, we terminate the process. If it was valid but we have not yet brought in that page, we now page it in. We find a free frame (by taking one from the free-frame list, for example). We schedule a secondary storage operation to read the desired page into the newly allocated frame. When the storage read is complete, we modify the internal table kept with the process and the page table to indicate that the page is now in memory. We restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. In the extreme case, we can start executing a process with no pages in memory. This scheme is pure demand paging : never bring a page into memory until it is required .","title":"Basic Concepts"},{"location":"osc/ch10/#free-frame-list","text":"Most operating system maintain a free-frame list , a pool of free frames for satisfying requests, that bring the desired pages from secondary storage into main memory. Operating system typically allocate free frames using a technique known as zero-fill-on-demand , which \"zero-out\" frames before it is allocated (i.e. erasing previous contents).","title":"Free-Frame List"},{"location":"osc/ch10/#performance-of-demand-paging","text":"Let p(0\\le p \\le 1) p(0\\le p \\le 1) be page-fault rage (\u7f3a\u9875\u7387), the probability of a page fault. We would expect p p to be close to zero. The effective access time, is \\text{effective access time} = (1-p)\\times ma + p\\times \\text{page fault time} \\text{effective access time} = (1-p)\\times ma + p\\times \\text{page fault time} where ma ma denotes the memory-access time. With an average page-fault service time of 8 milliseconds and a memory access time of 200 nanoseconds, the effective access time in nanoseconds is 200+7,999,800\\times p 200+7,999,800\\times p . The effective access time is directly proportional to the page-fault rate p p , so it is important to keep the page-fault rate low in a demand-paging system. An additional aspect of demand paging is the handling and overall use of swap space. I/O to swap space is generally faster than that to the file system, because swap space is allocated in much larger blocks, and file lookups an indirect allocation methods are not used ( see details in Chapter 11 ). One OPTION: copying an entire file image into the swap space at process startup and then performing demand paging from the swap space. Second OPTION: practiced by several operating system, including Linux and Windows \u2014\u2014\u2014 to demand-page from the file system initially but to write the pages to swap space as they are replaced. Mobile operating system typically do not support swapping [ Chapter 9 ]. Instead, these systems demand-page from the file system and reclaim read-only pages (such as code) from applications if memory becomes constrained.","title":"Performance of Demand Paging"},{"location":"osc/ch10/#3-copy-on-write","text":"Copy-on-write (\u5199\u65f6\u590d\u5236) works by allowing the parent and child processes initially to share the same pages, and if either process writes to a shared page, a copy of the shared page is created.","title":"3 Copy-on-Write"},{"location":"osc/ch10/#4-page-replacement","text":"When we increase degree of multiprogramming, over-allocating of memory results in page faults. The operating system determines where the desired page is residing on secondary storage but to find that there are no free frames on the free-frame list (i.e. all memory is in use). Most operating systems now combine swapping pages with page replacement (\u9875\u9762\u7f6e\u6362).","title":"4 Page Replacement"},{"location":"osc/ch10/#basic-page-replacement","text":"If no frame is free, we find one that is not currently being used and free it. When we select a page for replacement, We examine its modify bit (or dirty bit , see Example of Core i7 ). The modify bit for a page is set by the hardware whenever any byte in the page is written into, indicating that the page has been modified. If the bit is set, we must write the page to storage. Otherwise, we need not write the memory page to storage: it is already there. We must solve two major problems to implement demand paging: frame-allocation algorithm : decide how many frames allocate to each process page-replacement algorithm : select the frames that are to be replaced. In general, we want the one with the lowest page-fault rate.","title":"Basic Page Replacement"},{"location":"osc/ch10/#fifo-page-replacement","text":"The simplest page-replacement algorithm is a first-in, first-out(FIFO) algorithm. We can create a FIFO queue to hold all pages in memory. We replace the page at the head of the queue. When a page is brought into memory, we insert it at the tail of the queue. DISADVANTAGE: Its performance is not always good. Belady\u2019s anomaly : for some page-replacement algorithms, the page-fault rate may increase as the number of allocated frames increases. Figure below shows the curve of page faults for the reference string 1,2,3,4,1,2,5,1,2,3,4,5 versus the number of available frames, with a FIFO page-replacement algorithm.","title":"FIFO Page Replacement"},{"location":"osc/ch10/#optimal-page-replacement-opt","text":"Optimal page-replacement (OPT, \u6700\u4f73\u9875\u9762\u7f6e\u6362) algorithm replaces the page that will not be used for the longest period of time . It has the lowest page-fault rate of all algorithms. It will never suffer from Belady's anomaly. It is different to implement, because it requires future knowledge of the reference string. It is used mainly for comparison studies.","title":"Optimal Page Replacement, OPT"},{"location":"osc/ch10/#lru-page-replacement-lru","text":"Least recently used (LRU, \u6700\u8fd1\u6700\u5c11\u4f7f\u7528) algorithm chooses the page that has not been used for the longest period of time. We can think of this strategy as the optimal page-replacement algorithm looking backward in time. ( If we let S^R S^R be the reverse of a reference string S S , then the page-fault rate for the OPT algorithm on S S is the same as the page-fault rate of OPT algorithm on S^R S^R ). The LRU policy is often used as a page-replacement algorithm and is considered to be good. Best way to implement LRU replacement with a stack of page numbers is using a doubly linked list with a head pointer and a tail pointer. Note Implementation of LRU would be not conceivable without hardware assistance beyond the standard TLB registers. The updating of stack must be done for every memory reference. If we were to use an interrupt for every reference to allow software to update such data structures, it would slow every memory reference by a factor of at least ten, hence slowing every process by a factor of ten. Few systems could tolerate that level of overhead for memory management.","title":"LRU Page Replacement, LRU"},{"location":"osc/ch10/#lru-approximation-page-replacement","text":"In fact, some systems provide no hardware support, using the form of a reference bit instead. The reference bit for a page is set by the hardware whenever that page is referenced(either a read or a write to any byte in the page). Additional-Reference-Bits Algorithm We can gain additional ordering information by recording the reference bits at regular intervals. We can keep an 8-bit byte for each page in a table in memory. At regular intervals (say, every 100 milliseconds), a timer interrupt transfers control to the operating system. The operating system shifts the reference bit for each page into the high-order bit of its 8-bit byte, shifting the other bits right by 1 bit and discarding the low-order bit. These 8-bit shift registers contain the history of page use for the last eight time periods. If we interpret these 8-bit bytes as unsigned integers, the page with the lowest number is the LRU page, and it can be replaced. Second-Chance Algorithm The number of bits of history included in the shift register can be varied, the number can be reduced to zero, leaving only the reference bit itself. When a page has been selected, we inspect its reference bit. If the value is 0, we proceed to replace this page; but if the reference bit is set to 1, we give the page a second chance and move on to select the next FIFO page. When a page gets a second chance, its reference bit is cleared, and its arrival time is reset to the current time. Thus, a page that is given a second chance will not be replaced until all other pages have been replaced (or given second chances). I n addition, if a page is used often enough to keep its reference bit set, it will never be replaced. One way to implement the second-chance algorithm (sometimes referred to as the clock algorithm) is as a circular queue. A pointer (that is, a hand on the clock) indicates which page is to be replaced next. When a frame is needed, the pointer advances until it finds a page with a 0 reference bit. As it advances, it clears the reference bits. Once a victim page is found, the page is replaced, and the new page is inserted in the circular queue in that position. A simple example is illustrated by the figure below, in which small blue digits denotes the reference bit and green arrow denotes the pointer.","title":"LRU-Approximation Page Replacement"},{"location":"osc/ch10/#5-allocation-of-frames","text":"","title":"5 Allocation of Frames"},{"location":"osc/ch10/#minimum-number-of-frames","text":"We must allocate at least a minimum number of frames. One reason is performance. Obviously, as the number of frames allocated to each process decreases, the page-fault rate increases. Another reason is that when a page fault occurs before an executing instruction complete, the instruction must be restarted. So we must have enough frames to hold all the different pages that any single instruction can reference. The minimum number of frames is defined by the computer architecture.","title":"Minimum Number of Frames"},{"location":"osc/ch10/#allocation-algorithms","text":"Equal Allocation The easiest way to split m m frames among n n processes is to give everyone an equal share, m/n m/n frames (ignoring frames needed by the operating system for the moment). Proportional Allocation In proportional allocation, we allocate available memory to each process according to its size. Let the size of the virtual memory for process p_i p_i be s_i s_i , and define S=\\sum s_i S=\\sum s_i . Then, if the total number of available frames is m m , we allocate a_i a_i frames to process p_i p_i , where a_i a_i is approximately a_i=s_i/S\\times m a_i=s_i/S\\times m .","title":"Allocation Algorithms"},{"location":"osc/ch10/#global-versus-local-allocation","text":"Global replacement allows a process to select a replacement frame from the set of all frames, even if that frame is currently allocated to some other process; that is, one process can take a frame from another. Local replacement requires that each process select from only its own set of allocated frames. Local replacement might hinder a process, however, by not making available to it other, less used pages of memory. Thus, global replacement generally results in greater system throughput. It is therefore the more commonly used method. Global Page-Replacement Policy Rather than waiting for the free-frame list to drop to zero before we begin selecting pages for replacement, we trigger page replacement when the list falls below a certain threshold . It attempts to ensure there is always sufficient free memory to satisfy new requests. When the amount of free memory drops below minimum threshold, a kernel routine ( reapers ) is triggered that begins reclaiming pages from all processes in the system. When the amount of free memory reaches the maximum threshold, the reaper routine is suspended. The kernel reaper routine typically uses some form of LRU approximation.","title":"Global versus Local Allocation"},{"location":"osc/ch10/#6-thrashing","text":"","title":"6 Thrashing"},{"location":"osc/ch10/#cause-of-thrashing","text":"Thrashing may be caused by programs or workloads that present insufficient locality of reference (also principle of locality, \u8bbf\u95ee\u5c40\u90e8\u6027): if the working set (\u5de5\u4f5c\u96c6) of a program or a workload cannot be effectively held within physical memory, then constant data swapping, i.e., thrashing, may occur. Consider the following scenario, which is based on the actual behavior of early paging systems. The operating system monitors CPU utilization. If CPU utilizition is too low, we increase the degree of multiprogramming by introducing a new process to the system. A global page-replacement algorithm is used; it replaces pages without regard to the process to which they belong. Now suppose that a process enters a new phase in its execution and needs more frames. It starts faulting and taking frames away from other processes. These processes need those pages, however, and so they also fault, taking frames from other processes. These faulting processes must use the paging device to swap pages in and out. As they queue up for the paging device, the ready queue empties. As processes wait for the paging device, CPU utilization decreases. The CPU scheduler sees the decreasing CPU utilization and increases the degree of multiprogramming as a result. The new process tries to get started by taking frames from running processes, causing more page faults and a longer queue for the paging device. As a result, CPU utilization drops even further, and the CPU scheduler tries to increase the degree of multiprogramming even more. Thrashing has occurred, and system throughput plunges. QUESTION: To prevent thrashing, we must provide a process with as many frames as it needs. But how do we know how many frames it \"needs\"? The locality model of process execution, states that, as a process executes, it moves from locality to locality. A locality is a set of pages that are actively used together. A running program is generally composed of several different localities, which may overlap. If we do not allocate enough frames to accommodate the size of the current locality, the process will thrash, since it cannot keep in memory all the pages that it is actively using. Example Figure below illustrates the concept of locality and how a process\u2019s locality changes over time. At time (a), the locality is the set of pages {18, 19, 20, 21, 22, 23, 24, 29, 30, 33}. At time (b), the locality changes to {18, 19, 20, 24, 25, 26, 27, 28, 29, 31, 32, 33}. Notice the overlap, as some pages (for example, 18, 19, and 20) are part of both localities.","title":"Cause of Thrashing"},{"location":"osc/ch10/#working-set-model","text":"The group of physical memory pages currently dedicated to a specific process is known as the Working set (WS, \u5de5\u4f5c\u96c6) for that process. Example For example, the working set at time t_1 t_1 is {1, 2, 5, 6, 7}. By time t_2 t_2 , the working set has changed to {3, 4}. If we compute the working-set size WSS_i WSS_i for each process in the system, the total demand for frames D D is D=\\sum WSS_i D=\\sum WSS_i . If the total demand D D is greater than the total number of available frames ( D\\gt m D\\gt m ), thrashing will occur, because some processes will not have enough frames. WORKING SET SOLUTION TO THRASHING: The operating system monitors the working set of each process and allocates to that working set enough frames to provide it with its working-set size. If there are enough extra frames, another process can be initiated. If the sum of the working-set sizes increases, exceeding the total number of available frames, the operating system selects a process to suspend. The process\u2019s pages are written out (swapped), and its frames are reallocated to other processes. The suspended process can be restarted later.","title":"Working-Set Model"},{"location":"osc/ch10/#page-fault-frequency","text":"The working-set model is successful but seems a clumsy way to control thrashing. A strategy that uses the page-fault frequency (PFF) takes a more direct approach: Thrashing has a high page-fault rate. Thus, we control the page-fault rate. When the page fault rate is too high, we know that the process needs more frames. Conversely, if it too low, then the process may have too many frames. We can establish upper and lower bounds on the desired page-fault rate. If the actual page-fault rate exceeds the upper limit, we allocate the process. another frame. If the page-fault rate falls below the lower limit, we remove a frame from the process.","title":"Page-Fault Frequency"},{"location":"osc/ch10/#7-memory-compression","text":"When the number of free frames falls below a certain threshold that would triggers page replacement, rather than paging out modified frames to swap space, we compress several frames into a single frame ( memory compression , \u5185\u5b58\u538b\u7f29), enabling the system to reduce memory usage without resorting to swapping pages. Example For example, the free-frame list contains six frames: 7,2,9,21,27,16 , and the modified frame list contains four frames: 15, 3, 35, 26 . In Figure below, frame 7 is removed from the free-frame list. Frames 15, 3, and 35 are compressed and stored in frame 7, which is then stored in the list of compressed frames. The frames 15, 3, and 35 can now be moved to the free-frame list. If one of the three compressed frames is later referenced, a page fault occurs, and the compressed frame is decompressed, restoring the three pages 15, 3, and 35 in memory.","title":"7 Memory Compression"},{"location":"osc/ch10/#8-allocating-kernel-memory","text":"Kernel memory is often allocated from a free-memory pool different from the list used to satisfy ordinary user-mode processes discussed before. There are two primary reasons for this: The kernel requests memory for data structures of varying sizes, some of which are less than a page in size. As a result, the kernel must use memory conservatively and attempt to minimize waste due to fragmentation. Certain hardware devices interact directly with physical memory \u2014\u2014 without the benefit of a virtual memory interface \u2014\u2014 and consequently may require memory residing in physically contiguous pages.","title":"8 Allocating Kernel Memory"},{"location":"osc/ch10/#buddy-system","text":"The buddy system allocates memory from a fixed-size segment consisting of physically contiguous pages. Memory is allocated from this segment using a power-of-2 allocator, which satisfies requests in units sized as a power of 2 (4 KB, 8 KB, 16 KB, and so forth). A request in units not appropriately sized is rounded up to the next highest power of 2. Pro and Cons: An advantage of the buddy system is how quickly adjacent buddies can be combined to form larger segments using a technique known as coalescing (illustrated below in the Example section). Rounding up to the next highest power of 2 is very likely to cause internal fragmentation. Example Assume the size of a memory segment is initially 256 KB and the kernel requests 21 KB of memory. The segment is initially divided into two buddies\u2014which we will call A_L A_L and A_R A_R \u2014\u2014 each 128 KB in size. One of these buddies is further divided into two 64-KB buddies\u2014 B_L B_L and B_R B_R . However, the next-highest power of 2 from 21 KB is 32 KB so either B_L B_L or B_R B_R is again divided into two 32-KB buddies, C_L C_L and C_R C_R . One of these buddies is used to satisfy the 21-KB request. This scheme is illustrated in Figure below, where C_L C_L is the segment allocated to the 21-KB request. when the kernel releases the C_L C_L unit it was allocated, the system can coalesce C_L C_L and C_R C_R into a 64-KB segment. This segment, B_L B_L , can in turn be coalesced with its buddy B_R B_R to form a 128-KB segment. Ultimately, we can end up with the original 256-KB segment.","title":"Buddy System"},{"location":"osc/ch10/#slab-allocations","text":"A second strategy for allocating kernel memory is known as slab allocation . A slab is made up of one or more physically contiguous pages. A cache consists of one or more slabs. Each of caches stores a different type of object. There is one cache per object type.(e.g. a separate cache for the data structure representing process descriptors, a separate cache for file objects). Each cache is populated with objects that are instantiations of the kernel data structure the cache represents.(e.g. the cache representing semaphores stores instances of semaphore objects). Example Linux kernel adopted the slab allocator after Version 2.2. Each slab contains some number of objects, which are the data structures being cached. Each slab is in one of three states: full , partial , or empty . A full slab has no free objects. (All objects in the slab are allocated.) An empty slab has no allocated objects. (All objects in the slab are free.) A partial slab has some allocated objects and some free objects. When some part of the kernel requests a new object, the request is satisfied from a partial slab, if one exists. Otherwise, the request is satisfied from an empty slab. The slab allocator provides two main benefits: No memory is wasted due to fragmentation. Each unique kernel data structure has an associated cache, and each cache is made up of one or more slabs that are divided into chunks the size of the objects being represented. Memory requests can be satisfied quickly . Objects are created in advance and thus can be quickly allocated from the cache. When the kernel has finished with an object and releases it, it is marked as free and returned to its cache, thus making it immediately available for subsequent requests from the kernel.","title":"Slab Allocations"},{"location":"osc/ch13/","text":"Operating System Concepts 13 - File System Interface The file system consists of two distinct parts: a collection of files , each storing related data, and a directory structure, which organizes and provides information about all the files in the system. 1 File Concept A file is named collection of related information that is recorded on secondary storage.(\u6587\u4ef6\u662f\u8bb0\u5f55\u5728\u5916\u5b58\u4e0a\u7684\u76f8\u5173\u4fe1\u606f\u7684\u5177\u6709\u540d\u79f0\u7684\u96c6\u5408)\u3002 A file has a certain defined structure, which depends on its type. A text file is a sequence of characters organized into lines. An executable file is a series of code sections. File Attributes A file's attributes vary from one operating system to another but typically consist of these: Name . The symbolic file name is the only information kept in human-readable form. Identifier . This unique tag, usually a number, Identifier the file within the file system; it is the non-human-readable name for the file. Type . This information is needed for systems that support different types of files. Location . This information is a pointer to a device and to the location of the file on that device. Size . The current size of the file (in bytes, words, or blocks) and possibly the maximum allowed size are included in this attribute. Protection . Access-control information determines who can do reading, writing, executing, and so on. Timestamps and user identification . This information may be kept for creation, last modification, and last use. These data can be useful for protection, security, and usage monitoring. File Operations The operating system must do to perform each of these seven basic file operations. Creating a file . First, space must be allocated for the file. Second, an entry for the new file must be made in a directory. Opening a file . Check access permissions, and if successful, the open call returns a file handle that is used as an argument in the other calls. Writing a file : The system keeps a write pointer to the location in the file where the next write is to take place if it is sequential. Repositioning within a file : The current-file-position pointer of the open file is repositioned to a given value. Deleting a file : Release all file space. Truncating a file : The length of a file can be reset to zero, and its file space be released with all other attributes remain unchanged. These seven basic operations comprise the minimal set of required file operations. These primitive operations can then be combined to perform other file operations. The operating system keeps a table, called the open-file-table , containing information about all open files. When a file operation is requested, the file is specified via an index into this table. When the file is no longer being actively used, it is closed by the process, and the operating system removes its entry from the open-file table, potentially releasing locks. ISSUES: Several different applications open the same file at the same time. SOLUTION: The operating system uses two levels of internal tables: a per-process table and a system-wide table. The per-process table tracks all files that a process has open. It Stores information regarding the process's use of the file (e.g. the current file pointer for each file, access rights to the file and accounting information) Each entry in the per-process table in turn points to a system-wide open-file table. It contains process-independent information(e.g. the location of the file on disk, access dates, and file size). Once a file has been opened by one process, it includes an entry for the file. It also has an open count associated with each file to indicate how many processes have the file open. File locks (\u6587\u4ef6\u9501) are useful for files that are shared by several processes. For example, a system log file that can be accessed and modified by a number of processes in the system. File locks provide functionality similar to reader-writer locks in Chapter 7 . A shared lock (\u5171\u4eab\u9501) is akin to a reader lock in that several processes can acquire the lock concurrently. An exclusive lock (\u6392\u65a5\u9501) is akin to a writer lock in that only one process at a time can acquire such a lock. Furthermore, operating systems(e.g. Linux) may provide either mandatory or advisory file-locking mechanisms. With mandatory locking, once a process acquires an exclusive lock, the operating system will prevent any other process from accessing the locked file. with advisory locking, the operating system will not prevent other process from accessing to the locked file. For advisory locking, it is up to software developers to ensure that locks are appropriately acquired and released. The simple program in Java as follows demonstrating file locking. The program acquires an exclusive lock on the first half of the file and a shared lock on the second half. import java.io.* ; import java.nio.channels.* ; public class LockingExample { public static final boolean EXCLUSIVE = false ; public static final boolean SHARED = true ; public static void main ( String args []) throws IOException { if ( args . length != 1 ) { System . err . println ( Usage: java LockingExample input file ); System . exit ( 0 ); } FileLock sharedLock = null ; FileLock exclusiveLock = null ; try { RandomAccessFile raf = new RandomAccessFile ( args [ 0 ], rw ); // get the channel for the file FileChannel channel = raf . getChannel (); System . out . println ( trying to acquire lock ... ); // this locks the first half of the file - exclusive exclusiveLock = channel . lock ( 0 , raf . length ()/ 2 , EXCLUSIVE ); System . out . println ( lock acquired ... ); /** * Now modify the data . . . */ try { // sleep for 10 seconds Thread . sleep ( 10000 ); } catch ( InterruptedException ie ) { } // release the lock exclusiveLock . release (); System . out . println ( lock released ... ); // this locks the second half of the file - shared sharedLock = channel . lock ( raf . length ()/ 2 + 1 , raf . length (), SHARED ); /** * Now read the data . . . */ // release the lock exclusiveLock . release (); } catch ( java . io . IOException ioe ) { System . err . println ( ioe ); } finally { if ( exclusiveLock != null ) exclusiveLock . release (); if ( sharedLock != null ) sharedLock . release (); } } } File Types A common technique for implementing file types is to include the type as part of the file name. The name is split into two parts\u2014a name and an extension , usually separated by a period. The system uses the extension to indicate the type of the file and the type of operations that can be done on that file The UNIX system uses a magic number [ Wikipedia ] stored at the beginning of some binary files to indicate the type of data in the file (for example, the format of an image file). Not all files have magic numbers. File Structure Certain files must conform to a required structure that is understood by the operating system. For example, the operating system requires that an executable file have a specific structure so that it can determine where in memory to load the file and what the location of the first instruction is. One of the disadvantage of having the operating system support multiple file structures: it makes the operating system large and cumbersome. Some operating systems(UNIX, Windows) impose (and support) a minimal number of file structures. However, all operating systems must support at least one structure \u2014that of an executable file \u2014 so that the system is able to load and run programs. Internal File Structure ISSUE: All disk I/O is performed in units of one block (physical record), and all blocks are the same size. It is unlikely that the physical record size will exactly match the length of the desired logical record. SOLUTION: Packing a number of logical records into physical blocks . EXAMPLE: The UNIX operating system defines all files to be simply streams of bytes. Each byte is individually addressable by its offset from the beginning (or end) of the file. In this case, the logical record size is 1 byte. The file system automatically packs and unpacks bytes into physical disk blocks - say, 512 bytes per block \u2014 as necessary. 2 Access Methods Files store information. The information in the file can be accessed in several ways. Sequential Access The simplest access method is sequential access (\u987a\u5e8f\u8bbf\u95ee). Information in the file is processed in order, one record after the other. It is by far the most common access method. Direct Access For direct access (\u76f4\u63a5\u8bbf\u95ee) or relative access (\u76f8\u5bf9\u8bbf\u95ee), a file is made up of fixed-length logical records that allow programs to read and write records rapidly in no particular order. The file is viewed as a numbered sequence of blocks or records. Direct-access files are of great use for immediate access to large amounts of information. (e.g. databases, we compute which block contains the answer and then read that block directly when querying) A relative block number is provided by the user to the operating system. It is an index relative to the beginning of the file. Other Access Methods Other access methods generally involve the construction of an index for the file. The index contains pointers to the various blocks. To find a record in the file, we first search the index and then use the pointer to access the file directly and to find the desired record. 3 Directory Structure The directory can be viewed as a symbol table that translates file names into their file control blocks. The directory can be single-level (\u5355\u5c42), two-level (\u53cc\u5c42), and tree-structured (\u6811\u72b6). Within a file system, it is useful to create directories to allow files to be organized. A single-level directory in a multiuser system causes naming problems, since each file must have a unique name. A two-level directory solves this problem by creating a separate directory for each user\u2019s files. The directory lists the files by name and includes the file\u2019s location on the disk, length, type, owner, time of creation, time of last use, and so on. The natural generalization of a two-level directory is a tree-structured directory . A tree-structured directory allows a user to create subdirectories to organize files. Acyclic-graph directory structures enable users to share subdirectories and files but complicate searching and deletion. A general graph structure allows complete flexibility in the sharing of files and directories but sometimes requires garbage collection to recover unused disk space. 4 Protection When information is stored in a computer system, we want to keep it safe from physical damage (the issue of reliability ) and improper access (the issue of protection ). Types of Access Protection mechanisms provide controlled access by limiting the types of file access that can be made. Access is permitted or denied depending on several factors, one of which is the type of access requested. Several different types of operations may be controlled: Read . Read from the file. Write . Write or rewrite the file. Execute . Load the file into memory and execute it. Append . Write new information at the end of the file. Delete . Delete the file and free its space for possible reuse. List . List the name and attributes of the file. Attribute change . Changing the attributes of the file. Many protection mechanism have been proposed and we have a more detailed description in Chapter 17 . Access Control The most common approach to the protection problem is to make access dependent on the identity of the user. Different users may need different types of access to a file or directory. The most general scheme to implement identity dependent access is to associate with each file and directory an access-control list (ACL, \u8bbf\u95ee\u63a7\u5236\u5217\u8868) specifying user names and the types of access allowed for each user. If a user is listed for the requested access, the access is allowed. Otherwise, a protection violation occurs, and the user job is denied access to the file. ISSUES: If we want to allow everyone to read a file, we must list all users with read accesses. SOLUTION: A condensed version of the access list, many system recognize three classifications of users in connection with each file: Owner. The user who created the file is the owner. Group. A set of users who are sharing the file and need similar access is a group, or work group Other. All other users in the system. Other Protection Approaches Another approach to the protection problem, is to associate a password with each file. Permissions in UNIX In the UNIX system, directory protection and file protection are handled similarly. Associated with each file and directory are three fields - owner, group, and universe -- each consisting of the three bits rwx , where r controls read access, w controls write access, and x controls execution. A sample directory listing from a UNIX environment is shown in below: 5 Memory-Mapped Files Memory mapping (\u5185\u5b58\u6620\u5c04) a file is accomplished by mapping a disk block to a page in memory. Initial access to the file, results in a page fault. A page-sized portion of the file is read from the file system into a physical page. Subsequent reads and writes to the file are handled as routine memory accesses. Advantage Manipulating files through memory rather than incurring the overhead of using read() and write() system calls simplifies and speeds up file access and usuage. Multiple processes may be allowed to map the same file concurrently, to allow sharing of data. The virtual memory map of each sharing process points to the same page of physical memory. The memory-mapping system calls can also support copy-on-write functionality, allowing processes to share a file in read-only mode but to have their own copies of any data they modify. Quite often, shared memory is in fact implemented by memory mapping files. Under this scenario, processes can communicate using shared memory by having the communicating processes memory-map the same file into their virtual address spaces(e.g. POSIX shared memory ).","title":"Chapter 13: File-System Interfaces"},{"location":"osc/ch13/#operating-system-concepts-13-file-system-interface","text":"The file system consists of two distinct parts: a collection of files , each storing related data, and a directory structure, which organizes and provides information about all the files in the system.","title":"Operating System Concepts 13 - File System Interface"},{"location":"osc/ch13/#1-file-concept","text":"A file is named collection of related information that is recorded on secondary storage.(\u6587\u4ef6\u662f\u8bb0\u5f55\u5728\u5916\u5b58\u4e0a\u7684\u76f8\u5173\u4fe1\u606f\u7684\u5177\u6709\u540d\u79f0\u7684\u96c6\u5408)\u3002 A file has a certain defined structure, which depends on its type. A text file is a sequence of characters organized into lines. An executable file is a series of code sections.","title":"1 File Concept"},{"location":"osc/ch13/#file-attributes","text":"A file's attributes vary from one operating system to another but typically consist of these: Name . The symbolic file name is the only information kept in human-readable form. Identifier . This unique tag, usually a number, Identifier the file within the file system; it is the non-human-readable name for the file. Type . This information is needed for systems that support different types of files. Location . This information is a pointer to a device and to the location of the file on that device. Size . The current size of the file (in bytes, words, or blocks) and possibly the maximum allowed size are included in this attribute. Protection . Access-control information determines who can do reading, writing, executing, and so on. Timestamps and user identification . This information may be kept for creation, last modification, and last use. These data can be useful for protection, security, and usage monitoring.","title":"File Attributes"},{"location":"osc/ch13/#file-operations","text":"The operating system must do to perform each of these seven basic file operations. Creating a file . First, space must be allocated for the file. Second, an entry for the new file must be made in a directory. Opening a file . Check access permissions, and if successful, the open call returns a file handle that is used as an argument in the other calls. Writing a file : The system keeps a write pointer to the location in the file where the next write is to take place if it is sequential. Repositioning within a file : The current-file-position pointer of the open file is repositioned to a given value. Deleting a file : Release all file space. Truncating a file : The length of a file can be reset to zero, and its file space be released with all other attributes remain unchanged. These seven basic operations comprise the minimal set of required file operations. These primitive operations can then be combined to perform other file operations. The operating system keeps a table, called the open-file-table , containing information about all open files. When a file operation is requested, the file is specified via an index into this table. When the file is no longer being actively used, it is closed by the process, and the operating system removes its entry from the open-file table, potentially releasing locks. ISSUES: Several different applications open the same file at the same time. SOLUTION: The operating system uses two levels of internal tables: a per-process table and a system-wide table. The per-process table tracks all files that a process has open. It Stores information regarding the process's use of the file (e.g. the current file pointer for each file, access rights to the file and accounting information) Each entry in the per-process table in turn points to a system-wide open-file table. It contains process-independent information(e.g. the location of the file on disk, access dates, and file size). Once a file has been opened by one process, it includes an entry for the file. It also has an open count associated with each file to indicate how many processes have the file open. File locks (\u6587\u4ef6\u9501) are useful for files that are shared by several processes. For example, a system log file that can be accessed and modified by a number of processes in the system. File locks provide functionality similar to reader-writer locks in Chapter 7 . A shared lock (\u5171\u4eab\u9501) is akin to a reader lock in that several processes can acquire the lock concurrently. An exclusive lock (\u6392\u65a5\u9501) is akin to a writer lock in that only one process at a time can acquire such a lock. Furthermore, operating systems(e.g. Linux) may provide either mandatory or advisory file-locking mechanisms. With mandatory locking, once a process acquires an exclusive lock, the operating system will prevent any other process from accessing the locked file. with advisory locking, the operating system will not prevent other process from accessing to the locked file. For advisory locking, it is up to software developers to ensure that locks are appropriately acquired and released. The simple program in Java as follows demonstrating file locking. The program acquires an exclusive lock on the first half of the file and a shared lock on the second half. import java.io.* ; import java.nio.channels.* ; public class LockingExample { public static final boolean EXCLUSIVE = false ; public static final boolean SHARED = true ; public static void main ( String args []) throws IOException { if ( args . length != 1 ) { System . err . println ( Usage: java LockingExample input file ); System . exit ( 0 ); } FileLock sharedLock = null ; FileLock exclusiveLock = null ; try { RandomAccessFile raf = new RandomAccessFile ( args [ 0 ], rw ); // get the channel for the file FileChannel channel = raf . getChannel (); System . out . println ( trying to acquire lock ... ); // this locks the first half of the file - exclusive exclusiveLock = channel . lock ( 0 , raf . length ()/ 2 , EXCLUSIVE ); System . out . println ( lock acquired ... ); /** * Now modify the data . . . */ try { // sleep for 10 seconds Thread . sleep ( 10000 ); } catch ( InterruptedException ie ) { } // release the lock exclusiveLock . release (); System . out . println ( lock released ... ); // this locks the second half of the file - shared sharedLock = channel . lock ( raf . length ()/ 2 + 1 , raf . length (), SHARED ); /** * Now read the data . . . */ // release the lock exclusiveLock . release (); } catch ( java . io . IOException ioe ) { System . err . println ( ioe ); } finally { if ( exclusiveLock != null ) exclusiveLock . release (); if ( sharedLock != null ) sharedLock . release (); } } }","title":"File Operations"},{"location":"osc/ch13/#file-types","text":"A common technique for implementing file types is to include the type as part of the file name. The name is split into two parts\u2014a name and an extension , usually separated by a period. The system uses the extension to indicate the type of the file and the type of operations that can be done on that file The UNIX system uses a magic number [ Wikipedia ] stored at the beginning of some binary files to indicate the type of data in the file (for example, the format of an image file). Not all files have magic numbers.","title":"File Types"},{"location":"osc/ch13/#file-structure","text":"Certain files must conform to a required structure that is understood by the operating system. For example, the operating system requires that an executable file have a specific structure so that it can determine where in memory to load the file and what the location of the first instruction is. One of the disadvantage of having the operating system support multiple file structures: it makes the operating system large and cumbersome. Some operating systems(UNIX, Windows) impose (and support) a minimal number of file structures. However, all operating systems must support at least one structure \u2014that of an executable file \u2014 so that the system is able to load and run programs.","title":"File Structure"},{"location":"osc/ch13/#internal-file-structure","text":"ISSUE: All disk I/O is performed in units of one block (physical record), and all blocks are the same size. It is unlikely that the physical record size will exactly match the length of the desired logical record. SOLUTION: Packing a number of logical records into physical blocks . EXAMPLE: The UNIX operating system defines all files to be simply streams of bytes. Each byte is individually addressable by its offset from the beginning (or end) of the file. In this case, the logical record size is 1 byte. The file system automatically packs and unpacks bytes into physical disk blocks - say, 512 bytes per block \u2014 as necessary.","title":"Internal File Structure"},{"location":"osc/ch13/#2-access-methods","text":"Files store information. The information in the file can be accessed in several ways.","title":"2 Access Methods"},{"location":"osc/ch13/#sequential-access","text":"The simplest access method is sequential access (\u987a\u5e8f\u8bbf\u95ee). Information in the file is processed in order, one record after the other. It is by far the most common access method.","title":"Sequential Access"},{"location":"osc/ch13/#direct-access","text":"For direct access (\u76f4\u63a5\u8bbf\u95ee) or relative access (\u76f8\u5bf9\u8bbf\u95ee), a file is made up of fixed-length logical records that allow programs to read and write records rapidly in no particular order. The file is viewed as a numbered sequence of blocks or records. Direct-access files are of great use for immediate access to large amounts of information. (e.g. databases, we compute which block contains the answer and then read that block directly when querying) A relative block number is provided by the user to the operating system. It is an index relative to the beginning of the file.","title":"Direct Access"},{"location":"osc/ch13/#other-access-methods","text":"Other access methods generally involve the construction of an index for the file. The index contains pointers to the various blocks. To find a record in the file, we first search the index and then use the pointer to access the file directly and to find the desired record.","title":"Other Access Methods"},{"location":"osc/ch13/#3-directory-structure","text":"The directory can be viewed as a symbol table that translates file names into their file control blocks. The directory can be single-level (\u5355\u5c42), two-level (\u53cc\u5c42), and tree-structured (\u6811\u72b6). Within a file system, it is useful to create directories to allow files to be organized. A single-level directory in a multiuser system causes naming problems, since each file must have a unique name. A two-level directory solves this problem by creating a separate directory for each user\u2019s files. The directory lists the files by name and includes the file\u2019s location on the disk, length, type, owner, time of creation, time of last use, and so on. The natural generalization of a two-level directory is a tree-structured directory . A tree-structured directory allows a user to create subdirectories to organize files. Acyclic-graph directory structures enable users to share subdirectories and files but complicate searching and deletion. A general graph structure allows complete flexibility in the sharing of files and directories but sometimes requires garbage collection to recover unused disk space.","title":"3 Directory Structure"},{"location":"osc/ch13/#4-protection","text":"When information is stored in a computer system, we want to keep it safe from physical damage (the issue of reliability ) and improper access (the issue of protection ).","title":"4 Protection"},{"location":"osc/ch13/#types-of-access","text":"Protection mechanisms provide controlled access by limiting the types of file access that can be made. Access is permitted or denied depending on several factors, one of which is the type of access requested. Several different types of operations may be controlled: Read . Read from the file. Write . Write or rewrite the file. Execute . Load the file into memory and execute it. Append . Write new information at the end of the file. Delete . Delete the file and free its space for possible reuse. List . List the name and attributes of the file. Attribute change . Changing the attributes of the file. Many protection mechanism have been proposed and we have a more detailed description in Chapter 17 .","title":"Types of Access"},{"location":"osc/ch13/#access-control","text":"The most common approach to the protection problem is to make access dependent on the identity of the user. Different users may need different types of access to a file or directory. The most general scheme to implement identity dependent access is to associate with each file and directory an access-control list (ACL, \u8bbf\u95ee\u63a7\u5236\u5217\u8868) specifying user names and the types of access allowed for each user. If a user is listed for the requested access, the access is allowed. Otherwise, a protection violation occurs, and the user job is denied access to the file. ISSUES: If we want to allow everyone to read a file, we must list all users with read accesses. SOLUTION: A condensed version of the access list, many system recognize three classifications of users in connection with each file: Owner. The user who created the file is the owner. Group. A set of users who are sharing the file and need similar access is a group, or work group Other. All other users in the system.","title":"Access Control"},{"location":"osc/ch13/#other-protection-approaches","text":"Another approach to the protection problem, is to associate a password with each file.","title":"Other Protection Approaches"},{"location":"osc/ch13/#permissions-in-unix","text":"In the UNIX system, directory protection and file protection are handled similarly. Associated with each file and directory are three fields - owner, group, and universe -- each consisting of the three bits rwx , where r controls read access, w controls write access, and x controls execution. A sample directory listing from a UNIX environment is shown in below:","title":"Permissions in UNIX"},{"location":"osc/ch13/#5-memory-mapped-files","text":"Memory mapping (\u5185\u5b58\u6620\u5c04) a file is accomplished by mapping a disk block to a page in memory. Initial access to the file, results in a page fault. A page-sized portion of the file is read from the file system into a physical page. Subsequent reads and writes to the file are handled as routine memory accesses. Advantage Manipulating files through memory rather than incurring the overhead of using read() and write() system calls simplifies and speeds up file access and usuage. Multiple processes may be allowed to map the same file concurrently, to allow sharing of data. The virtual memory map of each sharing process points to the same page of physical memory. The memory-mapping system calls can also support copy-on-write functionality, allowing processes to share a file in read-only mode but to have their own copies of any data they modify. Quite often, shared memory is in fact implemented by memory mapping files. Under this scenario, processes can communicate using shared memory by having the communicating processes memory-map the same file into their virtual address spaces(e.g. POSIX shared memory ).","title":"5 Memory-Mapped Files"},{"location":"osc/ch17/","text":"Operating System Concepts 17 Protection Goals of Protection Protection was originally conceived as an adjunct to multiprogramming operating system, to allow untrustworthy users to safely share a common logical name space, such as a directory files, or a common physical name space, such as memory. Modern protection, to increase the reliability of any complex system that makes use of shared resources and is connected to insecure communications platforms such as Internet. Protection are needed for several reasons. Preventing the mischievous, intentional violation of an access restriction by a user is needed. Protection can improve reliability by detecting latent errors at the interfaces between component subsystems. provide a mechanism for the enforcement of the policies governing resource use","title":"Ch17"},{"location":"osc/ch17/#operating-system-concepts-17-protection","text":"","title":"Operating System Concepts 17 Protection"},{"location":"osc/ch17/#goals-of-protection","text":"Protection was originally conceived as an adjunct to multiprogramming operating system, to allow untrustworthy users to safely share a common logical name space, such as a directory files, or a common physical name space, such as memory. Modern protection, to increase the reliability of any complex system that makes use of shared resources and is connected to insecure communications platforms such as Internet. Protection are needed for several reasons. Preventing the mischievous, intentional violation of an access restriction by a user is needed. Protection can improve reliability by detecting latent errors at the interfaces between component subsystems. provide a mechanism for the enforcement of the policies governing resource use","title":"Goals of Protection"},{"location":"osc/ch9/","text":"Operating System Concepts 9 - Main Memory 1 Background Basic Hardware Main memory and the registers built into each processing core are the only general-purpose storage that the CPU can access directly. Registers that are built into each CPU core are generally accessible within one cycle of the CPU clock. Completing a memory access may take hundreds cycles of the CPU clock. In such cases, the processor normally needs to stall. The remedy is to add cache between the CPU and main memory for fast access. Table. Approximate timing for various operations on a typical PC operations approximate timing execute typical instruction 1/1,000,000,000 sec = 1 nanosec fetch from L1 cache memory 0.5 nanosec branch misprediction 5 nanosec fetch from L2 cache memory 7 nanosec Mutex lock/unlock 25 nanosec fetch from main memory 100 nanosec send 2K bytes over 1Gbps network 20,000 nanosec read 1MB sequentially from memory 250,000 nanosec fetch from new disk location (seek) 8,000,000 nanosec read 1MB sequentially from disk 20,000,000 nanosec send packet US to Europe and back 150 milliseconds = 150,000,000 nanosec We also must protect the operating system from access by user processes, as well as protect user processes from one another. This protection must be provided by the hardware, because the operating system doesn't usually intervene between the CPU and its memory access. Hardware implements this production in several different ways. We first need to make sure that each process have a separate memory space. Separate per-process memory space protects the processes from each other and is fundamental to having multiple processes loading in memory for concurrent execution. To separate memory spaces, we need the ability to determine the range of legal addresses that the process may access and to ensure that the process can access only these legal addresses. We can provide this protection by using two registers, usually a base and a limit. The base register holds the smallest legal physical memory address; the limit register specifies the size of the range. The base and limit registers can be loaded only by the operating system, which uses a special privileged instruction. Any attempt by a program executing in user mode to access operating-system memory or other users\u2019 memory results in a trap to the operating system, which treats the attempt as a fatal error. Address Binding In most cases, a user program goes through several steps. Addresses may be represented in different ways during these steps. Addresses in the source program are generally symbolic(\u7b26\u53f7). A compiler typically binds these symbolic addresses to relocatable addresses(\u53ef\u91cd\u5b9a\u4f4d\u5730\u5740). The linker or loader binds the relocatable addresses to absolute addresses(\u7edd\u5bf9\u5730\u5740)\u3002 Logical Versus Physical Address Space An address generated by the CPU is commonly referred to as a logical address (\u903b\u8f91\u5730\u5740), whereas an address loaded into memory-address register (MAR) of the memory is commonly referred to as a physical address (\u7269\u7406\u5730\u5740). We usually refer to the logical address as a virtual address (\u865a\u62df\u5730\u5740). The run-time mapping from virtual to physical addresses is done by a hardware device called the memory-management unit (MMU\uff0c\u5185\u5b58\u7ba1\u7406\u5355\u5143). Note Program counter points to the next instruction to be fetched/executed , whereas memory address register points to the memory location that contains data required (not an instruction).[ ref ] Dynamic Loading With dynamic loading (\u52a8\u6001\u52a0\u8f7d), a routine is not loaded until it is called. All routines are kept on disk in a relocatable load format. The main program is loaded into memory and is executed. When a routine needs to call another routine, the calling routine first checks to see whether the other routine has been loaded. If it has not, the relocatable linking loader is called to load the desired routine into memory and to update the program\u2019s address tables to reflect this change. Then control is passed to the newly loaded routine. The advantage of dynamic loading is that a routine is loaded only when it is needed. This method is particularly useful when large amounts of code are needed to handle infrequently occurring cases, such as error routines. A simple example: void * hndl = dlopen ( libnewshapes.so , RTLD_NOW ); if ( hndl == NULL ){ cerr dlerror () endl ; exit ( - 1 ); } void * mkr = dlsym ( hndl , maker ); Dynamic Linking and Shared Libraries Dynamically linked libraries (DLLs\uff0c\u52a8\u6001\u94fe\u63a5\u5e93) are system libraries that are linked to user programs when the programs are run Detailed information in CSAPP 2 Contiguous Memory Allocation In order to accommodate both the operating system and the various user processes, allocating main memory are needed to be in the most efficient way. In Contiguous memory allocation (\u8fde\u7eed\u5185\u5b58\u5206\u914d), one early method, each process is contained in a single section of memory that is contiguous to the section containing the next process . The memory is usually divided into two partitions: one for the operating system and one for the user processes. We can place the operating system in either low memory addresses or high memory addresses. Many operating system(including Linux and windows) place the operating system in high memory. Memory Protection Relocation register (\u91cd\u5b9a\u4f4d\u5bc4\u5b58\u5668) and limit register (\u754c\u9650\u5730\u5740\u5bc4\u5b58\u5668) are used to prevent a process from accessing memory that it does not own. The relocation register contains the value of the smallest physical address. The limit register contains the range of logical addresses. Each logical address must fall within the range specified by the limit register. The MMU maps the logical address dynamically by adding the value in the relocation register. Memory Allocation One of the simplest methods of allocating memory is to assign processes to variably sized partitions in memory, where each partition may contain exactly one process. The operating system keeps a table indicating which parts of memory are available and which are occupied. A block of available memory is a hole (\u5b54). When a process arrives and needs memory, the system searches for a hole that is large enough for this process. Placement policy (\u653e\u7f6e\u7b56\u7565)\uff1aSeveral Ways to satisfy a request of size n n from a list of free holes. First fit (\u9996\u6b21\u9002\u914d). Allocate the first hole that is big enough. Best fit (\u6700\u4f73\u9002\u914d). Allocate the smallest hole that is big enough. Worst fit (\u6700\u5dee\u9002\u914d). Allocate the largest hole. Both first fit and best fit are better than worst fit in terms of decreasing time and storage utilization. Neither first fit nor best fit is clearly better than the other in terms of storage utilization, but first fit is generally faster. Fragmentation External fragmentation (\u5916\u90e8\u788e\u7247) exists when there is enough total memory space to satisfy a request but the available spaces are not contiguous: storage is fragmented into a large number of small holes. One solution is compaction \uff0cwhich shuffles the memory contents so as to place all free memory together in one large block. Another solution is to permit the logical address space of processes to be noncontiguous, thus allowing a process to be allocated physical memory wherever such memory is available.( paging , \u5206\u9875, the most common memory-management technique for computer systems). Internal fragmentation (\u5185\u90e8\u788e\u7247) occurs when unused memory is internal to a partition, because of allocating memory in units based on block size. The memory must be partitioned into variable sized blocks and assign the best fit block to the process. 3 Paging Paging (\u5206\u9875), is a memory management scheme that permits a process's physical address space to be non-contiguous. Paging avoids external fragmentation and the associated need for compaction. Basic Method The basic method for implementing paging involves breaking physical memory info fixed-sized blocks called frames (\u5e27, also physical page), and breaking logical memory into blocks of the same size called pages (\u9875, also virtual page). The backing store is divided into fixed-sized blocks that are the same size as the memory frames or clusters of multiple frames. Every address generated by the CPU is divided into two parts: a page number (p, \u9875\u53f7) and a page offset (d, \u9875\u504f\u79fb)\u3002 The page number is used as an index into a per-process page table . Page table contains the base address of each frame in physical memory and the offset is the location in the frame being referenced. The base address of the frame is combined with the page offset to define the physical memory address. The MMU translates a logical address generated by the CPU to a physical address by: Extract the page number p p and use it as an index into the page table. Extract the corresponding frame number f f from the page table. Replace the page number p p in the logical address with the frame number f f . The page size is defined by the hardware, typically a power of 2. If the size of logical address space is $2^m $, and a page size is $2^n $ bytes, then the high-order m-n m-n bits of a logical address designate the page number, and the n n low-order bits designate the page offset. Internal fragmentation still exists\uff1a If process size is independent of page size, we expect internal fragmentation to average one-half page per process. This consideration suggests that small page sizes are desirable. However, overhead is involved in each page table entry, and this overhead is reduced as the size of the pages increases. Also, disk I/O is more efficient when the amount of data being transferred is larger Hardware Suport The hardware implementation of the page table can be done in several ways. In the simplest case, the page table is implemented as a set of dedicated high-speed hardware registers. It makes the page-address translation very efficient. It increases context-switch time, as each one of these registers must be exchanged during a context switch. The use of registers for the page table is satisfactory if the page table is reasonably small (for example, 256 entries). It is not feasible for most contemporary CPUs, since much larger page tables needed. For a much large page table, the page table is kept in main memory, and a page-table base register (PTBR, \u9875\u8868\u57fa\u5740\u5bc4\u5b58\u5668) points to the page table. ISSUE: Although storing the page table in main memory can yield faster context switches, it may also result in slower memory access times. Because using page-table to find a physical address need one memory access, since the page-table is kept in main memory. SOLUTION: To use a special, small, fast-lookup hardware cache called a translation look-aside buffer (TLB, \u8f6c\u8bd1\u540e\u5907\u7f13\u51b2\u5668). Each entry in the TLB consists of two parts: a key(tag) and a value. When the associative memory is presented with an item, the item is compared with all keys simultaneously . If the item is found, the corresponding value field is returned. TLB is kept small, typically between 32 and 1,024 entries in size. CPUs today may provide multiple levels of TLBs. We take the Intel Core i7 as an example [ see detail here ]: The Intel Core i7 CPU has a 128-entry L1 instruction TLB and a 64-entry L1 data TLB. In the case of a miss at L1, it takes the CPU six cycles to check for the entry in the L2 512-entry TLB. A miss in L2 means that the CPU must either walk through the page-table entries in memory to find the associated frame address, which can take hundreds of cycles, or interrupt to the operating system to have it do the work. Protection Memory protection in a paged environment is accomplished by protection bits associated with each frame. Normally, these bits are kept in the page table. One bit can define a page to be read\u2013write or read-only. One additional bit is generally attached to each entry in the page table: a valid\u2013invalid bit. (When the bit is set to invalid, the page is not in the process\u2019s logical address space). Shared Pages An advantage of paging is the possibility of sharing common code. Reentrant code is non-self-modifying code: it never changes during execution. If the code is reentrant code, it can be shared. Two or more processes can execute the same code at the same time. Each process has its own copy of registers and data storage to hold the data for the process\u2019s execution. The shared libraries are typically implemented with shared pages. Some operating systems implement shared memory using shared pages. Example: Sharing of standard C library in a paging environment. 4 Structure of the Page Table Hierarchical Paging Problem: Most modern computer systems support a large logical address space 2^{32}-2^{64} 2^{32}-2^{64} . In such an environment, the page table itself becomes excessively large. SOLUTION: Hierarchical Paging (\u5c42\u6b21\u5316\u5206\u9875) uses multi-level tables which break up the virtual address into multiple parts. The simplest way is to use a two-level paging algorithm, in which the page table itself is also paged. For a system with a 64-bit logical address space, a two-level paging scheme is no longer appropriate. So we can page the outer page table, giving us a three-level paging scheme. But the outer page table is still large in size. The next step would be a four-level paging scheme, where the second-level outer page table itself is also paged, and so forth. For 64-bit architectures, hierarchical page tables are generally considered inappropriate. Hashed Page Tables One approach for handling address spaces larger than 32 bits is to use a hashed page table (\u54c8\u5e0c\u9875\u8868), with the hash value being the virtual page number. Each entry in the hash table contains a linked list of elements that hash to the same location (to handle collisions). Each element consists of three fields: the virtual page number the value of the mapped page frame a pointer to the next element in the linked list. The algorithm works as follows: The virtual page number in the virtual address is hashed into the hash table. The virtual page number is compared with field 1 in the first element in the linked list. If there is a match, the corresponding page frame (field 2) is used to form the desired physical address. If there is no match, subsequent entries in the linked list are searched for a matching virtual page number. Clustered page tables , which are similar to hashed page tables except that each entry in the hash table refers to several pages (such as 16) rather than a single page. It is particularly useful for sparse address spaces, where memory references are noncontiguous and scattered throughout the address space. Inverted Page Tables Problem: One of the drawbacks of methods above is that each page table may consist of millions of entries. These tables may consume large amounts of physical memory just to keep track of how other physical memory is being used. SOLUTION: An inverted page table (\u53cd\u5411\u9875\u8868) has one entry for each real page (or frame) of memory. Each entry consists of the virtual address of the page stored in that real memory location, with information about the process that owns the page. Only one page table is in the system, and it has only one entry for each page of physical memory. DISADVANTAGE: It increases the amount of time needed to search the table when a page reference occurs. 5 Swapping A process, or a portion of a process, can be swapped temporarily out of memory to a backing store and then brought back into memory for continued execution. (The backing store is commonly fast secondary storage.) Swapping makes it possible for the total physical address space of all processes to exceed the real physical memory of the system, thus increasing the degree of multiprogramming in a system. Standard Swapping Standard swapping involves moving entire processes between main memory and a backing store. For a multithreaded process, all per-thread data structures must be swapped as well. The advantage of standard swapping is that it allows physical memory to be oversubscribed , so that the system can accommodate more processes than there is actual physical memory to store them. Idle or mostly idle processes are good candidates for swapping. Swapping with paging ISSUE: For standard swapping, the amount of time required to move entire processes between memory and the backing store is prohibitive. It was used in traditional UNIX systems, but it is generally no longer used in contemporary operating systems. SOLUTION: Most systems, including Linux and Windows, use a variation of swapping in which pages of a process\u2014rather than an entire process \u2014can be swapped. A page out operation moves a page from memory to the backing store; the reverse process is known as a page in . Swapping on Mobile Systems ISSUE: In contrast, mobile systems typically do not support swapping in any form. Space constraint: Mobile devices generally use flash memory rather than more spacious hard disks for nonvolatile storage. Other reasons include the limited number of writes that flash memory can tolerate before it becomes unreliable and the poor throughput between main memory and flash memory in these devices. SOLUTION: Instead of using swapping, when free memory falls below a certain threshold, Apple\u2019s iOS asks applications to voluntarily relinquish allocated memory. Any applications that fail to free up sufficient memory may be terminated by the operating system. 6 Example: Intel 64-bit Architectures Core i7 Memory System","title":"Chapter 9: Main Memory"},{"location":"osc/ch9/#operating-system-concepts-9-main-memory","text":"","title":"Operating System Concepts 9 - Main Memory"},{"location":"osc/ch9/#1-background","text":"","title":"1 Background"},{"location":"osc/ch9/#basic-hardware","text":"Main memory and the registers built into each processing core are the only general-purpose storage that the CPU can access directly. Registers that are built into each CPU core are generally accessible within one cycle of the CPU clock. Completing a memory access may take hundreds cycles of the CPU clock. In such cases, the processor normally needs to stall. The remedy is to add cache between the CPU and main memory for fast access. Table. Approximate timing for various operations on a typical PC operations approximate timing execute typical instruction 1/1,000,000,000 sec = 1 nanosec fetch from L1 cache memory 0.5 nanosec branch misprediction 5 nanosec fetch from L2 cache memory 7 nanosec Mutex lock/unlock 25 nanosec fetch from main memory 100 nanosec send 2K bytes over 1Gbps network 20,000 nanosec read 1MB sequentially from memory 250,000 nanosec fetch from new disk location (seek) 8,000,000 nanosec read 1MB sequentially from disk 20,000,000 nanosec send packet US to Europe and back 150 milliseconds = 150,000,000 nanosec We also must protect the operating system from access by user processes, as well as protect user processes from one another. This protection must be provided by the hardware, because the operating system doesn't usually intervene between the CPU and its memory access. Hardware implements this production in several different ways. We first need to make sure that each process have a separate memory space. Separate per-process memory space protects the processes from each other and is fundamental to having multiple processes loading in memory for concurrent execution. To separate memory spaces, we need the ability to determine the range of legal addresses that the process may access and to ensure that the process can access only these legal addresses. We can provide this protection by using two registers, usually a base and a limit. The base register holds the smallest legal physical memory address; the limit register specifies the size of the range. The base and limit registers can be loaded only by the operating system, which uses a special privileged instruction. Any attempt by a program executing in user mode to access operating-system memory or other users\u2019 memory results in a trap to the operating system, which treats the attempt as a fatal error.","title":"Basic Hardware"},{"location":"osc/ch9/#address-binding","text":"In most cases, a user program goes through several steps. Addresses may be represented in different ways during these steps. Addresses in the source program are generally symbolic(\u7b26\u53f7). A compiler typically binds these symbolic addresses to relocatable addresses(\u53ef\u91cd\u5b9a\u4f4d\u5730\u5740). The linker or loader binds the relocatable addresses to absolute addresses(\u7edd\u5bf9\u5730\u5740)\u3002","title":"Address Binding"},{"location":"osc/ch9/#logical-versus-physical-address-space","text":"An address generated by the CPU is commonly referred to as a logical address (\u903b\u8f91\u5730\u5740), whereas an address loaded into memory-address register (MAR) of the memory is commonly referred to as a physical address (\u7269\u7406\u5730\u5740). We usually refer to the logical address as a virtual address (\u865a\u62df\u5730\u5740). The run-time mapping from virtual to physical addresses is done by a hardware device called the memory-management unit (MMU\uff0c\u5185\u5b58\u7ba1\u7406\u5355\u5143). Note Program counter points to the next instruction to be fetched/executed , whereas memory address register points to the memory location that contains data required (not an instruction).[ ref ]","title":"Logical Versus Physical Address Space"},{"location":"osc/ch9/#dynamic-loading","text":"With dynamic loading (\u52a8\u6001\u52a0\u8f7d), a routine is not loaded until it is called. All routines are kept on disk in a relocatable load format. The main program is loaded into memory and is executed. When a routine needs to call another routine, the calling routine first checks to see whether the other routine has been loaded. If it has not, the relocatable linking loader is called to load the desired routine into memory and to update the program\u2019s address tables to reflect this change. Then control is passed to the newly loaded routine. The advantage of dynamic loading is that a routine is loaded only when it is needed. This method is particularly useful when large amounts of code are needed to handle infrequently occurring cases, such as error routines. A simple example: void * hndl = dlopen ( libnewshapes.so , RTLD_NOW ); if ( hndl == NULL ){ cerr dlerror () endl ; exit ( - 1 ); } void * mkr = dlsym ( hndl , maker );","title":"Dynamic Loading"},{"location":"osc/ch9/#dynamic-linking-and-shared-libraries","text":"Dynamically linked libraries (DLLs\uff0c\u52a8\u6001\u94fe\u63a5\u5e93) are system libraries that are linked to user programs when the programs are run Detailed information in CSAPP","title":"Dynamic Linking and Shared Libraries"},{"location":"osc/ch9/#2-contiguous-memory-allocation","text":"In order to accommodate both the operating system and the various user processes, allocating main memory are needed to be in the most efficient way. In Contiguous memory allocation (\u8fde\u7eed\u5185\u5b58\u5206\u914d), one early method, each process is contained in a single section of memory that is contiguous to the section containing the next process . The memory is usually divided into two partitions: one for the operating system and one for the user processes. We can place the operating system in either low memory addresses or high memory addresses. Many operating system(including Linux and windows) place the operating system in high memory.","title":"2 Contiguous Memory Allocation"},{"location":"osc/ch9/#memory-protection","text":"Relocation register (\u91cd\u5b9a\u4f4d\u5bc4\u5b58\u5668) and limit register (\u754c\u9650\u5730\u5740\u5bc4\u5b58\u5668) are used to prevent a process from accessing memory that it does not own. The relocation register contains the value of the smallest physical address. The limit register contains the range of logical addresses. Each logical address must fall within the range specified by the limit register. The MMU maps the logical address dynamically by adding the value in the relocation register.","title":"Memory Protection"},{"location":"osc/ch9/#memory-allocation","text":"One of the simplest methods of allocating memory is to assign processes to variably sized partitions in memory, where each partition may contain exactly one process. The operating system keeps a table indicating which parts of memory are available and which are occupied. A block of available memory is a hole (\u5b54). When a process arrives and needs memory, the system searches for a hole that is large enough for this process. Placement policy (\u653e\u7f6e\u7b56\u7565)\uff1aSeveral Ways to satisfy a request of size n n from a list of free holes. First fit (\u9996\u6b21\u9002\u914d). Allocate the first hole that is big enough. Best fit (\u6700\u4f73\u9002\u914d). Allocate the smallest hole that is big enough. Worst fit (\u6700\u5dee\u9002\u914d). Allocate the largest hole. Both first fit and best fit are better than worst fit in terms of decreasing time and storage utilization. Neither first fit nor best fit is clearly better than the other in terms of storage utilization, but first fit is generally faster.","title":"Memory Allocation"},{"location":"osc/ch9/#fragmentation","text":"External fragmentation (\u5916\u90e8\u788e\u7247) exists when there is enough total memory space to satisfy a request but the available spaces are not contiguous: storage is fragmented into a large number of small holes. One solution is compaction \uff0cwhich shuffles the memory contents so as to place all free memory together in one large block. Another solution is to permit the logical address space of processes to be noncontiguous, thus allowing a process to be allocated physical memory wherever such memory is available.( paging , \u5206\u9875, the most common memory-management technique for computer systems). Internal fragmentation (\u5185\u90e8\u788e\u7247) occurs when unused memory is internal to a partition, because of allocating memory in units based on block size. The memory must be partitioned into variable sized blocks and assign the best fit block to the process.","title":"Fragmentation"},{"location":"osc/ch9/#3-paging","text":"Paging (\u5206\u9875), is a memory management scheme that permits a process's physical address space to be non-contiguous. Paging avoids external fragmentation and the associated need for compaction.","title":"3 Paging"},{"location":"osc/ch9/#basic-method","text":"The basic method for implementing paging involves breaking physical memory info fixed-sized blocks called frames (\u5e27, also physical page), and breaking logical memory into blocks of the same size called pages (\u9875, also virtual page). The backing store is divided into fixed-sized blocks that are the same size as the memory frames or clusters of multiple frames. Every address generated by the CPU is divided into two parts: a page number (p, \u9875\u53f7) and a page offset (d, \u9875\u504f\u79fb)\u3002 The page number is used as an index into a per-process page table . Page table contains the base address of each frame in physical memory and the offset is the location in the frame being referenced. The base address of the frame is combined with the page offset to define the physical memory address. The MMU translates a logical address generated by the CPU to a physical address by: Extract the page number p p and use it as an index into the page table. Extract the corresponding frame number f f from the page table. Replace the page number p p in the logical address with the frame number f f . The page size is defined by the hardware, typically a power of 2. If the size of logical address space is $2^m $, and a page size is $2^n $ bytes, then the high-order m-n m-n bits of a logical address designate the page number, and the n n low-order bits designate the page offset. Internal fragmentation still exists\uff1a If process size is independent of page size, we expect internal fragmentation to average one-half page per process. This consideration suggests that small page sizes are desirable. However, overhead is involved in each page table entry, and this overhead is reduced as the size of the pages increases. Also, disk I/O is more efficient when the amount of data being transferred is larger","title":"Basic Method"},{"location":"osc/ch9/#hardware-suport","text":"The hardware implementation of the page table can be done in several ways. In the simplest case, the page table is implemented as a set of dedicated high-speed hardware registers. It makes the page-address translation very efficient. It increases context-switch time, as each one of these registers must be exchanged during a context switch. The use of registers for the page table is satisfactory if the page table is reasonably small (for example, 256 entries). It is not feasible for most contemporary CPUs, since much larger page tables needed. For a much large page table, the page table is kept in main memory, and a page-table base register (PTBR, \u9875\u8868\u57fa\u5740\u5bc4\u5b58\u5668) points to the page table. ISSUE: Although storing the page table in main memory can yield faster context switches, it may also result in slower memory access times. Because using page-table to find a physical address need one memory access, since the page-table is kept in main memory. SOLUTION: To use a special, small, fast-lookup hardware cache called a translation look-aside buffer (TLB, \u8f6c\u8bd1\u540e\u5907\u7f13\u51b2\u5668). Each entry in the TLB consists of two parts: a key(tag) and a value. When the associative memory is presented with an item, the item is compared with all keys simultaneously . If the item is found, the corresponding value field is returned. TLB is kept small, typically between 32 and 1,024 entries in size. CPUs today may provide multiple levels of TLBs. We take the Intel Core i7 as an example [ see detail here ]: The Intel Core i7 CPU has a 128-entry L1 instruction TLB and a 64-entry L1 data TLB. In the case of a miss at L1, it takes the CPU six cycles to check for the entry in the L2 512-entry TLB. A miss in L2 means that the CPU must either walk through the page-table entries in memory to find the associated frame address, which can take hundreds of cycles, or interrupt to the operating system to have it do the work.","title":"Hardware Suport"},{"location":"osc/ch9/#protection","text":"Memory protection in a paged environment is accomplished by protection bits associated with each frame. Normally, these bits are kept in the page table. One bit can define a page to be read\u2013write or read-only. One additional bit is generally attached to each entry in the page table: a valid\u2013invalid bit. (When the bit is set to invalid, the page is not in the process\u2019s logical address space).","title":"Protection"},{"location":"osc/ch9/#shared-pages","text":"An advantage of paging is the possibility of sharing common code. Reentrant code is non-self-modifying code: it never changes during execution. If the code is reentrant code, it can be shared. Two or more processes can execute the same code at the same time. Each process has its own copy of registers and data storage to hold the data for the process\u2019s execution. The shared libraries are typically implemented with shared pages. Some operating systems implement shared memory using shared pages. Example: Sharing of standard C library in a paging environment.","title":"Shared Pages"},{"location":"osc/ch9/#4-structure-of-the-page-table","text":"","title":"4 Structure of the Page Table"},{"location":"osc/ch9/#hierarchical-paging","text":"Problem: Most modern computer systems support a large logical address space 2^{32}-2^{64} 2^{32}-2^{64} . In such an environment, the page table itself becomes excessively large. SOLUTION: Hierarchical Paging (\u5c42\u6b21\u5316\u5206\u9875) uses multi-level tables which break up the virtual address into multiple parts. The simplest way is to use a two-level paging algorithm, in which the page table itself is also paged. For a system with a 64-bit logical address space, a two-level paging scheme is no longer appropriate. So we can page the outer page table, giving us a three-level paging scheme. But the outer page table is still large in size. The next step would be a four-level paging scheme, where the second-level outer page table itself is also paged, and so forth. For 64-bit architectures, hierarchical page tables are generally considered inappropriate.","title":"Hierarchical Paging"},{"location":"osc/ch9/#hashed-page-tables","text":"One approach for handling address spaces larger than 32 bits is to use a hashed page table (\u54c8\u5e0c\u9875\u8868), with the hash value being the virtual page number. Each entry in the hash table contains a linked list of elements that hash to the same location (to handle collisions). Each element consists of three fields: the virtual page number the value of the mapped page frame a pointer to the next element in the linked list. The algorithm works as follows: The virtual page number in the virtual address is hashed into the hash table. The virtual page number is compared with field 1 in the first element in the linked list. If there is a match, the corresponding page frame (field 2) is used to form the desired physical address. If there is no match, subsequent entries in the linked list are searched for a matching virtual page number. Clustered page tables , which are similar to hashed page tables except that each entry in the hash table refers to several pages (such as 16) rather than a single page. It is particularly useful for sparse address spaces, where memory references are noncontiguous and scattered throughout the address space.","title":"Hashed Page Tables"},{"location":"osc/ch9/#inverted-page-tables","text":"Problem: One of the drawbacks of methods above is that each page table may consist of millions of entries. These tables may consume large amounts of physical memory just to keep track of how other physical memory is being used. SOLUTION: An inverted page table (\u53cd\u5411\u9875\u8868) has one entry for each real page (or frame) of memory. Each entry consists of the virtual address of the page stored in that real memory location, with information about the process that owns the page. Only one page table is in the system, and it has only one entry for each page of physical memory. DISADVANTAGE: It increases the amount of time needed to search the table when a page reference occurs.","title":"Inverted Page Tables"},{"location":"osc/ch9/#5-swapping","text":"A process, or a portion of a process, can be swapped temporarily out of memory to a backing store and then brought back into memory for continued execution. (The backing store is commonly fast secondary storage.) Swapping makes it possible for the total physical address space of all processes to exceed the real physical memory of the system, thus increasing the degree of multiprogramming in a system.","title":"5 Swapping"},{"location":"osc/ch9/#standard-swapping","text":"Standard swapping involves moving entire processes between main memory and a backing store. For a multithreaded process, all per-thread data structures must be swapped as well. The advantage of standard swapping is that it allows physical memory to be oversubscribed , so that the system can accommodate more processes than there is actual physical memory to store them. Idle or mostly idle processes are good candidates for swapping.","title":"Standard Swapping"},{"location":"osc/ch9/#swapping-with-paging","text":"ISSUE: For standard swapping, the amount of time required to move entire processes between memory and the backing store is prohibitive. It was used in traditional UNIX systems, but it is generally no longer used in contemporary operating systems. SOLUTION: Most systems, including Linux and Windows, use a variation of swapping in which pages of a process\u2014rather than an entire process \u2014can be swapped. A page out operation moves a page from memory to the backing store; the reverse process is known as a page in .","title":"Swapping with paging"},{"location":"osc/ch9/#swapping-on-mobile-systems","text":"ISSUE: In contrast, mobile systems typically do not support swapping in any form. Space constraint: Mobile devices generally use flash memory rather than more spacious hard disks for nonvolatile storage. Other reasons include the limited number of writes that flash memory can tolerate before it becomes unreliable and the poor throughput between main memory and flash memory in these devices. SOLUTION: Instead of using swapping, when free memory falls below a certain threshold, Apple\u2019s iOS asks applications to voluntarily relinquish allocated memory. Any applications that fail to free up sufficient memory may be terminated by the operating system.","title":"Swapping on Mobile Systems"},{"location":"osc/ch9/#6-example-intel-64-bit-architectures","text":"Core i7 Memory System","title":"6 Example: Intel 64-bit Architectures"}]}