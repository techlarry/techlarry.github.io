<!doctype html>
<html class="no-js" lang="en">
  <head>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  techlarry
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?fdc936c9f5a3b72177541183cdeb8cb3";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
  </script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">HomePage</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_blank" href="wiki">WIKI</a></li>
        
        <li id=""><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        <li id=""><a target="_blank" href="note-os">NOTE-OS</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">HomePage</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_blank" href="wiki">WIKI</a></li>
        
        <li><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        
        <li><a target="_blank" href="note-os">NOTE-OS</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="Leetcode.html">Leetcode</a></li>
        
            <li><a href="programming_language.html">编程语言</a></li>
        
            <li><a href="data_structure_and_algorithm.html">数据结构和算法</a></li>
        
            <li><a href="Python%E7%89%B9%E6%80%A7.html">Python特性</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="English.html">English</a></li>
        
            <li><a href="Computer%20System.html">Computer System</a></li>
        
            <li><a href="Deep%20Learning.html">Deep Learning</a></li>
        
            <li><a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html">Linux 系统编程</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html">数据库</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="Big%20Data.html">Big Data</a></li>
        
            <li><a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html">文献阅读</a></li>
        
            <li><a href="Tools.html">Tools</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="recursion_iteration.html">
                
                  <h1>Recursion Iteration Performace</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>The performance of <code>Recursion</code> v.s. <code>Iteration</code> depends on the language being used.</p>

<p>In Java, C, and Python, <code>recursion</code> is fairly expensive compared to <code>iteration</code> (in general) because it requires the allocation of a new <code>stack frame</code>. In some C compilers, one can use a compiler flag to eliminate this overhead, which transforms certain types of recursion (actually, certain types of tail calls) into jumps instead of function calls.</p>

<h2 id="toc_0">Stack Frame</h2>

<p>When a function is called in Python, a <code>stack frame</code> is allocated to handle the local variables of the function. When the function returns, the return value is left on the top of the stack for the calling function to access.</p>

<p>The stack frame provide a <code>scope</code> for the variables used by the function. Even though we are calling the same function over and over, each call creates a new scope for the variables that are local to the function.</p>

<h3 id="toc_1">Get Stack Frame</h3>

<p>The <code>inspect</code> module in <code>python</code> provides several useful functions to help get information about live objects such as modules, classes, methods, functions, tracebacks, <code>frame objects</code>, and code objects.</p>

<pre><code class="language-python">import inspect

def f1():
    names = []
    # inpect.currentframe
    # Return the frame object for the caller’s stack frame.
    frame = inspect.currentframe()
  
    ## Keep moving to next outer frame
    while True:
        try:
            frame = frame.f_back # next outer frame object (this frame’s caller)
            name = frame.f_code.co_name # name with which this code object was defined
            names.append(name)
        except:
            break
    return names
    
def f2():
   return f1()

def f3():
   return f2()

def f4():
   return f3()

print(f4())
</code></pre>

<p>The results shows:</p>

<pre><code class="language-python">[&#39;f2&#39;, &#39;f3&#39;, &#39;f4&#39;, &#39;&lt;module&gt;&#39;]
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/7/17</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Python%E7%89%B9%E6%80%A7.html'>Python特性</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="overfiting_and_normalization.html">
                
                  <h1>Machine Learning (4): Overfitting and normalization</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">The problem of Overfitting</a>
</li>
<li>
<a href="#toc_1">Regularized Linear Regression</a>
<ul>
<li>
<a href="#toc_2">Normal Equation</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">Regularized Logistic Regression</a>
</li>
</ul>


<h2 id="toc_0">The problem of Overfitting</h2>

<p><strong>Underfitting</strong>, or <strong>high bias</strong>, is when the form of our hypothesis function \(h\) maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. </p>

<p><strong>Overfitting</strong>, or <strong>high variance</strong>, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.</p>

<p>There are two main options to address the issue of overfitting:</p>

<ol>
<li><p><strong><em>Reduce the number of features</em></strong>:<br/>
Manually select which features to keep.<br/>
(Use a model selection algorithm).</p></li>
<li><p><strong><em>Regularization</em></strong><br/>
Keep all the features, but reduce the magnitude of parameters \(\theta_j\). Regularization works well when we have a lot of slightly useful features.</p></li>
</ol>

<p><img src="media/14985711297859/14987109337751.png" alt="sd"/></p>

<p>The figure above shows the Underfitting, Normal, Overfitting.</p>

<h2 id="toc_1">Regularized Linear Regression</h2>

<p>We regularize all of theta parameters in a single summation as:</p>

<p>\[J(\theta)= \dfrac{1}{2m}[ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2]\]</p>

<p>where the \(\lambda\), or lambda, is the <strong>regularization parameter</strong>. It determines how much the costs of our theta parameters are inflated.  If \(\lambda\) is chosen to be too large, it may smooth out the function too much and cause underfitting. </p>

<p><strong>Note that you should not regularize the parameter \(\theta_0\)</strong>.</p>

<p>The corresponding gradient descent is</p>

<p>\[\begin{align*} &amp; \text{Repeat}\ \lbrace \newline &amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline &amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline &amp; \rbrace \end{align*}\]</p>

<p>With some manipulation our update rule can also be represented as:<br/>
\[\theta_j := \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\]</p>

<h3 id="toc_2">Normal Equation</h3>

<p>To add in regularization, the equation is the same as our original, except that we add another term inside the parentheses:</p>

<p>\[\begin{align*}&amp; \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline&amp; \text{where}\ \ L = \begin{bmatrix} 0 &amp; &amp; &amp; &amp; \newline &amp; 1 &amp; &amp; &amp; \newline &amp; &amp; 1 &amp; &amp; \newline &amp; &amp; &amp; \ddots &amp; \newline &amp; &amp; &amp; &amp; 1 \newline\end{bmatrix}\end{align*}\]</p>

<p>Recall that if \(m &lt; n\), then \(XTX\) is non-invertible. However, when we add the term \(\lambda L\), then \(XTX + \lambda L\) becomes invertible.</p>

<h2 id="toc_3">Regularized Logistic Regression</h2>

<p>We regularize all of \(\theta\) parameters in a single summation as:</p>

<p>\[ J(\theta) = -\dfrac{1}{m} \sum_{i=1}^m[ y ^{(i)}\log(h_\theta(x^{(i)}))+(1-y^{(i)}) \log(1-h_\theta(x^{(i)}))]+ \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2\]</p>

<p>The corresponding gradient descent is<br/>
\[\theta_j:=\theta_j-\frac{\alpha}{m}\Sigma^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j+\frac{\alpha\lambda}{m}\theta_j\]</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/29</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14987012023353.html">
                
                  <h1>口语</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ol>
<li>发音 native</li>
<li>流畅度</li>
<li>地道的语料</li>
</ol>

<p>方法</p>

<ol>
<li>逐句跟读   <strong>不看原文</strong></li>
<li>影子跟读</li>
<li>全文复数  用到关键词句</li>
</ol>

<p>每天2个小时，1个小时跟读，1个小时背诵和跟读</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/29</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='English.html'>English</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="linear_regression_with_multiple_variables.html">
                
                  <h1>Machine Learning (2): Linear Regression with Multiple Variables</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Multiple Features (variables)</a>
</li>
<li>
<a href="#toc_1">Gradient descent</a>
<ul>
<li>
<a href="#toc_2">Feature Scaling</a>
</li>
<li>
<a href="#toc_3">Learning rate</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">Computing Parameters Analytically</a>
<ul>
<li>
<a href="#toc_5">Normal equation:</a>
</li>
<li>
<a href="#toc_6">Gradient descent v.s. Normal equation</a>
</li>
<li>
<a href="#toc_7">Normal Equation Non-invertible</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">Multiple Features (variables)</h2>

<p>Notation:<br/>
\(m\) = the number of training examples<br/>
\(n\) = the number of features<br/>
\(x^{(i)}\)  = the input (feature) of \(i^{th}\) training example<br/>
\(x^{(i)}_j\) =  value of feature \(j\) of \(i^{th}\) training example</p>

<p>The multivariable form of the hypothesis function accommodating these multiple features is as follows:</p>

<p>\[h_θ(x)=θ_0+θ_1x_1+θ_2x_2+θ_3x_3+⋯+θ_nx_n=\theta^Tx\] (\(n+1\)- dimensional vector)</p>

<p>For convenience of notation, define \(x_0=1\)</p>

<h2 id="toc_1">Gradient descent</h2>

<p><strong>Hypothesis</strong>: <br/>
\[h_\theta(x) = \theta^Tx\]</p>

<p><strong>Parameters</strong>:<br/>
\[\theta\]</p>

<p><strong>Cost Function</strong>:<br/>
\[J(\theta)=\frac{1}{2m}\Sigma^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2\]</p>

<p><strong>Gradient descent</strong>:<br/>
Repeat until converge{</p>

<p>\(\theta_j := \theta_j -\alpha\frac{\partial}{\partial \theta_j}J(\theta)=\theta_j -\alpha\frac{1}{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\)  (simultaneously update for every j= 0,1,...,n)</p>

<h3 id="toc_2">Feature Scaling</h3>

<p>We can speed up gradient descent by having each of our input values in roughly the same range. This is because \(\theta\) will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</p>

<p><strong>Idea: Make sure features are on a similar scale.</strong></p>

<p>Two techniques to help with this are <strong>feature scaling</strong> and <strong>mean normalization</strong>.</p>

<ul>
<li><p><strong>Feature scaling</strong> involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. </p></li>
<li><p><strong>Mean normalization</strong> involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. </p></li>
</ul>

<p>Replace \(x_i\) with \( x_i -\mu_i\) to make features have approximately zero mean (Do not apply to \(x_0=1\))</p>

<p>\[x_i =\frac{ x_i - \mu_i}{S_i}\]</p>

<p>where  \(\mu_i\) is average value of \(x_i\) in training set, \(S_i\) is the range (max-min) or standard deviation of \(x_i\).</p>

<p>E.g. \[x_1=\frac{size-1000}{2000}\]<br/>
\[x_2=\frac{\#bedrooms-2}{5}\]</p>

<h3 id="toc_3">Learning rate</h3>

<p><strong>Debugging</strong>: How to make sure gradient descent is working correctly.</p>

<p>-- How to choose learning rate \(\alpha\).</p>

<p>Gradient descent is working correctly if \(J(\theta)\) decreases after every iteration.</p>

<p>Use smaller \(\alpha\). For sufficiently small \(\alpha\), \(J(\theta)\) should decrease on every iteration.</p>

<p><strong>Automatic convergence test</strong>. Declare convergence if \(J(\theta)\) decreases by less than \(E\) in one iteration, where \(E\) is some small value such as \(10^{−3}\). However in practice it&#39;s difficult to choose this threshold value.</p>

<p><strong>Summary</strong>:</p>

<ul>
<li>If  \(\alpha\)  is too small: slow convergence.</li>
<li>If \(\alpha\) is too large: may not decrease on every iteration; may not converge.</li>
</ul>

<h2 id="toc_4">Computing Parameters Analytically</h2>

<h3 id="toc_5">Normal equation:</h3>

<p>Gradient descent gives one way of minimizing \(J\). The &quot;Normal Equation&quot; method minimizes \(J\) by explicitly taking its derivatives with respect to the \(θj\) ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:</p>

<p>\[\theta = (X^TX)^{-1}X^Ty\]</p>

<p>Matlab command: </p>

<pre><code class="language-matlab">pinv(X&#39;*X)*X&#39;*y
</code></pre>

<p>where <code>pinv</code> is <code>peudoinversion</code> of matrix. It is different to <code>inv</code>.</p>

<h3 id="toc_6">Gradient descent v.s. Normal equation</h3>

<p><img src="media/14972614146835/Screen%20Shot%202017-06-27%20at%202.38.45%20PM.png" alt="Screen Shot 2017-06-27 at 2.38.45 P"/></p>

<h3 id="toc_7">Normal Equation Non-invertible</h3>

<p>The common reason causes non-invertible:</p>

<ul>
<li>Redundant features(linearly dependent)<br/>
E.g. \(x_1\) = size in feet\(^2\), \(x_2\) = size in m\(^2\)</li>
<li>Too many features(e.g. \(m&lt;=n\)).<br/>
-- Delete some features, or use regularization.</li>
</ul>

<p>where \(m\) is the number of training examples, \(n\) is the number of features.</p>

<p>Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/12</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14970829036232.html">
                
                  <h1>Python排序</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">排序算法的稳定性及意义</a>
</li>
<li>
<a href="#toc_1">冒泡排序</a>
<ul>
<li>
<a href="#toc_2">复杂度与稳定性</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">选择排序</a>
</li>
<li>
<a href="#toc_4">插入排序</a>
</li>
<li>
<a href="#toc_5">希尔排序</a>
</li>
<li>
<a href="#toc_6">快速排序</a>
</li>
<li>
<a href="#toc_7">归并排序</a>
<ul>
<li>
<a href="#toc_8">分治法</a>
</li>
</ul>
</li>
<li>
<a href="#toc_9">常见排序算法效率比较</a>
</li>
</ul>


<h2 id="toc_0">排序算法的稳定性及意义</h2>

<p>在待排序的序列中，存在具有相同关键字的记录，在排序后这些记录的相对次序保持不变，则排序算法是稳定的。</p>

<p>不稳定排序无法完成多个关键字的排序。例如整数排序，位数越高的数字优先级越高，从高位数到低位数一次排序。那么每一位的排序都需要稳定算法，否则无法得到正确的结果。</p>

<p>即，<strong>当要对多个关键词多次排序时，必须使用稳定算法</strong></p>

<h2 id="toc_1">冒泡排序</h2>

<p><img src="media/14970829036232/Screen%20Shot%202017-06-11%20at%2010.23.12%20AM.png" alt="Screen Shot 2017-06-11 at 10.23.12 A"/></p>

<pre><code class="language-python">def bubble_sort(alist):
    &quot;&quot;&quot;
    冒泡排序
    &quot;&quot;&quot;
    if len(alist) &lt;= 1:
        return alist

    for j in range(len(alist)-1,0,-1):
        for i in range(j):
            if alist[i] &gt; alist[i+1]:
                alist[i], alist[i+1] = alist[i+1], alist[i]

    return alist
</code></pre>

<h3 id="toc_2">复杂度与稳定性</h3>

<ul>
<li>最优时间复杂度：\(O(n)\) 遍历没有发现任何可以交换的元素，排序结束</li>
<li>最坏时间复杂度：\(O(n^2)\)</li>
<li>稳定性：稳定</li>
</ul>

<h2 id="toc_3">选择排序</h2>

<p>选择排序（Selection sort）是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。</p>

<h2 id="toc_4">插入排序</h2>

<p>插入排序通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。</p>

<p><img src="media/14970829036232/Screen%20Shot%202017-06-12%20at%207.07.03%20PM.png" alt="Screen Shot 2017-06-12 at 7.07.03 P"/></p>

<pre><code class="language-python">def insert_sort(alist):
    &quot;&quot;&quot;
    插入排序
    &quot;&quot;&quot;
    n = len(alist)
    if n &lt;= 1:
        return alist

    # 从第二个位置，即下表为1的元素开始向前插入
    for i in range(1, n):
        j = i
        # 向前向前比较，如果小于前一个元素，交换两个元素
        while alist[j] &lt; alist[j-1] and j &gt; 0:
            alist[j], alist[j-1] = alist[j-1], alist[j]
            j-=1
    return alist
</code></pre>

<p>复杂度与稳定性</p>

<ul>
<li>最优时间复杂度：O(\(n\)) （升序排列，序列已经处于升序状态）</li>
<li>最坏时间复杂度：O(\(n^2\))</li>
<li>稳定性：稳定</li>
</ul>

<h2 id="toc_5">希尔排序</h2>

<p>希尔排序(Shell Sort)是插入排序的改进, 排序非稳定。希尔排序是把记录按下标的一定<em>增量</em>分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。</p>

<pre><code class="language-python">def shell_sort(alist):
    
    n = len(alist)
    gap = n//2
    
    # gap 变化到0之前，插入算法之行的次数
    while gap &gt; 0:
        
        # 希尔排序， 与普通的插入算法的区别就是gap步长
        for i in range(gap,n):
            j = i
            while alist[j] &lt; alist[j-gap] and j &gt; 0:
                alist[j], alist[j-gap] = alist[j-gap], alist[j]
                j-=gap
    
        gap = gap//2

    return alist
</code></pre>

<p>复杂度与稳定性</p>

<ul>
<li>最优时间复杂度：\(O(n^{1.3})\) （不要求本身有序）</li>
<li>最坏时间复杂度：\(O(n^2)\)</li>
<li>稳定性：不稳定</li>
</ul>

<h2 id="toc_6">快速排序</h2>

<p>快速排序(Quicksort)，通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。</p>

<p>步骤为：</p>

<ol>
<li>从数列中挑出一个元素，称为&quot;基准&quot;(pivot)</li>
<li>重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区(partition)操作。</li>
<li>递归地(recursive)把小于基准值元素的子数列和大于基准值元素的子数列排序。</li>
</ol>

<p>递归的最底部情形，是数列的大小是零或一，也就是永远都已经被排序好了。虽然一直递归下去，但是这个算法总会结束，因为在每次的迭代(iteration)中，它至少会把一个元素摆到它最后的位置去。</p>

<h2 id="toc_7">归并排序</h2>

<p>归并排序是采用分治法的一个非常典型的应用。归并排序的思想就是先递归分解数组，再合并数组。</p>

<p>将数组分解最小之后，然后合并两个有序数组，基本思路是比较两个数组的最前面的数，谁小就先取谁，取了后相应的指针就往后移一位。然后再比较，直至一个数组为空，最后把另一个数组的剩余部分复制过来即可。</p>

<h3 id="toc_8">分治法</h3>

<p>分治法的思想：将原问题分解为几个规模较小但类似于原问题的子问题，递归地求解这些子问题，然后再合并这些子问题的解来建立原问题的解。</p>

<p>分治模式在每层递归时都有三个步骤：</p>

<ul>
<li><strong>分解</strong>原问题为若干子问题，这些子问题是原问题的规模较小的实例</li>
<li><strong>解决</strong>这些子问题，递归地求解各子问题。然而，若子问题的规模足够小，则直接求解</li>
<li><strong>合并</strong>这些子问题的解成原问题的解</li>
</ul>

<p>归并排序算法完全遵循分治模式。直观上其操作如下：</p>

<ul>
<li><strong>分解</strong>：分解待排序的n个元素的序列成各具n/2个元素的两个子序列</li>
<li><strong>解决</strong>：使用归并排序递归地排序两个子序列</li>
<li><strong>合并</strong>：合并两个已排序的子序列以产生已排序的答案。</li>
</ul>

<h2 id="toc_9">常见排序算法效率比较</h2>

<p><img src="media/14970829036232/14972715837154.jpg" alt=""/></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/10</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='data_structure_and_algorithm.html'>数据结构和算法</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all_25.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_27.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">他山之石，可以攻玉</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Leetcode.html"><strong>Leetcode</strong></a>
        
            <a href="programming_language.html"><strong>编程语言</strong></a>
        
            <a href="data_structure_and_algorithm.html"><strong>数据结构和算法</strong></a>
        
            <a href="Python%E7%89%B9%E6%80%A7.html"><strong>Python特性</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="English.html"><strong>English</strong></a>
        
            <a href="Computer%20System.html"><strong>Computer System</strong></a>
        
            <a href="Deep%20Learning.html"><strong>Deep Learning</strong></a>
        
            <a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html"><strong>Linux 系统编程</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html"><strong>数据库</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="Big%20Data.html"><strong>Big Data</strong></a>
        
            <a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html"><strong>文献阅读</strong></a>
        
            <a href="Tools.html"><strong>Tools</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15046649572570.html">Pandas</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="exceptional_control_flow.html">CSAPP - 异常控制流</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="introduction_to_computer_system_CMU.html">CMU 15-213 Introduction to Computer Systems</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os-concepts-os-structures.html">Operating System Concepts 2 - Operating System structures</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os-concets-processes.html">Operating System Concepts 3 - Processes</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
