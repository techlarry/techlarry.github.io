<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[techlarry]]></title>
  <link href="http://larryim.cc/atom.xml" rel="self"/>
  <link href="http://larryim.cc/"/>
  <updated>2018-10-21T12:22:09+08:00</updated>
  <id>http://larryim.cc/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Scrapy爬取MovieLens]]></title>
    <link href="http://larryim.cc/15400989593148.html"/>
    <updated>2018-10-21T13:15:59+08:00</updated>
    <id>http://larryim.cc/15400989593148.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">设置代理</h2>

<p>在下载中间件(Downloader Middleware)里设置代理，新建一个类<code>ProxyMiddleware</code>，具体内容如下：</p>

<pre><code class="language-python">class ProxyMiddleware(object):
    def process_request(self, request, spider):
        request.meta[&#39;proxy&#39;] = &quot;http://proxy.yourproxy:8001&quot;
</code></pre>

<p>别忘了在<code>settings.py</code>里设置下载中间件:</p>

<pre><code class="language-python">DOWNLOADER_MIDDLEWARES = {
    &#39;movielens.middlewares.ProxyMiddleware&#39;: 100,
}
</code></pre>

<p>其中movielens是项目名称，后面的数字代表中间件执行的优先级,官方文档中默认proxy中间件的优先级编号是750，这里的中间件优先级要高于默认的优先级。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scrapy爬取猫眼TOP100榜]]></title>
    <link href="http://larryim.cc/scrapy_top100.html"/>
    <updated>2018-10-21T09:50:40+08:00</updated>
    <id>http://larryim.cc/scrapy_top100.html</id>
    <content type="html"><![CDATA[
<p><a href="http://maoyan.com/board/4">猫眼电影TPO100排行榜</a>是根据观众打分进行排序的一个中国电影排行榜。本文试图爬取该榜单信息。</p>

<p><img src="media/15400866400516/maoyanTop100.png" alt="maoyanTop100"/></p>

<p>首先生成Scrapy项目和爬虫：</p>

<pre><code class="language-bash"># 生成scrapy项目
$ scrapy startproject movie 
# 生成scrapy爬虫
$ scrapy genspinder maoyan100 http://maoyan.com/board/4
</code></pre>

<p>从页面中可以观察到，排行榜中电影的基本信息有电影名称(title)，主演(star)，上映时间(release_time), 评分(score)，所以在<code>items.py</code>中定义要爬取的信息为</p>

<pre><code class="language-python">class MovieItem(scrapy.Item):
    title = scrapy.Field()
    star = scrapy.Field()
    release_date = scrapy.Field()
    score = scrapy.Field()
</code></pre>

<p>接下来一步就是提取信息，也是最关键的一步。为了试验提取信息，首先尝试用scrapy shell交互提取信息。</p>

<pre><code class="language-bash">$ scrapy shell &quot;http://maoyan.com/board/4&quot;
</code></pre>

<p>如果返回403信息，表明网站禁止了爬虫，需要改变user-agent信息，可以在site-packages\scrapy\settings\default_settings.py中配置默认的user_agent，省去每一次的配置，也可以直接添加命令行选项</p>

<pre><code class="language-bash">$ scrapy shell &quot;http://maoyan.com/board/4&quot; -s USER_AGENT=&#39;Mozilla/5.0&#39;
</code></pre>

<p>打开网站，选择Inspect，仔细观察排行榜的css源代码</p>

<p><img src="media/15400866400516/maoyanTop100css.png" alt="maoyanTop100css"/></p>

<p>可以发现每一步电影都以<code>&lt;dd&gt;</code>开头，电影所有详细信息都在<code>&lt;div class=&#39;board-item-content&#39;&gt;</code>里面。试着在scrapy shell中提取这些信息，验证我们的猜想是正确的。在<code>&lt;dd&gt;</code>处右键选择<code>copy-copy XPath</code>，复制的内容为<code>//*[@id=&quot;app&quot;]/div/div/div[1]/dl/dd[1]</code>，很明显最后一个<code>[1]</code>表明第一步电影，去掉<code>[1]</code>，可以匹配所有的电影。</p>

<p><img src="media/15400866400516/maoyanTop100cssXpath.png" alt="maoyanTop100cssXpath"/></p>

<p>在scrapy shell中试着提取电影信息</p>

<pre><code class="language-python"># 所有电影信息
movies = response.xpath(&#39;//*[@id=&quot;app&quot;]/div/div/div[1]/dl/dd&#39;)
# 第一步电影
movie = movies[0].css(&#39;.board-item-main .board-item-content&#39;)
# 提取电影名称
title = movie.css(&#39; .movie-item-info .name a::text&#39;).extract()[0]
# 提取主演
star = movie.css(&#39;.movie-item-info .star::text&#39;).extract()[0].strip().strip(&#39;\n&#39;)[3:]
# 提取上映时间
release_time = movie.css(&#39;.movie-item-info .releasetime::text&#39;).extract()[0][5:]
# 提取电影评分
ratings = movie.css(&#39;.movie-item-number .score&#39;)
score = ratings.css(&#39;.integer::text&#39;).extract()[0] +    
    ratings.css(&#39;.fraction::text&#39;).extract()[0]
</code></pre>

<p>在获取全部电影信息以后，选择其中一步电影，依次提取所需要的信息。试验完成了以后，我们就可以把正确的步骤写到爬虫中去，节省了反复调试的时间：</p>

<pre><code class="language-python">class Maoyan100Spider(scrapy.Spider):
    name = &#39;maoyan100&#39;
    allowed_domains = [&#39;maoyan.com&#39;]
    start_urls = [&#39;http://maoyan.com/board/4/&#39;]

    def parse(self, response):
        for movie_info in response.xpath(&#39;//*[@id=&quot;app&quot;]/div/div/div[1]/dl/dd&#39;):
            movie = movie_info.css(&#39;.board-item-main .board-item-content&#39;)
            ratings = movie.css(&#39;.movie-item-number .score&#39;)

            yield {
                # 提取电影名称
                &#39;title&#39;: movie.css(&#39; .movie-item-info .name a::text&#39;)
                    .extract_first(),
                # 提取主演
                &#39;star&#39;: movie.css(&#39;.movie-item-info .star::text&#39;)
                    .extract_first().strip().strip(&#39;\n&#39;)[3:],
                # 提取上映时间
                &#39;release_time&#39;: movie.css(&#39;.movie-item-info .releasetime::text&#39;)
                    .extract_first()[5:],
                # 提取电影评分
                &#39;score&#39;: ratings.css(&#39;.integer::text&#39;).extract_first() 
                    + ratings.css(&#39;.fraction::text&#39;).extract_first(),
            }
            
</code></pre>

<p>好了，写完爬虫了，开始正式运行scrapy爬取了！到根目录下，运行：</p>

<pre><code class="language-bash">scrapy crawl maoyan100 -o maoyan100.json
</code></pre>

<p>其中的<code>-o</code>选项表示输出到<code>maoyan100.json</code>文件。运行完成以后，可以看到在spiders目录中多了一个<code>maoyan100.json</code>文件。如果发现Json文件乱码，可以在<code>settings.py</code>中添加设置<code>FEED_EXPORT_ENCODING = &#39;utf-8&#39;</code>。</p>

<p>但是我们现在只爬取了第一页的信息，TOP100排行榜有10页的内容，每一页只有10部电影。那么，下一页的内容该如何抓取？我们试着点击下一页发现网址变成了<a href="http://maoyan.com/board/4?offset=10,%E5%86%8D%E6%AC%A1%E7%82%B9%E5%87%BB%EF%BC%8C%E5%8F%91%E7%8E%B0%E5%8F%98%E6%88%90%E4%BA%86http://maoyan.com/board/4?offset=20%E3%80%82%E7%AD%94%E6%A1%88%E9%9D%9E%E5%B8%B8%E6%98%8E%E6%98%BE%E4%BA%86%EF%BC%8C%E7%BD%91%E9%A1%B5%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E6%95%B0%E5%AD%97%E6%98%AF10%E7%9A%84%E5%80%8D%E6%95%B0%EF%BC%8C%E5%80%8D%E6%95%B0%E5%8D%B3%E9%A1%B5%E7%A0%81%E3%80%82%E5%9C%A8%60class">http://maoyan.com/board/4?offset=10,再次点击，发现变成了http://maoyan.com/board/4?offset=20。答案非常明显了，网页最后一个数字是10的倍数，倍数即页码。在`class</a> Maoyan100Spider`下增加一个方法:</p>

<pre><code class="language-python">def start_requests(self):
    for x in range(0, 10):
        url = &quot;http://maoyan.com/board/4?offset={0}&quot;
        yield scrapy.Request(url.format(x * 10))
</code></pre>

<p>方法<code>start_requests()</code>的目的与<code>start_urls</code>相同，写了<code>start_requests()</code>后需要把<code>start_urls</code>删除。<code>start_requests()</code>返回 iterable of Requests。好了，一个简单的scrapy爬虫完工了。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pandas]]></title>
    <link href="http://larryim.cc/15046649572570.html"/>
    <updated>2017-09-06T10:29:17+08:00</updated>
    <id>http://larryim.cc/15046649572570.html</id>
    <content type="html"><![CDATA[
<p>To convert a pandas <code>dataframe</code> (df) to a numpy <code>ndarray</code>, use this code:</p>

<pre><code class="language-text">df=df.values
</code></pre>

<p>df now becomes a numpy <code>ndarray</code>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CSAPP - 异常控制流]]></title>
    <link href="http://larryim.cc/exceptional_control_flow.html"/>
    <updated>2018-07-10T17:24:32+08:00</updated>
    <id>http://larryim.cc/exceptional_control_flow.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">1 异常</a>
<ul>
<li>
<a href="#toc_1">1.1 异常的处理</a>
</li>
<li>
<a href="#toc_2">1.2 异常的类别</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">2 进程</a>
<ul>
<li>
<a href="#toc_4">2.1 逻辑控制流</a>
</li>
<li>
<a href="#toc_5">2.2 并发流</a>
</li>
<li>
<a href="#toc_6">2.3 私有地址空间</a>
</li>
<li>
<a href="#toc_7">2.4 用户模式和内核模式</a>
</li>
</ul>
</li>
<li>
<a href="#toc_8">3 系统调用错误处理</a>
</li>
<li>
<a href="#toc_9">4 进程控制</a>
<ul>
<li>
<a href="#toc_10">4.1 获取进程ID</a>
</li>
<li>
<a href="#toc_11">4.2 创建和终止进程</a>
</li>
<li>
<a href="#toc_12">4.3 回收子进程</a>
</li>
<li>
<a href="#toc_13">4.4 进程休眠</a>
</li>
<li>
<a href="#toc_14">4.5  加载并运行程序</a>
</li>
</ul>
</li>
<li>
<a href="#toc_15">5 信号</a>
<ul>
<li>
<a href="#toc_16">5.1 发送/接收信号</a>
</li>
<li>
<a href="#toc_17">5.2 发送信号</a>
</li>
<li>
<a href="#toc_18">5.3 接收信号</a>
</li>
<li>
<a href="#toc_19">5.4 阻塞信号和进程回收</a>
<ul>
<li>
<a href="#toc_20">5.4.1 隐式阻塞机制</a>
</li>
<li>
<a href="#toc_21">5.4.2 显式阻塞机制</a>
</li>
</ul>
</li>
<li>
<a href="#toc_22">5.5 信号处理程序</a>
</li>
</ul>
</li>
<li>
<a href="#toc_23">7 操作进程的工具</a>
</li>
</ul>


<p>从给处理器加电开始，直到你断电为止，程序计数器假设成一个值的序列</p>

<p>\[a_0, a_1, ..., a_{n-1}\]</p>

<p>其中，每个\(a_k\)是某个相应的指令\(I_k\)的 <u>地址</u> 。每次从\(a_k\)到\(a_{k+1}\)的过渡称为<strong>控制转移</strong>(control transfer)。这样的控制转移序列叫做处理器的<strong>控制流</strong>(control flow)。</p>

<p>现在系统通过使控制流发生突变来应对系统状态的变化(eg.缺页异常，网络等待)，把这些突变称为<strong>异常控制流</strong>(Exceptional Control Flow, ECF)。</p>

<h2 id="toc_0">1 异常</h2>

<h3 id="toc_1">1.1 异常的处理</h3>

<p>系统为每<strong>类</strong>可能的异常都分配了一个唯一的非负整数的<strong>异常号</strong>(exception number)。在系统启动时，操作系统分配和初始化一张称为<strong>异常表</strong>的跳转表，使得表目\(k\)包含异常\(k\)的处理程序的地址。</p>

<p><img src="media/15312146721582/%E5%BC%82%E5%B8%B8%E8%A1%A8.png" alt="异常表"/></p>

<p>当检测到发生了一个事件，并且确定了相应的异常号\(k\)，处理器触发异常，执行间接过程调用，通过异常表的表目\(k\)，转到相应的处理程序。</p>

<h3 id="toc_2">1.2 异常的类别</h3>

<p>异常(exceptions)可以分为四类：中断(interrupt)、陷阱(trap)、故障(fault)和终止(abort)。<br/>
<img src="media/15312146721582/Exceptions.png" alt="Exceptions"/></p>

<ul>
<li><strong>中断</strong>是异步发生的，是来自处理器外部的I/O设备的信号的结果。</li>
<li><strong>陷阱</strong>是有意的异常，是执行一条指令的结果。
<ul>
<li>其用途是在用户程序和内核之间提供一个像过程一样的接口(系统调用)</li>
</ul></li>
<li><strong>故障</strong>是由错误情况引起的，可能能够被故障处理程序修正。
<ul>
<li>例如缺页异常</li>
</ul></li>
<li><strong>终止</strong>是不可恢复的致命错误造成的结果，通常是一些硬件错误。<br/></li>
</ul>

<h2 id="toc_3">2 进程</h2>

<p>进程(Process)的经典定义就是 <u>一个执行中程序的实例</u> (A process is a program in execuation) 。系统中的每个程序都运行在某个进程的上下文(context)中。上下文是由程序正确运行所需的状态组成的。这个状态包括存放在内存中的程序的代码和数据，它的栈、通用目的寄存器的内容、程序计数器、环境变量以及打开文件描述符的集合。</p>

<p>进程提供了应用程序两个关键抽象：</p>

<ul>
<li>一个<strong>独立</strong>的逻辑控制流，它提供一个假象，好像我们的程序独占地使用处理器。</li>
<li>一个<strong>私有</strong>的地址空间，它提供一个假象，好像我们的程序独占地使用内存系统。</li>
</ul>

<h3 id="toc_4">2.1 逻辑控制流</h3>

<p><strong>逻辑控制流</strong>(Logical Control Flow，简称逻辑流)是PC值的序列。</p>

<h3 id="toc_5">2.2 并发流</h3>

<p>一个逻辑流的执行在时间上与另一个流重叠，称为<strong>并发流</strong>(concurrent flow)，这两个流被称为<strong>并发地运行</strong>。</p>

<h3 id="toc_6">2.3 私有地址空间</h3>

<p>进程为每个程序提供它自己的<strong>私有地址空间</strong>。一般而言，和这个空间中某个地址相关联的那个内存字节是不能被其他进程读或者写的，从这个意义上说，这个地址空间是私有的。</p>

<h3 id="toc_7">2.4 用户模式和内核模式</h3>

<p>处理器通常是用某个控制寄存器中的一个<strong>模式位</strong>(mode bit)来控制用户/内核模式。当设置了模式位时，进程就运行在<strong>内核模式</strong>中，否则运行在<strong>用户模式</strong>中。</p>

<p>运行在内核模式的进程可以执行指令集中的任何指令，可以访问任何内存位置。用户模式中的进程不允许执行特权指令，也不允许直接引用地址空间中内核区的代码和数据。</p>

<h2 id="toc_8">3 系统调用错误处理</h2>

<h2 id="toc_9">4 进程控制</h2>

<p>进程控制包括获取进程ID、创建和终止进程、回收子进程、让进程休眠、加载并运行程序等。这一节将描述Unix提供了控制进程的系统调用。</p>

<h3 id="toc_10">4.1 获取进程ID</h3>

<p>每一个进程都有一个唯一的整数(非零)进程ID(PID)。<code>getpid</code>函数返回调用进程的PID。<code>getppid</code>函数返回它的父进程的PID。</p>

<pre><code class="language-c">#include &lt;sys/types.h&gt;
#include &lt;unistd.h&gt;

pid_t getpid(void);
pit_t getppid(void);
</code></pre>

<h3 id="toc_11">4.2 创建和终止进程</h3>

<p><strong>父进程</strong>通过调用fork函数创建一个新的运行的<strong>子进程</strong>。</p>

<pre><code class="language-c">#include &lt;sys/types.h&gt;
#include &lt;unistd.h&gt;

pid_t fork(void);
</code></pre>

<p>新创建的子进程几乎但不完全与父进程相同：</p>

<ul>
<li><strong>相同但是独立的地址空间</strong>：子进程获得父进程虚拟地址空间的一份副本</li>
<li><strong>共享文件</strong>：子进程获得父进程打开文件描述符相同的副本</li>
<li>子进程与父进程pid不同</li>
</ul>

<h3 id="toc_12">4.3 回收子进程</h3>

<p>进程在终止后，并不会被内核从系统中清除，而是保持这种状态，直到被它的父进程<strong>回收</strong>(reaped)。</p>

<ul>
<li>一个终止了但还未被回收的进程称为<strong>僵死进程</strong>(zombie)。</li>
<li>即使僵死进程没有运行，它仍然消耗系统的内存资源。</li>
</ul>

<p>通过调用<code>waitpid</code>函数来等待子进程终止或者停止。</p>

<h3 id="toc_13">4.4 进程休眠</h3>

<p><code>sleep</code>函数将一个进程挂起一段制定的时间。</p>

<pre><code class="language-c">#include &lt;unistd.n&gt;
unsigned int sleep(unsigned int secs);
</code></pre>

<h3 id="toc_14">4.5  加载并运行程序</h3>

<p><code>execve</code>函数在当前进程的上下文中加载并运行一个新程序。</p>

<ul>
<li><code>execve</code>调用一次并从不返回。</li>
</ul>

<h2 id="toc_15">5 信号</h2>

<p>Linux<strong>信号</strong>，通知进程系统中发生一个某种类型的事件。每种信号类型都对应于某种系统事件。低层的硬件异常是由内核异常处理程序处理的，正常情况下，对用户进程而言是不可见的。下面是Linux系统上常见的信号：</p>

<p><strong>常见的信号</strong>：</p>

<table>
<thead>
<tr>
<th>编号</th>
<th>名称</th>
<th>默认动作</th>
<th>对应事件</th>
</tr>
</thead>

<tbody>
<tr>
<td>2</td>
<td>SIGINT</td>
<td>终止</td>
<td>来自键盘的中断CTRL+C</td>
</tr>
<tr>
<td>3</td>
<td>SIGQUIT</td>
<td>终止</td>
<td>来自键盘的退出CTRL+\</td>
</tr>
<tr>
<td>9</td>
<td>SIGKILL</td>
<td>终止</td>
<td>杀死程序 <code>\bin\kill -9</code></td>
</tr>
<tr>
<td>11</td>
<td>SIGSEGV</td>
<td>终止并转储内存</td>
<td>段故障(无效的内存引用)</td>
</tr>
<tr>
<td>15</td>
<td>SIGTERM</td>
<td>终止</td>
<td>软件终止信号<code>\bin\kill</code></td>
</tr>
<tr>
<td>17</td>
<td>SIGCHLD</td>
<td>忽略</td>
<td>子进程停止或终止</td>
</tr>
<tr>
<td>18</td>
<td>SIGCONT</td>
<td>忽略</td>
<td>继续进程如果该进程停止</td>
</tr>
<tr>
<td>20</td>
<td>SIGTSTP</td>
<td>停止直到下一个SIGCONT</td>
<td>用户输入CTRL+Z</td>
</tr>
</tbody>
</table>

<p>详细信息可以通过<code>man 7 signal</code>查询。</p>

<h3 id="toc_16">5.1 发送/接收信号</h3>

<p>传送一个信号到目的进程由发送、接收信号两个步骤组成：</p>

<ul>
<li>发送信号。内核通过更新目的进程上下文中的某个状态，发送(递送)一个信号给目的进程。</li>
<li>接收信号。当目的进程被内核强迫已某种方式对信号的发送做出反应时，它就接收了信号。进程可以忽略这个信号，终止或者通过执行一个称为<strong>信号处理程序</strong>的用户层函数捕获这个信号。</li>
</ul>

<h3 id="toc_17">5.2 发送信号</h3>

<p>发送信号可以由以下原因引起：</p>

<ul>
<li>用户：用户能够通过输入<code>CTRL+c</code>(<code>SIGINT</code>)、<code>Ctrl+z</code>(<code>SIGTSTP</code>)，或者是终端驱动程序分配给信号控制字符的其他任何键来请求内核产生信号；</li>
<li>内核：当进程执行出错时，内核会给进程发送一个信号，例如非法段存取(内存访问违规)、浮点数溢出等；</li>
<li>进程：一个进程可以通过系统调用kill给另一个进程或自己发送信号。</li>
</ul>

<h3 id="toc_18">5.3 接收信号</h3>

<p>当内核把进程\(p\)从内核模式切换到用户模式时，它会检查进程\(p\)的未被阻塞的待处理信号的集合(<code>pending&amp;~blocked</code>,见下文)，如果集合非空，那么内核强制\(p\)接收信号，触发进程采取某种行为。</p>

<p>进程接收到信号以后，可以有如下3种选择进行处理：</p>

<ul>
<li>接收默认处理：接收默认处理的进程通常会导致进程本身消亡。例如连接到终端的进程，用户按下CTRL+c，将导致内核向进程发送一个SIGINT的信号，进程如果不对该信号做特殊的处理，系统将采用默认的方式处理该信号，即终止进程的执行；</li>
<li>忽略信号：进程可以通过代码，显示地忽略某个信号的处理，例如：<code>signal(SIGINT,SIGDEF)</code>；但是某些信号是不能被忽略的，</li>
<li>捕获信号并处理：当接收到信号时，由信号处理程序自动捕获并且处理信号。</li>
</ul>

<pre><code class="language-c">sighandler_t signal(int signum, sighandler_t handler);
</code></pre>

<p>有两个信号既不能被忽略也不能被捕获，它们是<code>SIGKILL</code>和<code>SIGSTOP</code>。即进程接收到这两个信号后，只能接受系统的默认处理，即终止线程。</p>

<h3 id="toc_19">5.4 阻塞信号和进程回收</h3>

<p>一个发出而没有被接受的信号叫做<strong>未处理信号</strong>（Pending Signal）。进程可以选择阻塞（Block）某个信号。被阻塞的信号产生时将保持在未处理状态，直到进程解除对此信号的阻塞，才执行接收的动作。阻塞和忽略是不同的，<strong>只要信号被阻塞就不会接收</strong>，而忽略是在接收之后可选的一种处理动作。</p>

<p>Linux提供阻塞信号的隐式和显式机制:</p>

<ul>
<li><strong>隐式阻塞机制</strong>：内核默认阻塞任何当前处理程序正在处理信号类型的待处理的信号。如果在进程解除对某信号的阻塞之前这种信号产生过多次，只计一次。因为每个信号只有一个bit的未处理标志(如下图)，非0即1，不记录该信号产生了多少次，阻塞标志也是这样表示的。</li>
<li><strong>显式阻塞机制</strong>：应用<code>sigprocmask</code>函数，明确地阻塞和解除阻塞选定的信号。</li>
</ul>

<p>内核为每个进程在<strong>pending位向量</strong>中维护着待处理信号的集合，而在<strong>blocked位向量</strong>中维护着被阻塞的信号集合。信号在内核中的表示可以看作是这样的：</p>

<p><img src="media/15312146721582/15327695584336.png" alt=""/></p>

<p>每个信号都有两个标志位分别表示阻塞和未处理，还有一个函数指针表示处理动作。信号产生时，内核在进程控制块中设置该信号的未处理标志，直到信号接收才清除该标志。在上图的例子中，</p>

<ul>
<li>SIGHUP信号未阻塞也未产生过，当它接收时执行默认处理动作。</li>
<li>SIGINT信号产生过，但正在被阻塞，所以暂时不能接收。虽然它的处理动作是忽略，但在没有解除阻塞之前不能忽略这个信号，因为进程仍有机会改变处理动作之后再解除阻塞。</li>
<li>SIGQUIT信号未产生过，一旦产生SIGQUIT信号将被阻塞，它调用信号处理程序<code>sighandler</code>。</li>
</ul>

<h4 id="toc_20">5.4.1 隐式阻塞机制</h4>

<p>当多个未处理信号(<code>pending signal</code>)到达时，由于信号并不会产生排队等待这样的情况，所以产生的效果仅相当于一个未处理信号(也就是对应的<code>pending</code>位标记为1，例如上图中的<code>SIGINT</code>信号)。</p>

<p>这样带来几个问题：</p>

<ul>
<li>不能用信号来对其他进程中发生的事件计数，这是显而易见的</li>
<li>在回收子进程时，要回收尽可能多的子进程。例如下面这个例子。</li>
</ul>

<pre><code class="language-c">void handler1(int sig)   
{  
    pid_t pid;  
  
    if ((pid = waitpid(-1, NULL, 0)) &lt; 0)  
        unix_error(&quot;waitpid error&quot;);  
    printf(&quot;Handler reaped child %d\n&quot;, (int)pid);  
    Sleep(2);  
    return;  
}  

/* $begin signal2 */
void handler2(int sig) 
{
    int olderrno = errno;

    while (waitpid(-1, NULL, 0) &gt; 0) {
        Sio_puts(&quot;Handler reaped child\n&quot;);
    }
    // waitpid()函数有可能因为找不到子进程而报ECHILD错误
    if (errno != ECHILD)
        Sio_error(&quot;waitpid error&quot;);
    Sleep(1);
    errno = olderrno;
}
/* $end signal2 */

int main() 
{
    int i, n;
    char buf[MAXBUF];

    if (signal(SIGCHLD, handler2) == SIG_ERR) //handler2 或者 handler1
        unix_error(&quot;signal error&quot;);

    /* Parent creates children */
    for (i = 0; i &lt; 3; i++) {
        if (Fork() == 0) {
            printf(&quot;Hello from child %d\n&quot;, (int)getpid());
            exit(0);
        }
    }

    /* Parent waits for terminal input and then processes it */
    if ((n = read(STDIN_FILENO, buf, sizeof(buf))) &lt; 0)
        unix_error(&quot;read&quot;);

    printf(&quot;Parent processing input\n&quot;);
    while (1)
        ;

    exit(0);
}
</code></pre>

<p>在上面这个例子中，父进程创建一些子进程，这些子进程各自独立运行一段时间，然后终止。用<code>SIGCHLD</code>处理程序来回收子进程，其中<code>handler1</code>是错误的，会产生僵死子进程。<code>handler2</code>是安全的。原因是在<code>handler1</code>中，可能存在子进程先被执行，产生<code>SIGCHLD</code>信号；但是在子进程还未被回收之前，又有多个子进程被执行，产生多个<code>SIGCHLD</code>信号。于是多余的未处理<code>SIGCHLD</code>信号就被抛弃，只相当于一个<code>SIGCHLD</code>信号。最终会造成有的子进程未被回收，产生僵死子进程。</p>

<p>执行的可能结果如下，可以看到父进程只回收了两个子进程。</p>

<pre><code class="language-text">Hello from child 5617
Hello from child 5616
Hello from child 5618
Handler reaped child
Handler reaped child

Parent processing input
</code></pre>

<h4 id="toc_21">5.4.2 显式阻塞机制</h4>

<p>有时候不希望在发送信号后就立即去接收、处理信号，同时也不希望忽略该信号，那么可以通过<code>sigprocmask</code>显式地阻塞信号从而实现延迟接收信号。</p>

<p>函数<code>sigprocmask</code>可以更改当前阻塞的信号集合(即blocked位向量):</p>

<pre><code class="language-c">int sigprocmask(int how, const sigset_t *set, sigset_t *oldset);
</code></pre>

<p>其具体行为依赖于how值：</p>

<pre><code class="language-text">SIG_BLOCK, blocked = blocked | set //添加set信号
SIG_UNBLOCK, blocked = blocked &amp; ~set //删除set信号
SIG_SETMASK, block = set //设置set信号为阻塞的信号
</code></pre>

<p>阻塞的信号集合其实就是一个无符号整型数组(在x86-64上，数组长度是16)。</p>

<pre><code class="language-c">/* A `sigset_t&#39; has a bit for each signal.  */
# define _SIGSET_NWORDS (1024 / (8 * sizeof (unsigned long int)))
typedef struct
{
    unsigned long int __val[_SIGSET_NWORDS];
} sigset_t;
</code></pre>

<p>还有其他的一些函数可以对信号集进行操作：</p>

<pre><code class="language-c">int sigfillset(sigset_t *set); // 信号集初始化, 然后把所有的信号加入到此信号集里
int sigemptyset(sigset_t *set); //信号集初始化为空
int sigaddset(sigset_t *set, int signo); //将信号signo添加到信号集中  
</code></pre>

<p>下面看个例子, 是一个具有细微同步错误的SHELL程序。如果子进程在父进程能够开始运行前就结束了，那么<br/>
<code>addjob()</code> 和 <code>deletejob()</code> 会以错误的方式被调用。这个程序希望父进程在一个作业列表中记录着它的当前子进程，每个作业条目。 <code>addjob()</code> 和 <code>deletejob()</code> 分别想这个作业列表添加和从中删除作业。当父进程创建一个新的子进程时，它就把这个子进程添加到作业列表中。当父进程在<code>SIGCHLD</code> 处理程序中回收一个终止的（僵死）子进程时，它就从作业列表中删除这个子进程。乍一看，这段代码是对的。不幸的是，可能发生下面的情况：</p>

<ul>
<li>1. 父进程执行<code>fork()</code>，内核调度新创建的子进程运行，而不是父进程</li>
<li>2. 在父进程能够再次运行之前，子进程就终止，并且变成一个僵死进程，使得内核传递一个<code>SIGCHLD</code>信号给父进程</li>
<li>3. 后来，当父进程再次变成可运行但又在它执行之前，内核注意到待处理的<code>SIGCHLD</code>信号，并通过在父进程中运行处理程序接收这个信号</li>
<li>4. 处理程序回收终止的子进程，并调用<code>deletejob()</code>，这个函数什么都不做，因为父进程还没有把该子进程添加到列表中</li>
<li>5. 在处理程序运行结束后，内核运行父进程，父进程从<code>fork()</code>返回，通过调用<code>addjob()</code> 错误地把（不存在的）子进程添加到作业列表中</li>
</ul>

<pre><code class="language-c">void handler(int sig)
{
        pid_t pid;
        while ((pid = waitpid(-1, NULL, 0)) &gt; 0) /* Reap a zombie child */
                deletejob(pid); /* Delete the child from the job list */
        if (errno != ECHILD)
                unix_error(&quot;waitpid error&quot;);
}

int main(int argc, char **argv)
{
        int pid;

        Signal(SIGCHLD, handler);
        initjobs();             /* Initialize the job list */

        while (1) {
                /* Child process */
                if ((pid = Fork()) == 0) {
                        Execve(&quot;/bin/date&quot;, argv, NULL);
                }

                /* Parent process */
                addjob(pid);    /* Add the child to the job list */
        }

        exit(0);
}
</code></pre>

<p>正确的做法应该如下,  通过在调用 <code>fork()</code> 之前，阻塞 <code>SIGCHLD</code> 信号，然后在我们调用了 <code>addjob()</code> 之后就取消阻塞这些信号，我们保证了在子进程被添加到作业列表之后回收该子进程。注意，子进程继承了它们父进程的被阻塞集合，所以我们必须在调用 <code>execve()</code> 之前，小心地解除子进程中阻塞的 <code>SIGCHLD</code> 信号。这样，父进程保证在相应的 <code>deletejob()</code> 之前执行 <code>addjob()</code>。</p>

<pre><code class="language-c">int main(int argc, char **argv)
{
    int pid;
    sigset_t mask_all, mask_one, prev_one;

    Sigfillset(&amp;mask_all);
    Sigemptyset(&amp;mask_one);
    Sigaddset(&amp;mask_one, SIGCHLD);
    Signal(SIGCHLD, handler);
    initjobs(); /* Initialize the job list */

    while (1) {
        Sigprocmask(SIG_BLOCK, &amp;mask_one, &amp;prev_one); /* Block SIGCHLD */
        if ((pid = Fork()) == 0) { /* Child process */
            Sigprocmask(SIG_SETMASK, &amp;prev_one, NULL); /* Unblock SIGCHLD */
            Execve(&quot;/bin/date&quot;, argv, NULL);
        }
        Sigprocmask(SIG_BLOCK, &amp;mask_all, NULL); /* Parent process */  
        addjob(pid);  /* Add the child to the job list */
        Sigprocmask(SIG_SETMASK, &amp;prev_one, NULL);  /* Unblock SIGCHLD */
    }
    exit(0);
}
</code></pre>

<h3 id="toc_22">5.5 信号处理程序</h3>

<p>信号处理程序(signal handler)是重要且棘手的一个问题。其难点在：</p>

<ul>
<li>处理程序与主程序并发运行，共享同样的全局变量，因此可能与主程序和其他处理程序相互干扰；</li>
<li>如何以及何时接收信号的规则常常违背人的直觉。</li>
</ul>

<h2 id="toc_23">7 操作进程的工具</h2>

<p>Linux系统提供了大量的监控和操作进程的有用工具。</p>

<ul>
<li>STRACE： 打印一个正在运行的程序和它的子进程调用的每个系统调用的轨迹</li>
<li>PS：列出当前系统中的进程(包括僵尸进程)</li>
<li>TOP: 打印出关于当前进程资源使用的信息</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CMU 15-213 Introduction to Computer Systems]]></title>
    <link href="http://larryim.cc/introduction_to_computer_system_CMU.html"/>
    <updated>2017-12-14T13:22:02+08:00</updated>
    <id>http://larryim.cc/introduction_to_computer_system_CMU.html</id>
    <content type="html"><![CDATA[
<p>CMU 15-213 <code>Introduction to Computer Systems</code>是最受欢迎的计算机课之一。与这门课对应的课本CSAPP被各大名校所采用，也被无数学生拜读。课程内容涉及广泛、由浅入深，是进入计算机科学的最佳课程，也是各种击破BAT笔试题的必备良药(<a href="https://book.douban.com/review/5627139/">这篇帖子描述了笔试题所对应的章节</a>)。</p>

<p>非常幸运的是，CMU在网上分享了几乎所有课程资料，想学习课程的同学几乎可以和CMU学生一样学习该课程。</p>

<p>课程资料：</p>

<ul>
<li><a href="https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22b96d90ae-9871-4fae-91e2-b1627b43e25e%22&amp;sortColumn=0&amp;sortAscending=true">课程视频</a></li>
<li><a href="http://www.cs.cmu.edu/afs/cs/academic/class/15213-f15/www/schedule.html">课程主页</a>，包括了PPT，代码</li>
<li><a href="http://csapp.cs.cmu.edu">课本CSAPP主页</a>，包括了Lab资源。</li>
</ul>

<p>Lab攻略:</p>

<ul>
<li><a href="http://larryim.cc/wiki/2017/12/30/CSAPP-Attack-Lab/">Lab3 Attack Lab</a></li>
<li><a href="http://larryim.cc/wiki/2017/12/30/CSAPP-Cache-Lab/">Lab4 Cache Lab</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Operating System Concepts 2 - Operating System structures]]></title>
    <link href="http://larryim.cc/os-concepts-os-structures.html"/>
    <updated>2018-07-16T18:16:02+08:00</updated>
    <id>http://larryim.cc/os-concepts-os-structures.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">Operating system service</a>
</li>
<li>
<a href="#toc_1">User Interface</a>
</li>
<li>
<a href="#toc_2">System call</a>
<ul>
<li>
<a href="#toc_3">API</a>
</li>
<li>
<a href="#toc_4">Types of system calls</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">System Service</a>
</li>
<li>
<a href="#toc_6">OS Design and Implementation</a>
</li>
<li>
<a href="#toc_7">Operating system structure</a>
<ul>
<li>
<a href="#toc_8">Monolithic structure</a>
</li>
<li>
<a href="#toc_9">Layered</a>
</li>
<li>
<a href="#toc_10">Microkernel</a>
</li>
<li>
<a href="#toc_11">Modules</a>
</li>
<li>
<a href="#toc_12">Hybrid systems</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">System boot</a>
</li>
</ul>


<h2 id="toc_0">Operating system service</h2>

<p>The figure below is a view of the various operating-system services and how they interrelate.</p>

<p><img src="http://or9a8nskt.bkt.clouddn.com/A_view_of_operating_system_services.png" alt="A_view_of_operating_system_services"/></p>

<h2 id="toc_1">User Interface</h2>

<p>There&#39;re mainly three ways for users to interface with the operating system:</p>

<ul>
<li>command interpreter</li>
<li>graphical user interface</li>
<li>touch-screen interface</li>
</ul>

<h2 id="toc_2">System call</h2>

<p>Purpose of System Call: System calls provide an <strong>interface to the services</strong> made available by an operating system.</p>

<h3 id="toc_3">API</h3>

<p>Typically, application developers design programs according to an application programming interface(<strong>API</strong>, 应用程序编程接口) rather than invoking <strong>actual system call</strong>.</p>

<ul>
<li>because even simple program may make heavy use of system call.</li>
<li><strong>program portabilit</strong>y: expect programs to compile and run other system that supports the same API</li>
<li><strong>run-time environment</strong>(RTE, 运行时环境) - the full suit of software needed to execute applications, including its compilers, interpreters, libraries, loaders.</li>
</ul>

<h3 id="toc_4">Types of system calls</h3>

<p>System calls can be grouped roughly into six major categories:<br/>
系统调用可分成六大类：进程控制，文件管理，设备管理，信息维护，通信和保护。</p>

<ul>
<li>process control</li>
<li>file management</li>
<li>device management</li>
<li>information maintenance</li>
<li>communications</li>
<li>protection</li>
</ul>

<p><img src="media/15317361625058/examplesofunixandlinuxsystemcalls.png" alt="Examples of Windows and Unix systemcalls"/></p>

<p>Three ways to pass parameters to the operating system:</p>

<ul>
<li>when less than five parameters, passing the parameters in registers</li>
<li>when more than five parameters, parameters are stored in a block, passing the address of the block in a register</li>
<li>using stack</li>
</ul>

<h2 id="toc_5">System Service</h2>

<p><strong>System services</strong>, also known as <strong>system utilities</strong>, provide a convenient environment for program development and execution.</p>

<h2 id="toc_6">OS Design and Implementation</h2>

<p>One important principle of OS design is <u>the separation of <strong>policy</strong> from <strong>mechanism</strong></u> . Mechanisms determine <strong>how</strong> to do something; policies determine <strong>what</strong> will be done.<br/>
操作系统设计的一个重要原则是策略（policy）和机制（mechanism）的分离。机制决定如何做，策略决定做什么。</p>

<ul>
<li>The separation of policy and mechanism is important for <strong>flexibility</strong>.</li>
</ul>

<h2 id="toc_7">Operating system structure</h2>

<h3 id="toc_8">Monolithic structure</h3>

<p>Operating systems with <strong>monolithic structure</strong> (单体结构) place all of the functionality of kernel into a <strong>single</strong>, <strong>static</strong> binary file that runs in a <strong>single</strong> address space.</p>

<ul>
<li>a common technique for designing operating system</li>
<li>e.g. original Unix operating system ( figure below)</li>
</ul>

<p><img src="http://or9a8nskt.bkt.clouddn.com/traditional_unix_system_structure.png" alt="traditional_unix_system_structure"/></p>

<ul>
<li>e.g. Linux is based on Unix and is structured similarly, as shown in figure below.</li>
</ul>

<p><img src="http://or9a8nskt.bkt.clouddn.com/linux_system_structure.png" alt="linux_system_structure"/></p>

<p>pros</p>

<ul>
<li>simplicity of kernels</li>
<li>a distinct performance advantage</li>
<li>very little overhead in the system-call interface</li>
<li>fast communication within the kernel</li>
</ul>

<p>cons</p>

<ul>
<li>difficult to implement and extend</li>
</ul>

<h3 id="toc_9">Layered</h3>

<p>A <strong>loosely coupled</strong> (松耦合) system is divided into separate, smaller components that have specific and limited functionality (<strong>modular</strong> approach). All these components together comprise the kernel .</p>

<ul>
<li>changes in one component affect only that component</li>
</ul>

<p>A system can be made modular in many ways.</p>

<ul>
<li>one way is the layered approach.</li>
</ul>

<p>For the <strong>layered operating system</strong> (层次式操作系统), it is broken into a number of layers.</p>

<ul>
<li>The bottom layer is the hardware; the highest is the user interface.</li>
<li>low-level layers can be invoked by higher-level layers</li>
</ul>

<p>pros</p>

<ul>
<li>simplicity of construction and debugging
<ul>
<li>each layer is implemented only with operations provided by lower-level layers. </li>
<li>higher-level layers can be debugged without any concern for the lower-level layers</li>
</ul></li>
</ul>

<p>cons</p>

<ul>
<li>difficulty of defining the functionality of each layer</li>
<li>poor performance
<ul>
<li>overhead of requiring a user program to traverse through multiple layers to obtain an operating-system service </li>
</ul></li>
</ul>

<p>Used in computer networks and web applications</p>

<p><img src="http://or9a8nskt.bkt.clouddn.com/A_layed_Operating_System.png" alt="A_layed_Operating_System"/></p>

<h3 id="toc_10">Microkernel</h3>

<p>Another way to modularized the kernel is using microkernel approach (微内核)。</p>

<ul>
<li><strong>removing all nonessential</strong> components from the kernel and implementing them as <strong>user-level</strong> programs the reside in <strong>separate</strong> address spaces.</li>
<li>smaller kernel</li>
</ul>

<p>A typical microkernel shown below.</p>

<p><img src="http://or9a8nskt.bkt.clouddn.com/A_typical_microkernel.png" alt="A_typical_microkernel"/></p>

<p>pros</p>

<ul>
<li>easy to extend the os
<ul>
<li>all new services added to user space do not require modification of the kernel.</li>
<li>when modification of kernel needed, changes tend to be fewer because of small kernel</li>
</ul></li>
<li>more security and reliability
<ul>
<li>since most services are running as user</li>
</ul></li>
</ul>

<p>cons</p>

<ul>
<li>performance may suffer due to increased system function overhead.
<ul>
<li>messages of user-level services to communicate must be copied between the services. </li>
</ul></li>
</ul>

<p>Best-known microkernel os is <strong>Darwin</strong>, the kernel component of the macOS and iOS.  </p>

<h3 id="toc_11">Modules</h3>

<p>Perhaps the best current methodology for operating system design involves using <strong>loadable kernel modules</strong>(LVMs, 可装载内核模块). Here, the kernel has a set of core components and can link in additional services via modules, either at boot time or during run time.</p>

<ul>
<li>design purpose: for the kernel to provide core services, while other services are implemented <strong>dynamically</strong>, as the kernel is running</li>
</ul>

<h3 id="toc_12">Hybrid systems</h3>

<p>In practice, <strong>very few</strong> operating system adopt a single, strictly defined structure. Instead, they <strong>combine different structures</strong><br/>
, resulting in <strong>hybrid systems</strong> that address performance, security, and usability issues.</p>

<p>Architecture of Apple’s macOS and iOS operating systems:<br/>
<img src="http://or9a8nskt.bkt.clouddn.com/Architecture_of_Apple%E2%80%99s_macOS_and_iOS_operating_systems.png" alt="Architecture of Apple’s macOS and iOS operating systems"/></p>

<p>Darwin provides two system-call interfaces: Mach system calls and BSD system calls.</p>

<p>The structure of Darwin:<br/>
<img src="http://or9a8nskt.bkt.clouddn.com/The_structure_of_Darwin.png" alt="The structure of Darwin."/></p>

<p>To address such performance problems, Darwin combines Mach, BSD, the I/O kit, and any kernel extensions into a <strong>single</strong> address space.</p>

<p><a href="https://developer.apple.com/library/archive/documentation/Darwin/Conceptual/KernelProgramming/Architecture/Architecture.html#//apple_ref/doc/uid/TP30000905-CH1g-CACDAEDC">detailed documents for Darwin kernel</a></p>

<h2 id="toc_13">System boot</h2>

<p>The process of starting a computer by loading the kernel is known as <strong>booting</strong> the system.</p>

<ol>
<li>A small piece of code known as the <strong>bootstrap program</strong>（引导程序） or boot loader locates the kernel.</li>
<li>The kernel is loaded into memory and started.</li>
<li>The kernel initializes hardware.</li>
<li>The root file system is mounted.</li>
</ol>

<p>bootstrap program:</p>

<ul>
<li>usually, bootstrap program located in BIOS( nonvolatile firmware(固件) on motherboard, <a href="https://en.wikipedia.org/wiki/BIOS">wiki</a>)</li>
<li><strong>GRUB</strong> is an open-source bootstrap program for Linux and Unix systems <a href="https://en.wikipedia.org/wiki/GNU_GRUB">wiki</a>.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Operating System Concepts 3 - Processes]]></title>
    <link href="http://larryim.cc/os-concets-processes.html"/>
    <updated>2018-07-17T00:28:20+08:00</updated>
    <id>http://larryim.cc/os-concets-processes.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">1 Process concept</a>
<ul>
<li>
<a href="#toc_1">1.1 The process</a>
</li>
<li>
<a href="#toc_2">1.2 Process state</a>
</li>
<li>
<a href="#toc_3">1.3 Process control block</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">2 Process scheduling 进程调度</a>
<ul>
<li>
<a href="#toc_5">2.1 Scheduling Queues</a>
</li>
<li>
<a href="#toc_6">2.2 context switch</a>
</li>
</ul>
</li>
<li>
<a href="#toc_7">3 Operating on Processes</a>
<ul>
<li>
<a href="#toc_8">3.1 Process creation</a>
</li>
<li>
<a href="#toc_9">3.2 Process termination</a>
</li>
</ul>
</li>
<li>
<a href="#toc_10">4 Interprocess communication</a>
</li>
<li>
<a href="#toc_11">5 IPC in shared-memory system</a>
</li>
<li>
<a href="#toc_12">6 IPC in message-passing system</a>
<ul>
<li>
<a href="#toc_13">6.1 Direct/Indirect communication</a>
<ul>
<li>
<a href="#toc_14">(1) Direct Communication</a>
</li>
<li>
<a href="#toc_15">(2) Indirect Communication</a>
</li>
</ul>
</li>
<li>
<a href="#toc_16">6.2 Synchronization</a>
</li>
<li>
<a href="#toc_17">6.3 Buffering</a>
</li>
</ul>
</li>
<li>
<a href="#toc_18">7 Examples of IPC</a>
<ul>
<li>
<a href="#toc_19">7.1 Mach Message Passing</a>
</li>
<li>
<a href="#toc_20">7.2 Pipes</a>
<ul>
<li>
<a href="#toc_21">(1) Ordinary pipes</a>
</li>
<li>
<a href="#toc_22">(2) Named pipes</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_23">8 Communication in Client-server system</a>
<ul>
<li>
<a href="#toc_24">8.1 Sockets</a>
</li>
<li>
<a href="#toc_25">8.2 Remote procedure calls</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">1 Process concept</h2>

<h3 id="toc_1">1.1 The process</h3>

<p><strong>Process</strong> (进程) is a program in execution.</p>

<ul>
<li>Process is the unit of work in a modern computing system</li>
</ul>

<p>The status of the <strong>current</strong> activity of a process is represented by the value of the <strong>program counter</strong> and the contents of the processor&#39;s <strong>registers</strong>.</p>

<p>A program by itself is not a process.</p>

<ul>
<li>A program is a <strong>passive</strong> entity, such as a file containing a list of instructions stored on disk</li>
<li>A process is an <strong>active</strong> entity, with a program counter specifying the next instruction to execute</li>
</ul>

<h3 id="toc_2">1.2 Process state</h3>

<p>A process may be in one of the following states:</p>

<ul>
<li><strong>New</strong>(新建). The process is being created. 进程正在被创建</li>
<li><strong>Running</strong>(运行). Instructions are being executed.指令正在被执行</li>
<li><strong>Waiting</strong>(等待). The process is waiting for some event to occur(such as an I/O completion or reception of a signal). 进程等待某些事件发生</li>
<li><strong>Ready</strong>(就绪). The process is waiting to be assigned to a processor.进程等待分配处理器</li>
<li><strong>Terminated</strong>(终止). The process has finished execution.进程执行完毕</li>
</ul>

<p>Diagram of process state:</p>

<p><img src="media/15317585001692/diagramofprocessstate.png" alt="Diagram of process state"/></p>

<h3 id="toc_3">1.3 Process control block</h3>

<p>Each process is represented by a <strong>process control block</strong>(PCB, 进程控制块), it contains</p>

<ul>
<li><strong>Process state</strong>(进程状态)</li>
<li><strong>Program counter</strong>(程序计数器)</li>
<li><strong>CPU registers</strong>(CPU寄存器)</li>
<li><strong>CPU-scheduling information</strong>(CPU调度信息): a process priority, pointers to scheduling queues, and any other scheduling parameters.</li>
<li><strong>Memory-management information</strong>(内存管理信息)</li>
<li><strong>Accounting information</strong>(记账信息): the amount of CPU and real time used, time limits, account numbers, process numbers and so on.</li>
<li><strong>I/O status information</strong>(I/O状态信息): the list of I/O devices allocated to the process, a list of open files</li>
</ul>

<p>Process Control Block:<br/>
<img src="media/15317585001692/processcontrolblock.png" alt="process control block"/></p>

<p>The process control block in Linux is represented by the C structure <code>task_struct</code> (&#39;include/linux/sched.h&#39;)， <a href="https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h#L592">CODE LINK</a></p>

<ul>
<li>Within the Linux kernel, all active processes are represented using a <strong>doubly linked list</strong> of task struct.</li>
</ul>

<p>Task_strut:</p>

<p><img src="http://or9a8nskt.bkt.clouddn.com/task_strcutinLinux.png" alt="task_strcut in Linux"/></p>

<h2 id="toc_4">2 Process scheduling 进程调度</h2>

<p>The <strong>process scheduler</strong>(进程调度程序) selects an available process for program execution on a core.</p>

<ul>
<li>Each CPU core can run one process at a time.</li>
<li>The number of processes currently in memory is known as the <strong>degree of multiprogramming</strong>.</li>
</ul>

<h3 id="toc_5">2.1 Scheduling Queues</h3>

<p><strong>Ready queue</strong>(就绪队列): the status of processes are ready.</p>

<ul>
<li>generally stored as a linked list, its header contains pointers to the first PCB in the list, each PCB includes a pointer field that points to next PCB in the ready queue.</li>
</ul>

<p><strong>Wait Queue</strong>(等待队列): the status of processes are waiting.</p>

<p>Queueing-diagram representation of process scheduling: <br/>
<img src="media/15317585001692/Queueing-diagram%20representation%20of%20process%20scheduling.png" alt="Queueing-diagram representation of process scheduling"/></p>

<h3 id="toc_6">2.2 context switch</h3>

<p>Here the <strong><em>context</em></strong> of a process is represented in the PCB of the process, including the value of the CPU registers, the process state, and memory-management information.</p>

<p>An operating system performs a <strong>context switch</strong>（上下文切换) when it switches from running one process to running another.</p>

<ul>
<li>The kernel <strong>saves</strong> the context of the old process into its PCB and <strong>restore</strong> the saved context of the new process scheduled to run.</li>
<li>Context-switch time is overhead; the system does no useful work while switching. 
<ul>
<li>A typical speed is a several microseconds. </li>
</ul></li>
<li>Context-switch times are <strong>highly</strong> dependent on hardware support.</li>
</ul>

<p>Context switch from an old process to a new process:</p>

<p><img src="http://or9a8nskt.bkt.clouddn.com/context_switch.png" alt="context_switch"/></p>

<h2 id="toc_7">3 Operating on Processes</h2>

<h3 id="toc_8">3.1 Process creation</h3>

<p>A process may <strong>create</strong> several new processes.</p>

<ul>
<li>the creating process is called a <strong>parent process</strong>.</li>
<li>the new process is called a <strong>child process</strong> .</li>
</ul>

<p><img src="media/15317585001692/process%20creating%20using%20the%20fork--%20system%20call.png" alt="process creating using the fork-- system cal"/></p>

<h3 id="toc_9">3.2 Process termination</h3>

<p>A process <strong>terminates</strong> when it finishes executing its final statement and asks the operating system to delete it by using the <code>exit()</code> system call.</p>

<ul>
<li><strong>cascading termination</strong>(级联终止):  if a process terminates (either normally or abnormally), then all its children must also be terminated. </li>
<li>A process that has terminated, but whose parent has not yet called <code>wait()</code>, is known as a <strong>zombie process</strong>(僵尸进程).</li>
<li>if a parent did not invoke <code>wait()</code> and instead terminated, then leaving its child processes as <strong>orphan processes</strong>(孤儿进程).
<ul>
<li>Unix system may assign the <code>init</code> process as the new parent to orphan processes, and the <code>init</code> process periodically invokes <code>wait()</code>.</li>
</ul></li>
</ul>

<h2 id="toc_10">4 Interprocess communication</h2>

<p>Processes may be either <strong>independent processes</strong>(独立进程) or <strong>cooperating processes</strong>(协同进程).</p>

<ul>
<li>A process is <strong><em>independent</em></strong> if it does not share data with any other processes executing in the system.</li>
<li>A process is <strong><em>cooperating</em></strong> if it can affect or be affected by the other processes executing in the system.</li>
</ul>

<p>Advantages of  process cooperation:</p>

<ul>
<li>Information sharing 信息共享</li>
<li>Computation speedup 加速运算</li>
<li>Modularity 模块化</li>
</ul>

<p>Cooperating process require an <strong>interprocess communication</strong> (IPC，进程间通信) mechanism that will allow them to <strong>exchange</strong> data. There are two fundamental models of IPC:</p>

<ul>
<li><strong>shared memory</strong>（共享内存）: a region of memory is shared by cooperating process. Process can exchange information by reading and writing data to the shared region.
<ul>
<li>Shared memory can be <strong>faster</strong> than message passing.</li>
</ul></li>
<li><strong>message passing</strong>(消息传递)： communication takes place by means of messages exchanged between the cooperating processes.
<ul>
<li>Message passing is useful for exchanging <strong>smaller</strong> amounts of data, because no conflicts need be avoided.</li>
<li>Message passing is easier to implement in a distributed system than shared memory.</li>
</ul></li>
</ul>

<p><img src="http://or9a8nskt.bkt.clouddn.com/shared_memory_and_message_passing.png" alt="shared memory and message passing"/></p>

<h2 id="toc_11">5 IPC in shared-memory system</h2>

<p>Here, we explore the POSIX API for shared memory. POSIX shared memory is organized using <strong>memory-mapped files</strong> (内存映射文件), which associate the region of shared memory with a file. A process must first create a shared-memory object using the <code>shm_open()</code> system call, as follows:</p>

<pre><code class="language-c">fd = shm_open(name, O_CREAT | O_RDWR, 0666);
ftruncate(fd, 4096);
mmap(0, SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
</code></pre>

<ul>
<li>A successful call to <code>shm_open()</code> returns an integer file descriptor for the shared-memory object.</li>
<li>Once the object is established, the <code>ftruncate</code> function is used to configure the size of the object in bytes.</li>
<li>Finally, the <code>mmap()</code> function establishes a memory-mapped file containing the shared-memory object. It returns a pointer to the shared</li>
</ul>

<h2 id="toc_12">6 IPC in message-passing system</h2>

<p>A message-passing facility provides at least two operations:</p>

<ul>
<li>send(message)</li>
<li>receive(message)</li>
</ul>

<p>If P and Q wish to communicate, they need to</p>

<ul>
<li>establish a <strong>communication link</strong>(通信连接) between them</li>
<li>exchange messages via send/receive </li>
</ul>

<p>Here are several methods for logically implementing a <em>communication link</em> between processes:</p>

<ul>
<li>Direct or indirect communication 直接/间接通信</li>
<li>Synchronous or asynchronous communication 同步/异步同步</li>
<li>Automatic or explicit buffering 自动/显式缓冲</li>
</ul>

<h3 id="toc_13">6.1 Direct/Indirect communication</h3>

<h4 id="toc_14">(1) Direct Communication</h4>

<p>Under <strong>direct communication</strong>, each process that wants to communicate must explicitly name the recipient or sender of the communication.</p>

<ul>
<li>send(P, message) - send a message to process P.</li>
<li>receive(Q, message) - receive a message from process Q</li>
</ul>

<p>A communication link in this scheme has the following properties:</p>

<ul>
<li>A link is established <strong>automatically</strong> between every pair of processes that want to communicate.</li>
<li>A link is associated with <strong>exactly two</strong> processes.</li>
<li>Between each pair of processes, there exists exactly one link.</li>
</ul>

<p>Cons:</p>

<ul>
<li>limited modularity of the resulting process definitions. Changing the identifier of a process may necessinate examining all other process definitions.</li>
<li>any such hard-coding techniques, are less desirable.</li>
</ul>

<h4 id="toc_15">(2) Indirect Communication</h4>

<p>With <strong>indirect communication</strong>, the message are sent to and receive from <strong>mailboxes</strong>, or <strong>ports</strong>.</p>

<ul>
<li>send(A, message) - send a message to mailbox A</li>
<li>receive(A, message) - receive a message from mailbox A</li>
</ul>

<p>A mailbox can be viewed abstractly as an object into which messages can be placed by processes and from which messages can be removed.</p>

<ul>
<li>Each mailbox has an <strong>unique</strong> identification.</li>
<li>Two processes can communicate only if they have a shared mailbox.</li>
</ul>

<p>In this scheme, a communication link has the following properties:</p>

<ul>
<li>A link is established between a pair of processes only if both members of the pair have a shared mailbox.</li>
<li>A link may be associated with more than two processes.</li>
<li>Between each pair of communicating processes, a number of different links may exist, with each link corresponding to one mailbox.</li>
</ul>

<p>A mailbox may be owned either by a process or by the operating system.</p>

<p>If the mailbox is owned by a process</p>

<ul>
<li>We distinguish between the <strong>owner</strong> (which can only receive messages through his mailbox) and the <strong>user</strong> (which can only send messages to the mailbox)</li>
<li>Each mailbox has a unique owner.</li>
<li>When a process that owns a mailbox terminates, the mailbox disappears.</li>
<li>The process that creates a new mailbox is that mailbox&#39;s owner by default.</li>
</ul>

<h3 id="toc_16">6.2 Synchronization</h3>

<p>Message passing may be either <strong>blocking</strong> or <strong>nonblocking</strong> - also known as <strong>synchronous</strong> and <strong>asynchronous</strong>.</p>

<h3 id="toc_17">6.3 Buffering</h3>

<p>Messages exchanged by communicating processes reside in a temporary queue, whether communication is direct or indirect. Basically, it can be implemented in three ways:</p>

<ul>
<li>Zero capacity（零容量）-- no buffering
<ul>
<li>The link cannot have any messages waiting in it.</li>
<li>The sender must block until the recipient receives the message. </li>
</ul></li>
<li>Bounded capacity（有界容量）-- automatic buffering
<ul>
<li>The queue has finite length n, at most n message can reside in it.<br/></li>
<li>The sender must block until space is available in the queue if the link is full.<br/></li>
</ul></li>
<li>Unbounded capacity （无界容量） -- automatic buffering
<ul>
<li>Any number of messages can wait in it.</li>
<li>The sender never blocks. </li>
</ul></li>
</ul>

<h2 id="toc_18">7 Examples of IPC</h2>

<h3 id="toc_19">7.1 Mach Message Passing</h3>

<p>Mach was especially designed for distributed systems. Its kernel supports the creation and destruction of multiple <strong>tasks</strong>, which are similar to processes but have multiple threads of control and fewer associated resources.  </p>

<p>Messages are sent to, and received from, mailboxes, which are called <strong>ports</strong> in Mach. </p>

<ul>
<li>Ports are <strong>finite in size</strong> and <strong>unidirectional</strong>.</li>
<li>For two-way communication, a message is sent to one port, and a response is sent to a separate <strong>reply</strong> port.</li>
<li>Associated with each port is a collection of <strong>port rights</strong>, which  identify the capabilities necessary for a task to interact with the port.</li>
</ul>

<p>Functions:</p>

<ul>
<li><code>mach_port_allocate()</code> creates a new port and allocates space for its queue of messages.</li>
<li><code>mach_msg()</code> is the standard API for both sending and receiving messages.</li>
</ul>

<pre><code class="language-c">#include &lt;mach/mach.h&gt;

struct message {
    mach_msg_header_t header;
    int data;
};

mach_port_t client;
mach_port_t server;

/* Client Code */

struct message message;

// construct the header
message.header.msgh_size = sizeof(message);
message.header.msgh_remote_port = server;
message.header.msgh_local_port = client;

// send the message
mach msg(&amp;message.header, // message header
         MACH_SEND_MSG, // sending a message
         sizeof(message), // size of message sent
         0, // maximum size of received message - unnecessary
         MACH_PORT_NULL, // name of receive port - unnecessary
         MACH_MSG_TIMEOUT_NONE, // no time outs MACH PORT NULL // no notify port
);

/* Server Code */

struct message message;

// receive the message
mach_msg(&amp;message.header, // message header
  MACH_RCV_MSG, // sending a message  0, // size of message sent
  sizeof(message), // maximum size of received message
  server, // name of receive port
  MACH_MSG_TIMEOUT_NONE, // no time outs
  MACH_PORT_NULL // no notify port
);
</code></pre>

<h3 id="toc_20">7.2 Pipes</h3>

<p>A <strong>pipe</strong> acts as a conduit allowing two processes to communicate. Pipes were one of the first IPC mechanisms in early UNIX systems. There are two common types of pipes used on both UNIX and Windows systems: <strong>ordinary pipes</strong> and <strong>named pipes</strong>.</p>

<h4 id="toc_21">(1) Ordinary pipes</h4>

<p><strong>Ordinary pipes</strong> allow two processes to communicate in standard producer-consumer fashion: the producer writes to one end of the pipe (the <strong>write end</strong>) and the consumer reads from the other end (the <strong>read end</strong>).</p>

<ul>
<li>Ordinary pipes are <strong>unidirectional</strong>, allowing only one-way communication.</li>
<li>Function <code>pipe(int fd[])</code> constructs an ordinary pipe, where <code>fd</code> is a file descriptor.</li>
<li>UNIX treats a pipe as <em>a special type of file</em>. Pipes can be accessed using ordinary <code>read()</code> and <code>write()</code> system calls.</li>
<li>Ordinary pipes <strong>exit only</strong> while the processes are communicating with each other.</li>
</ul>

<p><img src="media/15317585001692/file_descriptors_for_an_ordinary_pipes.png" alt="file descriptors for an ordinary pipes"/></p>

<pre><code class="language-c">#include &lt;sys/types.h&gt;
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;unistd.h&gt;

#define BUFFER_SIZE 25
#define READ_END 0
#define WRITE_END 1

int main(void)
{
        char write_msg[BUFFER_SIZE] = &quot;Greetings&quot;;
        char read_msg[BUFFER_SIZE];
        int fd[2];
        pid_t pid;

        /* create the pipe */
        if (pipe(fd) == -1){
                fprintf(stderr, &quot;Pipe failed&quot;);
                return 1;
        }

        /* fork a child process */
        pid = fork();

        if (pid&gt;0){ /* parent process */
                close(fd[READ_END]);/* close the unused end of the pipe */
                write(fd[WRITE_END], write_msg, strlen(write_msg)+1); /* write to the pipe */
                close(fd[WRITE_END]);  /* close the write end of the pipe */
        }
        else if (pid==0){ /* child process */
                close(fd[WRITE_END]); /* close the unused end of the pipe */
                read(fd[READ_END], read_msg, BUFFER_SIZE); /* read from the pipe */
                printf(&quot;read: %s\n&quot;, read_msg);
                close(fd[READ_END]); /* close the read end of the pipe */
        }
        return 0;

}
</code></pre>

<h4 id="toc_22">(2) Named pipes</h4>

<p><strong>Named pipes</strong>（命名管道） can be <strong>bidirectional</strong>, and no parent-child relationship is required.</p>

<ul>
<li>Named pipes are referred to as <strong>FIFOs</strong> in UNIX system.</li>
<li>Once created, they appear as typical files in the file system.</li>
<li>The communicating processes for named pipes must reside on the same machine.</li>
</ul>

<p>A FIFO is created with the <code>mkfifo()</code> system call and manipulated with the ordinary <code>open()</code>, <code>read()</code>, <code>write()</code>, and <code>close</code> system calls.：</p>

<pre><code class="language-c">int mkfifo(const char *filename, mode_t mode);
</code></pre>

<p><code>fifowrite.c</code>:</p>

<pre><code class="language-c">#include&lt;sys/types.h&gt;
#include&lt;stdlib.h&gt;
#include&lt;stdio.h&gt;
#include&lt;fcntl.h&gt;
#include&lt;limits.h&gt;
int main()
{
    const char *fifo_name = &quot;/tmp/my_fifo&quot;;
    int pipe_fd = -1;
    int data_fd = -1;
    int res = 0;
    const int open_mode = O_WRONLY;
    char buffer[PIPE_BUF+1];
    if(access(fifo_name,F_OK)==-1)
    {
        res = mkfifo(fifo_name,0777);
        if(res!=0)
        {
            fprintf(stderr,&quot;could not create fifo\n&quot;);
            exit(EXIT_FAILURE);
        }
    }
    printf(&quot;process %d opening fifo O_WRONLY\n&quot;,getpid());
    pipe_fd = open(fifo_name,open_mode);
    data_fd = open(&quot;data.txt&quot;,O_RDONLY);
    printf(&quot;process %d result %d\n&quot;,getpid(),pipe_fd);
    if(pipe_fd!=-1)
    {
        int bytes_read = 0;
        bytes_read = read(data_fd,buffer,PIPE_BUF);
        while(bytes_read&gt;0)
        {
            res = write(pipe_fd,buffer,bytes_read);
            if(res==-1)
            {
                fprintf(stderr,&quot;write error\n&quot;);
                exit(EXIT_FAILURE);
            }
            bytes_read = read(data_fd,buffer,PIPE_BUF);
            buffer[bytes_read]=&#39;\0&#39;;
        }
        close(pipe_fd);
        close(data_fd);
    }
    else{
        exit(EXIT_FAILURE);
    }
    printf(&quot;process %d finished.\n&quot;,getpid());
    exit(EXIT_SUCCESS);
}
</code></pre>

<p><code>fiforead.c</code>:</p>

<pre><code class="language-c">#include&lt;stdlib.h&gt;
#include&lt;stdio.h&gt;
#include&lt;sys/types.h&gt;
#include&lt;fcntl.h&gt;
#include&lt;limits.h&gt;
int main()
{
    const char *fifo_name = &quot;/tmp/my_fifo&quot;;
    int pipe_fd = -1;
    int data_fd = -1;
    int res = 0;
    int open_mode = O_RDONLY;
    char buffer[PIPE_BUF+1];
    int bytes_read = 0;
    int bytes_write = 0;
    memset(buffer,&#39;\0&#39;,sizeof(buffer));

    printf(&quot;process %d opening FIFO O_RDONLY\n&quot;,getpid());
    pipe_fd = open(fifo_name,open_mode);
    data_fd = open(&quot;dataformfifo.txt&quot;,O_WRONLY|O_CREAT,0644);
    printf(&quot;process %d result %d\n&quot;,getpid(),pipe_fd);
    if(pipe_fd!=-1)
    {
        do{
            res = read(pipe_fd,buffer,PIPE_BUF);
            bytes_write = write(data_fd,buffer,res);
            bytes_read +=res;
        }while(res&gt;0);
        close(pipe_fd);
        close(data_fd);
    }
    else{
        exit(EXIT_FAILURE);
    }
    printf(&quot;process %d finished,%d bytes read\n&quot;,getpid(),bytes_read);
    exit(EXIT_SUCCESS);
}
</code></pre>

<h2 id="toc_23">8 Communication in Client-server system</h2>

<p>In this section, we explore two other strategies for communication in client-server system: <strong>sockets</strong> and <strong>remote procedure calls</strong>(RPCs)</p>

<h3 id="toc_24">8.1 Sockets</h3>

<p>A <strong>socket</strong>（套接字）is defined as an endpoint for communication. A socket is identified by an IP address concatenated with a port number.</p>

<p>Communication using sockets：</p>

<p><img src="http://or9a8nskt.bkt.clouddn.com/communication_using_sockets.png" alt="communication using sockets"/></p>

<p>Servers implementing specific services (such as SSH, FTP, and HTTP) listen to well-known ports. Once a request is received, the server accepts a connection from the client socket to complete the connection.</p>

<h3 id="toc_25">8.2 Remote procedure calls</h3>

<p><strong>Remote Procedure Call</strong>（远程过程调用）allows programs on different machines to interact using simple procedure call/return semantics, just as if the two programs were in the same computer。</p>

<p>RPC between a client and a serve：</p>

<p><img src="http://or9a8nskt.bkt.clouddn.com/RPC_between_a_client_and_a_server.png" alt="RPC_between_a_client_and_a_server"/></p>

<p>RPC hides all the network code into the stub procedures. This prevents the application programs, the client and the server, from having to worry about details such as sockets, network byte order, and the like.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Operating System Concepts 7 - Synchronization Examples]]></title>
    <link href="http://larryim.cc/os_concepts_synchronization_examples.html"/>
    <updated>2018-07-27T08:50:40+08:00</updated>
    <id>http://larryim.cc/os_concepts_synchronization_examples.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">1 Classic Problems of Synchronization</h2>

<h2 id="toc_1">2 Synchronization within the Kernel</h2>

<h2 id="toc_2">3 POSIX Synchronization</h2>

<h2 id="toc_3">4 Synchronization in Java</h2>

<h2 id="toc_4">5 Alternative Approaches</h2>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Operating System Concepts 5 - CPU Scheduling]]></title>
    <link href="http://larryim.cc/os_concepts_CPU_scheduling.html"/>
    <updated>2018-07-27T19:12:13+08:00</updated>
    <id>http://larryim.cc/os_concepts_CPU_scheduling.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">1 Basic Concepts</a>
<ul>
<li>
<a href="#toc_1">1.1 CPU-I/O Burst Cycle</a>
</li>
<li>
<a href="#toc_2">1.2 CPU Scheduler</a>
</li>
<li>
<a href="#toc_3">1.3 Preemptive and Nonpreemptive Scheduling</a>
</li>
<li>
<a href="#toc_4">1.4 Dispatcher</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">2 Scheduling Criteria</a>
</li>
<li>
<a href="#toc_6">3 Scheduling Algorithms</a>
<ul>
<li>
<a href="#toc_7">3.1 First-Come,First-Served scheduling, FCFS</a>
</li>
<li>
<a href="#toc_8">3.2 Shortest-job-first scheduling, SJF</a>
</li>
<li>
<a href="#toc_9">3.3 Round-Robin scheduling, RR</a>
</li>
<li>
<a href="#toc_10">3.4 Priority scheduling algorithm</a>
</li>
<li>
<a href="#toc_11">3.5 Multilevel Queue Scheduling</a>
</li>
<li>
<a href="#toc_12">3.6 Multilevel Feedback-Queue Scheduling</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">4 Thread Scheduling</a>
<ul>
<li>
<a href="#toc_14">4.1 Contention Scope</a>
</li>
<li>
<a href="#toc_15">4.2 Pthread Scheduling</a>
</li>
</ul>
</li>
<li>
<a href="#toc_16">5 Multi-Processor Scheduling</a>
<ul>
<li>
<a href="#toc_17">5.1 Approaches to Multiple-Processor Scheduling</a>
</li>
<li>
<a href="#toc_18">5.2 Multicore Processors</a>
</li>
<li>
<a href="#toc_19">5.3 Load Balancing</a>
</li>
<li>
<a href="#toc_20">5.4 Processor Afﬁnity</a>
</li>
</ul>
</li>
<li>
<a href="#toc_21">6 Real-Time CPU Scheduling</a>
</li>
<li>
<a href="#toc_22">7 Linux Scheduling</a>
</li>
</ul>


<p>On modern operating systems it is <strong>kernel-level threads</strong> —not processes—that are in fact being scheduled by the operating system. </p>

<ul>
<li>User-level threads are managed by a thread library, and the kernel is <em>unaware</em> of them.</li>
<li>To run on a CPU, user-level threads must ultimately be mapped to an associated kernel-level thread, although this mapping may be indirect and may use a lightweight process (LWP).</li>
</ul>

<h2 id="toc_0">1 Basic Concepts</h2>

<h3 id="toc_1">1.1 CPU-I/O Burst Cycle</h3>

<p>Process execution consists of a <strong>cycle</strong> of CPU execution and I/O wait. 进程执行由CPU执行周期和I/O等待周期组成。</p>

<ul>
<li>Processes alternate between these two states. 进程在这两个状态之间切换。</li>
<li>Process execution begins with a <strong>CPU burst</strong>, which is followed by an <strong>I/O burst</strong> and so on. 进程执行从CPU区间开始，在这之后是I/O区间。</li>
</ul>

<p>进程在CPU区间和I/O区间之间切换：<br/>
<img src="media/15326899337167/alternating%20sequence%20of%20CPU%20and%20I:O%20bursts.png" alt="alternating sequence of CPU and I:O bursts"/></p>

<p>The durations of CPU bursts tend to have a frequency curve similar to the figure below. </p>

<ul>
<li>The curve is generally characterized as <strong>exponential</strong> or hyperexpoential(超指数).</li>
<li>A large number of short CPU bursts and a small number of long CPU burst.</li>
<li>An I/O-bounded program typically has many short CPU bursts. I/O密集程序通常具有很多短CPU区间。</li>
<li>A CPU-bound program might have a few long CPU bursts.CPU密集程序可能有少量的长CPU区间。</li>
<li>The distribution can be important when implementing a CPU-scheduling algorithm. 分布有助于选择合适的CPU调度算法。</li>
</ul>

<p><img src="media/15326899337167/Histogram%20of%20CPU-burst%20durations.png" alt="Histogram of CPU-burst durations"/></p>

<h3 id="toc_2">1.2 CPU Scheduler</h3>

<p>Whenever the CPU becomes idle, the operating system must select one of the processes in the <strong>ready queue</strong>(就绪队列) to be executed. 每当CPU空闲时，操作系统就必须从就绪队列中选择一个进程来执行。</p>

<ul>
<li>The selection process is carried out by the <strong>CPU scheduler</strong>(CPU调度程序).  进程选择由CPU调度程序执行。</li>
<li>CPU scheduler selects a process from the processes in memory that are ready to execute and allocates the CPU to that process. 调度程序从内存中选择一个能够执行的进程，并为之分配CPU。</li>
<li>A ready queue can be implemented as a FIFO queue, a priority queue, a tree, or simply an unordered linked list. 就绪队列可以是FIFO队列，优先队列、树或无序链表。</li>
</ul>

<h3 id="toc_3">1.3 Preemptive and Nonpreemptive Scheduling</h3>

<p>CPU-scheduling decisions may take place under the following four circumstances: </p>

<ol>
<li>When a process switches from the running state to the waiting state (for example, as the result of an I/O request or an invocation of <code>wait()</code> for the termination of a child process) 当一个进程从运行状态切换到等待状态（如：I/O请求，或者调用wait等待一个子进程的终止） </li>
<li>When a process switches from the running state to the ready state (for example, when an interrupt occurs) 当一个进程从运行状态切换到就绪状态（如：出现中断） </li>
<li>When a process switches from the waiting state to the ready state (for example, at completion of I/O) 当一个进程从等待状态切换到就绪状态（如：I/O完成） </li>
<li>When a process terminates 当一个进程终止时</li>
</ol>

<p>When scheduling takes place only under circumstances 1 and 4, the scheduling scheme is <strong>nonpreemptive</strong>(非抢占的) or <strong>cooperative</strong>(协作的). Otherwise, it is <strong>preemptive</strong>(抢占的).</p>

<ul>
<li>Under nonpreemptive scheduling, once the CPU has been allocated to a process, the process keeps the CPU until it releases it either by terminating or by switching to the waiting state.</li>
<li>Virtually all modern Operating systems use preemptive scheduling algorithms. </li>
</ul>

<h3 id="toc_4">1.4 Dispatcher</h3>

<p>The <strong>dispatcher</strong>(分派程序) is the module that gives control of the CPU&#39;s core to the process selected by the CPU scheduler. This function involves the following:</p>

<ul>
<li>Switching context from one process to another</li>
<li>Switching to user mode</li>
<li>Jumping to the proper location in the user program to resume that program</li>
</ul>

<p><strong>Dispatch latency</strong> (分派延迟) is the time it takes for the dispatcher to stop one process and start another running.</p>

<p><img src="media/15326899337167/the%20role%20of%20dispatcher.png" alt="the role of dispatcher"/></p>

<h2 id="toc_5">2 Scheduling Criteria</h2>

<p>Scheduling criteria（调度准则) include the following:</p>

<ul>
<li><strong>CPU utilization</strong> (CPU利用率)</li>
<li><strong>Throughput</strong> (吞吐量)： the number of processes that are completed per time unit.</li>
<li><strong>Turnaround time</strong> (周转时间): the interval from the time of submission of a process to the time of completion.</li>
<li><strong>Waiting time</strong> (等待时间): the sum of time spent waiting in the ready queue.</li>
<li><strong>Response time</strong> (响应时间): the time from the submission of a request until the first response is produced.</li>
</ul>

<h2 id="toc_6">3 Scheduling Algorithms</h2>

<h3 id="toc_7">3.1 First-Come,First-Served scheduling, FCFS</h3>

<p>By far the simplest CPU-scheduling algorithm is the <strong>first-come first serve scheduling</strong> (先到先服务调度, FCFS) algorithm.</p>

<ul>
<li>The implementation of FCFS policy is easily managed with a <strong>FIFO queue</strong>.</li>
<li>The average <strong>waiting time</strong> under the FCFS policy is often quite <strong>long</strong>.</li>
<li><strong>Convoy effect</strong>(护航效果) occurs when all the other processes wait for the one big process to get off the CPU. 所有其他进程都等待一个大进程释放CPU，这称之为护航效果。</li>
<li>The FCFS scheduling algorithm is <strong>nonpreemptive</strong>. FCFS调度算法是非抢占的。</li>
</ul>

<h3 id="toc_8">3.2 Shortest-job-first scheduling, SJF</h3>

<p>The <strong>shortest-job-first scheduling</strong> (最短作业优先调度, SJF) algorithm associates with each process the length of the process&#39;s next CPU burst.</p>

<ul>
<li>When the CPU is available, it is assigned to the process that has the smallest <strong>next</strong> CPU burst.</li>
<li>It gives the <strong>minimum</strong> average waiting time for a given set of processes.</li>
<li>The SJF algorithm can be either preemptive or nonpreemptive.
<ul>
<li>Preempt the currently executing process: when a new process arrives at the ready queue while a previous process is still executing. The next CPU burst of the newly arrived process may be shorter than what is left of the currently executing process. </li>
</ul></li>
</ul>

<p>The next CPU burst is generally predicted as an <strong>exponential average</strong> of the measured lengths of previous CPU bursts. Let \(t_n\) be the length of the \(n\)th CPU burst, and let \(\tau_{n+1}\) be predicted value for the next CPU burst:</p>

<p>\[\tau_{n+1}= \alpha \tau_n + (1-\alpha) \tau_n\]</p>

<p>where \(0\le\alpha \le 1\), commonly \(\alpha = 1/2\).</p>

<h3 id="toc_9">3.3 Round-Robin scheduling, RR</h3>

<p>The <strong>round-robin scheduling</strong>(轮转调度) algorithm is similar to FCFS scheduling, but switch occurs after 1 <strong>time quantum</strong> (时间片).</p>

<ul>
<li>Time quantum is a small unit of time, generally from 10 to 100 milliseconds in length.</li>
<li>The ready queue is treated as a circular queue.</li>
<li>If the process have a CPU burst of less than 1 time quantum, the  process itself will release the CPU voluntarily.</li>
<li>otherwise, a context switch will be executed, and the process will be put at the tail of the ready queue.</li>
</ul>

<p>The performance of the RR algorithm depends heavily on the size of the time quantum.</p>

<ul>
<li>If extremely large, the RR policy is the same as the FCFS policy.</li>
<li>If extremely small, it&#39;ll result in a large number of context switches.</li>
</ul>

<h3 id="toc_10">3.4 Priority scheduling algorithm</h3>

<p>The <strong>priority-scheduling</strong>(优先级调度) algorithm associate each process a priority, and the CPU allocated to the process with the highest priority.</p>

<ul>
<li>FCFS: equal-priority</li>
<li>SJF: the priority is the inverse of the next CPU burst.</li>
</ul>

<p>ISSUE: <strong>Indefinite blocking</strong>(无限阻塞), or <strong>starvation</strong>(饥饿) occurs when some low-priority processes waiting indefinitely.</p>

<p>SOLUTION: <strong>Aging</strong>(老化) involves gradually increasing the priority of processes that wait in the system for a long time.</p>

<h3 id="toc_11">3.5 Multilevel Queue Scheduling</h3>

<p>For <strong>multilevel queue scheduling</strong>(多级队列调度), there are separate queues for each distinct priority, and priority scheduling simply schedules the process in the highest-priority queue.</p>

<p>A multilevel queue scheduling algorithm can be used to partition processes into several separate queuse based on the process type.<br/>
<img src="media/15326899337167/multilevel-queue-scheduling.png" alt="multilevel-queue-scheduling"/></p>

<p>In addition, there must be scheduling <u><em>among the queues</em></u> :</p>

<ul>
<li><strong>Fixed-priority preemptive scheduling</strong>(固定优先级抢占调度): Each queue has absolute priority over lower-priority queues
<ul>
<li>eg. no process in the batch queue, could run unless the queues for real-time processes, system processes, and interactive processes were all empty. </li>
</ul></li>
<li><strong>Time-slice among queues</strong>(队列之间划分时间片): each queue gets a certain portion of the CPU time.
<ul>
<li>eg. the foreground queue can be given 80 percent of the CPU time for RR scheduling among its processes, while the background queue receives 20 percent of the CPU to give to its processes on an FCFS basis.</li>
</ul></li>
</ul>

<h3 id="toc_12">3.6 Multilevel Feedback-Queue Scheduling</h3>

<p>The <strong>multilevel feedback queue scheduling</strong>(多级反馈队列调度) algorithm allows a process to move between queues.</p>

<ul>
<li>If a process uses too much CPU time, it will be moved to a lower-priority queue.
<ul>
<li>It leaves I/O-bound and interactive processes—which are typically characterized by short CPU bursts —in the higher-priority queues. </li>
</ul></li>
<li>A process that waits too long in a lower-priority queue may be moved to a higher-priority queue.
<ul>
<li>This form of aging prevent starvation.</li>
</ul></li>
</ul>

<p>In general, a multilevel feedback queue scheduler is defined by the following parameters:</p>

<ul>
<li>The number of queues</li>
<li>The scheduling algorithm for each queue</li>
<li>The method used to determine when to upgrade a process to a higher priority queue</li>
<li>The method used to determine when to demote a process to a lower priority queue</li>
<li>The method used to determine which queue a process will enter when that process needs service</li>
</ul>

<h2 id="toc_13">4 Thread Scheduling</h2>

<h3 id="toc_14">4.1 Contention Scope</h3>

<p><strong>Process contention scope</strong> (PCS，进程竞争范围), occurs when competition for the CPU takes place among threads belonging to the same process.</p>

<ul>
<li>the thread library schedules user-level threads to run on an available LWP, on systems implementing the many-to-one and many-to-many models.</li>
</ul>

<p>To decide which kernel-level thread to schedule onto a CPU, the kernel uses <strong>system-contention scope</strong> (SCS, 系统竞争范围).</p>

<ul>
<li>Systems using the one-to-one model, such as Windows and Linux schedule threads using only SCS.</li>
</ul>

<h3 id="toc_15">4.2 Pthread Scheduling</h3>

<p><strong>Pthreads</strong> identifies the following contention scope values:</p>

<ul>
<li><code>PTHREAD_SCOPE_PROCESS</code> schedules threads using PCS scheduling.</li>
<li><code>PTHREAD_SCOPE_SYSTEM</code> schedules threads using SCS scheduling.</li>
</ul>

<p>The Pthread IPC (Interprocess Communication) provides two functions for setting—and getting—the contention scope policy:</p>

<ul>
<li><code>pthread_attr_setscope(pthread_attr_t *attr, int scope)</code></li>
<li><code>pthread_attr_getscope(pthread_attr_t *attr, int *scope)</code></li>
</ul>

<pre><code class="language-c">#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;
#define NUM_THREADS 5

/* the thread runs in this function */
void *runner(void *param); 

int main(int argc, char *argv[])
{
    int i, scope;
    pthread_t tid[NUM_THREADS];     /* the thread identifier */
    pthread_attr_t attr;        /* set of attributes for the thread */

    /* get the default attributes */
    pthread_attr_init(&amp;attr);

    /* first inquire on the current scope */
    if (pthread_attr_getscope(&amp;attr,&amp;scope) != 0)
        fprintf(stderr, &quot;Unable to get scheduling scope.\n&quot;);
    else {
        if (scope == PTHREAD_SCOPE_PROCESS)
            printf(&quot;PTHREAD_SCOPE_PROCESS\n&quot;);
        else if (scope == PTHREAD_SCOPE_SYSTEM)
            printf(&quot;PTHREAD_SCOPE_SYSTEM\n&quot;);
        else 
            fprintf(stderr,&quot;Illegal scope value.\n&quot;);
    }
    
    /* set the scheduling algorithm to PCS or SCS */
    if (pthread_attr_setscope(&amp;attr, PTHREAD_SCOPE_SYSTEM) != 0)
        printf(&quot;unable to set scheduling policy.\n&quot;);

    /* create the threads */
    for (i = 0; i &lt; NUM_THREADS; i++) 
        pthread_create(&amp;tid[i],&amp;attr,runner,NULL); 

    /**
     * Now join on each thread
     */
    for (i = 0; i &lt; NUM_THREADS; i++) 
        pthread_join(tid[i], NULL);
}

/**
 * The thread will begin control in this function.
 */
void *runner(void *param) 
{
    /* do some work ... */

    pthread_exit(0);
}
</code></pre>

<h2 id="toc_16">5 Multi-Processor Scheduling</h2>

<h3 id="toc_17">5.1 Approaches to Multiple-Processor Scheduling</h3>

<p><strong>Asymmetric multiprocessing</strong> (AMP，非对称多处理)</p>

<ul>
<li>all scheduling decisions, I/O processing, and other system activities handled by a single processor -- the master server; the other processors execute only user code.  让一个处理器（主服务器）处理所有的调度决定、I/O处理以及其他系统活动，其他的处理器只执行用户代码。</li>
<li>it is simple because only one core accesses the system data structures, reducing the need for data sharing. 简单，因为只有一个处理器访问系统数据结构，减轻了数据共享的需要。</li>
<li>the master server becomes a potential bottleneck where overall system performance may be reduced.</li>
</ul>

<p><strong>Symmetric multiprocessing</strong> (SMP， 对称多处理)</p>

<ul>
<li>each processor is self-scheduling</li>
<li>it provides two possible strategies for organizing the threads eligible to be scheduled:
<ul>
<li>All threads may be in a _common ready queue_.
<ul>
<li>use some form of locking to protect the common ready queue from race condition</li>
<li>all accesses to the queue would require lock ownership, it would be a performance bottleneck.</li>
</ul></li>
<li>Each processor may have its own <u>private queue</u> of threads.
<ul>
<li>most common approach on systems supporting SMP</li>
<li>more efficient use of cache memory.</li>
</ul></li>
</ul></li>
</ul>

<p><img src="media/15326899337167/organization%20of%20ready%20queues.png" alt="organization of ready queues"/></p>

<h3 id="toc_18">5.2 Multicore Processors</h3>

<p><u>Issue</u> : memory stalls occurs when a processor accesses memory, it spends a significant amount of time waiting for the data to become available.</p>

<ul>
<li>occurs primarily because modern processors operate at much faster speeds than memory</li>
<li>occur because of a cache miss</li>
</ul>

<p><img src="media/15326899337167/memory%20stall.png" alt="memory stall"/><br/>
<u>Solution</u> : many recent hardware designs have implemented multithreaded processing cores in which two (or more) <strong>hardware threads</strong>(硬件线程) are assigned to each core.</p>

<ul>
<li>If one hardware thread stalls while waiting for memory, the core can switch to another thread.</li>
<li>From an operating system perspective, each hardware thread maintains its architectural state, such as instruction pointer and register set, and thus appears as a logical CPU that is available to run a software thread. This technique is known as <strong>chip multithreading</strong> (CMT, 芯片多线程). Intel use the term <strong>hyper-threading</strong>(超线程).</li>
<li><strong>NOTE</strong>: the resources of the physical core (such as caches and pipelines) are shared among its hardware threads, and a processing core can only execute one hardware thread at a time.</li>
</ul>

<p><img src="media/15326899337167/Chip%20multithreading.png" alt="Chip multithreading"/></p>

<p>Two levels of scheduling needed:</p>

<ul>
<li>It chooses which software thread to run on each hardware thread.
<ul>
<li>It may choose any scheduling algorithm. </li>
</ul></li>
<li>It chooses which hardware thread to run on CPU.
<ul>
<li>Use a simple round-robin algorithm</li>
<li>assigned to each hardware thread a dynamic urgency value ranging from 0 to 7, with 0 representing the lowest urgency and 7 the highest. </li>
</ul></li>
</ul>

<p><img src="media/15326899337167/two%20levels%20of%20scheduling.png" alt="two levels of scheduling"/></p>

<h3 id="toc_19">5.3 Load Balancing</h3>

<p><strong>Load balancing</strong>(负载均衡) attempts to keep the workload evenly distributed across all processors in an SMP system.</p>

<p>Two general approaches to load balancing:</p>

<ul>
<li><strong>Push migration</strong>: a specific task periodically checks the load on each processor and -- if it finds an imbalance -- evenly distributes the load by moving (or pushing) threads from overloaded to idle or less-busy processors.</li>
<li><strong>Pull migration</strong>: an idle processor pulls a waiting task from a busy processor.</li>
<li>They are not mutually exclusive and are, in fact, often implemented in parallel on load-balancing systems.</li>
</ul>

<h3 id="toc_20">5.4 Processor Afﬁnity</h3>

<p>Because of the high cost of invalidating and repopulating caches, most operating systems with SMP support try to <u>avoid migrating</u> a thread from one processor to another and instead attempt to keep a thread running on the same processor and take advantage of a warm cache. This is known as <strong>processor affinity</strong>(处理器亲和性)。</p>

<p>Common ready queue and per-processor ready queue(section 5.1):</p>

<ul>
<li>If we adopt the approach of a common ready queue, a thread may be selected for execution by any processor. Thus, if a thread is scheduled on a new processor, that processor’s cache must be repopulated.</li>
<li>With private, per-processor ready queues, a thread is always scheduled on the same processor and can therefore benefit from the contents of a warm cache.</li>
</ul>

<p>The main-memory architecture of a system can affect processor affinity issues as well. <strong>Non-uniform memory access</strong>(NUMA, 非一致性内存访问) where there are two physical processor chips each with their own CPU and local memory. A CPU has faster access to its local memory than to memory local to another CPU.</p>

<p><img src="media/15326899337167/numa%20and%20CPU%20scheduling.png" alt="numa and CPU scheduling"/></p>

<p>Interestingly, load balancing often <strong>counteracts</strong> the benefits of processor affinity.</p>

<h2 id="toc_21">6 Real-Time CPU Scheduling</h2>

<p>[to be continued]</p>

<h2 id="toc_22">7 Linux Scheduling</h2>

<p>The <strong><em>Completely Fair Scheduler</em></strong>（CFS，完全公平调度算法) is the default Linux scheduling algorithm.</p>

<ul>
<li>Each task has a <strong>virtual runtime</strong> value, which is its actual runtime normalized to the number of ready tasks.</li>
<li>Task priority is incorporated as a <strong>decay factor</strong> into this<br/>
formula. 
<ul>
<li>Lower-priority tasks have higher rates of decay than higher-priority tasks.</li>
</ul></li>
<li>The CPU is allocated to the task with the smallest virtual<br/>
runtime value.</li>
</ul>

<p>Standard Linux kernels implement two <strong>scheduling classes</strong>(调度类): </p>

<ul>
<li>a default scheduling class using the CFS scheduling algorithm </li>
<li>a real-time scheduling class.</li>
</ul>

<p>Each runnable task is placed in a <strong>red-black tree</strong> - a balanced binary search tree whose key is based on the value of virtual runtime <code>vruntime</code>.</p>

<ul>
<li>discover the leftmost node will require \(O(\log N)\) operations.</li>
<li>Linux scheduler caches the leftmost node in the variable <code>rb_leftmost</code>, and requires only retrieving the cached value.</li>
</ul>

<p><img src="media/15326899337167/15327413379278.gif" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Operating System Concepts 6 - Synchronization Tools]]></title>
    <link href="http://larryim.cc/os_concepts_synchronization_tools.html"/>
    <updated>2018-07-27T08:49:59+08:00</updated>
    <id>http://larryim.cc/os_concepts_synchronization_tools.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">1 Background</a>
</li>
<li>
<a href="#toc_1">2 The Critical-Section problem</a>
</li>
<li>
<a href="#toc_2">3 Peterson&#39;s Solution</a>
</li>
<li>
<a href="#toc_3">4 Hardware support for Synchronization</a>
<ul>
<li>
<a href="#toc_4">4.1 Memory barriers</a>
</li>
<li>
<a href="#toc_5">4.2 Hardware instructions</a>
</li>
<li>
<a href="#toc_6">4.3 Atomic variables</a>
</li>
</ul>
</li>
<li>
<a href="#toc_7">5 Mutex locks</a>
</li>
<li>
<a href="#toc_8">6 Semaphores</a>
</li>
<li>
<a href="#toc_9">7 Monitors</a>
</li>
<li>
<a href="#toc_10">8 Liveness</a>
<ul>
<li>
<a href="#toc_11">8.1 Deadlock</a>
</li>
<li>
<a href="#toc_12">8.2 Priority Inversion</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">9 Evaluation</a>
</li>
</ul>


<h2 id="toc_0">1 Background</h2>

<p>A <strong>race condition</strong>(竞争条件) occurs when several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place.</p>

<p>多个进程并发访问和操作同一数据，且执行结果与访问发生的特定顺序有关，称之为竞争条件。</p>

<h2 id="toc_1">2 The Critical-Section problem</h2>

<p>A <strong>critical section</strong>（临界区） is a section of code, in which the process may be accessing and updating data that is shared with at least one other process.</p>

<ul>
<li>When one process is executing in its critical section, no other process is allowed to execute in its critical section.</li>
</ul>

<p>The <strong>critical-section problem</strong>（临界区问题） is to design a protocol that the processes can use to synchronize their activity so as to cooperatively share data.</p>

<ul>
<li>Each process must request permission to enter its critical section.</li>
<li>The section of code implementing this request is the <strong>entry section</strong>（进入区）</li>
<li>The critical section may be followed by an <strong>exit section</strong> (退出区)。</li>
<li>The remaining code is the **remainder section **（剩余区)。</li>
</ul>

<p><img src="media/15326525991493/general%20structure%20of%20a%20typical%20process.png" alt="general structure of a typical process"/></p>

<p>A solution to the critical-section problem must satisfy the following three requirements:</p>

<ol>
<li><strong>Mutual exclusion</strong> (互斥): If process \(P_i\) is executing in its critical section, then no other processes can be executing in their critical sections. 如果进程\(P_i\)在其临界区内执行，那么其他进程都不能在其临界区内执行；</li>
<li><p><strong>Progress</strong> (前进): If no process is executing in its critical section and some processes wish to enter their critical sections, then only those processes that are not executing in their remainder sections can participate in deciding which will enter its critical section next, and this selection cannot be postponed indefinitely. 如果没有进程在其临界区内执行且有进程需进入临界区，那么只有那么不在剩余区内执行的进程可参加选择，以确定谁能下一个进入临界区，且这种选择不能无限推迟；</p></li>
<li><p><strong>Bounded waiting</strong> (有限等待): There exists a bound, or limit, on the number of times that other processes are allowed to enter their critical sections after a process has made a request to enter its critical section and before that request is granted. 从一个进程做出进入临界区的请求，直到该请求允许为止，其他进程允许进入其临界区内的次数有上限。</p></li>
</ol>

<p>Two general approaches are used to handle critical sections in operating systems: <strong>preemptive kernels</strong>（抢占内核） and <strong>nonpreemptive kernels</strong>（非抢占内核）.</p>

<ul>
<li>A preemptive kernel allows a process to be preempted while it is running in kernel mode. 抢占内核允许处于内核模式的进程被抢占。</li>
<li>A nonpreemptive kernel does not allow a process running in kernel mode to be preempted.A kernel-model process will run until it exists kernel mode, blocks, or voluntarily yields control of the CPU.非抢占内核不允许内核模式的进程被抢占。</li>
<li>A nonpreemptive kernel is essentially free from race conditions on kernel data structures, as only on process is active in the kernel at at time. 非抢占内核的内核从根本上不会导致竞争条件，因为在内核中一次只有一个进程是活跃的。</li>
<li>Preemptive kernels must be carefully designed to ensure that shared kernel data are free from race conditions. 对于抢占内核需要认真设计以确保共享内和数据免于竞争条件。</li>
<li>A preemptive kernel may be more responsive, since there is less risk that a kernel-model process will run for an arbitrarily long period before relinquishing the processor to waiting process. 抢占内核的响应更快，因为处于内核模式的进程在释放CPU之前不会运行过久。</li>
<li>A preemptive kernel is more suitable for real-time programming, as it will allow a real-time process to preemptive a process currently running in the kernel. 抢占内核更适合实时编程，因为它能允许实时进程抢占处于内核模式运行的其他进程。</li>
</ul>

<h2 id="toc_2">3 Peterson&#39;s Solution</h2>

<p><strong>Peterson’s solution</strong>(Peterson 算法) is restricted to two processes that alternate execution between their critical sections and remainder sections. The processes are numbered \(P_0\) and \(P_1\). For convenience, when presenting \(P_i\), we use \(P_j\) to denote the other process; that is \(j\) equals \(1-i\).</p>

<p>Peterson&#39;s solution requires the two processes to share two data items:</p>

<pre><code class="language-c">int turn;
boolean flag[2];
</code></pre>

<p>The structure of process \(P_i\) in Peterson&#39;s solution.</p>

<pre><code class="language-c">while (true) {
    flag[i] = true; 
    turn = j; 
    while (flag[j] &amp;&amp; turn == j) 
        ;
    /* critical section */
    flag[i] = false;
    /*remainder section */
}
</code></pre>

<ul>
<li>The variable <code>turn</code> indicates whose turn it is to enter its critical section.</li>
<li>The <code>flag</code> array is used to indicate if a process is ready to enter its critical section.</li>
</ul>

<p><strong>Note</strong>:  Peterson’s solution is <strong>not guaranteed</strong> to work on modern computer architectures for the primary reason that, to improve system performance, <strong>processors and/or compilers may reorder read and write operations that have no dependencies</strong>.</p>

<p>If the assignments of the first two statements that appear in the entry section of Peterson&#39;s solution are reordered. It is possible that both threads may be active in their critical sections at the same time.<br/>
<img src="media/15326525991493/the%20effects%20of%20instruction%20reordering%20in%20Peterson&#x27;s%20solution.png" alt="the effects of instruction reordering in Peterson&#39;s solution"/></p>

<h2 id="toc_3">4 Hardware support for Synchronization</h2>

<p>Hardware support for the critical-section problem includes, </p>

<ul>
<li>memory barriers</li>
<li>hardware instructions</li>
<li>atomic variables</li>
</ul>

<h3 id="toc_4">4.1 Memory barriers</h3>

<p>How a computer architecture determines what memory guarantees it will provide to an application program is known as its <strong>memory model</strong>(内存模型). In general, a memory model falls into one of two categories:</p>

<ol>
<li><strong>Strongly ordered</strong>, where a memory modification on one processor is immediately visible to all other processors.</li>
<li><strong>Weakly ordered</strong>, where modifications to memory on one processor may not be immediately visible to other processors.</li>
</ol>

<p>Computer architectures provide instructions that can <em>force</em> any changes in memory to be propagated to all other processors, thereby ensuring that memory modifications are visible to threads running on other processors. Such instruction are known as <strong>memory barriers</strong>(内存屏障).</p>

<ul>
<li>When a memory barrier instruction is performed, the system ensures that all loads and stores are completed before any subsequent load or store operations are performed.</li>
</ul>

<h3 id="toc_5">4.2 Hardware instructions</h3>

<p>Many modern computer systems provide special hardware instructions that allow either to test and modify the content of a word or to swap the contents of two words atomically - that is, one uninterruptible unit.</p>

<p>The definition of the atomic <code>test_and_set()</code> instruction:</p>

<pre><code class="language-c">boolean test and set(boolean *target) { 
    boolean rv = *target; 
    *target = true;
    return rv;
}
</code></pre>

<p>Mutual-exclusion implementation with <code>test_and_set()</code>:</p>

<pre><code class="language-text">do {
    while (test and set(&amp;lock)) 
        ; /* do nothing */
    /* critical section */
    lock = false;
    /* remainder section */ } 
while (true);
</code></pre>

<p>The definition of the atomic <code>compare_and_swap()</code>（CAS）instruction:</p>

<pre><code class="language-c">int compare and swap(int *value, int expected, int new value) { 
    int temp = *value;
    if (*value == expected) 
        *value = new value;
    return temp;
}
</code></pre>

<p>Mutual exclusion with the <code>compare_and_swap()</code> instruction:</p>

<pre><code class="language-c">while (true) {
    while (compare and swap(&amp;lock, 0, 1) != 0) 
        ; /* do nothing */
    /* critical section */
    lock = 0;
    /* remainder section */
}
</code></pre>

<h3 id="toc_6">4.3 Atomic variables</h3>

<p><strong>Atomic variables</strong> (原子变量) provides atomic operations on basic data types such as integers and booleans. Their use is often limited to single updates of shared data such as counters and sequence generators.</p>

<h2 id="toc_7">5 Mutex locks</h2>

<p>ISSUE: The hardware-based solutions are complicated as well as generally inaccessible to application programmers.</p>

<p>SOLUTION: Operating-system designers build higher-level software tools. The simplest of these tools is the <strong>mutex lock</strong>(互斥锁)。</p>

<ul>
<li>A process must <strong>acquire</strong> the lock before entering a critical section; </li>
<li>A process <strong>releases</strong> the lock when it exists the critical section.</li>
<li>A mutex lock has a boolean variable <strong>available</strong>, whose value indicates if the lock is available or not.</li>
<li>Calls to either <code>acquire()</code> or <code>release()</code> must be performed atomically. Thus mutex locks can be implemented using the CAS operation.</li>
</ul>

<p>Solution to the critical-section problem using mutex locks:<br/>
<img src="media/15326525991493/mutex%20lock.png" alt="mutex lock"/></p>

<p>The definition of <code>acquire()</code> is as follows:</p>

<pre><code class="language-c">acquire() { 
    while (!available) ;
        /* busy wait */ 
    available = false; 
}
</code></pre>

<p>The definition of <code>release()</code> is as follows:</p>

<pre><code class="language-c">release(){
    available = true;
}
</code></pre>

<p>The main disadvantage of the implementation is that it requires <strong>busy waiting</strong>.</p>

<ul>
<li>while  a process is in its critical section, any other process that tries enter its critical section must loop continuously in the call to <code>acquire()</code>.</li>
<li>it wastes CPU cycles.</li>
</ul>

<p>Because the process &quot;spins&quot; while waiting for the lock to become available, this type of mutex lock is also called a <code>spinlock</code>（自旋锁）。</p>

<ul>
<li>advantage: no context switch is required</li>
</ul>

<p>Spinlocks are not appropriate for single-processor systems yet are often used in multiprocessor systems.</p>

<p>在UNIX中，自旋锁相关的API：</p>

<pre><code class="language-c">// 初始化自旋锁： 用来申请使用自旋锁所需要的资源并且将它初始化为非锁定状态
int pthread_spin_init(pthread_spinlock_t *, int);
// 获得一个自旋锁：如果该自旋锁当前没有被其它线程所持有，则调用该函数的线程获得该自旋锁.
// 否则该函数在获得自旋锁之前不会返回。
int pthread_spin_lock(pthread_spinlock_t *);
//释放指定的自旋锁
int pthread_spin_unlock(pthread_spinlock_t *);
// 销毁一个自旋锁
int pthread_spin_destroy(pthread_spinlock_t *);
</code></pre>

<h2 id="toc_8">6 Semaphores</h2>

<p>A <strong>semaphore</strong>(信号量) S is an integer variable that, apart from initialization, is accessed only through two standard atomic operations: <code>wait()</code> and <code>signal()</code>. 信号量S是个整数变量，除了初始化外，它只能通过两个标准原子操作：<code>wait()</code>和<code>signal()</code>来访问。</p>

<p>The definition of <code>wait()</code> is as follows:</p>

<pre><code class="language-c">wait(S){
    while (S &lt;= 0)
        ;// busy wait
    S--;
{
</code></pre>

<p>The definition of <code>signal()</code> is as follows:</p>

<pre><code class="language-c">signal(S){
    S++;
}
</code></pre>

<p>All modifications to the integer value of the semaphore in the <code>wait()</code> and <code>signal()</code> operations must be executed atomically.  在<code>wait()</code>和<code>signal()</code>操作中，对信号量整型值的修改必须不可分地执行。</p>

<p>Operating systems often distinguish between counting and binary semaphores.通常操作系统区分计数信号量和二进制信号量。</p>

<ul>
<li>The value of a <strong>counting semaphore</strong>(计数信号量) can range over an unrestricted domain.计数信号量的值域不受限制。</li>
<li>The value of a <strong>binary semaphore</strong>(二进制信号量) can range only between 0 and 1. 二进制信号量的值只能为0或1。</li>
</ul>

<p>Counting semaphores can be used to control access to  a given resource consisting of a finite number of instances.</p>

<ul>
<li>The semaphore is initialized to the number of resources available. </li>
<li>Each process that wishes to use a resource performs a <code>wait()</code>operation on the semaphore (thereby decrementing the count). </li>
<li>When a process releases a resource, it performs a <code>signal()</code> operation (incrementing the count). </li>
<li>When the count for the semaphore goes to 0, all resources are being used. After that, processes that wish to use a resource will block until the count becomes greater than 0.</li>
</ul>

<h2 id="toc_9">7 Monitors</h2>

<p>Issues: various types of errors can be generated easily when programmers use semaphores or mutex locks incorrectly to solve the critical-section problem.</p>

<ul>
<li>interchanges the order of <code>wait()</code> and <code>signal()</code></li>
<li>replaces <code>signal()</code> with <code>wait()</code></li>
<li>omits <code>wait()</code> or <code>signal()</code></li>
</ul>

<p>Solution: An abstract data type, <strong>monitor</strong>(管程), includes a set of programmer-defined operation related to mutual exclusion within the monitor. A monitor uses <strong>condition variables</strong> that allow processes to wait for certain conditions to become true and to signal one another when conditions have been set to true.</p>

<p>Pseudocode syntax of a monitor:</p>

<pre><code class="language-c">monitor monitor name { /* shared variable declarations */
    function P1 ( . . . ) { . . .}
    function P2 ( . . . ) { . . .}
        .
        .
    function Pn ( . . . ) { . . .}
    initialization code ( . . . ) { . . .}
}
</code></pre>

<p><img src="media/15326525991493/monitor%20with%20condition%20variables.png" alt="monitor with condition variables"/></p>

<h2 id="toc_10">8 Liveness</h2>

<h3 id="toc_11">8.1 Deadlock</h3>

<p><strong>deadlocked</strong>(死锁): two or more processes are waiting indefinitely for an event.</p>

<p>A set of processes is in a deadlocked state when every process in the set is waiting for an event that can be caused only by another process in the set.</p>

<h3 id="toc_12">8.2 Priority Inversion</h3>

<p>A scheduling challenge arises when a higher-priority process needs to read or modify kernel data that are currently being accessed by a lower-priority process—or a chain of lower-priority processes. </p>

<ul>
<li>Since kernel data are typically protected with a lock, the higher-priority process will have to wait for a lower-priority one to finish with the resource. </li>
<li>The situation becomes more complicated if the lower-priority process is preempted in favor of another process with a higher priority.</li>
</ul>

<p>As an example, assume we have three processes—\(L\), \(M\), and \(H\)—whose priorities follow the order \(L &lt; M &lt; H\). </p>

<ul>
<li>Assume that process \(H\) requires a semaphore \(S\), which is currently being accessed by process \(L\). </li>
<li>Ordinarily, process \(H\) would wait for \(L\) to finish using resource S. </li>
<li>However, now suppose that process \(M\) becomes runnable, thereby preempting process \(L\). </li>
<li>Indirectly, a process with a lower priority—process \(M\)—has affected how long process \(H\) must wait for \(L\) to relinquish resource \(S\).</li>
</ul>

<p>This liveness problem is known as <strong>priority inversion</strong>（优先级反转）, and it can occur only in systems with more than two priorities. </p>

<p>Solution：  priority-inheritance protocol(优先级继承协议)：</p>

<ul>
<li>All processes that are accessing resources needed by a higher-priority process inherit the higher priority until they are finished with the resources. </li>
<li>When they are finished, priorities revert to original values.</li>
</ul>

<h2 id="toc_13">9 Evaluation</h2>

<p>Performance differences between CAS-based synchronization and traditional synchronization (such as mutex locks and semaphores) under varying contention loads:</p>

<ul>
<li><strong>Uncontended</strong>： Although both options are generally fast, CAS protection will be somewhat faster than traditional synchronization.</li>
<li><strong>Moderate contention</strong>： CAS protection will be faster—possibly much faster —than traditional synchronization.</li>
<li><strong>High contention</strong>： Under very highly contended loads, traditional synchronization will ultimately be faster than CAS-based synchronization.</li>
</ul>

<p>Higher-level tools such as monitors and condition variables may have significant overhead, and may be less likely to scale in highly contended situations.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[图解TCP/IP]]></title>
    <link href="http://larryim.cc/diagrammatize_TCP_IP.html"/>
    <updated>2018-01-22T14:01:39+08:00</updated>
    <id>http://larryim.cc/diagrammatize_TCP_IP.html</id>
    <content type="html"><![CDATA[
<p>在计算机通信中，事先达成一个详细的约定，并遵循这一约定进行处理，这种约定其实就是<strong>协议</strong>。</p>

<p><strong>计算机网络体系结构</strong>将网络协议进行了系统的归纳。TCP/IP就是IP、TCP、HTTP等协议的集合。除此之外，还有很多其他类型的网络体系结构。</p>

<p><img src="media/15166008990128/15322220890961.jpg" alt=""/></p>

<h2 id="toc_0">OSI模型</h2>

<p><em><u>开放式系统互联通信参考模型</u></em>(Open System Interconnection Reference Model， 简称<strong>OSI模型</strong>)。在这一模型中，每个分层都接收由它下一层所提供的特定服务，并且负责为自己的上一层提供特定的服务。上下层之间进行交互时所遵循的约定叫做<strong>接口</strong>。统一层之间的交互所遵循的约定叫做<strong>协议</strong>。</p>

<ul>
<li>分层可以将每个独立使用，即使系统中某些分层发生变化，也不会波及整个系统</li>
<li>分层能够细分通信功能，更易于单独实现每个分层的协议，并界定各个分层的具体责任和义务。</li>
</ul>

<p><img src="media/15166008990128/15322230855436.jpg" alt=""/></p>

<ul>
<li>应⽤层：为应⽤程序提供服务并规定应⽤程序中通信相关的细节。包括⽂件传输、电⼦邮件、远程登录（虚拟终端）等协议。 </li>
<li>表⽰层：将应⽤处理的信息转换为适合⽹络传输的格式，或将来⾃下⼀层的数据转换为上层能够处理的格式。因此它主要负责数据格式的转换。具体来说，就是将设备固有的数据格式转换为⽹络标准传输格式。不同设备对同⼀⽐特流解释的结果可能会不同。因此，使它们保持⼀致是这 ⼀层的主要作⽤。 </li>
<li>会话层： 负责建⽴和断开通信连接（数据流动的逻辑通路），以及数据的分割等数据传输相关的管理。 </li>
<li>传输层： 起着可靠传输的作⽤。只在通信双⽅节点上进⾏处理，⽽⽆需在路由器上处理。 </li>
<li>⽹络层：将数据传输到⽬标地址。⽬标地址可以是多个⽹络通过路由器连接⽽成的某⼀个地址。因此这⼀层主要负责寻址和路由选择。 </li>
<li>数据链路层：负责物理层⾯上互连的、节点之间的通信传输。例如与1个以太⽹相连的2个节点之间的通信。 将0、1序列划分为具有意义的数据帧传送给对端（数据帧的⽣成与接收）。 </li>
<li>物理层：负责0、1⽐特流（0、1序列）与电压的⾼低、光的闪灭之间的互换。</li>
</ul>

<h2 id="toc_1">TCP/IP协议</h2>

<p><img src="media/15166008990128/15322239071091.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CSAPP - 网络编程]]></title>
    <link href="http://larryim.cc/csapp-internet-programming.html"/>
    <updated>2018-07-20T01:02:55+08:00</updated>
    <id>http://larryim.cc/csapp-internet-programming.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">1 客户端-服务器编程模型</a>
</li>
<li>
<a href="#toc_1">2 网络</a>
<ul>
<li>
<a href="#toc_2">2.1 网络层次系统</a>
</li>
<li>
<a href="#toc_3">2.2 网络协议</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">3 全球IP因特网</a>
<ul>
<li>
<a href="#toc_5">3.1 IP地址</a>
</li>
<li>
<a href="#toc_6">3.2 域名</a>
</li>
<li>
<a href="#toc_7">3.3 因特网连接</a>
</li>
</ul>
</li>
<li>
<a href="#toc_8">4 套接字接口</a>
<ul>
<li>
<a href="#toc_9">4.1 Echo客户端和服务器示例</a>
</li>
</ul>
</li>
<li>
<a href="#toc_10">5 Web服务器</a>
<ul>
<li>
<a href="#toc_11">5.1 Web基础</a>
</li>
<li>
<a href="#toc_12">5.2 Web内容</a>
</li>
<li>
<a href="#toc_13">5.3 HTTP事务</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">1 客户端-服务器编程模型</h2>

<p>每个网络应用都是基于<strong>客户端-服务器</strong>模型的。</p>

<p>客户端-服务器模型中的基本操作是<strong>事务</strong>(transaction)。一个客户端-服务器事务由以下四步组成：</p>

<ol>
<li>当一个客户端需要服务时，它向服务器发送一个请求，发起一个事务。</li>
<li>服务器收到请求后，解释它，并以适当的方式操作它的资源。</li>
<li>服务器给客户端发送一个响应，并等待下一个请求。</li>
<li>客户端收到响应并处理它。</li>
</ol>

<p><img src="media/15320197756867/client-server%20model.png" alt="client-server mode"/></p>

<p><strong>客户端和服务器是进程</strong>，而不是常提到的机器或者主机。</p>

<ul>
<li>一台主机可以同时运行许多不同的客户端和服务器</li>
<li>一个客户端和服务器的事务可以在同一台或是不同的主机上。</li>
</ul>

<h2 id="toc_1">2 网络</h2>

<p>对主机而言，网络只是又一种I/O设备，是数据源和数据接收方。物理上而言，网络是一个按照地理远近组成的层次系统。</p>

<h3 id="toc_2">2.1 网络层次系统</h3>

<p>(1) 最底层：以太网段<br/>
<strong>局域网</strong>(LAN, Local Area Network)的范围一般限制在一个建筑或者校园内。最流行的局域网技术是<strong>以太网</strong>(Ethernet)，由电缆和集线器(hub)组成一个以太网段。</p>

<p><img src="media/15320197756867/ethernet%20segment.png" alt="ethernet segment"/></p>

<p>(2) 桥接以太网<br/>
<strong>桥接以太网</strong>(bridged Ethernet)是将以太网段用电缆和网桥(bridge)连接成的较大的局域网。<br/>
<img src="media/15320197756867/bridged%20ethernet%20segment.png" alt="bridged ethernet segment"/></p>

<p>(3) 互联网络<br/>
多个不兼容的局域网可以通过路由器(routers)连接成互联网络(internets)。</p>

<p><img src="media/15320197756867/last_level_internets.png" alt="last_level_internets"/></p>

<h3 id="toc_3">2.2 网络协议</h3>

<p>互联网络是由各种局域网和广域网组成，它们采用完全不同且不兼容的技术。那么如何能让某台主机跨过所有不兼容的网络发送数据位到另一台目的主机呢？</p>

<p>解决方法：一层运行在每台主机和路由器上的<strong>协议</strong>软件，它消除了不同网络之间的差异。协议提供了两种基本能力：</p>

<ul>
<li>提供了命名机制
<ul>
<li>定义一致的<strong>主机地址</strong>(host adress)格式</li>
<li>每台主机会被分配至少一个<strong>互联网络地址</strong>(internet address)，地址唯一地标识了主机</li>
</ul></li>
<li><p>提供了传送机制</p>
<ul>
<li>定义了统一的基本传送单位-<strong>包</strong>(packet)</li>
<li>包由<strong>包头</strong>(header)和<strong>有效载荷</strong>(payload)组成
<ul>
<li>包头包括包的大小以及源主机和目的主机的地址</li>
<li>有效载荷包括从源主机发出的数据位</li>
</ul></li>
</ul>
<p><img src="media/15320197756867/Transferring%20Internet%20Data%20Via%20Encapsulation.png" alt="Transferring Internet Data Via Encapsulation"/></p></li>
</ul>

<p>PH: Internet packet header, 互联网络包头<br/>
FH: LAN frame header, 局域网帧头</p>

<h2 id="toc_4">3 全球IP因特网</h2>

<p>全球IP因特网(Global IP Internet)是最著名和最成功的互联网络(internet)实现。每台因特网主机都运行实现TCP/IP协议的软件，使用套接字接口(sockets interface)函数和Unix I/O函数来通信。</p>

<p><img src="media/15320197756867/hardware%20and%20software%20organization%20of%20an%20internet%20application.png" alt="hardware and software organization of an internet application"/></p>

<p>从程序员的角度：</p>

<ul>
<li>主机被映射为一组32位的<strong>IP地址</strong>(IP addresses)
<ul>
<li>128.2.203.179</li>
</ul></li>
<li>IP地址被映射为一组标识符，叫做<strong>域名</strong>(domain name)</li>
<li>因特网主机上的进程能够通过<strong>连接</strong>和任何其他因特网主机上的进程通信。</li>
</ul>

<h3 id="toc_5">3.1 IP地址</h3>

<p>32位IP地址存在一个IP地址结构(<code>in_addr</code>)中</p>

<ul>
<li>IP地址在内存中是以<strong>网络字节顺序</strong>(network byte order, 大端法)存放的</li>
</ul>

<pre><code class="language-c">/* Internet address structure */ 
struct in_addr { 
    uint32_t s_addr; /* network byte order (big-endian) */ 
};
</code></pre>

<h3 id="toc_6">3.2 域名</h3>

<p><strong>域名</strong>(domain names)是一串用句点分隔的单词(字母、数字和破折号)。域名集合形成了一个层次结构，可以表示为一棵树。</p>

<p><img src="media/15320197756867/Internet%20Domain%20Names.png" alt="Internet Domain Names"/></p>

<p><strong>域名系统</strong>(Domain Naming System, DNS)是映射IP地址和域名的数据库。可以把DNS数据库视为上百万的<strong>主机条目结构</strong>(host entry structure)的集合，其中每条定义了一组域名和一组IP地址之间的映射。</p>

<ul>
<li>DNS映射，可以通过<code>nsloopup</code>查看</li>
<li>在最简单的情况中，一个域名和一个IP地址之间是一一映射
<ul>
<li><code>nslookup whaleshark.ics.cs.cmu.edu</code> - <code>Address: 128.2.210.175</code></li>
</ul></li>
<li>然而，在某些情况下，多个域名可以映射为同一个IP地址
<ul>
<li><code>nslookup cs.mit.edu/ nslookup eecs.mit.edu</code> - <code>Address: 18.62.1.6</code></li>
</ul></li>
<li>在最通常的情况下，多个域名可以映射到同一组的多个IP地址
<ul>
<li><code>nslookup www.twitter.com</code> - <code>Address: 199.16.156.6</code>, <code>Address:199.16.156.70</code></li>
</ul></li>
</ul>

<h3 id="toc_7">3.3 因特网连接</h3>

<p>客户端和服务器通过<strong>连接</strong>(connections)发送字节流来通信，每一个连接都有如下特点：</p>

<ul>
<li>点对点(point-to-point)：连接一对进程</li>
<li>全双工(full-duplex)：数据可以同时在两个方向传送</li>
<li>可靠性(reliable)：发送和接收的字节流顺序相同</li>
</ul>

<p>套接字(sockets)是连接的端点，套接字地址用 “地址：端口”来表示。</p>

<ul>
<li>端口(port)是一个16位整数，标识了一个进程。
<ul>
<li>临时端口：当可会淡发起连接请求时，内核自动分配的端口</li>
<li>知名端口：和服务器提供的服务有短的端口 (
<ul>
<li>Web服务器使用端口80</li>
<li>ssh服务器使用端口22</li>
<li>email服务器使用端口25</li>
</ul></li>
</ul></li>
</ul>

<p>一个连接是由它两端的套接字地址唯一确定的（套接字对, socket pair）。</p>

<p><img src="media/15320197756867/socket%20pair.png" alt="socket pai"/></p>

<p>使用端口来识别服务</p>

<p><img src="media/15320197756867/using%20ports%20to%20identify%20services.png" alt="using ports to identify services"/></p>

<h2 id="toc_8">4 套接字接口</h2>

<p>什么是套接字？</p>

<ul>
<li>对于内核来说，套接字是通信的端点。 To the kernel, a socket is an endpoint of communication</li>
<li>对于应用来说，套接字是让应用从网络读写的文件描述符。 To an application, a socket is a file descriptor that lets the application read/write from/to the network.</li>
</ul>

<p>客户端和服务器通过对套接字描述符读写进行通信：</p>

<p><img src="media/15320197756867/client%20and%20servers%20communicate%20%20via%20socket%20descriptors.png" alt="client and servers communicate  via socket descriptors"/></p>

<p>(1) 通用套接字地址(generic socket address)：</p>

<ul>
<li>以套接字地址作为<code>connect()</code>, <code>bind()</code>, <code>accept</code>的实参</li>
<li>仅仅因为那时的C不存在<code>void *</code>指针，所以套接字接口被设计成这样。</li>
</ul>

<pre><code class="language-c">struct sockaddr { 
    uint16_t sa_family; /* Protocol family */
    char sa_data[14]; }; /* Address data. */
</code></pre>

<p>(2) 因特网的套接字地址</p>

<ul>
<li>必须将<code>struct sockaddr_in *</code> 转换为 <code>struct sockaddr *</code>才能以套接字地址作为函数实参</li>
</ul>

<pre><code class="language-c">struct sockaddr_in {
    uint16_t sin_family;
    uint16_t sin_port;
    struct in_addr sin_addr;
    unsigned char sin_zero[8];
    };
</code></pre>

<p><img src="media/15320197756867/sockets%20interface.png" alt="sockets interface"/></p>

<ol>
<li>开启服务器(start server)
<ul>
<li><code>getaddrinfo</code>: 把主机名(hostname）、主机地址(host addresses)、端口(ports)和服务名(service names)转换为套接字地址结构。</li>
<li><code>socket</code>: 创建一个套接字描述符(socket descriptor)，也就是之后用来读写的 file descriptor</li>
<li><code>bind</code>: 请求内核把套接字地址和套接字描述符绑定</li>
<li><code>listen</code>: 将套接字描述符从一个主动套接字转换为监听套接字(listening socket)，该套接字可以接受来自客户端的连接请求</li>
<li><code>accept</code>: 等待来自客户端的连接请求</li>
</ul></li>
<li>开启客户端(start client)
<ul>
<li><code>getaddrinfo</code>, <code>socket</code>与开启服务器相同</li>
<li><code>connect</code>: 试图与服务器建立连接</li>
</ul></li>
</ol>

<h3 id="toc_9">4.1 Echo客户端和服务器示例</h3>

<p>在和服务器建立连接之后，客户端进入一个循环，反复从标准输入读取文本行，发送文本行给服务器，从服务器读取回送的行，并输出结果到接准输出。</p>

<pre><code class="language-c">#include &quot;csapp.h&quot;
int main (int argc, char **argv) {
    int clientfd;
    char *host, *port, buf[MAXLINE];
    rio_t rio;
    
    host = argv[1];
    port = argv[2];
    
    //和服务器建立连接
    clientfd = Open_clientfd(host, port);
    Rio_readinitb(&amp;rio, clientfd);
    
    while (Fgets(buf, MAXLINE, stdin) != NULL) {
        // 写入，也就是向服务器发送信息
        Rio_writen(clientfd, buf, strlen(buf));
        // 读取，也就是从服务器接收信息
        Rio_readlineb(&amp;rio, buf, MAXLINE);
        // 把从服务器接收的信息显示在输出中
        Fputs(buf, stdout);
    }
    Close(clientfd);
    exit(0);
}
</code></pre>

<p>服务器在打开监听描述符后，进入一个无限循环。每次循环都等待一个来自客户端的连接请求，输出已连接客户端的域名和IP地址，并调用echo函数为这些客户端服务。在echo程序返回后，主程序关闭已连接描述符。</p>

<pre><code class="language-c">#include &quot;csapp.h&quot;
void echo(int connfd);
int main(int argc, char **argv){
    int listenfd, connfd;
    socklen_t clientlen;
    struct sockaddr_storage clientaddr; // Enough room for any addr
    char client_hostname[MAXLINE], client_port[MAXLINE];
    
    // 开启监听端口，注意只开这么一次
    listenfd = Open_listenfd(argv[1]);
    while (1) {
        // 需要具体的大小
        clientlen = sizeof(struct sockaddr_storage); // Important!
        // 等待连接
        connfd = Accept(listenfd, (SA *)&amp;clientaddr, &amp;clientlen);
        // 获取客户端相关信息
        Getnameinfo((SA *) &amp;clientaddr, clientlen, client_hostname,
                     MAXLINE, client_port, MAXLINE, 0);
        printf(&quot;Connected to (%s, %s)\n&quot;, client_hostname, client_port);
        // 服务器具体完成的工作
        echo(coonfd);
        Close(connfd);
    }
    exit(0);
}
void echo(int connfd) {
    size_t n;
    char buf[MAXLINE];
    rio_t rio;
    
    // 读取从客户端传输过来的数据
    Rio_readinitb(&amp;rio, connfd);
    while((n = Rio_readlineb(&amp;rio, buf, MAXLINE)) != 0) {
        printf(&quot;server received %d bytes\n&quot;, (int)n);
        // 把从 client 接收到的信息再写回去
        Rio_writen(connfd, buf, n);
    }
}
</code></pre>

<h2 id="toc_10">5 Web服务器</h2>

<h3 id="toc_11">5.1 Web基础</h3>

<p>Web客户端和服务器之间的交互用的是<strong>HTTP协议</strong>(超文本传输协议)，交互的基本过程为：</p>

<ul>
<li>客户端和服务器建立TCP连接</li>
<li>客户端请求内容</li>
<li>服务器响应请求的内容</li>
<li>服务器和客户端最终关闭 连接</li>
</ul>

<p><img src="media/15320197756867/web%20server%20basics.png" alt="web server basics"/></p>

<h3 id="toc_12">5.2 Web内容</h3>

<p>Web服务器返回内容给客户端，内容是与一个<strong>MIME</strong>类型相关的字节序列。(MIME -  Multipurpose Internet Mail Extensions)</p>

<p>HTTP响应返回的类型可以是静态的，也可以是动态的：</p>

<ul>
<li>静态内容：内容存储在文件中，响应HTTP请求后返回给客户端
<ul>
<li>例如HTML文件，图片，声音</li>
</ul></li>
<li>动态内容：运行一个可执行文件产生输出，返回给客户端</li>
</ul>

<h3 id="toc_13">5.3 HTTP事务</h3>

<p>一个<strong>HTTP请求</strong>(request)是一个<strong>请求行</strong>(request line)，后面跟随着零个或多个<strong>请求报头</strong>(request header)，再跟随一个终止报头的空行。</p>

<p><strong>请求行</strong>的格式是<code>&lt;method&gt; &lt;uri&gt; &lt;version&gt;</code>。</p>

<ul>
<li><code>&lt;method&gt;</code>可以是GET, POST, OPTIONS, HEAD, PUT, DELETE, TRAXE</li>
<li><code>&lt;uri&gt;</code>是响应的URL的后缀，包括文件名和可选的参数</li>
<li><code>&lt;version&gt;</code>是该请求遵循的HTTP的版本(HTTP/1.0或者HTTP/1.1)</li>
</ul>

<p><strong>请求报头</strong>的格式是<code>&lt;header name&gt;:&lt;header data&gt;</code></p>

<ul>
<li>为服务器提供额外信息，例如浏览器的商标名</li>
</ul>

<p><strong>HTTP响应</strong>与HTTP请求类似，是一个<strong>响应行</strong>(response line)，后面跟着零个或者多个<strong>响应报头</strong>(response header)，再跟随一个终止报头的空行，再跟随一个响应主体(response body)。</p>

<p><strong>响应行</strong>的格式是<code>&lt;version&gt; &lt;status code&gt; &lt;status message&gt;</code></p>

<ul>
<li><code>&lt;version&gt;</code>是响应所遵循的HTTP版本</li>
<li><code>&lt;status-code&gt;</code>是一个3位的正整数，指明对请求的处理</li>
<li><code>&lt;status-message&gt;</code> 英文描述</li>
</ul>

<p><strong>响应报头</strong>的格式是<code>&lt;header name&gt;:&lt;header data&gt;</code></p>

<p>下面是HTTP请求的一个实例<br/>
<img src="media/15320197756867/example%20HTTP%20transaction.png" alt="example HTTP transaction"/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning with large datasets]]></title>
    <link href="http://larryim.cc/15323640524161.html"/>
    <updated>2018-07-24T00:40:52+08:00</updated>
    <id>http://larryim.cc/15323640524161.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">1 overview</h2>

<p>Why use big data?</p>

<ul>
<li>Simple learning methods with large data sets can outperform complex learners with smaller datasets</li>
<li>The ordering of learning methods, best-to-worst, can be different for small datasets than from large datasets</li>
<li>The best way to improve performance for a learning system is often to collect more data</li>
<li>Large datasets often imply large classifiers</li>
</ul>

<p>Asymptotic analysis</p>

<ul>
<li>It measures number of operations as function of problem size</li>
<li>Different operations (eg disk seeking, scanning, memory access) can have very very different costs</li>
<li>Disk access is cheapest when you scan sequentially</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Operating System Concepts 4 - Threads & Concurrency]]></title>
    <link href="http://larryim.cc/os_concepts_threads_and_concurrency.html"/>
    <updated>2018-07-19T17:40:56+08:00</updated>
    <id>http://larryim.cc/os_concepts_threads_and_concurrency.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">1 Overview</a>
<ul>
<li>
<a href="#toc_1">1.1 Motivation</a>
</li>
<li>
<a href="#toc_2">1.2 Benefits</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">2 Multicore Programming</a>
<ul>
<li>
<a href="#toc_4">2.1 Programming Challenges</a>
</li>
<li>
<a href="#toc_5">2.2 Types of Parallelism</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">3 Multithreading Models</a>
<ul>
<li>
<a href="#toc_7">3.1 Many-to-One Model</a>
</li>
<li>
<a href="#toc_8">3.2 One-to-One Model</a>
</li>
<li>
<a href="#toc_9">3.3 Many-to-Many Model</a>
</li>
<li>
<a href="#toc_10">3.4 User/Kernel-Level threads</a>
</li>
</ul>
</li>
<li>
<a href="#toc_11">4 Thread Libraries</a>
</li>
<li>
<a href="#toc_12">5 Implicit threading</a>
<ul>
<li>
<a href="#toc_13">5.1 Thread Pools</a>
</li>
<li>
<a href="#toc_14">5.4 Grand Central Dispatch</a>
</li>
</ul>
</li>
<li>
<a href="#toc_15">6 Threading Issues</a>
<ul>
<li>
<a href="#toc_16">6.1 Light Weight Process</a>
</li>
<li>
<a href="#toc_17">6.2 Scheduler activation</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">1 Overview</h2>

<p>A <strong>thread</strong> is a basic unit of CPU utilization; it comprises a thread ID, a program counter, a register set, and a stack.</p>

<p>线程是一个CPU利用的基本单元，它由线程ID，程序计数器、寄存器集合和栈组成。</p>

<p>A traditional process has a single thread of control. If a process has <strong>multiple threads of control</strong>, it can perform more than one task at a time. <br/>
一个传统的进程只有单个控制线程，如果进程有多个控制线程，那么它能一次处理多个任务。</p>

<p>The figure below illustrates the difference between a traditional <strong>single-threaded</strong> process and a <strong>multithreaded</strong> process.</p>

<p><img src="media/15319932561412/single%20threaded%20and%20multithreaded%20processes.png" alt="single threaded and multithreaded processes"/></p>

<h3 id="toc_1">1.1 Motivation</h3>

<p>Process creation is <strong>time consuming</strong> and <strong>resource intensive</strong>. It is generally more efficient to use one process that contains multiple threads.</p>

<h3 id="toc_2">1.2 Benefits</h3>

<p>The benefits of  multithreaded programming can be broken down into four major categories:</p>

<ol>
<li><p><strong>Responsiveness</strong></p>
<ul>
<li>It allows a program to continue running even if part of it is blocked or is performing a lengthy operation, thereby increasing responsiveness to the user.</li>
<li>响应度高：即使程序部分阻塞或执行较冗长操作，该程序仍能继续执行，从而增加了对用户的相应程度。</li>
</ul></li>
<li><p><strong>Resource sharing</strong></p>
<ul>
<li>Processes can share resources only through techniques such as shared memory and message. Such techniques must be explicitly arranged by the programmer.</li>
<li>Threads share the memory and the resources of the process to which they belong by default.</li>
<li>资源共享：线程默认共享它们所属进程的内存和资源。代码和数据共享的优点是它允许一个应用程序在同一地址空间有多个不同的活动线程。</li>
</ul></li>
<li><p><strong>Economy</strong></p>
<ul>
<li>Allocating memory and resources for process creation is costly. Because threads share the resources of the process to which they belong, it is more economical to create and context-switch threads.</li>
<li>经济：进程创建所需要的内存和资源的分配比较昂贵。由于线程能共享它们所属进程的资源，所以创建和切换线程会更为经济。</li>
</ul></li>
<li><p><strong>Scalability</strong></p>
<ul>
<li>The benefits of multithreading can be even greater in a multiprocessor architecture, where threads may be running in parallel on different processing cores.</li>
<li>可扩展性: 多线程的优点之一是能充分使用多处理器体系结构。以便每个进程能并行运行在不同的处理器上。</li>
</ul></li>
</ol>

<h2 id="toc_3">2 Multicore Programming</h2>

<p>On a system with a <strong>single</strong> computing core, concurrency merely means that the execution of the threads will be <strong>interleaved</strong> over time, because the processing core is capable of executing only one thread at a time.</p>

<p>On a system with <strong>multiple</strong> cores, however, concurrency means that some threads can run in <strong>parallel</strong>, because the system can assign a separate thread to each core.</p>

<p><img src="media/15319932561412/concurrency%20of%20single%20and%20multiple%20core.png" alt="concurrency of single and multiple core"/></p>

<p>Note: <strong><em>Concurrency</em></strong> v.s. <strong><em>Parallelism</em></strong></p>

<ul>
<li>Concurrency: supports more than one task by allowing all the tasks to make progress.</li>
<li>Parallelism: perform more than one task simultaneously.</li>
</ul>

<h3 id="toc_4">2.1 Programming Challenges</h3>

<ol>
<li><strong>Identifying tasks</strong>. This involves examining applications to find areas that can be divided into separate, concurrent tasks.</li>
<li><strong>Balance</strong>. Programmer must ensure that the tasks perform equal work of equal value.</li>
<li><strong>Data splitting</strong>. The data accessed and manipulated by the tasks must be divided to run on separate cores.</li>
<li><strong>Data dependency</strong>. The data accessed by the tasks must be examined for dependencies between two or more tasks.</li>
<li><strong>Testing and debugging</strong>. Testing and debugging such concurrent programs is inherently more difficult than testing and debugging single-threaded applications.</li>
</ol>

<h3 id="toc_5">2.2 Types of Parallelism</h3>

<p>In general, there are two types of parallelism: data parallelism and task parallelism.</p>

<ul>
<li><strong>Data parallelism</strong> focuses on distributing subsets of the same data across multiple computing cores and performing the same operation on each core. </li>
<li><strong>Task parallelism</strong> involves distributing not data but tasks (threads) across multiple computing cores.</li>
<li>However, data and task parallelism are not mutually exclusive, and an application may in fact use a hybrid of these two strategies.</li>
</ul>

<p>Data and task parallelism<br/>
<img src="media/15319932561412/data%20and%20task%20parallellism.png" alt="data and task parallelism"/></p>

<h2 id="toc_6">3 Multithreading Models</h2>

<p>Support for threads may be provided either at the user level, for <strong><em>user threads</em></strong>, or by the kernel, for <strong><em>kernel threads</em></strong>. </p>

<ul>
<li>User threads are supported above the kernel and are managed without kernel support.</li>
<li>Kernel threads are supported and managed directly by the operating system.</li>
</ul>

<p>有两种不同的方法来提供线程支持：用户层的用户级线程和内核层的内核级线程。用户级线程受内核支持，而无需内核管理；而内核级线程由操作系统直接支持和管理。事实上所有当代操作系统都支持内核级线程。</p>

<p>Ultimately, a relationship must exist between user threads and kernel threads. There are three common ways of establishing such a relationship: the many-to-one model, the one-to-one model, and the many-to-many model.<br/>
在用户级线程和内核级线程之间必然存在一种关系。有三种普遍建立这种关系的方法：多对一模型、一对一模型、多对多模型。</p>

<h3 id="toc_7">3.1 Many-to-One Model</h3>

<p>The <strong><u>many-to-one model</u></strong> maps many user-level threads to one kernel thread. 多对一模型将许多用户级线程映射到一个内核线程。</p>

<ul>
<li>Thread management is done by the thread library in user space, so it is efficient. 线程管理由线程库在用户空间进行的，因而效率比较高。</li>
<li>Also, because only one thread can access the kernel at a time, multiple threads are unable to run in parallel on multicore systems. 因为任意时刻只能有一个线程能够访问内核，多个线程不能并行运行在多处理器上。</li>
</ul>

<p><img src="media/15319932561412/many_to_one_model.png" alt="many_to_one_mode"/></p>

<h3 id="toc_8">3.2 One-to-One Model</h3>

<p>The <strong>one-to-one model</strong> maps each user thread to a kernel thread. 一对一模型每个用户线程映射到一个内核线程。</p>

<ul>
<li>It provides more concurrency by allowing another thread to run when a thread makes a blocking system call. 该模型在一个线程执行阻塞系统调用时，能允许另一个线程继续执行，提供了更高的并发性。</li>
<li>It also allows multiple threads to run in parallel on multiprocessors. 它也允许多个线程能并行运行在多处理器系统上。</li>
<li>The only drawback to this model is that creating a user thread requires creating the corresponding kernel thread, and a large number of kernel threads may burden the performance of a system. 这种模型的唯一缺点是每创建一个用户线程就会创建一个相应的内核线程, 大量内核线程会影响系统性能。</li>
</ul>

<p><img src="media/15319932561412/one_to_one_model.png" alt="one_to_one_mode"/></p>

<h3 id="toc_9">3.3 Many-to-Many Model</h3>

<p>The <strong>many-to-many model</strong> multiplexes many user-level threads to a smaller or equal number of kernel threads. 多对多模型多路复用了许多用户线程到同样数量或更小数量的内核线程上。</p>

<ul>
<li>Developers can create as many user threads as necessary. 开发人员可创建任意多的用户线程。</li>
<li>The corresponding kernel threads can run in parallel on a multiprocessor. 相应内核线程能在多处理器系统上并发执行。</li>
<li>Also, when a thread performs a blocking system call, the kernel can schedule another thread for execution. 而且当一个线程执行阻塞系统调用时，内核能调度另一个线程来执行。</li>
<li>In practice it is <strong>difficult to implement</strong>. 实际上难以实施。</li>
</ul>

<p><img src="media/15319932561412/many_to_many_model.png" alt="many_to_many_mode"/></p>

<p><u><strong><em>Most operating systems now use the one-to-one model.</em></strong></u></p>

<h3 id="toc_10">3.4 User/Kernel-Level threads</h3>

<p><a href="https://cs.nyu.edu/rgrimm/teaching/sp07-os/activations.pdf">ref: Scheduler Activations</a></p>

<p>(1) <strong>User-Level Threads</strong>(用户级线程)</p>

<p>Advantages</p>

<ul>
<li>Common operations can be implemented <strong>efficiently</strong> </li>
<li>Interface can be tailored to application needs</li>
</ul>

<p>Issues:</p>

<ul>
<li>A blocking system call blocks all user-level threads. 阻塞系统调用能够阻塞所有用户级线程。</li>
<li>Asynchronous system calls can provide partial work-around. 非同步系统调用能提供部分work-around. <a href="https://en.wikipedia.org/wiki/Workaround">view the definition of work-around here</a></li>
<li>A page fault blocks all user-level threads. 缺页异常阻塞所有用户级线程。</li>
<li>Matching threads to CPUs in a multiprocessor is hard：
<ul>
<li>No knowledge about the numbers of CPUs available to address space </li>
<li>No knowledge when a thread blocks</li>
</ul></li>
</ul>

<p>(2) <strong>Kernel-Level Threads</strong> (内核级线程)</p>

<p>Primary advantage</p>

<ul>
<li>Blocking system calls and page faults handled correctly</li>
</ul>

<p>Issues</p>

<ul>
<li>Cost of performing thread operations</li>
<li>Create, exit, lock, signal, wait all require user/kernel crossings</li>
</ul>

<p><strong>NOTE</strong>: The term <strong><u>virtual processor</u></strong> is often used instead of kernel thread.</p>

<h2 id="toc_11">4 Thread Libraries</h2>

<p>A thread library provides the programmer with an API for creating and managing threads. 线程库为程序员提供了创建和管理线程的API。</p>

<p><strong>Pthreads</strong>, the threads extension of the POSIX standard, may be provided as either a user-level or a kernel-level library. Pthread作为POSIX标准扩展，可以提供用户级或内核级的库。</p>

<ul>
<li><code>pthread_t tid</code>: declares the identifier for the thread</li>
<li><code>pthread attr_t attr</code>: declares the attributes for the thread</li>
<li><code>pthread_attr_init(&amp;attr)</code>: initialize thread attributes object</li>
<li><code>pthread_create()</code>: create a new thread</li>
<li><code>pthread_join()</code>:  join with a terminated thread<br/></li>
<li><code>pthread_exit()</code>:  terminate calling thread<br/></li>
</ul>

<p>Note: Compile and link with <code>-pthread</code>.</p>

<p><code>pthread</code>详细用法和实例, 见<a href="https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/pthreads.html">POSIX thread (pthread) libraries</a></p>

<h2 id="toc_12">5 Implicit threading</h2>

<p><strong>Implicit threading</strong>(隐式线程): Transfers the creation and management of threading from application developers to compilers and run-time libraries.</p>

<ul>
<li>One way to address difficulties and better support the design of concurrent and parallel applications</li>
<li>The advantage of this approach is that developers <strong><em><u>only need to identify parallel tasks</u></em></strong>, and the libraries determine the specific details of thread creation and management.</li>
</ul>

<p>In this section, we explore four alternative approaches to designing applications that can take advantage of multicore processors through implicit threading:</p>

<ul>
<li>Thread Pools</li>
<li>Fork Join</li>
<li>OpemMP</li>
<li>Grand Central Dispatch</li>
</ul>

<h3 id="toc_13">5.1 Thread Pools</h3>

<p>Two main issues exist:</p>

<ul>
<li>The thread will be <strong>discarded</strong> once it has completed its work. 线程在完成工作之后就要被丢弃</li>
<li><strong>Unlimited</strong> threads could exhaust system resources. 无限制的线程会耗尽系统资源</li>
</ul>

<p>Solution -&gt; <strong>thread pool</strong>(线程池)</p>

<ul>
<li>It creates a number of threads at start-up, and places them into a pool, where they sit and wait for work.</li>
<li>When a server receives a request, it submits the request to the thread pool and resumes waiting for additional requests.</li>
<li>If there is an available thread in the pool, it is awakened, and the request is serviced immediately. </li>
<li>If the pool contains no available thread, the task is queued until one becomes free. </li>
</ul>

<p>线程池的思想是在进程开始时创建一定数量的线程，并放入到池中以等待工作。当服务器收到请求时，它会唤醒线程池中的一个线程，并将要处理的请求传递给它，一旦线程完成了服务，它会返回到池中在等待工作。如果池中没有可用的线程，那么服务器会一直等待直到有空线程为止。</p>

<p>Thread pools offer these benefits:</p>

<ol>
<li>Servicing a request with an existing thread is often <strong>faster</strong> than waiting to create a thread. 通常用现有线程处理请求要比等待创建新的线程要快.</li>
<li>A thread pool <strong>limits</strong> the number of threads that exist at any one point.  线程池限制了在任何时候可用线程的数量.</li>
<li>Separating the task to be performed from the mechanics of creating the task allows us to use different strategies for running the task.</li>
</ol>

<p>Java线程库的一个例子：</p>

<pre><code class="language-java">import java.util.concurrent.*; 
public class ThreadPoolExample 
{ 
    public static void main(String[] args) {
        int numTasks = Integer.parseInt(args[0].trim()); 
        
        /* Create the thread pool */ 
        ExecutorService pool = Executors.newCachedThreadPool(); 
        /* Run each task using a thread in the pool */ 
        for (int i = 0; i &lt; numTasks; i++) 
            pool.execute(new Task()); 
        
         /* Shut down the pool once all threads have completed */            
        pool.shutdown();
}
</code></pre>

<h3 id="toc_14">5.4 Grand Central Dispatch</h3>

<p><strong>Grand Central Dispatch</strong> (GCD) is a technology for Apple&#39;s Mac OS X and iOS operating systems. It is a combination of extensions to the C languages, an API, and a run-time library that allows application developers identify sections of code to run in parallel.</p>

<ul>
<li>GCD identifies two types of dispatch queues: serial and concurrent.</li>
</ul>

<h2 id="toc_15">6 Threading Issues</h2>

<h3 id="toc_16">6.1 Light Weight Process</h3>

<p>Many systems implementing either the many-to-many or the two-level model place an <em>intermediate</em> data structure between the user and kernel threads. This data structure—typically known as a <strong><u><em>lightweight process</em></u></strong>（轻量级进程）, or <strong>LWP</strong>.</p>

<ul>
<li>To the user-thread library, the LWP appears to be a <strong>virtual</strong> processor on which the application can schedule a user thread to run.</li>
<li>Each LWP is attached to a kernel thread.</li>
<li>If a kernel thread blocks, the LWP blocks as well. Up the chain, the user-level thread attached to the LWP also blocks.</li>
</ul>

<p><img src="media/15319932561412/light-weight%20process.png" alt="light-weight process"/></p>

<p><img src="media/15319932561412/15320591793590.jpg" alt=""/></p>

<p>where K denotes kernel threads and  P denotes Process.</p>

<p>下面是一个测试LINUX中LWP的C程序</p>

<pre><code class="language-c">/* filename: test_LWP.c */
#include &lt;stdlib.h&gt;
#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;
#include &lt;unistd.h&gt;

int* thread(void* arg)
{
    pthread_t tid; // the ID of a thread
    tid = pthread_self();//get the current thread&#39;s id
 
    printf(&quot;The ID of new thread is =%lu\n&quot;, tid);
    sleep(500); //sleep for 500 seconds
    return NULL; 
}
  
int main()
{
    pthread_t tid;
    printf(&quot;The ID of main thread is %lu\n&quot;, pthread_self()); //get the main thread&#39;s id
  if (pthread_create(&amp;tid, NULL, (void *) thread, NULL) !=0) 
    {
       printf(&quot;Thread creation failed\n&quot;);
        exit(1);
    }
      
    printf(&quot;my Id is %lu, new thread ID is %lu\n&quot;, pthread_self(), tid);
    sleep(1000);
    return 0;
}
</code></pre>

<p>运行<code>ps -efL</code> 可以看到, <code>test_LWP</code>进程(PID=1953)有两个LWP，即NLWP(number of light weight process)=2。</p>

<pre><code class="language-text">UID        PID  PPID   LWP  C NLWP STIME TTY          TIME CMD
vagrant   1953  1644  1953  0    2 04:16 pts/0    00:00:00 ./test_LWP
vagrant   1953  1644  1954  0    2 04:16 pts/0    00:00:00 ./test_LWP
vagrant   2028  1839  2028  0    1 04:18 pts/1    00:00:00 ps -efL
</code></pre>

<h3 id="toc_17">6.2 Scheduler activation</h3>

<p><strong>Problems</strong>:<br/>
内核线程在各方面都比较灵活，但是性能不高，经常会出现请求在用户空间和内核空间的传递。那么如何在拥有内核空间线程的灵活性的同时又提高性能呢?</p>

<p><strong>Solution</strong>:</p>

<p><strong>Scheduler activation</strong>（调度器激活）are a threading mechanism that, when implemented in an operating system&#39;s process scheduler, provide <em><u>kernel-level</u></em> thread functionality with <u><em>user-level</em></u> thread flexibility and performance [<a href="https://en.wikipedia.org/wiki/Scheduler_activations">ref</a>]. </p>

<p>It works as follows: </p>

<ul>
<li>The kernel provides an application with a set of virtual processors (LWPs), and the application can <strong>schedule</strong> user threads onto an available virtual processor. </li>
<li>Furthermore, the kernel must inform an application about certain events. This procedure is known as an <strong>upcall</strong>(向上调用). </li>
<li>Upcalls are handled by the thread library with an upcall handler, and upcall handlers must run on a virtual processor.</li>
<li>While the user threading library will schedule user threads, the kernel will schedule the underlying LWPs.</li>
</ul>

<p><strong>Example</strong> [<a href="http://www.it.uu.se/education/course/homepage/os/vt18/module-4/implementing-threads/">ref</a>]: </p>

<p>Let’s study an example of how scheduler activations can be used. The kernel has allocated one kernel thread (1) to a process with three user-level threads (2). The three user level threads take turn executing on the single kernel-level thread.</p>

<p><img src="media/15319932561412/scheduler-activations-1-2.png" alt="scheduler-activations-1-2"/></p>

<ul>
<li>(3) The executing thread makes a <strong>blocking system call</strong>.</li>
<li>(4) And the the kernel blocks the calling user-level thread and the kernel-level thread used to execute the user-level thread .</li>
<li>(5) Scheduler activation: the kernel decides to allocate a new kernel-level thread to the process . </li>
<li>(6) Upcall: the kernel notifies the user-level thread manager which user-level thread that is now blocked and that a new kernel-level thread is available. </li>
<li>(7) The user-level thread manager move the other threads to the new kernel thread and resumes one of the ready threads.</li>
</ul>

<p><img src="media/15319932561412/scheduler-activations-3-7.png" alt="scheduler-activations-3-7"/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[NOTE] C++ By Dissection]]></title>
    <link href="http://larryim.cc/cpp_by_diessection.html"/>
    <updated>2018-07-21T12:02:43+08:00</updated>
    <id>http://larryim.cc/cpp_by_diessection.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Chapter 1 Writing an ANSI C++ Program</h2>

<h2 id="toc_1">Chapter 2 Native Types and Statements</h2>

<h2 id="toc_2">Chapter 3 Functions, Pointers, and Arrays</h2>

<h2 id="toc_3">Chapter 4 Classes and Abstract Data Types</h2>

<h2 id="toc_4">Chapter 5 Ctors, Dtors, Converisons, and Operator Overloading</h2>

<p><strong>Polymorphism</strong> means giving different meanings to the same function name or operator, dependent on context. Overloading of functions gives the same function name different meanings. The name has several interpretations that depend on function selection. This is called ad hoc polymorphism.</p>

<ul>
<li>ad hoc polymorphism: function overloading, operator overloading
<ul>
<li> functions can be applied to arguments of different types</li>
</ul></li>
<li>parametric polymorphism: using templates
<ul>
<li>allows the same code to be used with respect to various types, in which the type is a parameter of the code body.</li>
</ul></li>
<li>pure polymorphism: using virtual functions</li>
</ul>

<p>Chapter 11</p>

<p>OOP Language Characteristics</p>

<ul>
<li>Encapsulation with data hiding: the ability to distinguish an object&#39;s internal state and behavior from its external state and behavior.</li>
<li>Type extensibility: the ability to add user-defined types to augment the native types</li>
<li>Inheritance: the ability to create new types by importing or reusing the description of existing types</li>
<li>Polymorphism with dynamic binding: the ability of objects to be responsible for interpreting function invocation</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CSAPP - 链接]]></title>
    <link href="http://larryim.cc/Linking.html"/>
    <updated>2018-01-06T05:42:41+08:00</updated>
    <id>http://larryim.cc/Linking.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">1 编译器驱动程序</a>
</li>
<li>
<a href="#toc_1">2 静态链接</a>
</li>
<li>
<a href="#toc_2">3 目标文件</a>
</li>
<li>
<a href="#toc_3">4 可重定位目标文件</a>
</li>
<li>
<a href="#toc_4">5 符号和符号表</a>
</li>
<li>
<a href="#toc_5">6 符号解析</a>
</li>
<li>
<a href="#toc_6">7 重定位</a>
</li>
<li>
<a href="#toc_7">8 可执行目标文件</a>
</li>
<li>
<a href="#toc_8">9 加载可执行目标文件</a>
</li>
<li>
<a href="#toc_9">10 动态链接共享库</a>
</li>
<li>
<a href="#toc_10">14 处理目标文件的工具</a>
</li>
</ul>


<p><strong>链接</strong>(Linking)是将各种<strong>代码</strong>和<strong>数据片段</strong>收集并组合成为一个单一文件的过程。链接可以在编译、加载、运行时执行。在现代系统中，链接由<strong>链接器</strong>(Linker)自动执行。</p>

<p>链接器使得<strong>分离编译</strong>(separate compilation)成为可能：</p>

<ul>
<li>可以将源文件分解为更小、更好管理的模块，可以独立地修改和编译这些模块</li>
<li>修改一个模块后，只需重新编译它，并重新链接，不必编译其他文件</li>
</ul>

<h2 id="toc_0">1 编译器驱动程序</h2>

<p><strong>编译器驱动程序</strong>(<code>compiler driver</code>)，代表用户在需要时调用预处理器(cpp)、编译器(ccl)、汇编器(as)和链接器(ld)。典型的编译器驱动程序，包括GNU GCC, Clang。</p>

<p>例如，一个简单打印hello的<code>hello.c</code>程序，经过下面四个阶段，生成可执行目标文件：</p>

<pre><code class="language-c">//file: hello.c
#include &lt;stdio.h&gt;

int main()
{
    int i;
    printf(&quot;Hello World&quot;);
}
</code></pre>

<pre><code class="language-bash">linux &gt; gcc -o hello hello.c
</code></pre>

<p><img src="media/15151885614329/compiler_system.jpeg" alt="compiler_syste"/></p>

<h2 id="toc_1">2 静态链接</h2>

<p>静态链接器有两个主要任务：</p>

<ul>
<li><strong>符号解析</strong>(symbol resolution): 将每个符号 <u>引用</u> 正好和一个符号 <u>定义</u> 关联起来。</li>
<li><strong>重定位</strong>(relocation): 把每个符号定义与一个内存位置关联起来，并修改所有对这些符号的引用，使得它们指向这个内存位置。</li>
</ul>

<h2 id="toc_2">3 目标文件</h2>

<p>目标文件有三种格式：<strong>可重定位目标文件</strong>(<code>.o</code>)，<strong>可执行目标文件</strong>(<code>.out</code>)，<strong>共享目标文件</strong>(<code>.so</code>)</p>

<ul>
<li><strong>可重定位目标文件</strong>(.o文件)。包含二进制代码和数据，其形式可以在编译时与其他可重定位目标文件合并起来，创建一个可执行目标文件。</li>
<li><strong>可执行目标文件</strong>(a.out文件)。包含二进制代码和数据，其形式可以被直接复制到内存并执行。</li>
<li><strong>共享目标文件</strong>(.so文件)。在加载或者运行时被动态地加载进内存并链接</li>
</ul>

<p>各个系统的目标文件格式不同，Windows使用<strong>可移植可执行</strong>(Portable Executable, <code>PE</code>)格式。现代x86-64系统使用<strong>可执行可链接格式</strong>(Executable and Linkable Format, <code>ELF</code>)。</p>

<h2 id="toc_3">4 可重定位目标文件</h2>

<p>以可执行可链接(ELF)格式为例，一个典型的可重定位目标文件包括以下几个节：</p>

<ul>
<li>ELF头和节头部表</li>
<li><code>.text</code> 已编译程序的机器代码</li>
<li><code>.rodata</code> 只读数据</li>
<li><code>.data</code>  已初始化的全局和静态C变量</li>
<li><code>.bss</code>  未初始化的全局和静态C变量</li>
<li><code>.symtab</code> 一个符号表</li>
<li><code>.rel.text</code> 一个.text节中位置的列表</li>
<li><code>.rel.data</code> 重定位信息</li>
<li><code>.debug</code> 调试符号表</li>
<li><code>.line</code>  原始程序行号和机器指令之间的映射</li>
<li><code>.strtab</code>  字符串表</li>
</ul>

<p><img src="media/15151885614329/elf.png" alt="elf"/></p>

<p>利用<code>READELF</code>程序可以显示程序<code>hello.c</code>生成的可执行可链接文件的信息：</p>

<pre><code class="language-bash">gcc hello.c -c
readelf -a hello.o ## UNIX/LINUX
greadelf -a hello.o ## MAC, after brew install binutils
</code></pre>

<pre><code class="language-text">ELF Header:
  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00
  Class:                             ELF64
  Data:                              2&#39;s complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              REL (Relocatable file)
  Machine:                           Advanced Micro Devices X86-64
  Version:                           0x1
  Entry point address:               0x0
  Start of program headers:          0 (bytes into file)
  Start of section headers:          304 (bytes into file)
  Flags:                             0x0
  Size of this header:               64 (bytes)
  Size of program headers:           0 (bytes)
  Number of program headers:         0
  Size of section headers:           64 (bytes)
  Number of section headers:         13
  Section header string table index: 10

Section Headers:
  [Nr] Name              Type             Address           Offset
       Size              EntSize          Flags  Link  Info  Align
  [ 0]                   NULL             0000000000000000  00000000
       0000000000000000  0000000000000000           0     0     0
  [ 1] .text             PROGBITS         0000000000000000  00000040
       0000000000000015  0000000000000000  AX       0     0     1
  [ 2] .rela.text        RELA             0000000000000000  00000590
       0000000000000030  0000000000000018          11     1     8
  [ 3] .data             PROGBITS         0000000000000000  00000055
       0000000000000000  0000000000000000  WA       0     0     1
  [ 4] .bss              NOBITS           0000000000000000  00000055
       0000000000000000  0000000000000000  WA       0     0     1
  [ 5] .rodata           PROGBITS         0000000000000000  00000055
       000000000000000c  0000000000000000   A       0     0     1
  [ 6] .comment          PROGBITS         0000000000000000  00000061
       000000000000002c  0000000000000001  MS       0     0     1
  [ 7] .note.GNU-stack   PROGBITS         0000000000000000  0000008d
       0000000000000000  0000000000000000           0     0     1
  [ 8] .eh_frame         PROGBITS         0000000000000000  00000090
       0000000000000038  0000000000000000   A       0     0     8
  [ 9] .rela.eh_frame    RELA             0000000000000000  000005c0
       0000000000000018  0000000000000018          11     8     8
  [10] .shstrtab         STRTAB           0000000000000000  000000c8
       0000000000000061  0000000000000000           0     0     1
  [11] .symtab           SYMTAB           0000000000000000  00000470
       0000000000000108  0000000000000018          12     9     8
  [12] .strtab           STRTAB           0000000000000000  00000578
       0000000000000015  0000000000000000           0     0     1
Key to Flags:
  W (write), A (alloc), X (execute), M (merge), S (strings), l (large)
  I (info), L (link order), G (group), T (TLS), E (exclude), x (unknown)
  O (extra OS processing required) o (OS specific), p (processor specific)

There are no section groups in this file.

There are no program headers in this file.

Relocation section &#39;.rela.text&#39; at offset 0x590 contains 2 entries:
  Offset          Info           Type           Sym. Value    Sym. Name + Addend
000000000005  00050000000a R_X86_64_32       0000000000000000 .rodata + 0
00000000000f  000a00000002 R_X86_64_PC32     0000000000000000 printf - 4

Relocation section &#39;.rela.eh_frame&#39; at offset 0x5c0 contains 1 entries:
  Offset          Info           Type           Sym. Value    Sym. Name + Addend
000000000020  000200000002 R_X86_64_PC32     0000000000000000 .text + 0

The decoding of unwind sections for machine type Advanced Micro Devices X86-64 is not currently supported.

Symbol table &#39;.symtab&#39; contains 11 entries:
   Num:    Value          Size Type    Bind   Vis      Ndx Name
     0: 0000000000000000     0 NOTYPE  LOCAL  DEFAULT  UND
     1: 0000000000000000     0 FILE    LOCAL  DEFAULT  ABS hello.c
     2: 0000000000000000     0 SECTION LOCAL  DEFAULT    1
     3: 0000000000000000     0 SECTION LOCAL  DEFAULT    3
     4: 0000000000000000     0 SECTION LOCAL  DEFAULT    4
     5: 0000000000000000     0 SECTION LOCAL  DEFAULT    5
     6: 0000000000000000     0 SECTION LOCAL  DEFAULT    7
     7: 0000000000000000     0 SECTION LOCAL  DEFAULT    8
     8: 0000000000000000     0 SECTION LOCAL  DEFAULT    6
     9: 0000000000000000    21 FUNC    GLOBAL DEFAULT    1 main
    10: 0000000000000000     0 NOTYPE  GLOBAL DEFAULT  UND printf

No version information found in this file.
</code></pre>

<h2 id="toc_4">5 符号和符号表</h2>

<p><code>.symtab</code>中的<strong>符号表</strong>，有三种不同的符号(不包括本地非静态变量)：</p>

<ul>
<li>由模块\(m\)定义并能被其他模块引用的<strong>全局符号</strong>。
<ul>
<li>非静态C函数和全局变量</li>
</ul></li>
<li>由其他模块定义并被模块\(m\)引用的全局符号。
<ul>
<li>对应于其他模块中定义的非静态C函数和全局变量</li>
</ul></li>
<li>只被模块\(m\)定义和引用的局部符号。
<ul>
<li>静态C函数和全局变量 </li>
</ul></li>
</ul>

<h2 id="toc_5">6 符号解析</h2>

<p><strong>符号解析</strong>是将每个<strong>符号引用</strong>和可重定位目标文件中的<strong>符号定义</strong>关联起来。链接器的输入是一组可重定位目标文件(模块)，有些是局部的( <u>局部符号</u> ，只对定义该符号的模块可见)，有些是全局的( <u>全局符号</u> ，对其他模块可见)。</p>

<ul>
<li><strong>局部符号</strong>：每个模块中每个局部符号有一个定义</li>
<li><p><strong>全局符号</strong>：可重定位目标文件的符号表里的全局符号是区分<strong>强</strong>和<strong>弱</strong>的，链接器根据以下规则来处理多重定义的符号名：</p>
<ul>
<li>规则1: 不允许有多个同名的强符号</li>
<li>规则2: 如果有一个强符号和多个弱符号同名，那么选择强符号</li>
<li>规则3：如果有多个弱符号同名，那么任选一个 </li>
</ul></li>
</ul>

<h2 id="toc_6">7 重定位</h2>

<p>重定位合并输入模块，并为每个符号分配运行时地址：</p>

<ul>
<li>重定位节和符号定义：将所有相同类型的节合并为同一类型的新的聚合节，并将运行时内存地址赋给新的聚合节和每个符号定义。
<ul>
<li>例如，来自所有输入模块的<code>.data</code>节被全部合并成输出的可执行目标文件的<code>.data</code>节<br/></li>
</ul></li>
<li>重定位节中的符号引用：将运行时地址付给每个符号引用</li>
</ul>

<h2 id="toc_7">8 可执行目标文件</h2>

<p>下图概括了一个典型的ELF可执行文件的给类信息。</p>

<p><img src="media/15151885614329/%E5%8F%AF%E6%89%A7%E8%A1%8C%E7%9B%AE%E6%A0%87%E6%96%87%E4%BB%B6.png" alt="可执行目标文件"/></p>

<h2 id="toc_8">9 加载可执行目标文件</h2>

<p>当在shell中执行目标文件时，首先通过调用<strong>加载器</strong>(<code>loader</code>)的操作系统代码来运行它，加载器将可执行目标文件的代码和数据复制到主存，跳转到程序的第一条指令(入口点，<code>_start_</code>函数的地址)运行该程序。</p>

<p>在Unix系统中，加载器是系统调用(system call)<code>execve()</code>的回调(call back)，其任务包括：</p>

<ul>
<li>确认(权限，内存要求等)</li>
<li>复制程序到主存</li>
<li>复制命令行参数到栈</li>
<li>初始化寄存器(例如栈针)</li>
<li>跳到入口点(<code>_start_</code>)</li>
</ul>

<h2 id="toc_9">10 动态链接共享库</h2>

<p>静态库有2大缺陷：</p>

<ul>
<li>静态库更新时，需要显示地将程序与更新了的库重新链接</li>
<li>浪费内存资源：几乎每个C程序都使用标准I/O函数，这些函数代码会被复制到每个运行进程的文本段中</li>
</ul>

<p>共享库(shared library)是致力于解决静态库缺陷的产物。</p>

<p><strong>动态链接</strong>(dynamic linking)：共享库在运行或加载时，可以加载到任意的内存地址，并和一个在内存中的程序链接起来。</p>

<ul>
<li>由动态链接器(dynamic linke)执行；</li>
<li>在linux系统中常用<code>.so</code>后缀表示。</li>
</ul>

<p><img src="media/15151885614329/dynamic_linking.png" alt="dynamic_linking"/></p>

<h2 id="toc_10">14 处理目标文件的工具</h2>

<p>Unix系统提供了一系列命令帮助理解和处理目标文件。这些工具包括：</p>

<ul>
<li><code>ar</code> ：创建静态库，插入、删除、列出和提取成员；</li>
<li><code>STRINGS</code> ：列出目标文件中所有可以打印的字符串；</li>
<li><code>STRIP</code> ：从目标文件中删除符号表信息；</li>
<li><code>NM</code> ：列出目标文件符号表中定义的符号；</li>
<li><code>SIZE</code> ：列出目标文件中节的名字和大小；</li>
<li><code>READELF</code> ：显示一个目标文件的完整结构，包括ELF 头中编码的所有信息。</li>
<li><code>OBJDUMP</code> ：显示目标文件的所有信息，最有用的功能是反汇编.text节中的二进制指令。</li>
<li><code>LDD</code> ：列出可执行文件在运行时需要的共享库。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Operating System Concepts 1 - Introduction]]></title>
    <link href="http://larryim.cc/os-concepts-introduction.html"/>
    <updated>2018-07-16T18:07:17+08:00</updated>
    <id>http://larryim.cc/os-concepts-introduction.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">what operating system do</a>
</li>
<li>
<a href="#toc_1">Computer-system organisation</a>
</li>
<li>
<a href="#toc_2">Interrupt</a>
<ul>
<li>
<a href="#toc_3">interrupt, exception, trap</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">multiprogramming and multitasking</a>
</li>
<li>
<a href="#toc_5">dual-mode</a>
</li>
<li>
<a href="#toc_6">timer</a>
</li>
<li>
<a href="#toc_7">virtualization</a>
</li>
<li>
<a href="#toc_8">Free and Open-Source OS</a>
</li>
</ul>


<h2 id="toc_0">what operating system do</h2>

<p>There is no completely adequate definition of operating system. A simple viewpoint is that it includes everything a vendor ships. A more common definition is that the <u>operating system is the one program running at all times on computer - usually  called <strong>kernel</strong></u> . </p>

<p>Three main <strong>purposes</strong> of an operating system are,</p>

<ul>
<li>manages a computer&#39;s hardware</li>
<li>provides a basis for application programs</li>
<li>acts as an intermediary between the user and hardware</li>
</ul>

<p>The operating system includes the always running <strong>kernel</strong>, <strong>middleware</strong> frameworks that ease application development and provide features, and <strong>system programs</strong> that aid in managing the system while it is running.</p>

<p>Anything between the kernel and user applications is considered <strong>middleware</strong>(中间件) [<a href="https://en.wikipedia.org/wiki/Middleware">1</a>].</p>

<h2 id="toc_1">Computer-system organisation</h2>

<p>A computer system can be divided roughly into four components: the <strong>hardware</strong>, the <strong>operating system</strong>, the <strong>application programs</strong>, and a <strong>user</strong>.</p>

<p><img src="http://or9a8nskt.bkt.clouddn.com/abstractviewofcomputersytem.png" alt="Abstract view of the components of a computer system"/></p>

<p>A <strong>computer system</strong>(计算机系统) consists of one or more <strong>CPUs</strong> and a number of <strong>device controllers</strong>(设备控制器) connected through a common <strong>bus</strong>(总线) that provides access between components and shared <strong>memory</strong>.</p>

<p>A <strong>device controller</strong> maintains some <strong>local buffer storage</strong>(局部缓冲存储) and a set of special-purpose <strong>registers</strong>.</p>

<hr/>

<p>Typically, operating systems have a <strong>device driver</strong>(设备驱动) for each device controller. This device driver understands the device controller and provides the rest of the operating system with a uniform interface to the device</p>

<p><img src="http://or9a8nskt.bkt.clouddn.com/AtypicalPCcomputerSystem.png" alt="一个典型的PC计算机系统"/></p>

<h2 id="toc_2">Interrupt</h2>

<p>When the CPU is <strong>interrupted</strong>, it stops what it is doing and immediately transfers execution to a fixed location. The fixed location usually contains the starting address where the service routine for the interrupt is located.</p>

<p>The <strong>interrupt routine</strong>(中断程序) is called indirectly through the interrupt vector table（中断向量表).</p>

<ul>
<li>Generally, the table of pointers is stored in low memory (the first hundred or so locations).</li>
<li>These locations hold the addresses of the interrupt service routines for the various devices.</li>
<li>Interrupt vector is then indexed by a unique number(interrupt vector number, 中断向量号)</li>
<li>interrupt priority levels(中断优先级)</li>
</ul>

<p><img src="http://or9a8nskt.bkt.clouddn.com/interruptvectortable.png" alt="中断向量号"/></p>

<p>Some <strong>services</strong> are provided outside of the kernel by system programs that are loaded into memory at boot time to become system <strong>daemons</strong>, which run the entire time the kernel is running.</p>

<p><img src="http://or9a8nskt.bkt.clouddn.com/interrupt-driven%20I:O%20cycle.png" alt="interrupt-driven I:O cycle"/></p>

<h3 id="toc_3">interrupt, exception, trap</h3>

<p>Unfortunately, there is no clear consensus as to the exact meaning of these terms(exceptions, faults, aborts, traps, and interrupts). Different authors adopt different terms to their own use [<a href="http://www.plantation-productions.com/Webster/www.artofasm.com/DOS/pdf/ch17.pdf">ref</a>].</p>

<p><strong>trap</strong>(陷阱) or <strong>exception</strong>(异常): a software-generated interrupt either by an error（e.g. division by zero, or invalid memory access or by a system call.</p>

<ul>
<li>usual way to invoke a kernel routine (a system call) </li>
</ul>

<p><strong>interrupt</strong>(中断):  generated by the hardware (devices like the hard disk, graphics card, I/O ports, etc).</p>

<h2 id="toc_4">multiprogramming and multitasking</h2>

<p><strong>Multiprogramming</strong>(多道程序) explained:</p>

<ul>
<li>The operating system <strong>keeps several processes in memory</strong> simultaneously. </li>
<li>The operating system picks and begins to execute one of these processes.</li>
<li>Eventually, the process may have to wait for some task, such as an I/O operation, to complete.</li>
<li>When that process needs to wait, the CPU <strong>switches</strong> to another process, and so on.</li>
<li> Eventually, the first process finishes waiting and gets the CPU back. As long as at least one process needs to execute, the <strong>CPU is never idle</strong>.</li>
</ul>

<p><strong>Multitasking</strong>(多任务) is a logical <strong>extension</strong> of multiprogramming. In multitasking systems, the CPU executes multiple processes by switching among them, but the switches occur <strong>frequently</strong>, providing the user with a <strong>fast</strong> response time.</p>

<h2 id="toc_5">dual-mode</h2>

<p>In order to ensure the proper execution of the system, we must be able to distinguish between the execution of operating-system code（<strong>kernel mode</strong>）and user-defined code (<strong>user mode</strong>).</p>

<p><img src="http://or9a8nskt.bkt.clouddn.com/transitionfromusermodetokernelmode.png" alt="Transition from user mode to kernel mode"/></p>

<p><strong>Mode bit</strong>(模式位), is added to the hardware of the computer to indicate the current mode: kernel (0) or user (1).</p>

<p>The concept of modes can be <strong>extended</strong> beyond two modes. </p>

<ul>
<li><p><strong>protection rings</strong>（保护环) are mechanisms to protect data and functionality from faults (by improving fault tolerance) and malicious behavior (by providing computer security). </p></li>
<li><p>For intel processors, ring 0 is kernel mode and ring 3 is user mode</p></li>
</ul>

<p><img src="http://or9a8nskt.bkt.clouddn.com/15317318589869.jpg" alt=""/></p>

<h2 id="toc_6">timer</h2>

<p>A timer (定时器) can  be set to interrupt the computer after a specified period( usually, 100s hz)</p>

<ul>
<li>A variable timer is generally implemented by a fixed-rate clock and a counter. </li>
<li>The operating system sets the counter. Every time the clock ticks, the counter is decremented. </li>
<li>When the counter reaches 0, an interrupt occurs.</li>
</ul>

<h2 id="toc_7">virtualization</h2>

<p><strong>virtualization</strong>(虚拟化) is a technology that allows us to abstract the hardware of a single computer into several different execution environments, thereby creating the illusion that <u><em>each separate environment is running on its own private computer</em></u> .</p>

<ul>
<li>v.s. [different] Emulation involves simulating computer handware in software.</li>
</ul>

<p><img src="http://or9a8nskt.bkt.clouddn.com/virtualmachines.png" alt="A computer running (a) a single operating system and (b) three virtual machines"/></p>

<h2 id="toc_8">Free and Open-Source OS</h2>

<p>Open-source OS</p>

<ul>
<li>source code available</li>
<li>opposite: closed-source OS</li>
</ul>

<p>Free OS</p>

<ul>
<li>source code available</li>
<li>allow no-cost use, redistribution, and modification</li>
</ul>

<p>Arguably, open-source code is <strong>more secure</strong> than closed-source code because many more eyes are viewing the code.</p>

<p>e.g. OS</p>

<ul>
<li> GNU/Linux</li>
<li> FreeBSD</li>
<li> Solaris</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Overhead]]></title>
    <link href="http://larryim.cc/overhead.html"/>
    <updated>2018-01-21T12:39:28+08:00</updated>
    <id>http://larryim.cc/overhead.html</id>
    <content type="html"><![CDATA[
<p><strong>overhead</strong>这个词在计算机中经常出现，那么它到底是什么意思呢？</p>

<p>overhead视风格可以翻译成「额外开销」、「额外消耗」、「虚耗」。</p>

<p>额外开销是建立一个操作所需的资源，它可能看起来不相关，但是却是必须的。就好像当你需要去某个地方的时候，你可能需要一辆车。但是如果你开车上街这件事是大量的额外开销，你可能会想要走路；但是如果你要开车穿越一个国家，额外开销是值得的。</p>

<p>The meaning of the word can differ a lot with context. In general, it&#39;s resources (most often memory and CPU time) that are used, which do not contribute directly to the intended result, but are required by the technology or method that is being used. Examples:</p>

<ul>
<li><p>Protocol overhead: Ethernet frames, IP packets and TCP segments all have headers, TCP connections require handshake packets. Thus, you cannot use the entire bandwidth the hardware is capable of for your actual data. You can reduce the overhead by using larger packet sizes and UDP has a smaller header and no handshake.</p></li>
<li><p>Data structure memory overhead: A linked list requires at least one pointer for each element it contains. If the elements are the same size as a pointer, this means a 50% memory overhead, whereas an array can potentially have 0% overhead.</p></li>
<li><p>Method call overhead: A well-designed program is broken down into lots of short methods. But each method call requires setting up a stack frame, copying parameters and a return address. This represents CPU overhead compared to a program that does everything in a single monolithic function. Of course, the added maintainability makes it very much worth it, but in some cases, excessive method calls can have a significant performance impact.</p></li>
</ul>

<h2 id="toc_0">参考</h2>

<ul>
<li><a href="https://stackoverflow.com/questions/2860234/what-is-overhead">https://stackoverflow.com/questions/2860234/what-is-overhead</a></li>
<li><a href="https://www.zhihu.com/question/20596199?sort=created">https://www.zhihu.com/question/20596199?sort=created</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CSAPP - x86-64汇编]]></title>
    <link href="http://larryim.cc/15146536465849.html"/>
    <updated>2017-12-31T01:07:26+08:00</updated>
    <id>http://larryim.cc/15146536465849.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">汇编代码格式</h2>

<p>现在主要存在<code>ATT</code>和<code>Intel</code>两种汇编代码格式。<code>ATT</code>格式是<code>GCC</code>、 <code>OBJDUMP</code>常用工具的默认格式。其他的诸如Microsoft的工具和来自Intel的文档都是<code>Intel</code>格式的。本文使用<code>ATT</code>格式。</p>

<p><code>ATT</code>汇编格式的注释格式有两种</p>

<pre><code class="language-text"># this is a comment
/* this is a comment */
</code></pre>

<h2 id="toc_1">寄存器</h2>

<p><code>x86-64</code>体系结构广泛存在于个人电脑中。它拥有16个整数寄存器，分别存储64位的值。这些寄存器可以存储地址或整数数据，其分布如下：</p>

<p><img src="media/15146536465849/sfd.png" alt="sfd"/></p>

<p>根据惯例，寄存器%rbx, %rbp和%r12~%r15被划分为<strong>被调用者保存寄存器</strong>。所有其他的寄存器，除了栈指针%rsp都分类为<strong>调用者保存寄存器</strong>。</p>

<ul>
<li>在函数被调用时，不能改变<strong>被</strong>调用者寄存器；</li>
<li>如果要改变的话，只能把<strong>被</strong>调用者寄存器的值压入栈中，在使用后，从栈中恢复<strong>被</strong>调用者寄存器。</li>
</ul>

<h3 id="toc_2">rip 寄存器与PC相对寻址</h3>

<p>%rip 的名称来自于(instruction pointer register，指令指针寄存器)。%rip其实就是<strong>程序计数器</strong>(Program Counter, PC), <u><em>存放着下一条指令的地址</em></u> 。不可以直接修改%rip。</p>

<p>-&gt; <code>instruction pointer = program counter = %rip</code></p>

<p>%rip的其他很重要的一个用法就是RIP/<strong>PC相对寻址</strong>(RIP/PC relative addressing)。即<code>%rip + displacement</code>的用法。</p>

<p>例如，</p>

<pre><code class="language-assembly">mov    0x202a62(%rip),%rdi        # 6044d0 &lt;infile&gt;  rdi = infile
</code></pre>

<p>表示传输%rip+0x202a62的地址对应的内存上的内容到%rdi。</p>

<p>下面说说它是怎么进行PC相对寻址的。</p>

<ul>
<li>源文件经过预处理器、编译器、汇编器处理，输出<strong>可重定位目标文件</strong></li>
<li>再经过<strong>符号解析</strong>(Symbol resolution)把代码中的每个符号引用和一个符号定义关联起来之后，要完成<strong>重定位</strong>(Relocation)任务，最终输出<strong>可执行目标文件</strong>。
<ul>
<li>在<strong>重定位</strong>阶段，ELF(可重定位目标文件在LINUX系统上的一种格式)文件中的<code>R_X86_64_PC32</code>重定位类型重定位了一个使用32位PC相对地址的引用。</li>
<li>当CPU执行一条使用PC相对寻址的指令时，它就将在指令中编码的32位值加上PC的当前运行时值，得到<strong>有效地址</strong>。</li>
</ul></li>
</ul>

<h2 id="toc_3">指令</h2>

<p>指令主要有<code>mov</code>数据传送指令，<code>push</code>、<code>pop</code>压入和压出栈数据，<code>add</code>,<code>sub</code>等算数操作指令，<code>ret</code>, <code>call</code>等转移控制指令。</p>

<h3 id="toc_4">数据传送指令</h3>

<table>
<thead>
<tr>
<th>指令</th>
<th>效果</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>MOV S, D</td>
<td>将S中数据传送到D中</td>
<td>传送</td>
</tr>
<tr>
<td>movb</td>
<td>传送字节</td>
<td></td>
</tr>
<tr>
<td>movw</td>
<td>传送字(双字节)</td>
<td></td>
</tr>
<tr>
<td>movl</td>
<td>传送双字(四字节)</td>
<td></td>
</tr>
<tr>
<td>MOVS S, D</td>
<td>将S中数据传送到D，过程中做了符号扩展处理 传送需要符号扩展的字节</td>
<td></td>
</tr>
<tr>
<td>movsbw</td>
<td>将做了符号扩展的字节传送到字</td>
<td></td>
</tr>
<tr>
<td>movsbl</td>
<td>将做了符号扩展的字节传送到双字</td>
<td></td>
</tr>
<tr>
<td>movswl</td>
<td>将做了符号扩展的字传送到双字</td>
<td></td>
</tr>
<tr>
<td>MOVZ S, D</td>
<td>将S中数据传送到D，过程中做了零扩展处理</td>
<td>传送需要零扩展的字节</td>
</tr>
<tr>
<td>movzbw</td>
<td>将做了零扩展的字节传送到</td>
<td></td>
</tr>
<tr>
<td>movzbl</td>
<td>将做了零扩展的字节传送到双字</td>
<td></td>
</tr>
<tr>
<td>movzwl</td>
<td>将做了零扩展的字传送到双字</td>
<td></td>
</tr>
<tr>
<td>pushl S</td>
<td>R[%esp] &lt;- R[%esp] - 4; M[R[%esp]] &lt;- S;</td>
<td>将双字压栈</td>
</tr>
<tr>
<td>popl D</td>
<td>D &lt;- M[R[%esp]]; R[%esp] &lt;- R[%esp] + 4;</td>
<td>将双字出栈</td>
</tr>
</tbody>
</table>

<h3 id="toc_5">ret, call指令</h3>

<p>在x86-64上，<code>ret</code>指令，相当于从栈中弹出地址A，然后把PC设置为A。<br/>
<code>pop %rip</code><br/>
而<code>call</code>指令，刚好相反，把%rip 压入栈中，然后跳到函数对应的地址。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Objdump 反汇编]]></title>
    <link href="http://larryim.cc/objdump_disassembler.html"/>
    <updated>2018-01-01T03:54:09+08:00</updated>
    <id>http://larryim.cc/objdump_disassembler.html</id>
    <content type="html"><![CDATA[
<p><code>objdump</code>是一个反汇编器(<code>disassembler</code>)，可以将机器语言生成对应的汇编文件。常用的命令是</p>

<pre><code class="language-text">objdump -d filename.o
</code></pre>

<p>其选项有：</p>

<pre><code class="language-text">--archive-headers 
-a 
显示档案库的成员信息,类似ls -l将lib*.a的信息列出。 

-b bfdname 
--target=bfdname 
指定目标码格式。这不是必须的，objdump能自动识别许多格式，比如： 

objdump -b oasys -m vax -h fu.o 
显示fu.o的头部摘要信息，明确指出该文件是Vax系统下用Oasys编译器生成的目标文件。objdump -i将给出这里可以指定的目标码格式列表。 

-C 
--demangle 
将底层的符号名解码成用户级名字，除了去掉所开头的下划线之外，还使得C++函数名以可理解的方式显示出来。 

--debugging 
-g 
显示调试信息。企图解析保存在文件中的调试信息并以C语言的语法显示出来。仅仅支持某些类型的调试信息。有些其他的格式被readelf -w支持。 

-e 
--debugging-tags 
类似-g选项，但是生成的信息是和ctags工具相兼容的格式。 

--disassemble 
-d 
从objfile中反汇编那些特定指令机器码的section。 

-D 
--disassemble-all 
与 -d 类似，但反汇编所有section. 

--prefix-addresses 
反汇编的时候，显示每一行的完整地址。这是一种比较老的反汇编格式。 

-EB 
-EL 
--endian={big|little} 
指定目标文件的小端。这个项将影响反汇编出来的指令。在反汇编的文件没描述小端信息的时候用。例如S-records. 

-f 
--file-headers 
显示objfile中每个文件的整体头部摘要信息。 

-h 
--section-headers 
--headers 
显示目标文件各个section的头部摘要信息。 

-H 
--help 
简短的帮助信息。 

-i 
--info 
显示对于 -b 或者 -m 选项可用的架构和目标格式列表。 

-j name
--section=name 
仅仅显示指定名称为name的section的信息 

-l
--line-numbers 
用文件名和行号标注相应的目标代码，仅仅和-d、-D或者-r一起使用使用-ld和使用-d的区别不是很大，在源码级调试的时候有用，要求编译时使用了-g之类的调试编译选项。 

-m machine 
--architecture=machine 
指定反汇编目标文件时使用的架构，当待反汇编文件本身没描述架构信息的时候(比如S-records)，这个选项很有用。可以用-i选项列出这里能够指定的架构. 

--reloc 
-r 
显示文件的重定位入口。如果和-d或者-D一起使用，重定位部分以反汇编后的格式显示出来。 

--dynamic-reloc 
-R 
显示文件的动态重定位入口，仅仅对于动态目标文件意义，比如某些共享库。 

-s 
--full-contents 
显示指定section的完整内容。默认所有的非空section都会被显示。 

-S 
--source 
尽可能反汇编出源代码，尤其当编译的时候指定了-g这种调试参数时，效果比较明显。隐含了-d参数。 

--show-raw-insn 
反汇编的时候，显示每条汇编指令对应的机器码，如不指定--prefix-addresses，这将是缺省选项。 

--no-show-raw-insn 
反汇编时，不显示汇编指令的机器码，如不指定--prefix-addresses，这将是缺省选项。 

--start-address=address 
从指定地址开始显示数据，该选项影响-d、-r和-s选项的输出。 

--stop-address=address 
显示数据直到指定地址为止，该项影响-d、-r和-s选项的输出。 

-t 
--syms 
显示文件的符号表入口。类似于nm -s提供的信息 

-T 
--dynamic-syms 
显示文件的动态符号表入口，仅仅对动态目标文件意义，比如某些共享库。它显示的信息类似于 nm -D|--dynamic 显示的信息。 

-V 
--version 
版本信息 

--all-headers 
-x 
显示所可用的头信息，包括符号表、重定位入口。-x 等价于-a -f -h -r -t 同时指定。 

-z 
--disassemble-zeroes 
一般反汇编输出将省略大块的零，该选项使得这些零块也被反汇编。 

@file 可以将选项集中到一个文件中，然后使用这个@file选项载入。
</code></pre>

<p>下面通过一个简单的例子来说明一下<code>objdump</code>的常见用法，以及它生成的文件的格式。</p>

<p>假设写一个简单的C程序<code>test.c</code>如下：</p>

<pre><code class="language-c">int foo()
{
    int a = 5;
    int b = 0;
    b = a + 3;
}
</code></pre>

<p>用<code>gcc</code>命令<code>gcc test.c -c</code>生成目标文件<code>test.o</code>后, 利用<code>objdump -d test.o &gt; test.s</code>生成类似汇编文件：</p>

<pre><code class="language-text">test.o:     file format elf64-x86-64


Disassembly of section .text:

0000000000000000 &lt;foo&gt;:
   0:   55                      push   %rbp
   1:   48 89 e5                mov    %rsp,%rbp
   4:   c7 45 f8 05 00 00 00    movl   $0x5,-0x8(%rbp)
   b:   c7 45 fc 00 00 00 00    movl   $0x0,-0x4(%rbp)
  12:   8b 45 f8                mov    -0x8(%rbp),%eax
  15:   83 c0 03                add    $0x3,%eax
  18:   89 45 fc                mov    %eax,-0x4(%rbp)
  1b:   5d                      pop    %rbp
  1c:   c3                      retq
</code></pre>

<p>可以看到的其实这个类似于汇编文件的格式是<code>elf64-x86-64</code>, 下一小节会简单的介绍这个格式。其中最重要的内容是从<code>0000000000000000 &lt;foo&gt;</code>开始到结束的部分。这部分从左到右依次是</p>

<ul>
<li>指令开始的地址<code>memory staring addresses</code></li>
<li>汇编代码对应的二进制指令<code>byte codes for instruction</code></li>
<li>汇编代码<code>assembly codes</code></li>
</ul>

<h3 id="toc_0">显示文件的符号表入口</h3>

<p><code>objdump -t</code> 命令会打印文件的符号表<code>symbol table</code>. 输出的文件一共有7列，从左到右依次是</p>

<ul>
<li>value</li>
<li>class</li>
<li>type</li>
<li>size</li>
<li>line</li>
<li>section</li>
<li>symbol-name</li>
</ul>

<h2 id="toc_1">elf64-x86-64 文件</h2>

<p><code>elf</code>是<code>Executable and Linkable Format</code>(可执行和可链接格式，<a href="http://larryim.cc/Linking.html">看本文</a>)的简称。<code>elf</code>文件格式及其复杂，如果只需要研究<code>objdump</code>产生的反汇编文件没有必要去专门学习<code>elf</code>格式。掌握下面几点，就可以阅读<code>objdump</code>产生的反汇编文件了。<code>objdump</code>产生的<code>elf</code>文件，主要包括以下几个部分：</p>

<ul>
<li>Disassembly of section .init</li>
<li>Disassembly of section .plt</li>
<li>Disassembly of section .text</li>
<li>Disassembly of section .fini</li>
</ul>

<p>下面是一个具体的文件，为了更简洁的展示，每一部分只保留了一小段内容：</p>

<pre><code class="language-text">ctarget:     file format elf64-x86-64


Disassembly of section .init:

0000000000400c48 &lt;_init&gt;:
  400c48:   48 83 ec 08             sub    $0x8,%rsp
  400c4c:   e8 6b 02 00 00          callq  400ebc &lt;call_gmon_start&gt;
  400c51:   48 83 c4 08             add    $0x8,%rsp
  400c55:   c3                      retq   

Disassembly of section .plt:

0000000000400cb0 &lt;strcpy@plt&gt;:
  400cb0:   ff 25 6a 33 20 00       jmpq   *0x20336a(%rip)        # 604020 &lt;_GLOBAL_OFFSET_TABLE_+0x38&gt;
  400cb6:   68 04 00 00 00          pushq  $0x4
  400cbb:   e9 a0 ff ff ff          jmpq   400c60 &lt;_init+0x18&gt;


Disassembly of section .text:

00000000004011ad &lt;main&gt;:
  4011bb:   be c5 1d 40 00          mov    $0x401dc5,%esi
  4011c0:   bf 0b 00 00 00          mov    $0xb,%edi
  4011c5:   e8 86 fb ff ff          callq  400d50 &lt;signal@plt&gt;
  4011cf:   bf 07 00 00 00       
  401384:   c3                      retq   


Disassembly of section .fini:

0000000000402d74 &lt;_fini&gt;:
  402d74:   48 83 ec 08             sub    $0x8,%rsp
  402d78:   48 83 c4 08             add    $0x8,%rsp
  402d7c:   c3                      retq   

</code></pre>

<p>其中<code>.fini</code>部分是有关进程结束的指令。<code>.init</code>部分是有关进程启动的指令，在<code>main()</code>函数执行前会执行。<code>PLT</code>代表<code>Procedure Linkage Table</code>(过程链接表),用来调用在链接阶段未知的外部函数/过程的，在运行时它会动态链接。所以最重要的内容都在<code>.text</code>部分中。</p>

]]></content>
  </entry>
  
</feed>
